model : table13slim
N : 10
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 2.0473763698939185

The number of parameters is: 252462

The number of individual parameters is:

17
306
17
17
25
38250
25
25
50
112500
50
50
64
96000
64
64
4096
64
640
10
64
64

nonzero elements in E: 12623098
elements in E: 12623100
fraction nonzero: 0.9999998415603141
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.54; acc: 0.06
Batch: 20; loss: 2.42; acc: 0.09
Batch: 40; loss: 2.47; acc: 0.05
Batch: 60; loss: 2.36; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.29; acc: 0.09
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.19; acc: 0.23
Batch: 160; loss: 2.19; acc: 0.17
Batch: 180; loss: 2.15; acc: 0.22
Batch: 200; loss: 2.16; acc: 0.25
Batch: 220; loss: 2.11; acc: 0.28
Batch: 240; loss: 2.13; acc: 0.31
Batch: 260; loss: 2.19; acc: 0.22
Batch: 280; loss: 2.14; acc: 0.17
Batch: 300; loss: 2.08; acc: 0.31
Batch: 320; loss: 2.03; acc: 0.34
Batch: 340; loss: 2.05; acc: 0.34
Batch: 360; loss: 2.05; acc: 0.3
Batch: 380; loss: 2.13; acc: 0.23
Batch: 400; loss: 2.16; acc: 0.23
Batch: 420; loss: 2.01; acc: 0.31
Batch: 440; loss: 2.03; acc: 0.36
Batch: 460; loss: 2.08; acc: 0.31
Batch: 480; loss: 2.07; acc: 0.28
Batch: 500; loss: 2.05; acc: 0.3
Batch: 520; loss: 1.98; acc: 0.34
Batch: 540; loss: 2.07; acc: 0.27
Batch: 560; loss: 1.99; acc: 0.39
Batch: 580; loss: 1.99; acc: 0.28
Batch: 600; loss: 1.96; acc: 0.33
Batch: 620; loss: 2.11; acc: 0.25
Batch: 640; loss: 1.99; acc: 0.27
Batch: 660; loss: 1.99; acc: 0.41
Batch: 680; loss: 1.92; acc: 0.42
Batch: 700; loss: 1.9; acc: 0.41
Batch: 720; loss: 1.93; acc: 0.44
Batch: 740; loss: 1.99; acc: 0.36
Batch: 760; loss: 1.98; acc: 0.33
Batch: 780; loss: 1.97; acc: 0.38
Train Epoch over. train_loss: 2.11; train_accuracy: 0.27 

2.2596637791139074e-05
5.1511779020074755e-06
Batch: 0; loss: 2.04; acc: 0.3
Batch: 20; loss: 2.14; acc: 0.28
Batch: 40; loss: 1.79; acc: 0.53
Batch: 60; loss: 1.94; acc: 0.34
Batch: 80; loss: 1.89; acc: 0.52
Batch: 100; loss: 1.91; acc: 0.39
Batch: 120; loss: 2.02; acc: 0.31
Batch: 140; loss: 1.79; acc: 0.58
Val Epoch over. val_loss: 1.9527875686147411; val_accuracy: 0.39798964968152867 

The current subspace-distance is: 5.1511779020074755e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.93; acc: 0.38
Batch: 20; loss: 1.91; acc: 0.42
Batch: 40; loss: 1.9; acc: 0.41
Batch: 60; loss: 1.87; acc: 0.47
Batch: 80; loss: 1.94; acc: 0.36
Batch: 100; loss: 1.94; acc: 0.28
Batch: 120; loss: 1.9; acc: 0.47
Batch: 140; loss: 1.93; acc: 0.41
Batch: 160; loss: 1.92; acc: 0.39
Batch: 180; loss: 1.87; acc: 0.44
Batch: 200; loss: 1.84; acc: 0.45
Batch: 220; loss: 1.94; acc: 0.42
Batch: 240; loss: 1.93; acc: 0.42
Batch: 260; loss: 1.96; acc: 0.36
Batch: 280; loss: 1.86; acc: 0.45
Batch: 300; loss: 1.92; acc: 0.47
Batch: 320; loss: 1.9; acc: 0.34
Batch: 340; loss: 1.84; acc: 0.42
Batch: 360; loss: 1.97; acc: 0.34
Batch: 380; loss: 1.78; acc: 0.53
Batch: 400; loss: 1.96; acc: 0.41
Batch: 420; loss: 1.88; acc: 0.41
Batch: 440; loss: 2.03; acc: 0.27
Batch: 460; loss: 1.91; acc: 0.38
Batch: 480; loss: 1.88; acc: 0.44
Batch: 500; loss: 1.83; acc: 0.5
Batch: 520; loss: 1.92; acc: 0.45
Batch: 540; loss: 1.81; acc: 0.55
Batch: 560; loss: 1.84; acc: 0.39
Batch: 580; loss: 1.86; acc: 0.45
Batch: 600; loss: 1.95; acc: 0.39
Batch: 620; loss: 1.92; acc: 0.36
Batch: 640; loss: 1.84; acc: 0.39
Batch: 660; loss: 1.84; acc: 0.47
Batch: 680; loss: 1.95; acc: 0.41
Batch: 700; loss: 1.84; acc: 0.44
Batch: 720; loss: 1.8; acc: 0.5
Batch: 740; loss: 1.8; acc: 0.53
Batch: 760; loss: 1.92; acc: 0.47
Batch: 780; loss: 1.81; acc: 0.55
Train Epoch over. train_loss: 1.91; train_accuracy: 0.43 

2.553517151682172e-05
7.4025515459652524e-06
Batch: 0; loss: 1.94; acc: 0.41
Batch: 20; loss: 2.14; acc: 0.31
Batch: 40; loss: 1.69; acc: 0.62
Batch: 60; loss: 1.8; acc: 0.45
Batch: 80; loss: 1.76; acc: 0.58
Batch: 100; loss: 1.83; acc: 0.5
Batch: 120; loss: 1.92; acc: 0.42
Batch: 140; loss: 1.69; acc: 0.62
Val Epoch over. val_loss: 1.8356507629345937; val_accuracy: 0.4785031847133758 

The current subspace-distance is: 7.4025515459652524e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.95; acc: 0.41
Batch: 20; loss: 1.86; acc: 0.45
Batch: 40; loss: 1.86; acc: 0.52
Batch: 60; loss: 1.77; acc: 0.53
Batch: 80; loss: 1.88; acc: 0.41
Batch: 100; loss: 1.92; acc: 0.47
Batch: 120; loss: 1.86; acc: 0.5
Batch: 140; loss: 1.74; acc: 0.56
Batch: 160; loss: 1.9; acc: 0.42
Batch: 180; loss: 1.87; acc: 0.39
Batch: 200; loss: 1.84; acc: 0.48
Batch: 220; loss: 1.87; acc: 0.44
Batch: 240; loss: 1.89; acc: 0.42
Batch: 260; loss: 1.96; acc: 0.36
Batch: 280; loss: 1.9; acc: 0.42
Batch: 300; loss: 1.79; acc: 0.47
Batch: 320; loss: 1.89; acc: 0.38
Batch: 340; loss: 1.85; acc: 0.52
Batch: 360; loss: 1.9; acc: 0.41
Batch: 380; loss: 1.81; acc: 0.47
Batch: 400; loss: 1.75; acc: 0.47
Batch: 420; loss: 1.9; acc: 0.38
Batch: 440; loss: 1.8; acc: 0.59
Batch: 460; loss: 1.82; acc: 0.48
Batch: 480; loss: 1.85; acc: 0.47
Batch: 500; loss: 1.9; acc: 0.39
Batch: 520; loss: 1.88; acc: 0.48
Batch: 540; loss: 1.84; acc: 0.39
Batch: 560; loss: 1.85; acc: 0.45
Batch: 580; loss: 1.85; acc: 0.45
Batch: 600; loss: 1.82; acc: 0.48
Batch: 620; loss: 1.82; acc: 0.53
Batch: 640; loss: 1.87; acc: 0.47
Batch: 660; loss: 1.87; acc: 0.42
Batch: 680; loss: 1.77; acc: 0.47
Batch: 700; loss: 1.72; acc: 0.47
Batch: 720; loss: 1.74; acc: 0.52
Batch: 740; loss: 1.98; acc: 0.3
Batch: 760; loss: 1.88; acc: 0.47
Batch: 780; loss: 1.79; acc: 0.42
Train Epoch over. train_loss: 1.85; train_accuracy: 0.46 

2.7952064556302503e-05
7.68782956583891e-06
Batch: 0; loss: 1.89; acc: 0.39
Batch: 20; loss: 2.11; acc: 0.28
Batch: 40; loss: 1.67; acc: 0.62
Batch: 60; loss: 1.76; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.62
Batch: 100; loss: 1.83; acc: 0.52
Batch: 120; loss: 1.86; acc: 0.53
Batch: 140; loss: 1.62; acc: 0.64
Val Epoch over. val_loss: 1.80032170578173; val_accuracy: 0.4880573248407643 

The current subspace-distance is: 7.68782956583891e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.9; acc: 0.34
Batch: 20; loss: 1.71; acc: 0.5
Batch: 40; loss: 1.78; acc: 0.48
Batch: 60; loss: 1.78; acc: 0.5
Batch: 80; loss: 1.86; acc: 0.38
Batch: 100; loss: 1.87; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.55
Batch: 140; loss: 1.85; acc: 0.45
Batch: 160; loss: 1.78; acc: 0.47
Batch: 180; loss: 1.75; acc: 0.47
Batch: 200; loss: 1.72; acc: 0.59
Batch: 220; loss: 1.81; acc: 0.45
Batch: 240; loss: 1.76; acc: 0.44
Batch: 260; loss: 1.69; acc: 0.61
Batch: 280; loss: 1.69; acc: 0.55
Batch: 300; loss: 1.79; acc: 0.48
Batch: 320; loss: 1.68; acc: 0.67
Batch: 340; loss: 1.82; acc: 0.48
Batch: 360; loss: 1.79; acc: 0.45
Batch: 380; loss: 1.79; acc: 0.52
Batch: 400; loss: 1.71; acc: 0.61
Batch: 420; loss: 1.78; acc: 0.55
Batch: 440; loss: 1.76; acc: 0.58
Batch: 460; loss: 1.83; acc: 0.47
Batch: 480; loss: 1.78; acc: 0.45
Batch: 500; loss: 1.7; acc: 0.58
Batch: 520; loss: 1.75; acc: 0.56
Batch: 540; loss: 1.71; acc: 0.61
Batch: 560; loss: 1.82; acc: 0.44
Batch: 580; loss: 1.83; acc: 0.53
Batch: 600; loss: 1.81; acc: 0.47
Batch: 620; loss: 1.74; acc: 0.55
Batch: 640; loss: 1.77; acc: 0.52
Batch: 660; loss: 1.69; acc: 0.59
Batch: 680; loss: 1.87; acc: 0.47
Batch: 700; loss: 1.7; acc: 0.64
Batch: 720; loss: 1.79; acc: 0.52
Batch: 740; loss: 1.81; acc: 0.47
Batch: 760; loss: 1.8; acc: 0.42
Batch: 780; loss: 1.72; acc: 0.48
Train Epoch over. train_loss: 1.8; train_accuracy: 0.48 

2.9214354071882553e-05
8.6185418695095e-06
Batch: 0; loss: 1.83; acc: 0.42
Batch: 20; loss: 2.08; acc: 0.33
Batch: 40; loss: 1.6; acc: 0.66
Batch: 60; loss: 1.69; acc: 0.53
Batch: 80; loss: 1.66; acc: 0.66
Batch: 100; loss: 1.81; acc: 0.48
Batch: 120; loss: 1.83; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.69
Val Epoch over. val_loss: 1.7501376654691756; val_accuracy: 0.5225915605095541 

The current subspace-distance is: 8.6185418695095e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.45
Batch: 20; loss: 1.88; acc: 0.42
Batch: 40; loss: 1.79; acc: 0.47
Batch: 60; loss: 1.81; acc: 0.48
Batch: 80; loss: 1.73; acc: 0.48
Batch: 100; loss: 1.7; acc: 0.55
Batch: 120; loss: 1.7; acc: 0.52
Batch: 140; loss: 1.89; acc: 0.41
Batch: 160; loss: 1.73; acc: 0.52
Batch: 180; loss: 1.77; acc: 0.47
Batch: 200; loss: 1.95; acc: 0.34
Batch: 220; loss: 1.69; acc: 0.53
Batch: 240; loss: 1.77; acc: 0.56
Batch: 260; loss: 1.93; acc: 0.38
Batch: 280; loss: 1.73; acc: 0.56
Batch: 300; loss: 1.73; acc: 0.53
Batch: 320; loss: 1.74; acc: 0.5
Batch: 340; loss: 1.75; acc: 0.58
Batch: 360; loss: 1.76; acc: 0.45
Batch: 380; loss: 1.73; acc: 0.59
Batch: 400; loss: 1.78; acc: 0.48
Batch: 420; loss: 1.74; acc: 0.5
Batch: 440; loss: 1.82; acc: 0.48
Batch: 460; loss: 1.62; acc: 0.56
Batch: 480; loss: 1.68; acc: 0.58
Batch: 500; loss: 1.67; acc: 0.53
Batch: 520; loss: 1.78; acc: 0.45
Batch: 540; loss: 1.87; acc: 0.41
Batch: 560; loss: 1.77; acc: 0.44
Batch: 580; loss: 1.77; acc: 0.52
Batch: 600; loss: 1.75; acc: 0.52
Batch: 620; loss: 1.85; acc: 0.47
Batch: 640; loss: 1.66; acc: 0.56
Batch: 660; loss: 1.71; acc: 0.53
Batch: 680; loss: 1.77; acc: 0.48
Batch: 700; loss: 1.74; acc: 0.47
Batch: 720; loss: 1.77; acc: 0.52
Batch: 740; loss: 1.77; acc: 0.47
Batch: 760; loss: 1.69; acc: 0.56
Batch: 780; loss: 1.72; acc: 0.44
Train Epoch over. train_loss: 1.77; train_accuracy: 0.49 

3.1846055208006874e-05
1.1497388186398894e-05
Batch: 0; loss: 1.79; acc: 0.48
Batch: 20; loss: 2.07; acc: 0.31
Batch: 40; loss: 1.57; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.64; acc: 0.61
Batch: 100; loss: 1.85; acc: 0.45
Batch: 120; loss: 1.81; acc: 0.55
Batch: 140; loss: 1.55; acc: 0.7
Val Epoch over. val_loss: 1.7332752085035774; val_accuracy: 0.522093949044586 

The current subspace-distance is: 1.1497388186398894e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.53
Batch: 20; loss: 1.87; acc: 0.33
Batch: 40; loss: 1.73; acc: 0.48
Batch: 60; loss: 1.83; acc: 0.39
Batch: 80; loss: 1.77; acc: 0.45
Batch: 100; loss: 1.85; acc: 0.44
Batch: 120; loss: 1.63; acc: 0.61
Batch: 140; loss: 1.65; acc: 0.58
Batch: 160; loss: 1.9; acc: 0.42
Batch: 180; loss: 1.75; acc: 0.45
Batch: 200; loss: 1.65; acc: 0.64
Batch: 220; loss: 1.92; acc: 0.33
Batch: 240; loss: 1.89; acc: 0.39
Batch: 260; loss: 1.78; acc: 0.45
Batch: 280; loss: 1.9; acc: 0.5
Batch: 300; loss: 1.68; acc: 0.56
Batch: 320; loss: 1.7; acc: 0.5
Batch: 340; loss: 1.75; acc: 0.47
Batch: 360; loss: 1.66; acc: 0.58
Batch: 380; loss: 1.76; acc: 0.48
Batch: 400; loss: 1.84; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.85; acc: 0.48
Batch: 460; loss: 1.72; acc: 0.55
Batch: 480; loss: 1.82; acc: 0.38
Batch: 500; loss: 1.78; acc: 0.56
Batch: 520; loss: 1.78; acc: 0.42
Batch: 540; loss: 1.77; acc: 0.47
Batch: 560; loss: 1.72; acc: 0.5
Batch: 580; loss: 1.78; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.55
Batch: 620; loss: 1.73; acc: 0.52
Batch: 640; loss: 1.67; acc: 0.59
Batch: 660; loss: 1.69; acc: 0.53
Batch: 680; loss: 1.76; acc: 0.48
Batch: 700; loss: 1.84; acc: 0.42
Batch: 720; loss: 1.76; acc: 0.48
Batch: 740; loss: 1.71; acc: 0.53
Batch: 760; loss: 1.84; acc: 0.39
Batch: 780; loss: 1.87; acc: 0.45
Train Epoch over. train_loss: 1.75; train_accuracy: 0.49 

3.122800262644887e-05
1.0067815310321748e-05
Batch: 0; loss: 1.77; acc: 0.48
Batch: 20; loss: 2.05; acc: 0.28
Batch: 40; loss: 1.56; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.64; acc: 0.56
Batch: 100; loss: 1.84; acc: 0.45
Batch: 120; loss: 1.81; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.69
Val Epoch over. val_loss: 1.721884490578038; val_accuracy: 0.5177149681528662 

The current subspace-distance is: 1.0067815310321748e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.7; acc: 0.55
Batch: 20; loss: 1.75; acc: 0.48
Batch: 40; loss: 1.8; acc: 0.44
Batch: 60; loss: 1.74; acc: 0.52
Batch: 80; loss: 1.73; acc: 0.53
Batch: 100; loss: 1.73; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.59
Batch: 140; loss: 1.69; acc: 0.59
Batch: 160; loss: 1.74; acc: 0.52
Batch: 180; loss: 1.82; acc: 0.48
Batch: 200; loss: 1.65; acc: 0.56
Batch: 220; loss: 1.74; acc: 0.47
Batch: 240; loss: 1.61; acc: 0.62
Batch: 260; loss: 1.81; acc: 0.48
Batch: 280; loss: 1.66; acc: 0.55
Batch: 300; loss: 1.76; acc: 0.38
Batch: 320; loss: 1.79; acc: 0.44
Batch: 340; loss: 1.84; acc: 0.48
Batch: 360; loss: 1.8; acc: 0.39
Batch: 380; loss: 1.76; acc: 0.42
Batch: 400; loss: 1.71; acc: 0.55
Batch: 420; loss: 1.64; acc: 0.61
Batch: 440; loss: 1.77; acc: 0.42
Batch: 460; loss: 1.75; acc: 0.47
Batch: 480; loss: 1.74; acc: 0.5
Batch: 500; loss: 1.64; acc: 0.52
Batch: 520; loss: 1.75; acc: 0.53
Batch: 540; loss: 1.72; acc: 0.45
Batch: 560; loss: 1.81; acc: 0.47
Batch: 580; loss: 1.83; acc: 0.36
Batch: 600; loss: 1.67; acc: 0.58
Batch: 620; loss: 1.85; acc: 0.38
Batch: 640; loss: 1.66; acc: 0.47
Batch: 660; loss: 1.76; acc: 0.47
Batch: 680; loss: 1.68; acc: 0.59
Batch: 700; loss: 1.76; acc: 0.52
Batch: 720; loss: 1.82; acc: 0.53
Batch: 740; loss: 1.65; acc: 0.64
Batch: 760; loss: 1.74; acc: 0.53
Batch: 780; loss: 1.77; acc: 0.52
Train Epoch over. train_loss: 1.75; train_accuracy: 0.49 

3.1784558814251795e-05
8.866230018611532e-06
Batch: 0; loss: 1.76; acc: 0.48
Batch: 20; loss: 2.07; acc: 0.28
Batch: 40; loss: 1.57; acc: 0.64
Batch: 60; loss: 1.68; acc: 0.61
Batch: 80; loss: 1.66; acc: 0.59
Batch: 100; loss: 1.84; acc: 0.42
Batch: 120; loss: 1.81; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.62
Val Epoch over. val_loss: 1.7316961736436103; val_accuracy: 0.5128383757961783 

The current subspace-distance is: 8.866230018611532e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.77; acc: 0.56
Batch: 20; loss: 1.77; acc: 0.52
Batch: 40; loss: 1.74; acc: 0.53
Batch: 60; loss: 1.78; acc: 0.39
Batch: 80; loss: 1.67; acc: 0.52
Batch: 100; loss: 1.74; acc: 0.44
Batch: 120; loss: 1.81; acc: 0.45
Batch: 140; loss: 1.72; acc: 0.48
Batch: 160; loss: 1.8; acc: 0.44
Batch: 180; loss: 1.58; acc: 0.56
Batch: 200; loss: 1.84; acc: 0.38
Batch: 220; loss: 1.84; acc: 0.41
Batch: 240; loss: 1.71; acc: 0.5
Batch: 260; loss: 1.76; acc: 0.48
Batch: 280; loss: 1.74; acc: 0.56
Batch: 300; loss: 1.68; acc: 0.58
Batch: 320; loss: 1.64; acc: 0.61
Batch: 340; loss: 1.66; acc: 0.56
Batch: 360; loss: 1.73; acc: 0.42
Batch: 380; loss: 1.76; acc: 0.53
Batch: 400; loss: 1.71; acc: 0.44
Batch: 420; loss: 1.66; acc: 0.53
Batch: 440; loss: 1.76; acc: 0.45
Batch: 460; loss: 1.79; acc: 0.52
Batch: 480; loss: 1.72; acc: 0.53
Batch: 500; loss: 1.82; acc: 0.45
Batch: 520; loss: 1.72; acc: 0.5
Batch: 540; loss: 1.76; acc: 0.45
Batch: 560; loss: 1.65; acc: 0.58
Batch: 580; loss: 1.87; acc: 0.39
Batch: 600; loss: 1.88; acc: 0.36
Batch: 620; loss: 1.78; acc: 0.44
Batch: 640; loss: 1.78; acc: 0.42
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.63; acc: 0.58
Batch: 700; loss: 1.73; acc: 0.55
Batch: 720; loss: 1.77; acc: 0.41
Batch: 740; loss: 1.58; acc: 0.61
Batch: 760; loss: 1.53; acc: 0.62
Batch: 780; loss: 1.73; acc: 0.55
Train Epoch over. train_loss: 1.74; train_accuracy: 0.49 

3.290957465651445e-05
1.108538344851695e-05
Batch: 0; loss: 1.74; acc: 0.5
Batch: 20; loss: 2.05; acc: 0.34
Batch: 40; loss: 1.54; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.61
Batch: 80; loss: 1.63; acc: 0.55
Batch: 100; loss: 1.81; acc: 0.45
Batch: 120; loss: 1.79; acc: 0.5
Batch: 140; loss: 1.53; acc: 0.61
Val Epoch over. val_loss: 1.7084188347409486; val_accuracy: 0.5213972929936306 

The current subspace-distance is: 1.108538344851695e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.59
Batch: 20; loss: 1.68; acc: 0.53
Batch: 40; loss: 1.61; acc: 0.59
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.86; acc: 0.41
Batch: 100; loss: 1.63; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.52
Batch: 140; loss: 1.79; acc: 0.53
Batch: 160; loss: 1.64; acc: 0.59
Batch: 180; loss: 1.79; acc: 0.47
Batch: 200; loss: 1.7; acc: 0.53
Batch: 220; loss: 1.7; acc: 0.52
Batch: 240; loss: 1.74; acc: 0.44
Batch: 260; loss: 1.78; acc: 0.52
Batch: 280; loss: 1.71; acc: 0.53
Batch: 300; loss: 1.71; acc: 0.48
Batch: 320; loss: 1.66; acc: 0.55
Batch: 340; loss: 1.7; acc: 0.48
Batch: 360; loss: 1.82; acc: 0.48
Batch: 380; loss: 1.71; acc: 0.47
Batch: 400; loss: 1.68; acc: 0.52
Batch: 420; loss: 1.68; acc: 0.55
Batch: 440; loss: 1.78; acc: 0.5
Batch: 460; loss: 1.63; acc: 0.53
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.79; acc: 0.41
Batch: 520; loss: 1.74; acc: 0.53
Batch: 540; loss: 1.74; acc: 0.39
Batch: 560; loss: 1.61; acc: 0.58
Batch: 580; loss: 1.78; acc: 0.53
Batch: 600; loss: 1.69; acc: 0.61
Batch: 620; loss: 1.69; acc: 0.47
Batch: 640; loss: 1.65; acc: 0.55
Batch: 660; loss: 1.52; acc: 0.62
Batch: 680; loss: 1.77; acc: 0.44
Batch: 700; loss: 1.59; acc: 0.61
Batch: 720; loss: 1.67; acc: 0.58
Batch: 740; loss: 1.78; acc: 0.36
Batch: 760; loss: 1.72; acc: 0.53
Batch: 780; loss: 1.72; acc: 0.47
Train Epoch over. train_loss: 1.74; train_accuracy: 0.5 

3.345115692354739e-05
1.0614087841531727e-05
Batch: 0; loss: 1.74; acc: 0.53
Batch: 20; loss: 2.04; acc: 0.34
Batch: 40; loss: 1.53; acc: 0.67
Batch: 60; loss: 1.67; acc: 0.66
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.78; acc: 0.44
Batch: 120; loss: 1.79; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.62
Val Epoch over. val_loss: 1.701813494323925; val_accuracy: 0.5265724522292994 

The current subspace-distance is: 1.0614087841531727e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.45
Batch: 20; loss: 1.78; acc: 0.47
Batch: 40; loss: 1.72; acc: 0.48
Batch: 60; loss: 1.78; acc: 0.52
Batch: 80; loss: 1.77; acc: 0.44
Batch: 100; loss: 1.7; acc: 0.5
Batch: 120; loss: 1.8; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.59
Batch: 160; loss: 1.61; acc: 0.58
Batch: 180; loss: 1.69; acc: 0.55
Batch: 200; loss: 1.72; acc: 0.48
Batch: 220; loss: 1.75; acc: 0.48
Batch: 240; loss: 1.62; acc: 0.62
Batch: 260; loss: 1.64; acc: 0.58
Batch: 280; loss: 1.63; acc: 0.59
Batch: 300; loss: 1.75; acc: 0.5
Batch: 320; loss: 1.79; acc: 0.44
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.7; acc: 0.55
Batch: 380; loss: 1.82; acc: 0.44
Batch: 400; loss: 1.69; acc: 0.53
Batch: 420; loss: 1.73; acc: 0.52
Batch: 440; loss: 1.65; acc: 0.58
Batch: 460; loss: 1.83; acc: 0.45
Batch: 480; loss: 1.59; acc: 0.56
Batch: 500; loss: 1.67; acc: 0.55
Batch: 520; loss: 1.74; acc: 0.53
Batch: 540; loss: 1.69; acc: 0.55
Batch: 560; loss: 1.74; acc: 0.48
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.73; acc: 0.56
Batch: 620; loss: 1.79; acc: 0.45
Batch: 640; loss: 1.72; acc: 0.61
Batch: 660; loss: 1.79; acc: 0.45
Batch: 680; loss: 1.67; acc: 0.5
Batch: 700; loss: 1.69; acc: 0.58
Batch: 720; loss: 1.67; acc: 0.53
Batch: 740; loss: 1.64; acc: 0.53
Batch: 760; loss: 1.61; acc: 0.53
Batch: 780; loss: 1.81; acc: 0.5
Train Epoch over. train_loss: 1.72; train_accuracy: 0.5 

3.4644643164938316e-05
1.111030996980844e-05
Batch: 0; loss: 1.74; acc: 0.52
Batch: 20; loss: 1.99; acc: 0.39
Batch: 40; loss: 1.5; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.49; acc: 0.66
Val Epoch over. val_loss: 1.6807889346104519; val_accuracy: 0.5437898089171974 

The current subspace-distance is: 1.111030996980844e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.65; acc: 0.55
Batch: 40; loss: 1.63; acc: 0.59
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.77; acc: 0.47
Batch: 100; loss: 1.74; acc: 0.48
Batch: 120; loss: 1.77; acc: 0.42
Batch: 140; loss: 1.75; acc: 0.42
Batch: 160; loss: 1.76; acc: 0.59
Batch: 180; loss: 1.58; acc: 0.61
Batch: 200; loss: 1.86; acc: 0.44
Batch: 220; loss: 1.83; acc: 0.5
Batch: 240; loss: 1.85; acc: 0.44
Batch: 260; loss: 1.7; acc: 0.5
Batch: 280; loss: 1.64; acc: 0.55
Batch: 300; loss: 1.74; acc: 0.44
Batch: 320; loss: 1.72; acc: 0.47
Batch: 340; loss: 1.84; acc: 0.42
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.6; acc: 0.53
Batch: 400; loss: 1.79; acc: 0.48
Batch: 420; loss: 1.66; acc: 0.55
Batch: 440; loss: 1.87; acc: 0.44
Batch: 460; loss: 1.75; acc: 0.48
Batch: 480; loss: 1.68; acc: 0.52
Batch: 500; loss: 1.72; acc: 0.5
Batch: 520; loss: 1.75; acc: 0.42
Batch: 540; loss: 1.73; acc: 0.47
Batch: 560; loss: 1.64; acc: 0.56
Batch: 580; loss: 1.82; acc: 0.41
Batch: 600; loss: 1.68; acc: 0.56
Batch: 620; loss: 1.67; acc: 0.52
Batch: 640; loss: 1.79; acc: 0.47
Batch: 660; loss: 1.72; acc: 0.52
Batch: 680; loss: 1.69; acc: 0.48
Batch: 700; loss: 1.8; acc: 0.45
Batch: 720; loss: 1.7; acc: 0.52
Batch: 740; loss: 1.75; acc: 0.48
Batch: 760; loss: 1.69; acc: 0.56
Batch: 780; loss: 1.59; acc: 0.59
Train Epoch over. train_loss: 1.72; train_accuracy: 0.51 

3.488679067231715e-05
1.2097278158762492e-05
Batch: 0; loss: 1.75; acc: 0.5
Batch: 20; loss: 1.99; acc: 0.36
Batch: 40; loss: 1.5; acc: 0.7
Batch: 60; loss: 1.67; acc: 0.59
Batch: 80; loss: 1.59; acc: 0.56
Batch: 100; loss: 1.7; acc: 0.56
Batch: 120; loss: 1.79; acc: 0.44
Batch: 140; loss: 1.48; acc: 0.69
Val Epoch over. val_loss: 1.679086947896678; val_accuracy: 0.5403065286624203 

The current subspace-distance is: 1.2097278158762492e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.55
Batch: 20; loss: 1.76; acc: 0.52
Batch: 40; loss: 1.7; acc: 0.53
Batch: 60; loss: 1.8; acc: 0.47
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.59
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.84; acc: 0.42
Batch: 160; loss: 1.8; acc: 0.41
Batch: 180; loss: 1.67; acc: 0.55
Batch: 200; loss: 1.71; acc: 0.48
Batch: 220; loss: 1.59; acc: 0.67
Batch: 240; loss: 1.69; acc: 0.41
Batch: 260; loss: 1.66; acc: 0.58
Batch: 280; loss: 1.77; acc: 0.44
Batch: 300; loss: 1.84; acc: 0.39
Batch: 320; loss: 1.63; acc: 0.5
Batch: 340; loss: 1.64; acc: 0.56
Batch: 360; loss: 1.67; acc: 0.55
Batch: 380; loss: 1.61; acc: 0.58
Batch: 400; loss: 1.75; acc: 0.52
Batch: 420; loss: 1.66; acc: 0.48
Batch: 440; loss: 1.76; acc: 0.53
Batch: 460; loss: 1.82; acc: 0.48
Batch: 480; loss: 1.7; acc: 0.52
Batch: 500; loss: 1.72; acc: 0.52
Batch: 520; loss: 1.7; acc: 0.48
Batch: 540; loss: 1.61; acc: 0.58
Batch: 560; loss: 1.74; acc: 0.44
Batch: 580; loss: 1.88; acc: 0.36
Batch: 600; loss: 1.61; acc: 0.64
Batch: 620; loss: 1.8; acc: 0.44
Batch: 640; loss: 1.67; acc: 0.59
Batch: 660; loss: 1.62; acc: 0.48
Batch: 680; loss: 1.71; acc: 0.5
Batch: 700; loss: 1.74; acc: 0.42
Batch: 720; loss: 1.91; acc: 0.38
Batch: 740; loss: 1.75; acc: 0.58
Batch: 760; loss: 1.62; acc: 0.52
Batch: 780; loss: 1.81; acc: 0.48
Train Epoch over. train_loss: 1.71; train_accuracy: 0.51 

3.546604420989752e-05
1.1535104022186715e-05
Batch: 0; loss: 1.75; acc: 0.5
Batch: 20; loss: 1.97; acc: 0.39
Batch: 40; loss: 1.49; acc: 0.69
Batch: 60; loss: 1.67; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.61
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.78; acc: 0.42
Batch: 140; loss: 1.48; acc: 0.66
Val Epoch over. val_loss: 1.67186127498651; val_accuracy: 0.5462778662420382 

The current subspace-distance is: 1.1535104022186715e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.75; acc: 0.47
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.76; acc: 0.44
Batch: 60; loss: 1.61; acc: 0.64
Batch: 80; loss: 1.78; acc: 0.42
Batch: 100; loss: 1.71; acc: 0.45
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.75; acc: 0.48
Batch: 160; loss: 1.65; acc: 0.59
Batch: 180; loss: 1.59; acc: 0.64
Batch: 200; loss: 1.75; acc: 0.48
Batch: 220; loss: 1.67; acc: 0.58
Batch: 240; loss: 1.64; acc: 0.55
Batch: 260; loss: 1.7; acc: 0.53
Batch: 280; loss: 1.71; acc: 0.48
Batch: 300; loss: 1.71; acc: 0.44
Batch: 320; loss: 1.63; acc: 0.59
Batch: 340; loss: 1.62; acc: 0.55
Batch: 360; loss: 1.81; acc: 0.45
Batch: 380; loss: 1.82; acc: 0.48
Batch: 400; loss: 1.68; acc: 0.55
Batch: 420; loss: 1.69; acc: 0.53
Batch: 440; loss: 1.65; acc: 0.56
Batch: 460; loss: 1.59; acc: 0.61
Batch: 480; loss: 1.52; acc: 0.62
Batch: 500; loss: 1.71; acc: 0.47
Batch: 520; loss: 1.69; acc: 0.47
Batch: 540; loss: 1.73; acc: 0.53
Batch: 560; loss: 1.72; acc: 0.42
Batch: 580; loss: 1.67; acc: 0.52
Batch: 600; loss: 1.89; acc: 0.36
Batch: 620; loss: 1.74; acc: 0.52
Batch: 640; loss: 1.85; acc: 0.45
Batch: 660; loss: 1.72; acc: 0.48
Batch: 680; loss: 1.7; acc: 0.52
Batch: 700; loss: 1.75; acc: 0.53
Batch: 720; loss: 1.81; acc: 0.45
Batch: 740; loss: 1.58; acc: 0.58
Batch: 760; loss: 1.77; acc: 0.5
Batch: 780; loss: 1.66; acc: 0.56
Train Epoch over. train_loss: 1.71; train_accuracy: 0.51 

3.582900171750225e-05
1.3472574210027233e-05
Batch: 0; loss: 1.74; acc: 0.5
Batch: 20; loss: 1.96; acc: 0.41
Batch: 40; loss: 1.48; acc: 0.69
Batch: 60; loss: 1.67; acc: 0.59
Batch: 80; loss: 1.56; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.77; acc: 0.39
Batch: 140; loss: 1.47; acc: 0.69
Val Epoch over. val_loss: 1.6681496337720543; val_accuracy: 0.5476711783439491 

The current subspace-distance is: 1.3472574210027233e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.64; acc: 0.5
Batch: 40; loss: 1.61; acc: 0.48
Batch: 60; loss: 1.67; acc: 0.59
Batch: 80; loss: 1.62; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.56; acc: 0.64
Batch: 140; loss: 1.61; acc: 0.61
Batch: 160; loss: 1.68; acc: 0.56
Batch: 180; loss: 1.62; acc: 0.64
Batch: 200; loss: 1.63; acc: 0.55
Batch: 220; loss: 1.7; acc: 0.39
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.65; acc: 0.55
Batch: 280; loss: 1.53; acc: 0.58
Batch: 300; loss: 1.56; acc: 0.61
Batch: 320; loss: 1.65; acc: 0.52
Batch: 340; loss: 1.76; acc: 0.53
Batch: 360; loss: 1.69; acc: 0.55
Batch: 380; loss: 1.65; acc: 0.58
Batch: 400; loss: 1.6; acc: 0.58
Batch: 420; loss: 1.65; acc: 0.56
Batch: 440; loss: 1.82; acc: 0.52
Batch: 460; loss: 1.69; acc: 0.44
Batch: 480; loss: 1.74; acc: 0.5
Batch: 500; loss: 1.69; acc: 0.53
Batch: 520; loss: 1.72; acc: 0.48
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.8; acc: 0.45
Batch: 580; loss: 1.84; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.39
Batch: 620; loss: 1.75; acc: 0.45
Batch: 640; loss: 1.8; acc: 0.48
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.8; acc: 0.55
Batch: 700; loss: 1.76; acc: 0.5
Batch: 720; loss: 1.65; acc: 0.56
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.9; acc: 0.42
Batch: 780; loss: 1.69; acc: 0.56
Train Epoch over. train_loss: 1.7; train_accuracy: 0.51 

3.678132270579226e-05
1.3772105376119725e-05
Batch: 0; loss: 1.74; acc: 0.5
Batch: 20; loss: 1.96; acc: 0.44
Batch: 40; loss: 1.47; acc: 0.72
Batch: 60; loss: 1.67; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.66; acc: 0.58
Batch: 120; loss: 1.77; acc: 0.41
Batch: 140; loss: 1.48; acc: 0.69
Val Epoch over. val_loss: 1.6653439391190838; val_accuracy: 0.5427945859872612 

The current subspace-distance is: 1.3772105376119725e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.66; acc: 0.56
Batch: 40; loss: 1.72; acc: 0.53
Batch: 60; loss: 1.6; acc: 0.62
Batch: 80; loss: 1.7; acc: 0.62
Batch: 100; loss: 1.71; acc: 0.45
Batch: 120; loss: 1.72; acc: 0.56
Batch: 140; loss: 1.6; acc: 0.59
Batch: 160; loss: 1.63; acc: 0.59
Batch: 180; loss: 1.74; acc: 0.5
Batch: 200; loss: 1.74; acc: 0.45
Batch: 220; loss: 1.83; acc: 0.48
Batch: 240; loss: 1.58; acc: 0.59
Batch: 260; loss: 1.55; acc: 0.59
Batch: 280; loss: 1.72; acc: 0.47
Batch: 300; loss: 1.61; acc: 0.56
Batch: 320; loss: 1.73; acc: 0.45
Batch: 340; loss: 1.64; acc: 0.55
Batch: 360; loss: 1.71; acc: 0.48
Batch: 380; loss: 1.7; acc: 0.55
Batch: 400; loss: 1.72; acc: 0.5
Batch: 420; loss: 1.73; acc: 0.47
Batch: 440; loss: 1.75; acc: 0.44
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.62; acc: 0.53
Batch: 500; loss: 1.72; acc: 0.45
Batch: 520; loss: 1.72; acc: 0.52
Batch: 540; loss: 1.64; acc: 0.59
Batch: 560; loss: 1.7; acc: 0.55
Batch: 580; loss: 1.77; acc: 0.39
Batch: 600; loss: 1.56; acc: 0.66
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.65; acc: 0.59
Batch: 660; loss: 1.66; acc: 0.56
Batch: 680; loss: 1.64; acc: 0.61
Batch: 700; loss: 1.68; acc: 0.5
Batch: 720; loss: 1.63; acc: 0.48
Batch: 740; loss: 1.67; acc: 0.53
Batch: 760; loss: 1.56; acc: 0.62
Batch: 780; loss: 1.71; acc: 0.45
Train Epoch over. train_loss: 1.7; train_accuracy: 0.51 

3.773556454689242e-05
1.4628653843828943e-05
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.95; acc: 0.45
Batch: 40; loss: 1.45; acc: 0.72
Batch: 60; loss: 1.66; acc: 0.62
Batch: 80; loss: 1.54; acc: 0.64
Batch: 100; loss: 1.65; acc: 0.55
Batch: 120; loss: 1.76; acc: 0.44
Batch: 140; loss: 1.47; acc: 0.69
Val Epoch over. val_loss: 1.6539496259324868; val_accuracy: 0.5472730891719745 

The current subspace-distance is: 1.4628653843828943e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.55
Batch: 20; loss: 1.75; acc: 0.5
Batch: 40; loss: 1.67; acc: 0.59
Batch: 60; loss: 1.69; acc: 0.56
Batch: 80; loss: 1.73; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.75; acc: 0.48
Batch: 180; loss: 1.67; acc: 0.58
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.84; acc: 0.45
Batch: 240; loss: 1.71; acc: 0.47
Batch: 260; loss: 1.69; acc: 0.53
Batch: 280; loss: 1.76; acc: 0.5
Batch: 300; loss: 1.67; acc: 0.55
Batch: 320; loss: 1.77; acc: 0.47
Batch: 340; loss: 1.69; acc: 0.55
Batch: 360; loss: 1.79; acc: 0.44
Batch: 380; loss: 1.71; acc: 0.52
Batch: 400; loss: 1.68; acc: 0.47
Batch: 420; loss: 1.66; acc: 0.56
Batch: 440; loss: 1.64; acc: 0.67
Batch: 460; loss: 1.64; acc: 0.53
Batch: 480; loss: 1.81; acc: 0.44
Batch: 500; loss: 1.67; acc: 0.47
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.72; acc: 0.47
Batch: 560; loss: 1.61; acc: 0.59
Batch: 580; loss: 1.62; acc: 0.58
Batch: 600; loss: 1.64; acc: 0.48
Batch: 620; loss: 1.71; acc: 0.47
Batch: 640; loss: 1.85; acc: 0.44
Batch: 660; loss: 1.7; acc: 0.47
Batch: 680; loss: 1.62; acc: 0.61
Batch: 700; loss: 1.63; acc: 0.56
Batch: 720; loss: 1.72; acc: 0.48
Batch: 740; loss: 1.68; acc: 0.53
Batch: 760; loss: 1.8; acc: 0.52
Batch: 780; loss: 1.67; acc: 0.53
Train Epoch over. train_loss: 1.69; train_accuracy: 0.51 

3.6976147384848446e-05
1.3119883078616112e-05
Batch: 0; loss: 1.73; acc: 0.48
Batch: 20; loss: 1.93; acc: 0.44
Batch: 40; loss: 1.44; acc: 0.75
Batch: 60; loss: 1.66; acc: 0.66
Batch: 80; loss: 1.53; acc: 0.61
Batch: 100; loss: 1.65; acc: 0.58
Batch: 120; loss: 1.77; acc: 0.42
Batch: 140; loss: 1.48; acc: 0.69
Val Epoch over. val_loss: 1.6460851939620487; val_accuracy: 0.5500597133757962 

The current subspace-distance is: 1.3119883078616112e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.74; acc: 0.47
Batch: 20; loss: 1.75; acc: 0.45
Batch: 40; loss: 1.74; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.64; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.69; acc: 0.55
Batch: 140; loss: 1.58; acc: 0.58
Batch: 160; loss: 1.63; acc: 0.53
Batch: 180; loss: 1.6; acc: 0.61
Batch: 200; loss: 1.6; acc: 0.59
Batch: 220; loss: 1.74; acc: 0.45
Batch: 240; loss: 1.63; acc: 0.55
Batch: 260; loss: 1.87; acc: 0.36
Batch: 280; loss: 1.72; acc: 0.53
Batch: 300; loss: 1.7; acc: 0.48
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 1.68; acc: 0.59
Batch: 360; loss: 1.68; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.73; acc: 0.47
Batch: 420; loss: 1.66; acc: 0.55
Batch: 440; loss: 1.72; acc: 0.44
Batch: 460; loss: 1.7; acc: 0.47
Batch: 480; loss: 1.55; acc: 0.7
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 1.65; acc: 0.52
Batch: 540; loss: 1.85; acc: 0.44
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.64; acc: 0.47
Batch: 600; loss: 1.5; acc: 0.64
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.74; acc: 0.48
Batch: 660; loss: 1.67; acc: 0.47
Batch: 680; loss: 1.66; acc: 0.58
Batch: 700; loss: 1.59; acc: 0.55
Batch: 720; loss: 1.94; acc: 0.36
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.68; acc: 0.56
Batch: 780; loss: 1.68; acc: 0.53
Train Epoch over. train_loss: 1.68; train_accuracy: 0.51 

3.787631430895999e-05
1.436089769413229e-05
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.93; acc: 0.45
Batch: 40; loss: 1.43; acc: 0.77
Batch: 60; loss: 1.66; acc: 0.64
Batch: 80; loss: 1.52; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.49; acc: 0.7
Val Epoch over. val_loss: 1.6421852339604857; val_accuracy: 0.5513535031847133 

The current subspace-distance is: 1.436089769413229e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.68; acc: 0.55
Batch: 40; loss: 1.63; acc: 0.55
Batch: 60; loss: 1.76; acc: 0.48
Batch: 80; loss: 1.72; acc: 0.47
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.62; acc: 0.55
Batch: 140; loss: 1.75; acc: 0.53
Batch: 160; loss: 1.62; acc: 0.66
Batch: 180; loss: 1.84; acc: 0.47
Batch: 200; loss: 1.56; acc: 0.56
Batch: 220; loss: 1.81; acc: 0.41
Batch: 240; loss: 1.66; acc: 0.53
Batch: 260; loss: 1.77; acc: 0.47
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.68; acc: 0.52
Batch: 320; loss: 1.72; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.52
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.68; acc: 0.52
Batch: 400; loss: 1.66; acc: 0.45
Batch: 420; loss: 1.63; acc: 0.52
Batch: 440; loss: 1.76; acc: 0.47
Batch: 460; loss: 1.74; acc: 0.5
Batch: 480; loss: 1.61; acc: 0.59
Batch: 500; loss: 1.92; acc: 0.31
Batch: 520; loss: 1.78; acc: 0.38
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.6; acc: 0.59
Batch: 580; loss: 1.6; acc: 0.58
Batch: 600; loss: 1.59; acc: 0.56
Batch: 620; loss: 1.65; acc: 0.56
Batch: 640; loss: 1.63; acc: 0.53
Batch: 660; loss: 1.67; acc: 0.44
Batch: 680; loss: 1.59; acc: 0.55
Batch: 700; loss: 1.63; acc: 0.55
Batch: 720; loss: 1.75; acc: 0.45
Batch: 740; loss: 1.66; acc: 0.56
Batch: 760; loss: 1.59; acc: 0.61
Batch: 780; loss: 1.62; acc: 0.55
Train Epoch over. train_loss: 1.68; train_accuracy: 0.52 

3.8626461901003495e-05
1.5677202100050636e-05
Batch: 0; loss: 1.71; acc: 0.5
Batch: 20; loss: 1.91; acc: 0.42
Batch: 40; loss: 1.39; acc: 0.75
Batch: 60; loss: 1.65; acc: 0.62
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.47; acc: 0.7
Val Epoch over. val_loss: 1.6238307223957815; val_accuracy: 0.5561305732484076 

The current subspace-distance is: 1.5677202100050636e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.71; acc: 0.5
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.86; acc: 0.39
Batch: 80; loss: 1.65; acc: 0.47
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.74; acc: 0.52
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.6; acc: 0.53
Batch: 220; loss: 1.78; acc: 0.45
Batch: 240; loss: 1.72; acc: 0.52
Batch: 260; loss: 1.68; acc: 0.44
Batch: 280; loss: 1.56; acc: 0.61
Batch: 300; loss: 1.72; acc: 0.52
Batch: 320; loss: 1.61; acc: 0.62
Batch: 340; loss: 1.68; acc: 0.5
Batch: 360; loss: 1.63; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.59
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.56; acc: 0.67
Batch: 440; loss: 1.7; acc: 0.58
Batch: 460; loss: 1.76; acc: 0.56
Batch: 480; loss: 1.62; acc: 0.52
Batch: 500; loss: 1.68; acc: 0.53
Batch: 520; loss: 1.54; acc: 0.58
Batch: 540; loss: 1.6; acc: 0.59
Batch: 560; loss: 1.66; acc: 0.52
Batch: 580; loss: 1.6; acc: 0.56
Batch: 600; loss: 1.65; acc: 0.58
Batch: 620; loss: 1.71; acc: 0.52
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.57; acc: 0.58
Batch: 680; loss: 1.79; acc: 0.45
Batch: 700; loss: 1.62; acc: 0.48
Batch: 720; loss: 1.72; acc: 0.44
Batch: 740; loss: 1.59; acc: 0.56
Batch: 760; loss: 1.75; acc: 0.44
Batch: 780; loss: 1.64; acc: 0.56
Train Epoch over. train_loss: 1.67; train_accuracy: 0.52 

3.8066820707172155e-05
1.254189555766061e-05
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.67; acc: 0.59
Batch: 80; loss: 1.54; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.48; acc: 0.7
Val Epoch over. val_loss: 1.6284420353591822; val_accuracy: 0.5594148089171974 

The current subspace-distance is: 1.254189555766061e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.58
Batch: 40; loss: 1.58; acc: 0.55
Batch: 60; loss: 1.82; acc: 0.41
Batch: 80; loss: 1.67; acc: 0.45
Batch: 100; loss: 1.66; acc: 0.55
Batch: 120; loss: 1.7; acc: 0.5
Batch: 140; loss: 1.52; acc: 0.67
Batch: 160; loss: 1.72; acc: 0.45
Batch: 180; loss: 1.64; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.58
Batch: 240; loss: 1.71; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.58
Batch: 280; loss: 1.64; acc: 0.55
Batch: 300; loss: 1.65; acc: 0.55
Batch: 320; loss: 1.72; acc: 0.48
Batch: 340; loss: 1.58; acc: 0.59
Batch: 360; loss: 1.6; acc: 0.61
Batch: 380; loss: 1.69; acc: 0.39
Batch: 400; loss: 1.72; acc: 0.47
Batch: 420; loss: 1.78; acc: 0.39
Batch: 440; loss: 1.67; acc: 0.52
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.64
Batch: 500; loss: 1.64; acc: 0.59
Batch: 520; loss: 1.71; acc: 0.47
Batch: 540; loss: 1.66; acc: 0.5
Batch: 560; loss: 1.57; acc: 0.56
Batch: 580; loss: 1.67; acc: 0.59
Batch: 600; loss: 1.63; acc: 0.59
Batch: 620; loss: 1.65; acc: 0.62
Batch: 640; loss: 1.68; acc: 0.53
Batch: 660; loss: 1.61; acc: 0.5
Batch: 680; loss: 1.78; acc: 0.47
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.71; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.62
Batch: 760; loss: 1.66; acc: 0.5
Batch: 780; loss: 1.53; acc: 0.59
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.942287730751559e-05
1.5066329069668427e-05
Batch: 0; loss: 1.71; acc: 0.52
Batch: 20; loss: 1.92; acc: 0.38
Batch: 40; loss: 1.36; acc: 0.73
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.58
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.47; acc: 0.69
Val Epoch over. val_loss: 1.6087471687110366; val_accuracy: 0.5592157643312102 

The current subspace-distance is: 1.5066329069668427e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.65; acc: 0.5
Batch: 60; loss: 1.68; acc: 0.55
Batch: 80; loss: 1.72; acc: 0.48
Batch: 100; loss: 1.64; acc: 0.56
Batch: 120; loss: 1.68; acc: 0.56
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.63; acc: 0.56
Batch: 180; loss: 1.65; acc: 0.48
Batch: 200; loss: 1.67; acc: 0.52
Batch: 220; loss: 1.51; acc: 0.64
Batch: 240; loss: 1.78; acc: 0.42
Batch: 260; loss: 1.84; acc: 0.45
Batch: 280; loss: 1.65; acc: 0.52
Batch: 300; loss: 1.64; acc: 0.55
Batch: 320; loss: 1.68; acc: 0.5
Batch: 340; loss: 1.75; acc: 0.48
Batch: 360; loss: 1.68; acc: 0.5
Batch: 380; loss: 1.65; acc: 0.48
Batch: 400; loss: 1.79; acc: 0.44
Batch: 420; loss: 1.67; acc: 0.45
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.46; acc: 0.66
Batch: 480; loss: 1.69; acc: 0.45
Batch: 500; loss: 1.66; acc: 0.55
Batch: 520; loss: 1.54; acc: 0.55
Batch: 540; loss: 1.67; acc: 0.44
Batch: 560; loss: 1.67; acc: 0.47
Batch: 580; loss: 1.69; acc: 0.47
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.63; acc: 0.56
Batch: 640; loss: 1.74; acc: 0.44
Batch: 660; loss: 1.81; acc: 0.44
Batch: 680; loss: 1.65; acc: 0.48
Batch: 700; loss: 1.46; acc: 0.64
Batch: 720; loss: 1.6; acc: 0.59
Batch: 740; loss: 1.76; acc: 0.48
Batch: 760; loss: 1.55; acc: 0.62
Batch: 780; loss: 1.66; acc: 0.48
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.955412103096023e-05
1.528457141830586e-05
Batch: 0; loss: 1.72; acc: 0.53
Batch: 20; loss: 1.9; acc: 0.41
Batch: 40; loss: 1.38; acc: 0.73
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.49; acc: 0.67
Val Epoch over. val_loss: 1.6212692799841522; val_accuracy: 0.5527468152866242 

The current subspace-distance is: 1.528457141830586e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.58
Batch: 40; loss: 1.55; acc: 0.59
Batch: 60; loss: 1.67; acc: 0.47
Batch: 80; loss: 1.56; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.47
Batch: 120; loss: 1.64; acc: 0.53
Batch: 140; loss: 1.67; acc: 0.5
Batch: 160; loss: 1.65; acc: 0.47
Batch: 180; loss: 1.68; acc: 0.47
Batch: 200; loss: 1.56; acc: 0.61
Batch: 220; loss: 1.6; acc: 0.56
Batch: 240; loss: 1.75; acc: 0.47
Batch: 260; loss: 1.53; acc: 0.58
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.75; acc: 0.48
Batch: 320; loss: 1.68; acc: 0.45
Batch: 340; loss: 1.6; acc: 0.53
Batch: 360; loss: 1.73; acc: 0.48
Batch: 380; loss: 1.69; acc: 0.47
Batch: 400; loss: 1.72; acc: 0.52
Batch: 420; loss: 1.62; acc: 0.53
Batch: 440; loss: 1.65; acc: 0.47
Batch: 460; loss: 1.7; acc: 0.56
Batch: 480; loss: 1.62; acc: 0.56
Batch: 500; loss: 1.59; acc: 0.61
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.6; acc: 0.55
Batch: 560; loss: 1.57; acc: 0.58
Batch: 580; loss: 1.63; acc: 0.61
Batch: 600; loss: 1.69; acc: 0.47
Batch: 620; loss: 1.72; acc: 0.48
Batch: 640; loss: 1.94; acc: 0.41
Batch: 660; loss: 1.62; acc: 0.56
Batch: 680; loss: 1.74; acc: 0.52
Batch: 700; loss: 1.79; acc: 0.36
Batch: 720; loss: 1.49; acc: 0.56
Batch: 740; loss: 1.66; acc: 0.5
Batch: 760; loss: 1.64; acc: 0.55
Batch: 780; loss: 1.62; acc: 0.55
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

3.9941507566254586e-05
1.3810362361255102e-05
Batch: 0; loss: 1.71; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.73
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.72; acc: 0.45
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 1.6162564602627116; val_accuracy: 0.5559315286624203 

The current subspace-distance is: 1.3810362361255102e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.64
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.59; acc: 0.5
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.64; acc: 0.5
Batch: 100; loss: 1.74; acc: 0.44
Batch: 120; loss: 1.55; acc: 0.62
Batch: 140; loss: 1.6; acc: 0.56
Batch: 160; loss: 1.58; acc: 0.56
Batch: 180; loss: 1.7; acc: 0.47
Batch: 200; loss: 1.62; acc: 0.55
Batch: 220; loss: 1.79; acc: 0.47
Batch: 240; loss: 1.73; acc: 0.52
Batch: 260; loss: 1.67; acc: 0.52
Batch: 280; loss: 1.62; acc: 0.52
Batch: 300; loss: 1.69; acc: 0.52
Batch: 320; loss: 1.71; acc: 0.39
Batch: 340; loss: 1.54; acc: 0.61
Batch: 360; loss: 1.73; acc: 0.44
Batch: 380; loss: 1.62; acc: 0.44
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.78; acc: 0.5
Batch: 440; loss: 1.72; acc: 0.48
Batch: 460; loss: 1.55; acc: 0.61
Batch: 480; loss: 1.64; acc: 0.53
Batch: 500; loss: 1.55; acc: 0.56
Batch: 520; loss: 1.72; acc: 0.48
Batch: 540; loss: 1.63; acc: 0.55
Batch: 560; loss: 1.53; acc: 0.59
Batch: 580; loss: 1.66; acc: 0.48
Batch: 600; loss: 1.68; acc: 0.53
Batch: 620; loss: 1.64; acc: 0.48
Batch: 640; loss: 1.68; acc: 0.48
Batch: 660; loss: 1.6; acc: 0.55
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.67; acc: 0.45
Batch: 720; loss: 1.58; acc: 0.56
Batch: 740; loss: 1.75; acc: 0.47
Batch: 760; loss: 1.59; acc: 0.58
Batch: 780; loss: 1.66; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

3.992151323473081e-05
1.5114958841877524e-05
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.9; acc: 0.36
Batch: 40; loss: 1.35; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.61
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.47; acc: 0.66
Val Epoch over. val_loss: 1.6017593707248663; val_accuracy: 0.5605095541401274 

The current subspace-distance is: 1.5114958841877524e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.49; acc: 0.62
Batch: 60; loss: 1.72; acc: 0.42
Batch: 80; loss: 1.52; acc: 0.56
Batch: 100; loss: 1.65; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.66
Batch: 140; loss: 1.68; acc: 0.45
Batch: 160; loss: 1.6; acc: 0.56
Batch: 180; loss: 1.55; acc: 0.58
Batch: 200; loss: 1.77; acc: 0.42
Batch: 220; loss: 1.72; acc: 0.5
Batch: 240; loss: 1.56; acc: 0.58
Batch: 260; loss: 1.75; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.55
Batch: 300; loss: 1.73; acc: 0.48
Batch: 320; loss: 1.72; acc: 0.5
Batch: 340; loss: 1.59; acc: 0.53
Batch: 360; loss: 1.64; acc: 0.55
Batch: 380; loss: 1.54; acc: 0.56
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.44
Batch: 440; loss: 1.65; acc: 0.48
Batch: 460; loss: 1.67; acc: 0.47
Batch: 480; loss: 1.66; acc: 0.56
Batch: 500; loss: 1.8; acc: 0.44
Batch: 520; loss: 1.68; acc: 0.41
Batch: 540; loss: 1.7; acc: 0.52
Batch: 560; loss: 1.75; acc: 0.39
Batch: 580; loss: 1.63; acc: 0.55
Batch: 600; loss: 1.69; acc: 0.53
Batch: 620; loss: 1.65; acc: 0.44
Batch: 640; loss: 1.63; acc: 0.56
Batch: 660; loss: 1.65; acc: 0.52
Batch: 680; loss: 1.55; acc: 0.59
Batch: 700; loss: 1.62; acc: 0.58
Batch: 720; loss: 1.62; acc: 0.55
Batch: 740; loss: 1.68; acc: 0.47
Batch: 760; loss: 1.62; acc: 0.55
Batch: 780; loss: 1.62; acc: 0.5
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

4.0056602301774547e-05
1.175094712380087e-05
Batch: 0; loss: 1.71; acc: 0.52
Batch: 20; loss: 1.9; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.72
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.47; acc: 0.66
Val Epoch over. val_loss: 1.6104937085680142; val_accuracy: 0.5570262738853503 

The current subspace-distance is: 1.175094712380087e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.58
Batch: 20; loss: 1.79; acc: 0.45
Batch: 40; loss: 1.72; acc: 0.45
Batch: 60; loss: 1.55; acc: 0.58
Batch: 80; loss: 1.54; acc: 0.56
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.67; acc: 0.53
Batch: 160; loss: 1.71; acc: 0.45
Batch: 180; loss: 1.58; acc: 0.58
Batch: 200; loss: 1.6; acc: 0.55
Batch: 220; loss: 1.79; acc: 0.42
Batch: 240; loss: 1.76; acc: 0.44
Batch: 260; loss: 1.73; acc: 0.53
Batch: 280; loss: 1.67; acc: 0.53
Batch: 300; loss: 1.7; acc: 0.52
Batch: 320; loss: 1.67; acc: 0.53
Batch: 340; loss: 1.56; acc: 0.5
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.69; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.48
Batch: 420; loss: 1.64; acc: 0.55
Batch: 440; loss: 1.71; acc: 0.56
Batch: 460; loss: 1.69; acc: 0.52
Batch: 480; loss: 1.66; acc: 0.52
Batch: 500; loss: 1.71; acc: 0.55
Batch: 520; loss: 1.7; acc: 0.47
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.68; acc: 0.45
Batch: 580; loss: 1.54; acc: 0.64
Batch: 600; loss: 1.6; acc: 0.55
Batch: 620; loss: 1.63; acc: 0.58
Batch: 640; loss: 1.78; acc: 0.44
Batch: 660; loss: 1.58; acc: 0.59
Batch: 680; loss: 1.55; acc: 0.62
Batch: 700; loss: 1.72; acc: 0.47
Batch: 720; loss: 1.74; acc: 0.47
Batch: 740; loss: 1.64; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.55
Batch: 780; loss: 1.45; acc: 0.64
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

4.056011312059127e-05
1.3722277799388394e-05
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.9; acc: 0.39
Batch: 40; loss: 1.36; acc: 0.73
Batch: 60; loss: 1.63; acc: 0.56
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.66
Val Epoch over. val_loss: 1.603631015795811; val_accuracy: 0.5588176751592356 

The current subspace-distance is: 1.3722277799388394e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.61
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.47
Batch: 60; loss: 1.66; acc: 0.45
Batch: 80; loss: 1.67; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.71; acc: 0.52
Batch: 160; loss: 1.52; acc: 0.61
Batch: 180; loss: 1.62; acc: 0.58
Batch: 200; loss: 1.73; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.39
Batch: 240; loss: 1.74; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.52; acc: 0.61
Batch: 300; loss: 1.57; acc: 0.62
Batch: 320; loss: 1.59; acc: 0.52
Batch: 340; loss: 1.73; acc: 0.48
Batch: 360; loss: 1.67; acc: 0.45
Batch: 380; loss: 1.7; acc: 0.55
Batch: 400; loss: 1.54; acc: 0.61
Batch: 420; loss: 1.63; acc: 0.58
Batch: 440; loss: 1.65; acc: 0.56
Batch: 460; loss: 1.64; acc: 0.48
Batch: 480; loss: 1.66; acc: 0.56
Batch: 500; loss: 1.61; acc: 0.56
Batch: 520; loss: 1.69; acc: 0.5
Batch: 540; loss: 1.7; acc: 0.53
Batch: 560; loss: 1.62; acc: 0.52
Batch: 580; loss: 1.65; acc: 0.53
Batch: 600; loss: 1.74; acc: 0.44
Batch: 620; loss: 1.7; acc: 0.52
Batch: 640; loss: 1.64; acc: 0.45
Batch: 660; loss: 1.76; acc: 0.5
Batch: 680; loss: 1.68; acc: 0.52
Batch: 700; loss: 1.6; acc: 0.55
Batch: 720; loss: 1.64; acc: 0.48
Batch: 740; loss: 1.62; acc: 0.48
Batch: 760; loss: 1.65; acc: 0.55
Batch: 780; loss: 1.7; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

4.089934373041615e-05
1.5790736142662354e-05
Batch: 0; loss: 1.7; acc: 0.53
Batch: 20; loss: 1.9; acc: 0.41
Batch: 40; loss: 1.38; acc: 0.73
Batch: 60; loss: 1.64; acc: 0.53
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.71; acc: 0.5
Batch: 140; loss: 1.49; acc: 0.67
Val Epoch over. val_loss: 1.6072258022940082; val_accuracy: 0.5604100318471338 

The current subspace-distance is: 1.5790736142662354e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.36
Batch: 40; loss: 1.58; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.5; acc: 0.66
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.74; acc: 0.44
Batch: 160; loss: 1.66; acc: 0.48
Batch: 180; loss: 1.62; acc: 0.52
Batch: 200; loss: 1.69; acc: 0.52
Batch: 220; loss: 1.62; acc: 0.55
Batch: 240; loss: 1.88; acc: 0.39
Batch: 260; loss: 1.43; acc: 0.62
Batch: 280; loss: 1.58; acc: 0.69
Batch: 300; loss: 1.75; acc: 0.47
Batch: 320; loss: 1.68; acc: 0.45
Batch: 340; loss: 1.66; acc: 0.58
Batch: 360; loss: 1.75; acc: 0.39
Batch: 380; loss: 1.5; acc: 0.58
Batch: 400; loss: 1.83; acc: 0.48
Batch: 420; loss: 1.79; acc: 0.42
Batch: 440; loss: 1.61; acc: 0.52
Batch: 460; loss: 1.64; acc: 0.52
Batch: 480; loss: 1.59; acc: 0.58
Batch: 500; loss: 1.63; acc: 0.58
Batch: 520; loss: 1.54; acc: 0.58
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.63; acc: 0.56
Batch: 580; loss: 1.68; acc: 0.47
Batch: 600; loss: 1.59; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.55
Batch: 640; loss: 1.67; acc: 0.48
Batch: 660; loss: 1.48; acc: 0.62
Batch: 680; loss: 1.75; acc: 0.48
Batch: 700; loss: 1.57; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.62
Batch: 740; loss: 1.67; acc: 0.53
Batch: 760; loss: 1.68; acc: 0.45
Batch: 780; loss: 1.61; acc: 0.56
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

4.0152684960048646e-05
1.1643365724012256e-05
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.9; acc: 0.41
Batch: 40; loss: 1.35; acc: 0.73
Batch: 60; loss: 1.63; acc: 0.56
Batch: 80; loss: 1.51; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 1.5969807653670098; val_accuracy: 0.5548367834394905 

The current subspace-distance is: 1.1643365724012256e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.52
Batch: 20; loss: 1.66; acc: 0.53
Batch: 40; loss: 1.53; acc: 0.53
Batch: 60; loss: 1.7; acc: 0.48
Batch: 80; loss: 1.65; acc: 0.47
Batch: 100; loss: 1.7; acc: 0.5
Batch: 120; loss: 1.56; acc: 0.61
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.59; acc: 0.55
Batch: 180; loss: 1.67; acc: 0.45
Batch: 200; loss: 1.69; acc: 0.5
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.65; acc: 0.53
Batch: 260; loss: 1.63; acc: 0.48
Batch: 280; loss: 1.65; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.5
Batch: 320; loss: 1.7; acc: 0.53
Batch: 340; loss: 1.6; acc: 0.56
Batch: 360; loss: 1.6; acc: 0.56
Batch: 380; loss: 1.6; acc: 0.59
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.69; acc: 0.5
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.67; acc: 0.48
Batch: 480; loss: 1.68; acc: 0.55
Batch: 500; loss: 1.69; acc: 0.5
Batch: 520; loss: 1.64; acc: 0.61
Batch: 540; loss: 1.61; acc: 0.53
Batch: 560; loss: 1.55; acc: 0.59
Batch: 580; loss: 1.66; acc: 0.48
Batch: 600; loss: 1.7; acc: 0.55
Batch: 620; loss: 1.54; acc: 0.59
Batch: 640; loss: 1.67; acc: 0.52
Batch: 660; loss: 1.66; acc: 0.48
Batch: 680; loss: 1.55; acc: 0.58
Batch: 700; loss: 1.74; acc: 0.42
Batch: 720; loss: 1.46; acc: 0.58
Batch: 740; loss: 1.64; acc: 0.5
Batch: 760; loss: 1.61; acc: 0.59
Batch: 780; loss: 1.8; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

4.042723594466224e-05
1.2603006325662136e-05
Batch: 0; loss: 1.69; acc: 0.52
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.34; acc: 0.73
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.57; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.59248629648974; val_accuracy: 0.5600119426751592 

The current subspace-distance is: 1.2603006325662136e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.72; acc: 0.47
Batch: 100; loss: 1.54; acc: 0.61
Batch: 120; loss: 1.7; acc: 0.55
Batch: 140; loss: 1.71; acc: 0.52
Batch: 160; loss: 1.64; acc: 0.5
Batch: 180; loss: 1.64; acc: 0.56
Batch: 200; loss: 1.68; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.55
Batch: 240; loss: 1.56; acc: 0.59
Batch: 260; loss: 1.51; acc: 0.64
Batch: 280; loss: 1.54; acc: 0.58
Batch: 300; loss: 1.52; acc: 0.67
Batch: 320; loss: 1.56; acc: 0.58
Batch: 340; loss: 1.77; acc: 0.47
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.69; acc: 0.55
Batch: 400; loss: 1.74; acc: 0.47
Batch: 420; loss: 1.63; acc: 0.5
Batch: 440; loss: 1.74; acc: 0.47
Batch: 460; loss: 1.53; acc: 0.55
Batch: 480; loss: 1.8; acc: 0.39
Batch: 500; loss: 1.67; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.52
Batch: 540; loss: 1.61; acc: 0.61
Batch: 560; loss: 1.73; acc: 0.52
Batch: 580; loss: 1.81; acc: 0.44
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.73; acc: 0.53
Batch: 640; loss: 1.59; acc: 0.52
Batch: 660; loss: 1.55; acc: 0.56
Batch: 680; loss: 1.6; acc: 0.58
Batch: 700; loss: 1.59; acc: 0.58
Batch: 720; loss: 1.63; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.55
Batch: 760; loss: 1.74; acc: 0.45
Batch: 780; loss: 1.62; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.52 

4.056092075188644e-05
1.579670060891658e-05
Batch: 0; loss: 1.68; acc: 0.53
Batch: 20; loss: 1.89; acc: 0.41
Batch: 40; loss: 1.34; acc: 0.75
Batch: 60; loss: 1.6; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.61
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.585818347657562; val_accuracy: 0.5625995222929936 

The current subspace-distance is: 1.579670060891658e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.61
Batch: 40; loss: 1.61; acc: 0.48
Batch: 60; loss: 1.66; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.66
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 1.64; acc: 0.52
Batch: 160; loss: 1.63; acc: 0.62
Batch: 180; loss: 1.73; acc: 0.42
Batch: 200; loss: 1.66; acc: 0.48
Batch: 220; loss: 1.72; acc: 0.5
Batch: 240; loss: 1.7; acc: 0.33
Batch: 260; loss: 1.72; acc: 0.44
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.62; acc: 0.52
Batch: 320; loss: 1.66; acc: 0.47
Batch: 340; loss: 1.69; acc: 0.39
Batch: 360; loss: 1.46; acc: 0.69
Batch: 380; loss: 1.61; acc: 0.52
Batch: 400; loss: 1.68; acc: 0.59
Batch: 420; loss: 1.63; acc: 0.53
Batch: 440; loss: 1.55; acc: 0.52
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.57; acc: 0.59
Batch: 500; loss: 1.63; acc: 0.53
Batch: 520; loss: 1.71; acc: 0.42
Batch: 540; loss: 1.62; acc: 0.59
Batch: 560; loss: 1.51; acc: 0.67
Batch: 580; loss: 1.64; acc: 0.58
Batch: 600; loss: 1.71; acc: 0.48
Batch: 620; loss: 1.67; acc: 0.52
Batch: 640; loss: 1.68; acc: 0.53
Batch: 660; loss: 1.67; acc: 0.53
Batch: 680; loss: 1.7; acc: 0.47
Batch: 700; loss: 1.59; acc: 0.56
Batch: 720; loss: 1.72; acc: 0.55
Batch: 740; loss: 1.66; acc: 0.48
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.69; acc: 0.56
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

4.076490949955769e-05
1.3135148947185371e-05
Batch: 0; loss: 1.69; acc: 0.52
Batch: 20; loss: 1.87; acc: 0.39
Batch: 40; loss: 1.35; acc: 0.73
Batch: 60; loss: 1.61; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.61
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.5880254795596858; val_accuracy: 0.5644904458598726 

The current subspace-distance is: 1.3135148947185371e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.0473763698939185

The number of parameters is: 252462

The number of individual parameters is:

17
306
17
17
25
38250
25
25
50
112500
50
50
64
96000
64
64
4096
64
640
10
64
64

nonzero elements in E: 25246197
elements in E: 25246200
fraction nonzero: 0.9999998811702355
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.44; acc: 0.12
Batch: 20; loss: 2.35; acc: 0.16
Batch: 40; loss: 2.28; acc: 0.17
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.13; acc: 0.27
Batch: 100; loss: 2.06; acc: 0.34
Batch: 120; loss: 2.14; acc: 0.27
Batch: 140; loss: 1.97; acc: 0.41
Batch: 160; loss: 2.14; acc: 0.23
Batch: 180; loss: 2.08; acc: 0.31
Batch: 200; loss: 1.9; acc: 0.44
Batch: 220; loss: 1.97; acc: 0.42
Batch: 240; loss: 1.92; acc: 0.45
Batch: 260; loss: 1.94; acc: 0.41
Batch: 280; loss: 1.94; acc: 0.41
Batch: 300; loss: 1.88; acc: 0.52
Batch: 320; loss: 1.93; acc: 0.38
Batch: 340; loss: 1.88; acc: 0.45
Batch: 360; loss: 1.76; acc: 0.56
Batch: 380; loss: 1.88; acc: 0.38
Batch: 400; loss: 1.82; acc: 0.48
Batch: 420; loss: 1.89; acc: 0.39
Batch: 440; loss: 1.81; acc: 0.5
Batch: 460; loss: 1.87; acc: 0.52
Batch: 480; loss: 1.78; acc: 0.52
Batch: 500; loss: 1.78; acc: 0.52
Batch: 520; loss: 1.9; acc: 0.45
Batch: 540; loss: 1.79; acc: 0.48
Batch: 560; loss: 1.83; acc: 0.5
Batch: 580; loss: 1.77; acc: 0.55
Batch: 600; loss: 1.73; acc: 0.58
Batch: 620; loss: 1.7; acc: 0.56
Batch: 640; loss: 1.71; acc: 0.53
Batch: 660; loss: 1.72; acc: 0.59
Batch: 680; loss: 1.67; acc: 0.59
Batch: 700; loss: 1.62; acc: 0.64
Batch: 720; loss: 1.61; acc: 0.67
Batch: 740; loss: 1.93; acc: 0.36
Batch: 760; loss: 1.73; acc: 0.56
Batch: 780; loss: 1.74; acc: 0.5
Train Epoch over. train_loss: 1.9; train_accuracy: 0.44 

5.256441727397032e-05
4.701536090578884e-05
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.74; acc: 0.52
Batch: 40; loss: 1.47; acc: 0.73
Batch: 60; loss: 1.64; acc: 0.62
Batch: 80; loss: 1.61; acc: 0.59
Batch: 100; loss: 1.7; acc: 0.58
Batch: 120; loss: 1.75; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.67
Val Epoch over. val_loss: 1.673536093371689; val_accuracy: 0.5906648089171974 

The current subspace-distance is: 4.701536090578884e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.77; acc: 0.5
Batch: 20; loss: 1.72; acc: 0.52
Batch: 40; loss: 1.7; acc: 0.56
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.73; acc: 0.56
Batch: 100; loss: 1.78; acc: 0.48
Batch: 120; loss: 1.9; acc: 0.36
Batch: 140; loss: 1.75; acc: 0.48
Batch: 160; loss: 1.69; acc: 0.58
Batch: 180; loss: 1.54; acc: 0.66
Batch: 200; loss: 1.63; acc: 0.55
Batch: 220; loss: 1.63; acc: 0.64
Batch: 240; loss: 1.58; acc: 0.64
Batch: 260; loss: 1.61; acc: 0.62
Batch: 280; loss: 1.67; acc: 0.5
Batch: 300; loss: 1.47; acc: 0.7
Batch: 320; loss: 1.61; acc: 0.48
Batch: 340; loss: 1.61; acc: 0.53
Batch: 360; loss: 1.7; acc: 0.52
Batch: 380; loss: 1.63; acc: 0.53
Batch: 400; loss: 1.61; acc: 0.59
Batch: 420; loss: 1.45; acc: 0.72
Batch: 440; loss: 1.63; acc: 0.55
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.58
Batch: 500; loss: 1.44; acc: 0.75
Batch: 520; loss: 1.63; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.61
Batch: 560; loss: 1.53; acc: 0.64
Batch: 580; loss: 1.57; acc: 0.67
Batch: 600; loss: 1.41; acc: 0.8
Batch: 620; loss: 1.62; acc: 0.55
Batch: 640; loss: 1.4; acc: 0.75
Batch: 660; loss: 1.68; acc: 0.55
Batch: 680; loss: 1.45; acc: 0.66
Batch: 700; loss: 1.53; acc: 0.62
Batch: 720; loss: 1.54; acc: 0.62
Batch: 740; loss: 1.5; acc: 0.58
Batch: 760; loss: 1.5; acc: 0.69
Batch: 780; loss: 1.49; acc: 0.67
Train Epoch over. train_loss: 1.6; train_accuracy: 0.59 

7.210061448859051e-05
6.706979183945805e-05
Batch: 0; loss: 1.61; acc: 0.62
Batch: 20; loss: 1.59; acc: 0.58
Batch: 40; loss: 1.27; acc: 0.8
Batch: 60; loss: 1.48; acc: 0.64
Batch: 80; loss: 1.41; acc: 0.7
Batch: 100; loss: 1.46; acc: 0.75
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.64
Val Epoch over. val_loss: 1.4962930451532839; val_accuracy: 0.658937101910828 

The current subspace-distance is: 6.706979183945805e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.73
Batch: 20; loss: 1.45; acc: 0.7
Batch: 40; loss: 1.43; acc: 0.72
Batch: 60; loss: 1.56; acc: 0.61
Batch: 80; loss: 1.48; acc: 0.64
Batch: 100; loss: 1.48; acc: 0.67
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.43; acc: 0.62
Batch: 200; loss: 1.63; acc: 0.55
Batch: 220; loss: 1.48; acc: 0.67
Batch: 240; loss: 1.39; acc: 0.73
Batch: 260; loss: 1.53; acc: 0.56
Batch: 280; loss: 1.46; acc: 0.58
Batch: 300; loss: 1.5; acc: 0.67
Batch: 320; loss: 1.48; acc: 0.67
Batch: 340; loss: 1.42; acc: 0.66
Batch: 360; loss: 1.56; acc: 0.58
Batch: 380; loss: 1.37; acc: 0.78
Batch: 400; loss: 1.54; acc: 0.64
Batch: 420; loss: 1.44; acc: 0.73
Batch: 440; loss: 1.47; acc: 0.67
Batch: 460; loss: 1.54; acc: 0.59
Batch: 480; loss: 1.49; acc: 0.64
Batch: 500; loss: 1.36; acc: 0.75
Batch: 520; loss: 1.34; acc: 0.77
Batch: 540; loss: 1.52; acc: 0.61
Batch: 560; loss: 1.48; acc: 0.69
Batch: 580; loss: 1.57; acc: 0.64
Batch: 600; loss: 1.46; acc: 0.72
Batch: 620; loss: 1.48; acc: 0.62
Batch: 640; loss: 1.52; acc: 0.64
Batch: 660; loss: 1.41; acc: 0.77
Batch: 680; loss: 1.36; acc: 0.64
Batch: 700; loss: 1.49; acc: 0.62
Batch: 720; loss: 1.4; acc: 0.7
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.53; acc: 0.62
Batch: 780; loss: 1.38; acc: 0.7
Train Epoch over. train_loss: 1.48; train_accuracy: 0.65 

8.697028533788398e-05
8.22511938167736e-05
Batch: 0; loss: 1.52; acc: 0.58
Batch: 20; loss: 1.52; acc: 0.67
Batch: 40; loss: 1.16; acc: 0.83
Batch: 60; loss: 1.38; acc: 0.73
Batch: 80; loss: 1.34; acc: 0.77
Batch: 100; loss: 1.31; acc: 0.75
Batch: 120; loss: 1.55; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.77
Val Epoch over. val_loss: 1.398609686808981; val_accuracy: 0.7064092356687898 

The current subspace-distance is: 8.22511938167736e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.72
Batch: 20; loss: 1.43; acc: 0.67
Batch: 40; loss: 1.41; acc: 0.62
Batch: 60; loss: 1.55; acc: 0.64
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.75
Batch: 120; loss: 1.38; acc: 0.66
Batch: 140; loss: 1.46; acc: 0.66
Batch: 160; loss: 1.38; acc: 0.67
Batch: 180; loss: 1.49; acc: 0.64
Batch: 200; loss: 1.41; acc: 0.67
Batch: 220; loss: 1.34; acc: 0.73
Batch: 240; loss: 1.38; acc: 0.67
Batch: 260; loss: 1.28; acc: 0.77
Batch: 280; loss: 1.34; acc: 0.69
Batch: 300; loss: 1.44; acc: 0.69
Batch: 320; loss: 1.39; acc: 0.7
Batch: 340; loss: 1.45; acc: 0.64
Batch: 360; loss: 1.39; acc: 0.7
Batch: 380; loss: 1.4; acc: 0.72
Batch: 400; loss: 1.4; acc: 0.69
Batch: 420; loss: 1.36; acc: 0.75
Batch: 440; loss: 1.46; acc: 0.61
Batch: 460; loss: 1.48; acc: 0.62
Batch: 480; loss: 1.42; acc: 0.67
Batch: 500; loss: 1.37; acc: 0.66
Batch: 520; loss: 1.42; acc: 0.67
Batch: 540; loss: 1.39; acc: 0.69
Batch: 560; loss: 1.49; acc: 0.59
Batch: 580; loss: 1.54; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.69
Batch: 620; loss: 1.3; acc: 0.73
Batch: 640; loss: 1.31; acc: 0.78
Batch: 660; loss: 1.35; acc: 0.7
Batch: 680; loss: 1.47; acc: 0.61
Batch: 700; loss: 1.36; acc: 0.67
Batch: 720; loss: 1.31; acc: 0.7
Batch: 740; loss: 1.31; acc: 0.8
Batch: 760; loss: 1.4; acc: 0.67
Batch: 780; loss: 1.29; acc: 0.8
Train Epoch over. train_loss: 1.4; train_accuracy: 0.68 

0.00010054140875581652
9.548180969431996e-05
Batch: 0; loss: 1.43; acc: 0.66
Batch: 20; loss: 1.46; acc: 0.67
Batch: 40; loss: 1.08; acc: 0.88
Batch: 60; loss: 1.3; acc: 0.77
Batch: 80; loss: 1.26; acc: 0.72
Batch: 100; loss: 1.27; acc: 0.78
Batch: 120; loss: 1.49; acc: 0.62
Batch: 140; loss: 1.26; acc: 0.81
Val Epoch over. val_loss: 1.3191791188185382; val_accuracy: 0.7249203821656051 

The current subspace-distance is: 9.548180969431996e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.62
Batch: 20; loss: 1.21; acc: 0.86
Batch: 40; loss: 1.52; acc: 0.61
Batch: 60; loss: 1.45; acc: 0.64
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.33; acc: 0.7
Batch: 120; loss: 1.32; acc: 0.72
Batch: 140; loss: 1.37; acc: 0.64
Batch: 160; loss: 1.21; acc: 0.75
Batch: 180; loss: 1.37; acc: 0.64
Batch: 200; loss: 1.33; acc: 0.73
Batch: 220; loss: 1.35; acc: 0.67
Batch: 240; loss: 1.29; acc: 0.73
Batch: 260; loss: 1.28; acc: 0.72
Batch: 280; loss: 1.27; acc: 0.67
Batch: 300; loss: 1.36; acc: 0.69
Batch: 320; loss: 1.42; acc: 0.69
Batch: 340; loss: 1.28; acc: 0.72
Batch: 360; loss: 1.24; acc: 0.78
Batch: 380; loss: 1.33; acc: 0.73
Batch: 400; loss: 1.26; acc: 0.69
Batch: 420; loss: 1.47; acc: 0.53
Batch: 440; loss: 1.44; acc: 0.64
Batch: 460; loss: 1.2; acc: 0.81
Batch: 480; loss: 1.28; acc: 0.75
Batch: 500; loss: 1.21; acc: 0.8
Batch: 520; loss: 1.34; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.78
Batch: 560; loss: 1.3; acc: 0.69
Batch: 580; loss: 1.29; acc: 0.72
Batch: 600; loss: 1.45; acc: 0.61
Batch: 620; loss: 1.3; acc: 0.73
Batch: 640; loss: 1.47; acc: 0.66
Batch: 660; loss: 1.43; acc: 0.59
Batch: 680; loss: 1.21; acc: 0.77
Batch: 700; loss: 1.34; acc: 0.7
Batch: 720; loss: 1.34; acc: 0.69
Batch: 740; loss: 1.24; acc: 0.75
Batch: 760; loss: 1.25; acc: 0.73
Batch: 780; loss: 1.43; acc: 0.67
Train Epoch over. train_loss: 1.33; train_accuracy: 0.7 

0.00011148786143166944
0.00010675023077055812
Batch: 0; loss: 1.38; acc: 0.67
Batch: 20; loss: 1.37; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.91
Batch: 60; loss: 1.23; acc: 0.75
Batch: 80; loss: 1.16; acc: 0.73
Batch: 100; loss: 1.22; acc: 0.73
Batch: 120; loss: 1.42; acc: 0.64
Batch: 140; loss: 1.19; acc: 0.83
Val Epoch over. val_loss: 1.2500110538142502; val_accuracy: 0.7350716560509554 

The current subspace-distance is: 0.00010675023077055812 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.72
Batch: 20; loss: 1.41; acc: 0.66
Batch: 40; loss: 1.43; acc: 0.62
Batch: 60; loss: 1.22; acc: 0.8
Batch: 80; loss: 1.28; acc: 0.7
Batch: 100; loss: 1.36; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.69
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.24; acc: 0.77
Batch: 180; loss: 1.35; acc: 0.67
Batch: 200; loss: 1.13; acc: 0.8
Batch: 220; loss: 1.21; acc: 0.77
Batch: 240; loss: 1.24; acc: 0.69
Batch: 260; loss: 1.22; acc: 0.77
Batch: 280; loss: 1.18; acc: 0.75
Batch: 300; loss: 1.26; acc: 0.72
Batch: 320; loss: 1.29; acc: 0.67
Batch: 340; loss: 1.18; acc: 0.8
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.24; acc: 0.73
Batch: 400; loss: 1.36; acc: 0.75
Batch: 420; loss: 1.35; acc: 0.73
Batch: 440; loss: 1.3; acc: 0.69
Batch: 460; loss: 1.36; acc: 0.62
Batch: 480; loss: 1.36; acc: 0.58
Batch: 500; loss: 1.26; acc: 0.75
Batch: 520; loss: 1.23; acc: 0.81
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.28; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.83
Batch: 600; loss: 1.47; acc: 0.61
Batch: 620; loss: 1.17; acc: 0.8
Batch: 640; loss: 1.15; acc: 0.77
Batch: 660; loss: 1.18; acc: 0.77
Batch: 680; loss: 1.26; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.8
Batch: 720; loss: 1.18; acc: 0.72
Batch: 740; loss: 1.4; acc: 0.64
Batch: 760; loss: 1.3; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.8
Train Epoch over. train_loss: 1.26; train_accuracy: 0.71 

0.00012694350152742118
0.00012146782682975754
Batch: 0; loss: 1.35; acc: 0.61
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 1.0; acc: 0.88
Batch: 60; loss: 1.16; acc: 0.78
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.15; acc: 0.77
Batch: 120; loss: 1.37; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.8
Val Epoch over. val_loss: 1.1894051220007003; val_accuracy: 0.7394506369426752 

The current subspace-distance is: 0.00012146782682975754 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.28; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.26; acc: 0.64
Batch: 80; loss: 1.16; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.75
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.19; acc: 0.73
Batch: 180; loss: 1.22; acc: 0.7
Batch: 200; loss: 1.12; acc: 0.78
Batch: 220; loss: 1.1; acc: 0.86
Batch: 240; loss: 1.19; acc: 0.7
Batch: 260; loss: 1.23; acc: 0.78
Batch: 280; loss: 1.23; acc: 0.69
Batch: 300; loss: 1.15; acc: 0.75
Batch: 320; loss: 1.21; acc: 0.73
Batch: 340; loss: 1.31; acc: 0.64
Batch: 360; loss: 1.03; acc: 0.81
Batch: 380; loss: 1.14; acc: 0.77
Batch: 400; loss: 1.18; acc: 0.7
Batch: 420; loss: 1.1; acc: 0.73
Batch: 440; loss: 1.31; acc: 0.61
Batch: 460; loss: 1.29; acc: 0.66
Batch: 480; loss: 1.24; acc: 0.66
Batch: 500; loss: 1.23; acc: 0.72
Batch: 520; loss: 1.35; acc: 0.66
Batch: 540; loss: 1.09; acc: 0.77
Batch: 560; loss: 1.05; acc: 0.78
Batch: 580; loss: 1.17; acc: 0.73
Batch: 600; loss: 1.04; acc: 0.78
Batch: 620; loss: 1.15; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 1.21; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.73
Batch: 700; loss: 1.12; acc: 0.77
Batch: 720; loss: 1.2; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.7
Batch: 780; loss: 1.34; acc: 0.66
Train Epoch over. train_loss: 1.2; train_accuracy: 0.72 

0.00013800154556520283
0.00013352837413549423
Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.94; acc: 0.89
Batch: 60; loss: 1.09; acc: 0.77
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 1.03; acc: 0.78
Val Epoch over. val_loss: 1.128409454397335; val_accuracy: 0.7456210191082803 

The current subspace-distance is: 0.00013352837413549423 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.78
Batch: 20; loss: 1.14; acc: 0.78
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.15; acc: 0.75
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.07; acc: 0.7
Batch: 140; loss: 1.17; acc: 0.72
Batch: 160; loss: 1.18; acc: 0.64
Batch: 180; loss: 1.25; acc: 0.62
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.24; acc: 0.66
Batch: 240; loss: 1.1; acc: 0.75
Batch: 260; loss: 1.2; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.73
Batch: 300; loss: 1.03; acc: 0.8
Batch: 320; loss: 1.09; acc: 0.75
Batch: 340; loss: 1.3; acc: 0.64
Batch: 360; loss: 1.15; acc: 0.73
Batch: 380; loss: 1.2; acc: 0.77
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 1.29; acc: 0.64
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.14; acc: 0.77
Batch: 480; loss: 1.27; acc: 0.61
Batch: 500; loss: 1.15; acc: 0.73
Batch: 520; loss: 1.24; acc: 0.72
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.18; acc: 0.69
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.32; acc: 0.64
Batch: 620; loss: 1.03; acc: 0.81
Batch: 640; loss: 1.19; acc: 0.72
Batch: 660; loss: 1.26; acc: 0.69
Batch: 680; loss: 1.06; acc: 0.8
Batch: 700; loss: 1.03; acc: 0.81
Batch: 720; loss: 1.24; acc: 0.7
Batch: 740; loss: 1.12; acc: 0.69
Batch: 760; loss: 1.14; acc: 0.7
Batch: 780; loss: 1.16; acc: 0.7
Train Epoch over. train_loss: 1.15; train_accuracy: 0.72 

0.00014916170039214194
0.00014471464965026826
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.31; acc: 0.58
Batch: 40; loss: 0.88; acc: 0.88
Batch: 60; loss: 1.06; acc: 0.75
Batch: 80; loss: 0.9; acc: 0.84
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.73
Val Epoch over. val_loss: 1.095726143023011; val_accuracy: 0.7426353503184714 

The current subspace-distance is: 0.00014471464965026826 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.7
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 0.96; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 1.21; acc: 0.66
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 1.24; acc: 0.66
Batch: 220; loss: 1.02; acc: 0.81
Batch: 240; loss: 1.3; acc: 0.66
Batch: 260; loss: 1.14; acc: 0.69
Batch: 280; loss: 1.15; acc: 0.69
Batch: 300; loss: 1.17; acc: 0.69
Batch: 320; loss: 0.98; acc: 0.83
Batch: 340; loss: 1.25; acc: 0.69
Batch: 360; loss: 1.2; acc: 0.72
Batch: 380; loss: 1.03; acc: 0.73
Batch: 400; loss: 1.18; acc: 0.69
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.69
Batch: 460; loss: 1.1; acc: 0.77
Batch: 480; loss: 1.06; acc: 0.73
Batch: 500; loss: 1.22; acc: 0.69
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.1; acc: 0.75
Batch: 560; loss: 1.08; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.64
Batch: 600; loss: 1.01; acc: 0.84
Batch: 620; loss: 1.06; acc: 0.73
Batch: 640; loss: 1.19; acc: 0.67
Batch: 660; loss: 1.12; acc: 0.77
Batch: 680; loss: 1.15; acc: 0.73
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.73
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.72 

0.00016082622460089624
0.00015559911844320595
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.88
Batch: 100; loss: 1.05; acc: 0.69
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.7
Val Epoch over. val_loss: 1.0671480506848379; val_accuracy: 0.7466162420382165 

The current subspace-distance is: 0.00015559911844320595 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.08; acc: 0.72
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.7
Batch: 140; loss: 1.3; acc: 0.58
Batch: 160; loss: 1.07; acc: 0.73
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.0; acc: 0.81
Batch: 220; loss: 1.2; acc: 0.67
Batch: 240; loss: 1.12; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.77
Batch: 280; loss: 1.06; acc: 0.73
Batch: 300; loss: 1.22; acc: 0.62
Batch: 320; loss: 1.43; acc: 0.61
Batch: 340; loss: 1.09; acc: 0.73
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.22; acc: 0.73
Batch: 400; loss: 1.04; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.72
Batch: 480; loss: 1.17; acc: 0.62
Batch: 500; loss: 1.03; acc: 0.73
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.56
Batch: 560; loss: 1.12; acc: 0.73
Batch: 580; loss: 1.1; acc: 0.61
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.38; acc: 0.55
Batch: 640; loss: 1.15; acc: 0.72
Batch: 660; loss: 1.02; acc: 0.77
Batch: 680; loss: 1.08; acc: 0.75
Batch: 700; loss: 1.03; acc: 0.8
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.15; acc: 0.73
Batch: 780; loss: 1.15; acc: 0.69
Train Epoch over. train_loss: 1.1; train_accuracy: 0.72 

0.00017180772556457669
0.0001647603785386309
Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.87; acc: 0.84
Batch: 100; loss: 1.0; acc: 0.7
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 0.99; acc: 0.72
Val Epoch over. val_loss: 1.0458049876674724; val_accuracy: 0.7515923566878981 

The current subspace-distance is: 0.0001647603785386309 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 1.14; acc: 0.72
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.86
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.75
Batch: 160; loss: 1.26; acc: 0.66
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.8
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.69
Batch: 300; loss: 1.21; acc: 0.69
Batch: 320; loss: 1.0; acc: 0.77
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 0.96; acc: 0.81
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.06; acc: 0.78
Batch: 420; loss: 1.18; acc: 0.64
Batch: 440; loss: 1.05; acc: 0.69
Batch: 460; loss: 1.06; acc: 0.69
Batch: 480; loss: 1.27; acc: 0.69
Batch: 500; loss: 1.04; acc: 0.8
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 1.02; acc: 0.73
Batch: 560; loss: 1.27; acc: 0.62
Batch: 580; loss: 1.03; acc: 0.81
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.02; acc: 0.78
Batch: 640; loss: 1.09; acc: 0.73
Batch: 660; loss: 1.15; acc: 0.72
Batch: 680; loss: 1.2; acc: 0.67
Batch: 700; loss: 1.1; acc: 0.64
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.13; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.72 

0.00017297036538366228
0.0001660622947383672
Batch: 0; loss: 1.28; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.58
Batch: 140; loss: 0.99; acc: 0.72
Val Epoch over. val_loss: 1.0436831238163504; val_accuracy: 0.7482085987261147 

The current subspace-distance is: 0.0001660622947383672 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.75
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 1.21; acc: 0.7
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.73
Batch: 160; loss: 0.96; acc: 0.81
Batch: 180; loss: 1.25; acc: 0.67
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.15; acc: 0.77
Batch: 240; loss: 1.16; acc: 0.67
Batch: 260; loss: 0.94; acc: 0.78
Batch: 280; loss: 1.05; acc: 0.75
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 0.98; acc: 0.81
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 1.11; acc: 0.69
Batch: 420; loss: 1.14; acc: 0.69
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.03; acc: 0.75
Batch: 480; loss: 0.9; acc: 0.8
Batch: 500; loss: 1.2; acc: 0.64
Batch: 520; loss: 1.09; acc: 0.69
Batch: 540; loss: 1.17; acc: 0.72
Batch: 560; loss: 1.13; acc: 0.67
Batch: 580; loss: 1.03; acc: 0.77
Batch: 600; loss: 1.27; acc: 0.67
Batch: 620; loss: 0.95; acc: 0.8
Batch: 640; loss: 1.09; acc: 0.67
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.07; acc: 0.78
Batch: 700; loss: 1.16; acc: 0.66
Batch: 720; loss: 1.0; acc: 0.73
Batch: 740; loss: 1.19; acc: 0.62
Batch: 760; loss: 1.02; acc: 0.75
Batch: 780; loss: 1.0; acc: 0.78
Train Epoch over. train_loss: 1.08; train_accuracy: 0.72 

0.0001771412935340777
0.0001703045709291473
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.88
Batch: 60; loss: 1.0; acc: 0.78
Batch: 80; loss: 0.86; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 0.96; acc: 0.75
Val Epoch over. val_loss: 1.0275039991755395; val_accuracy: 0.7529856687898089 

The current subspace-distance is: 0.0001703045709291473 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.83
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 1.16; acc: 0.7
Batch: 140; loss: 1.17; acc: 0.61
Batch: 160; loss: 1.03; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.72
Batch: 200; loss: 0.96; acc: 0.81
Batch: 220; loss: 1.07; acc: 0.7
Batch: 240; loss: 1.11; acc: 0.75
Batch: 260; loss: 1.02; acc: 0.75
Batch: 280; loss: 1.0; acc: 0.73
Batch: 300; loss: 1.17; acc: 0.69
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.01; acc: 0.77
Batch: 380; loss: 1.26; acc: 0.64
Batch: 400; loss: 1.06; acc: 0.75
Batch: 420; loss: 1.18; acc: 0.59
Batch: 440; loss: 1.06; acc: 0.81
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.07; acc: 0.69
Batch: 500; loss: 1.03; acc: 0.8
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.73
Batch: 560; loss: 1.12; acc: 0.73
Batch: 580; loss: 1.28; acc: 0.58
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 0.9; acc: 0.84
Batch: 640; loss: 1.09; acc: 0.73
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 1.0; acc: 0.77
Batch: 700; loss: 1.05; acc: 0.7
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 1.04; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.7
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 1.08; train_accuracy: 0.72 

0.00018127790826838464
0.0001719864521874115
Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.7
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.73
Val Epoch over. val_loss: 1.0356110638114298; val_accuracy: 0.74890525477707 

The current subspace-distance is: 0.0001719864521874115 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.89; acc: 0.89
Batch: 40; loss: 1.1; acc: 0.69
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.23; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.73
Batch: 180; loss: 1.12; acc: 0.67
Batch: 200; loss: 1.4; acc: 0.53
Batch: 220; loss: 1.08; acc: 0.69
Batch: 240; loss: 1.02; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.83
Batch: 280; loss: 1.07; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.73
Batch: 320; loss: 1.05; acc: 0.7
Batch: 340; loss: 1.09; acc: 0.72
Batch: 360; loss: 1.01; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 1.07; acc: 0.7
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.08; acc: 0.7
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 0.92; acc: 0.78
Batch: 520; loss: 1.11; acc: 0.73
Batch: 540; loss: 1.28; acc: 0.62
Batch: 560; loss: 1.05; acc: 0.81
Batch: 580; loss: 1.25; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.75
Batch: 620; loss: 0.99; acc: 0.75
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.05; acc: 0.73
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.05; acc: 0.8
Batch: 720; loss: 1.12; acc: 0.7
Batch: 740; loss: 1.05; acc: 0.73
Batch: 760; loss: 1.1; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.72 

0.0001797191216610372
0.00017206896154675633
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.89
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.8
Batch: 100; loss: 1.01; acc: 0.7
Batch: 120; loss: 1.24; acc: 0.59
Batch: 140; loss: 0.99; acc: 0.72
Val Epoch over. val_loss: 1.0318326699505946; val_accuracy: 0.746218152866242 

The current subspace-distance is: 0.00017206896154675633 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 0.87; acc: 0.84
Batch: 40; loss: 0.97; acc: 0.78
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.64
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.02; acc: 0.81
Batch: 140; loss: 0.99; acc: 0.75
Batch: 160; loss: 1.21; acc: 0.62
Batch: 180; loss: 1.01; acc: 0.73
Batch: 200; loss: 1.04; acc: 0.73
Batch: 220; loss: 1.06; acc: 0.7
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 1.12; acc: 0.67
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 1.12; acc: 0.73
Batch: 320; loss: 1.0; acc: 0.81
Batch: 340; loss: 1.1; acc: 0.72
Batch: 360; loss: 0.95; acc: 0.86
Batch: 380; loss: 1.25; acc: 0.69
Batch: 400; loss: 1.07; acc: 0.72
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 1.12; acc: 0.72
Batch: 460; loss: 1.05; acc: 0.7
Batch: 480; loss: 1.15; acc: 0.64
Batch: 500; loss: 1.15; acc: 0.69
Batch: 520; loss: 0.99; acc: 0.7
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.06; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.86
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 0.94; acc: 0.84
Batch: 660; loss: 0.93; acc: 0.84
Batch: 680; loss: 1.1; acc: 0.75
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.11; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.04; acc: 0.66
Train Epoch over. train_loss: 1.07; train_accuracy: 0.72 

0.0001856432354543358
0.0001791148679330945
Batch: 0; loss: 1.27; acc: 0.58
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.79; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.73
Val Epoch over. val_loss: 1.025651238146861; val_accuracy: 0.7418391719745223 

The current subspace-distance is: 0.0001791148679330945 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.8
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 0.97; acc: 0.78
Batch: 160; loss: 0.93; acc: 0.83
Batch: 180; loss: 1.18; acc: 0.66
Batch: 200; loss: 1.12; acc: 0.67
Batch: 220; loss: 0.97; acc: 0.81
Batch: 240; loss: 1.03; acc: 0.78
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 0.96; acc: 0.81
Batch: 300; loss: 1.11; acc: 0.66
Batch: 320; loss: 1.08; acc: 0.7
Batch: 340; loss: 1.22; acc: 0.62
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.15; acc: 0.64
Batch: 400; loss: 1.22; acc: 0.69
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.14; acc: 0.73
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 0.89; acc: 0.81
Batch: 520; loss: 1.02; acc: 0.72
Batch: 540; loss: 1.05; acc: 0.72
Batch: 560; loss: 1.17; acc: 0.75
Batch: 580; loss: 1.14; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 0.98; acc: 0.8
Batch: 660; loss: 1.16; acc: 0.64
Batch: 680; loss: 1.06; acc: 0.75
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 1.15; acc: 0.69
Batch: 740; loss: 0.88; acc: 0.83
Batch: 760; loss: 1.08; acc: 0.69
Batch: 780; loss: 1.06; acc: 0.69
Train Epoch over. train_loss: 1.07; train_accuracy: 0.72 

0.0001869613624876365
0.00017889367882162333
Batch: 0; loss: 1.25; acc: 0.61
Batch: 20; loss: 1.21; acc: 0.59
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.8
Batch: 100; loss: 0.98; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.0167070634805473; val_accuracy: 0.7501990445859873 

The current subspace-distance is: 0.00017889367882162333 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.09; acc: 0.7
Batch: 40; loss: 1.12; acc: 0.78
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 1.21; acc: 0.64
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 0.96; acc: 0.73
Batch: 200; loss: 1.01; acc: 0.8
Batch: 220; loss: 1.07; acc: 0.75
Batch: 240; loss: 0.94; acc: 0.75
Batch: 260; loss: 1.27; acc: 0.64
Batch: 280; loss: 1.12; acc: 0.75
Batch: 300; loss: 1.0; acc: 0.8
Batch: 320; loss: 1.13; acc: 0.73
Batch: 340; loss: 0.99; acc: 0.8
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 0.96; acc: 0.78
Batch: 400; loss: 1.14; acc: 0.64
Batch: 420; loss: 1.24; acc: 0.64
Batch: 440; loss: 1.0; acc: 0.77
Batch: 460; loss: 0.92; acc: 0.77
Batch: 480; loss: 1.04; acc: 0.77
Batch: 500; loss: 1.11; acc: 0.67
Batch: 520; loss: 1.0; acc: 0.77
Batch: 540; loss: 1.07; acc: 0.72
Batch: 560; loss: 1.03; acc: 0.73
Batch: 580; loss: 1.03; acc: 0.73
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 1.12; acc: 0.64
Batch: 640; loss: 1.0; acc: 0.75
Batch: 660; loss: 1.01; acc: 0.78
Batch: 680; loss: 1.27; acc: 0.61
Batch: 700; loss: 0.99; acc: 0.77
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 1.12; acc: 0.69
Batch: 760; loss: 0.9; acc: 0.83
Batch: 780; loss: 0.98; acc: 0.72
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00018662777438294142
0.00018158293096348643
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 0.99; acc: 0.8
Batch: 80; loss: 0.87; acc: 0.78
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.97; acc: 0.77
Val Epoch over. val_loss: 1.0206393664050255; val_accuracy: 0.7517914012738853 

The current subspace-distance is: 0.00018158293096348643 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.0; acc: 0.72
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.21; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.72
Batch: 160; loss: 1.2; acc: 0.61
Batch: 180; loss: 0.92; acc: 0.8
Batch: 200; loss: 0.99; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 0.96; acc: 0.78
Batch: 260; loss: 1.17; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.07; acc: 0.72
Batch: 320; loss: 1.08; acc: 0.73
Batch: 340; loss: 1.26; acc: 0.64
Batch: 360; loss: 1.03; acc: 0.72
Batch: 380; loss: 1.05; acc: 0.77
Batch: 400; loss: 1.27; acc: 0.61
Batch: 420; loss: 1.09; acc: 0.75
Batch: 440; loss: 1.11; acc: 0.73
Batch: 460; loss: 1.05; acc: 0.8
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.02; acc: 0.72
Batch: 540; loss: 1.11; acc: 0.73
Batch: 560; loss: 1.19; acc: 0.66
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.72
Batch: 640; loss: 1.06; acc: 0.75
Batch: 660; loss: 1.09; acc: 0.67
Batch: 680; loss: 0.95; acc: 0.75
Batch: 700; loss: 0.95; acc: 0.81
Batch: 720; loss: 1.09; acc: 0.73
Batch: 740; loss: 0.85; acc: 0.86
Batch: 760; loss: 1.07; acc: 0.75
Batch: 780; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00018636550521478057
0.00018105456547345966
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.97; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.95; acc: 0.75
Val Epoch over. val_loss: 1.004025761109249; val_accuracy: 0.7546775477707006 

The current subspace-distance is: 0.00018105456547345966 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.91; acc: 0.75
Batch: 20; loss: 1.1; acc: 0.7
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.81
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 1.07; acc: 0.69
Batch: 160; loss: 0.99; acc: 0.75
Batch: 180; loss: 0.91; acc: 0.8
Batch: 200; loss: 1.16; acc: 0.66
Batch: 220; loss: 0.98; acc: 0.78
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.94; acc: 0.78
Batch: 280; loss: 0.91; acc: 0.8
Batch: 300; loss: 1.02; acc: 0.72
Batch: 320; loss: 0.95; acc: 0.77
Batch: 340; loss: 1.0; acc: 0.78
Batch: 360; loss: 0.93; acc: 0.77
Batch: 380; loss: 1.16; acc: 0.62
Batch: 400; loss: 1.1; acc: 0.66
Batch: 420; loss: 1.06; acc: 0.69
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.05; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 0.95; acc: 0.73
Batch: 540; loss: 1.12; acc: 0.69
Batch: 560; loss: 1.12; acc: 0.69
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 1.11; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.84
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.08; acc: 0.64
Batch: 680; loss: 1.02; acc: 0.69
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 1.08; acc: 0.72
Batch: 740; loss: 1.03; acc: 0.72
Batch: 760; loss: 1.32; acc: 0.62
Batch: 780; loss: 1.1; acc: 0.75
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00018886274483520538
0.00018421611457597464
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 0.77; acc: 0.91
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.99; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.013120624669798; val_accuracy: 0.7464171974522293 

The current subspace-distance is: 0.00018421611457597464 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.01; acc: 0.72
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.67
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.05; acc: 0.72
Batch: 160; loss: 0.99; acc: 0.77
Batch: 180; loss: 1.09; acc: 0.72
Batch: 200; loss: 1.0; acc: 0.81
Batch: 220; loss: 0.97; acc: 0.77
Batch: 240; loss: 1.1; acc: 0.73
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.96; acc: 0.77
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 0.93; acc: 0.83
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 1.25; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.67
Batch: 400; loss: 1.05; acc: 0.66
Batch: 420; loss: 1.03; acc: 0.8
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 1.0; acc: 0.78
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 1.08; acc: 0.72
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.05; acc: 0.75
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.04; acc: 0.73
Batch: 600; loss: 1.14; acc: 0.62
Batch: 620; loss: 1.0; acc: 0.78
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.66
Batch: 680; loss: 0.87; acc: 0.8
Batch: 700; loss: 0.86; acc: 0.83
Batch: 720; loss: 0.98; acc: 0.73
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.73
Batch: 780; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00019341419101692736
0.0001847696112236008
Batch: 0; loss: 1.25; acc: 0.61
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.0123537862376801; val_accuracy: 0.7525875796178344 

The current subspace-distance is: 0.0001847696112236008 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.8
Batch: 40; loss: 1.06; acc: 0.72
Batch: 60; loss: 0.95; acc: 0.84
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.17; acc: 0.69
Batch: 160; loss: 0.95; acc: 0.8
Batch: 180; loss: 0.88; acc: 0.88
Batch: 200; loss: 1.07; acc: 0.7
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.04; acc: 0.72
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 0.99; acc: 0.69
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.36; acc: 0.56
Batch: 380; loss: 1.1; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 0.95; acc: 0.77
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.03; acc: 0.72
Batch: 500; loss: 0.97; acc: 0.77
Batch: 520; loss: 0.94; acc: 0.83
Batch: 540; loss: 0.96; acc: 0.77
Batch: 560; loss: 0.93; acc: 0.86
Batch: 580; loss: 1.17; acc: 0.67
Batch: 600; loss: 0.98; acc: 0.8
Batch: 620; loss: 0.96; acc: 0.83
Batch: 640; loss: 1.11; acc: 0.73
Batch: 660; loss: 1.05; acc: 0.7
Batch: 680; loss: 1.08; acc: 0.69
Batch: 700; loss: 0.99; acc: 0.77
Batch: 720; loss: 1.11; acc: 0.7
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.66
Batch: 780; loss: 0.83; acc: 0.83
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00019098630582448095
0.00018536571587901562
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.91
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.78
Val Epoch over. val_loss: 1.0090993642807007; val_accuracy: 0.7504976114649682 

The current subspace-distance is: 0.00018536571587901562 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 0.95; acc: 0.78
Batch: 180; loss: 1.13; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 0.97; acc: 0.78
Batch: 240; loss: 1.24; acc: 0.66
Batch: 260; loss: 0.99; acc: 0.77
Batch: 280; loss: 0.9; acc: 0.88
Batch: 300; loss: 0.96; acc: 0.8
Batch: 320; loss: 0.91; acc: 0.78
Batch: 340; loss: 0.88; acc: 0.88
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 0.93; acc: 0.81
Batch: 400; loss: 0.97; acc: 0.75
Batch: 420; loss: 1.06; acc: 0.67
Batch: 440; loss: 1.2; acc: 0.61
Batch: 460; loss: 1.07; acc: 0.72
Batch: 480; loss: 1.02; acc: 0.7
Batch: 500; loss: 1.0; acc: 0.75
Batch: 520; loss: 1.08; acc: 0.72
Batch: 540; loss: 0.89; acc: 0.89
Batch: 560; loss: 1.14; acc: 0.67
Batch: 580; loss: 1.04; acc: 0.73
Batch: 600; loss: 0.95; acc: 0.8
Batch: 620; loss: 0.92; acc: 0.81
Batch: 640; loss: 1.23; acc: 0.64
Batch: 660; loss: 0.88; acc: 0.72
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 1.02; acc: 0.77
Batch: 720; loss: 1.02; acc: 0.78
Batch: 740; loss: 0.98; acc: 0.75
Batch: 760; loss: 0.96; acc: 0.73
Batch: 780; loss: 0.98; acc: 0.77
Train Epoch over. train_loss: 1.05; train_accuracy: 0.73 

0.00019528699340298772
0.0001868718973128125
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.91
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.0039035108438723; val_accuracy: 0.7552746815286624 

The current subspace-distance is: 0.0001868718973128125 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.99; acc: 0.78
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.04; acc: 0.75
Batch: 180; loss: 0.99; acc: 0.75
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 1.05; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.78
Batch: 260; loss: 0.94; acc: 0.81
Batch: 280; loss: 0.97; acc: 0.78
Batch: 300; loss: 1.18; acc: 0.64
Batch: 320; loss: 1.06; acc: 0.69
Batch: 340; loss: 1.07; acc: 0.69
Batch: 360; loss: 1.02; acc: 0.77
Batch: 380; loss: 1.03; acc: 0.73
Batch: 400; loss: 1.16; acc: 0.58
Batch: 420; loss: 0.96; acc: 0.77
Batch: 440; loss: 1.03; acc: 0.66
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 1.07; acc: 0.72
Batch: 500; loss: 1.09; acc: 0.73
Batch: 520; loss: 1.07; acc: 0.73
Batch: 540; loss: 1.2; acc: 0.59
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.75
Batch: 600; loss: 0.99; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.8
Batch: 640; loss: 1.13; acc: 0.61
Batch: 660; loss: 1.08; acc: 0.77
Batch: 680; loss: 0.87; acc: 0.84
Batch: 700; loss: 1.0; acc: 0.75
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.11; acc: 0.78
Batch: 760; loss: 1.25; acc: 0.58
Batch: 780; loss: 0.99; acc: 0.77
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00019705729209817946
0.00018898000416811556
Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 0.9971816801721123; val_accuracy: 0.7548765923566879 

The current subspace-distance is: 0.00018898000416811556 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 0.98; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.83
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 1.04; acc: 0.75
Batch: 160; loss: 1.03; acc: 0.78
Batch: 180; loss: 1.21; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 0.92; acc: 0.83
Batch: 240; loss: 1.07; acc: 0.73
Batch: 260; loss: 0.94; acc: 0.78
Batch: 280; loss: 1.04; acc: 0.66
Batch: 300; loss: 1.1; acc: 0.73
Batch: 320; loss: 1.02; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.75
Batch: 360; loss: 0.97; acc: 0.73
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 1.03; acc: 0.72
Batch: 420; loss: 1.19; acc: 0.62
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 1.05; acc: 0.77
Batch: 480; loss: 1.0; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.9; acc: 0.81
Batch: 540; loss: 1.06; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.67
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 1.0; acc: 0.78
Batch: 620; loss: 1.03; acc: 0.7
Batch: 640; loss: 0.94; acc: 0.8
Batch: 660; loss: 1.04; acc: 0.77
Batch: 680; loss: 0.99; acc: 0.78
Batch: 700; loss: 0.92; acc: 0.78
Batch: 720; loss: 1.01; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.67
Batch: 760; loss: 1.15; acc: 0.64
Batch: 780; loss: 1.14; acc: 0.73
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00019312194490339607
0.000187866942724213
Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.0043022936316812; val_accuracy: 0.7514928343949044 

The current subspace-distance is: 0.000187866942724213 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.07; acc: 0.7
Batch: 40; loss: 1.08; acc: 0.77
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 1.01; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 1.12; acc: 0.72
Batch: 180; loss: 1.09; acc: 0.69
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 0.95; acc: 0.77
Batch: 240; loss: 1.09; acc: 0.69
Batch: 260; loss: 0.95; acc: 0.83
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.72
Batch: 320; loss: 1.06; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 1.14; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.72
Batch: 420; loss: 1.18; acc: 0.67
Batch: 440; loss: 0.96; acc: 0.78
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 1.05; acc: 0.66
Batch: 500; loss: 1.16; acc: 0.69
Batch: 520; loss: 0.96; acc: 0.8
Batch: 540; loss: 1.23; acc: 0.66
Batch: 560; loss: 0.9; acc: 0.81
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.03; acc: 0.78
Batch: 620; loss: 1.1; acc: 0.78
Batch: 640; loss: 1.07; acc: 0.7
Batch: 660; loss: 1.15; acc: 0.72
Batch: 680; loss: 1.05; acc: 0.77
Batch: 700; loss: 1.15; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.7
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.26; acc: 0.64
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.05; train_accuracy: 0.73 

0.0001963559043360874
0.0001885688106995076
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.24; acc: 0.56
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 0.95; acc: 0.75
Val Epoch over. val_loss: 0.9990066084892127; val_accuracy: 0.7517914012738853 

The current subspace-distance is: 0.0001885688106995076 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.77
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 1.06; acc: 0.73
Batch: 180; loss: 1.04; acc: 0.72
Batch: 200; loss: 0.92; acc: 0.81
Batch: 220; loss: 1.2; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.73
Batch: 260; loss: 0.95; acc: 0.83
Batch: 280; loss: 1.14; acc: 0.72
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 0.99; acc: 0.83
Batch: 360; loss: 1.18; acc: 0.62
Batch: 380; loss: 1.0; acc: 0.7
Batch: 400; loss: 1.08; acc: 0.69
Batch: 420; loss: 1.07; acc: 0.69
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.73
Batch: 480; loss: 0.95; acc: 0.81
Batch: 500; loss: 1.04; acc: 0.72
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 1.11; acc: 0.7
Batch: 560; loss: 1.01; acc: 0.81
Batch: 580; loss: 1.07; acc: 0.72
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 0.98; acc: 0.83
Batch: 640; loss: 0.94; acc: 0.78
Batch: 660; loss: 1.0; acc: 0.77
Batch: 680; loss: 1.1; acc: 0.78
Batch: 700; loss: 0.98; acc: 0.78
Batch: 720; loss: 1.11; acc: 0.7
Batch: 740; loss: 1.11; acc: 0.64
Batch: 760; loss: 1.08; acc: 0.72
Batch: 780; loss: 1.09; acc: 0.73
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00019902882922906429
0.0001882595388451591
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 0.96; acc: 0.69
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.97; acc: 0.78
Val Epoch over. val_loss: 1.0030872924312664; val_accuracy: 0.7494028662420382 

The current subspace-distance is: 0.0001882595388451591 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.81
Batch: 20; loss: 0.99; acc: 0.77
Batch: 40; loss: 1.07; acc: 0.66
Batch: 60; loss: 1.05; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.61
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.2; acc: 0.66
Batch: 180; loss: 1.03; acc: 0.77
Batch: 200; loss: 1.06; acc: 0.73
Batch: 220; loss: 1.12; acc: 0.72
Batch: 240; loss: 1.0; acc: 0.75
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 1.16; acc: 0.7
Batch: 300; loss: 0.94; acc: 0.72
Batch: 320; loss: 0.87; acc: 0.78
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 1.02; acc: 0.7
Batch: 380; loss: 1.1; acc: 0.7
Batch: 400; loss: 1.23; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.0; acc: 0.81
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.24; acc: 0.64
Batch: 540; loss: 0.9; acc: 0.84
Batch: 560; loss: 0.99; acc: 0.69
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.21; acc: 0.62
Batch: 640; loss: 1.28; acc: 0.53
Batch: 660; loss: 1.16; acc: 0.66
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 1.13; acc: 0.66
Batch: 720; loss: 0.98; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 1.1; acc: 0.64
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.0001946375850820914
0.00018740806262940168
Batch: 0; loss: 1.25; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 0.9962755863074284; val_accuracy: 0.7557722929936306 

The current subspace-distance is: 0.00018740806262940168 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.8
Batch: 60; loss: 0.94; acc: 0.81
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.04; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.84
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 1.04; acc: 0.7
Batch: 200; loss: 0.97; acc: 0.72
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 0.98; acc: 0.8
Batch: 280; loss: 0.88; acc: 0.83
Batch: 300; loss: 1.03; acc: 0.73
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 0.93; acc: 0.83
Batch: 360; loss: 1.05; acc: 0.7
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 0.9; acc: 0.73
Batch: 420; loss: 1.12; acc: 0.62
Batch: 440; loss: 1.14; acc: 0.64
Batch: 460; loss: 1.11; acc: 0.61
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 0.9; acc: 0.8
Batch: 520; loss: 1.01; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.66
Batch: 560; loss: 1.12; acc: 0.66
Batch: 580; loss: 1.07; acc: 0.72
Batch: 600; loss: 0.93; acc: 0.81
Batch: 620; loss: 0.99; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.04; acc: 0.73
Batch: 680; loss: 0.97; acc: 0.7
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 0.9; acc: 0.8
Batch: 740; loss: 1.06; acc: 0.73
Batch: 760; loss: 1.12; acc: 0.72
Batch: 780; loss: 1.12; acc: 0.75
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00019678416720125824
0.00018736465426627547
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.81
Batch: 100; loss: 0.96; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 0.9944509742366281; val_accuracy: 0.7530851910828026 

The current subspace-distance is: 0.00018736465426627547 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.78
Batch: 20; loss: 0.91; acc: 0.8
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.0; acc: 0.69
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.78
Batch: 160; loss: 1.01; acc: 0.8
Batch: 180; loss: 1.09; acc: 0.72
Batch: 200; loss: 1.06; acc: 0.78
Batch: 220; loss: 1.15; acc: 0.67
Batch: 240; loss: 0.99; acc: 0.78
Batch: 260; loss: 1.08; acc: 0.73
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 0.95; acc: 0.78
Batch: 320; loss: 1.17; acc: 0.67
Batch: 340; loss: 0.98; acc: 0.77
Batch: 360; loss: 0.89; acc: 0.83
Batch: 380; loss: 1.1; acc: 0.7
Batch: 400; loss: 1.09; acc: 0.66
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.02; acc: 0.78
Batch: 500; loss: 1.14; acc: 0.72
Batch: 520; loss: 0.84; acc: 0.81
Batch: 540; loss: 1.02; acc: 0.72
Batch: 560; loss: 1.1; acc: 0.67
Batch: 580; loss: 0.97; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 0.94; acc: 0.77
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 0.99; acc: 0.77
Batch: 740; loss: 1.27; acc: 0.61
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 1.13; acc: 0.64
Train Epoch over. train_loss: 1.05; train_accuracy: 0.73 

0.00020088061864953488
0.0001916405017254874
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 0.999258635909694; val_accuracy: 0.7522890127388535 

The current subspace-distance is: 0.0001916405017254874 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.7
Batch: 160; loss: 1.17; acc: 0.61
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 0.98; acc: 0.78
Batch: 220; loss: 1.1; acc: 0.72
Batch: 240; loss: 1.11; acc: 0.7
Batch: 260; loss: 1.05; acc: 0.73
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 0.9; acc: 0.81
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 0.98; acc: 0.72
Batch: 360; loss: 1.06; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.66
Batch: 400; loss: 1.03; acc: 0.73
Batch: 420; loss: 1.17; acc: 0.62
Batch: 440; loss: 0.99; acc: 0.8
Batch: 460; loss: 1.18; acc: 0.66
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.73
Batch: 540; loss: 0.98; acc: 0.77
Batch: 560; loss: 0.86; acc: 0.84
Batch: 580; loss: 0.95; acc: 0.78
Batch: 600; loss: 1.13; acc: 0.67
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.03; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.73
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.73
Batch: 740; loss: 1.33; acc: 0.61
Batch: 760; loss: 1.02; acc: 0.69
Batch: 780; loss: 1.15; acc: 0.66
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00019800565496552736
0.00019078546029049903
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.83
Batch: 100; loss: 0.97; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 0.9919794224629737; val_accuracy: 0.7552746815286624 

The current subspace-distance is: 0.00019078546029049903 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.0473763698939185

The number of parameters is: 252462

The number of individual parameters is:

17
306
17
17
25
38250
25
25
50
112500
50
50
64
96000
64
64
4096
64
640
10
64
64

nonzero elements in E: 50492396
elements in E: 50492400
fraction nonzero: 0.999999920780157
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.42; acc: 0.06
Batch: 20; loss: 2.28; acc: 0.14
Batch: 40; loss: 2.03; acc: 0.38
Batch: 60; loss: 2.0; acc: 0.27
Batch: 80; loss: 1.86; acc: 0.48
Batch: 100; loss: 1.75; acc: 0.55
Batch: 120; loss: 1.81; acc: 0.55
Batch: 140; loss: 1.61; acc: 0.64
Batch: 160; loss: 1.73; acc: 0.55
Batch: 180; loss: 1.58; acc: 0.73
Batch: 200; loss: 1.56; acc: 0.7
Batch: 220; loss: 1.54; acc: 0.67
Batch: 240; loss: 1.56; acc: 0.66
Batch: 260; loss: 1.51; acc: 0.75
Batch: 280; loss: 1.55; acc: 0.67
Batch: 300; loss: 1.56; acc: 0.67
Batch: 320; loss: 1.5; acc: 0.62
Batch: 340; loss: 1.49; acc: 0.7
Batch: 360; loss: 1.45; acc: 0.77
Batch: 380; loss: 1.45; acc: 0.7
Batch: 400; loss: 1.52; acc: 0.59
Batch: 420; loss: 1.49; acc: 0.73
Batch: 440; loss: 1.45; acc: 0.7
Batch: 460; loss: 1.41; acc: 0.77
Batch: 480; loss: 1.48; acc: 0.69
Batch: 500; loss: 1.45; acc: 0.73
Batch: 520; loss: 1.44; acc: 0.64
Batch: 540; loss: 1.32; acc: 0.8
Batch: 560; loss: 1.28; acc: 0.77
Batch: 580; loss: 1.34; acc: 0.78
Batch: 600; loss: 1.32; acc: 0.75
Batch: 620; loss: 1.37; acc: 0.75
Batch: 640; loss: 1.35; acc: 0.67
Batch: 660; loss: 1.41; acc: 0.7
Batch: 680; loss: 1.38; acc: 0.72
Batch: 700; loss: 1.39; acc: 0.61
Batch: 720; loss: 1.23; acc: 0.84
Batch: 740; loss: 1.33; acc: 0.72
Batch: 760; loss: 1.28; acc: 0.77
Batch: 780; loss: 1.28; acc: 0.73
Train Epoch over. train_loss: 1.54; train_accuracy: 0.64 

5.906569640501402e-05
5.383257303037681e-05
Batch: 0; loss: 1.27; acc: 0.84
Batch: 20; loss: 1.41; acc: 0.61
Batch: 40; loss: 0.99; acc: 0.94
Batch: 60; loss: 1.19; acc: 0.83
Batch: 80; loss: 1.14; acc: 0.84
Batch: 100; loss: 1.19; acc: 0.86
Batch: 120; loss: 1.45; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.84
Val Epoch over. val_loss: 1.2645671963691711; val_accuracy: 0.7685111464968153 

The current subspace-distance is: 5.383257303037681e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.77
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 1.18; acc: 0.83
Batch: 80; loss: 1.24; acc: 0.8
Batch: 100; loss: 1.31; acc: 0.77
Batch: 120; loss: 1.23; acc: 0.81
Batch: 140; loss: 1.12; acc: 0.88
Batch: 160; loss: 1.26; acc: 0.81
Batch: 180; loss: 1.28; acc: 0.78
Batch: 200; loss: 1.19; acc: 0.81
Batch: 220; loss: 1.18; acc: 0.78
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.16; acc: 0.83
Batch: 280; loss: 1.05; acc: 0.89
Batch: 300; loss: 1.25; acc: 0.78
Batch: 320; loss: 1.16; acc: 0.84
Batch: 340; loss: 1.13; acc: 0.83
Batch: 360; loss: 1.21; acc: 0.77
Batch: 380; loss: 1.29; acc: 0.67
Batch: 400; loss: 1.14; acc: 0.8
Batch: 420; loss: 1.12; acc: 0.83
Batch: 440; loss: 1.26; acc: 0.77
Batch: 460; loss: 1.14; acc: 0.8
Batch: 480; loss: 1.2; acc: 0.72
Batch: 500; loss: 1.19; acc: 0.75
Batch: 520; loss: 1.18; acc: 0.78
Batch: 540; loss: 1.17; acc: 0.78
Batch: 560; loss: 1.27; acc: 0.75
Batch: 580; loss: 1.07; acc: 0.86
Batch: 600; loss: 1.17; acc: 0.72
Batch: 620; loss: 1.14; acc: 0.8
Batch: 640; loss: 1.17; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.75
Batch: 680; loss: 1.13; acc: 0.75
Batch: 700; loss: 1.21; acc: 0.77
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.11; acc: 0.8
Batch: 760; loss: 1.14; acc: 0.77
Batch: 780; loss: 1.13; acc: 0.75
Train Epoch over. train_loss: 1.22; train_accuracy: 0.76 

8.209990483010188e-05
7.6944830652792e-05
Batch: 0; loss: 1.14; acc: 0.84
Batch: 20; loss: 1.19; acc: 0.72
Batch: 40; loss: 0.82; acc: 0.89
Batch: 60; loss: 1.05; acc: 0.8
Batch: 80; loss: 0.91; acc: 0.92
Batch: 100; loss: 1.03; acc: 0.83
Batch: 120; loss: 1.3; acc: 0.72
Batch: 140; loss: 0.87; acc: 0.91
Val Epoch over. val_loss: 1.0890140541040214; val_accuracy: 0.794187898089172 

The current subspace-distance is: 7.6944830652792e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.78
Batch: 40; loss: 1.11; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.89
Batch: 120; loss: 1.06; acc: 0.8
Batch: 140; loss: 1.07; acc: 0.81
Batch: 160; loss: 1.11; acc: 0.78
Batch: 180; loss: 1.15; acc: 0.7
Batch: 200; loss: 1.12; acc: 0.78
Batch: 220; loss: 1.04; acc: 0.81
Batch: 240; loss: 1.22; acc: 0.72
Batch: 260; loss: 1.27; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.8
Batch: 300; loss: 1.06; acc: 0.78
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.0; acc: 0.83
Batch: 360; loss: 1.1; acc: 0.78
Batch: 380; loss: 1.05; acc: 0.84
Batch: 400; loss: 1.09; acc: 0.77
Batch: 420; loss: 1.07; acc: 0.83
Batch: 440; loss: 1.14; acc: 0.78
Batch: 460; loss: 1.13; acc: 0.73
Batch: 480; loss: 1.12; acc: 0.72
Batch: 500; loss: 0.9; acc: 0.92
Batch: 520; loss: 1.01; acc: 0.81
Batch: 540; loss: 0.91; acc: 0.92
Batch: 560; loss: 0.95; acc: 0.86
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 1.01; acc: 0.77
Batch: 620; loss: 1.18; acc: 0.7
Batch: 640; loss: 1.03; acc: 0.83
Batch: 660; loss: 1.02; acc: 0.81
Batch: 680; loss: 1.04; acc: 0.81
Batch: 700; loss: 0.98; acc: 0.83
Batch: 720; loss: 1.02; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.81
Batch: 760; loss: 0.9; acc: 0.8
Batch: 780; loss: 0.97; acc: 0.78
Train Epoch over. train_loss: 1.06; train_accuracy: 0.79 

0.00010139708319911733
9.654237510403618e-05
Batch: 0; loss: 0.98; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.81
Batch: 40; loss: 0.65; acc: 0.91
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.91
Batch: 100; loss: 0.91; acc: 0.89
Batch: 120; loss: 1.16; acc: 0.69
Batch: 140; loss: 0.69; acc: 0.92
Val Epoch over. val_loss: 0.9323792191827374; val_accuracy: 0.8215565286624203 

The current subspace-distance is: 9.654237510403618e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.95; acc: 0.8
Batch: 60; loss: 1.02; acc: 0.83
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 1.0; acc: 0.77
Batch: 160; loss: 1.03; acc: 0.77
Batch: 180; loss: 1.06; acc: 0.77
Batch: 200; loss: 0.87; acc: 0.84
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 1.02; acc: 0.77
Batch: 260; loss: 1.01; acc: 0.8
Batch: 280; loss: 0.9; acc: 0.83
Batch: 300; loss: 0.75; acc: 0.94
Batch: 320; loss: 1.12; acc: 0.66
Batch: 340; loss: 0.88; acc: 0.8
Batch: 360; loss: 1.01; acc: 0.78
Batch: 380; loss: 0.83; acc: 0.88
Batch: 400; loss: 1.06; acc: 0.73
Batch: 420; loss: 1.04; acc: 0.7
Batch: 440; loss: 0.83; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.88
Batch: 480; loss: 0.99; acc: 0.78
Batch: 500; loss: 0.88; acc: 0.86
Batch: 520; loss: 0.9; acc: 0.8
Batch: 540; loss: 0.95; acc: 0.84
Batch: 560; loss: 0.78; acc: 0.86
Batch: 580; loss: 0.86; acc: 0.84
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 0.91; acc: 0.83
Batch: 640; loss: 0.91; acc: 0.86
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.92
Batch: 700; loss: 0.86; acc: 0.84
Batch: 720; loss: 0.98; acc: 0.77
Batch: 740; loss: 0.86; acc: 0.86
Batch: 760; loss: 0.92; acc: 0.78
Batch: 780; loss: 0.98; acc: 0.78
Train Epoch over. train_loss: 0.94; train_accuracy: 0.8 

0.00011719414032995701
0.00011316472227917984
Batch: 0; loss: 0.89; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.84; acc: 0.91
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.94
Val Epoch over. val_loss: 0.8409375852080667; val_accuracy: 0.837281050955414 

The current subspace-distance is: 0.00011316472227917984 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.81
Batch: 40; loss: 0.86; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 0.93; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 0.98; acc: 0.83
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 0.91; acc: 0.81
Batch: 220; loss: 0.96; acc: 0.73
Batch: 240; loss: 0.92; acc: 0.78
Batch: 260; loss: 0.91; acc: 0.78
Batch: 280; loss: 1.03; acc: 0.8
Batch: 300; loss: 0.82; acc: 0.84
Batch: 320; loss: 0.82; acc: 0.86
Batch: 340; loss: 0.91; acc: 0.75
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.86; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.88
Batch: 460; loss: 0.87; acc: 0.8
Batch: 480; loss: 0.89; acc: 0.77
Batch: 500; loss: 0.98; acc: 0.8
Batch: 520; loss: 0.92; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.81; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.84
Batch: 620; loss: 0.94; acc: 0.75
Batch: 640; loss: 0.81; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 0.89; acc: 0.75
Batch: 700; loss: 0.94; acc: 0.72
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.82; acc: 0.83
Batch: 760; loss: 0.85; acc: 0.8
Batch: 780; loss: 0.73; acc: 0.92
Train Epoch over. train_loss: 0.86; train_accuracy: 0.82 

0.0001311971718678251
0.00012575479922816157
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.88
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.7815097340732623; val_accuracy: 0.8366839171974523 

The current subspace-distance is: 0.00012575479922816157 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.84; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.89
Batch: 200; loss: 0.99; acc: 0.75
Batch: 220; loss: 0.8; acc: 0.88
Batch: 240; loss: 0.85; acc: 0.77
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 0.91; acc: 0.72
Batch: 300; loss: 0.77; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.86
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.89
Batch: 380; loss: 0.83; acc: 0.78
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.96; acc: 0.73
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.86
Batch: 500; loss: 0.87; acc: 0.78
Batch: 520; loss: 0.91; acc: 0.73
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.88
Batch: 580; loss: 0.78; acc: 0.84
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.77
Batch: 660; loss: 0.73; acc: 0.84
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.75; acc: 0.89
Batch: 720; loss: 0.76; acc: 0.86
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.00014367922267410904
0.00013688867329619825
Batch: 0; loss: 0.76; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.78; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.7310603042696692; val_accuracy: 0.8481289808917197 

The current subspace-distance is: 0.00013688867329619825 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.84
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.89; acc: 0.77
Batch: 180; loss: 0.92; acc: 0.78
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.81
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.95; acc: 0.73
Batch: 300; loss: 0.81; acc: 0.78
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.83
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.91; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 1.06; acc: 0.7
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.89; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.7; acc: 0.89
Train Epoch over. train_loss: 0.78; train_accuracy: 0.82 

0.0001524561084806919
0.00014583609299734235
Batch: 0; loss: 0.71; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.7106802860263047; val_accuracy: 0.8458399681528662 

The current subspace-distance is: 0.00014583609299734235 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.83; acc: 0.73
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.82; acc: 0.8
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.8; acc: 0.84
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.88
Batch: 420; loss: 0.84; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.88; acc: 0.7
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.73; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.88
Batch: 600; loss: 0.77; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.71; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00016391757526434958
0.00015819165855646133
Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.6732242050444245; val_accuracy: 0.8493232484076433 

The current subspace-distance is: 0.00015819165855646133 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.66; acc: 0.91
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.91
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.78
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.94
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.89
Batch: 480; loss: 0.71; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.73
Batch: 520; loss: 0.82; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.84; acc: 0.81
Batch: 580; loss: 0.82; acc: 0.81
Batch: 600; loss: 0.53; acc: 0.95
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 0.82; acc: 0.8
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.67; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 0.63; acc: 0.91
Batch: 760; loss: 0.79; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00017003850371111184
0.00016236539522651583
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.6567152379804356; val_accuracy: 0.8485270700636943 

The current subspace-distance is: 0.00016236539522651583 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.89
Batch: 160; loss: 0.82; acc: 0.77
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.77
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.59; acc: 0.95
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.9; acc: 0.72
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.84; acc: 0.78
Batch: 520; loss: 0.89; acc: 0.78
Batch: 540; loss: 0.66; acc: 0.89
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 0.76; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00017613927775528282
0.00017013782053254545
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.6441867264213076; val_accuracy: 0.8490246815286624 

The current subspace-distance is: 0.00017013782053254545 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.94
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.85; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.8
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.72
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00017907528672367334
0.00017142352589871734
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.6421676287605504; val_accuracy: 0.8495222929936306 

The current subspace-distance is: 0.00017142352589871734 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.65; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.79; acc: 0.77
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.44; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.8; acc: 0.77
Batch: 460; loss: 0.7; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.71; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.89
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00018417539831716567
0.00017874968762043864
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.92
Val Epoch over. val_loss: 0.6311403558512402; val_accuracy: 0.8530055732484076 

The current subspace-distance is: 0.00017874968762043864 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.9; acc: 0.72
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.92
Batch: 260; loss: 0.77; acc: 0.75
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.74; acc: 0.78
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.74; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.81; acc: 0.77
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.91; acc: 0.78
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.77; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.7; acc: 0.84
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00018331700994167477
0.00017569601186551154
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.92
Val Epoch over. val_loss: 0.6300439513792657; val_accuracy: 0.8524084394904459 

The current subspace-distance is: 0.00017569601186551154 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.74; acc: 0.75
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.73; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.84
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.75
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.75
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00018596023437567055
0.00017765829397831112
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.6168586209321477; val_accuracy: 0.8545979299363057 

The current subspace-distance is: 0.00017765829397831112 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 1.05; acc: 0.69
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.51; acc: 0.92
Batch: 320; loss: 0.75; acc: 0.8
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.56; acc: 0.91
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.85; acc: 0.78
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.73
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.0001880768104456365
0.00017850560834631324
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.92
Val Epoch over. val_loss: 0.6203799209776958; val_accuracy: 0.853702229299363 

The current subspace-distance is: 0.00017850560834631324 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.76; acc: 0.75
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.76; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.77; acc: 0.8
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00019114317547064275
0.00018324701522942632
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.6140864212421855; val_accuracy: 0.8536027070063694 

The current subspace-distance is: 0.00018324701522942632 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.67
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.76; acc: 0.86
Batch: 160; loss: 0.88; acc: 0.73
Batch: 180; loss: 0.59; acc: 0.91
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.49; acc: 0.92
Batch: 260; loss: 0.55; acc: 0.91
Batch: 280; loss: 0.76; acc: 0.73
Batch: 300; loss: 0.63; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.78
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.7; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.77
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00019244021677877754
0.0001827148225856945
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.6094034403372722; val_accuracy: 0.8532046178343949 

The current subspace-distance is: 0.0001827148225856945 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.9; acc: 0.73
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.94
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.86; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.88
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00019528080883901566
0.0001869757252279669
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.6074600513953312; val_accuracy: 0.8531050955414012 

The current subspace-distance is: 0.0001869757252279669 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.87; acc: 0.72
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.88; acc: 0.78
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.92
Batch: 460; loss: 0.81; acc: 0.75
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.88
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.76; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.84
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.71; acc: 0.77
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00019183078256901354
0.00018495249969419092
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.5992414512831694; val_accuracy: 0.8553941082802548 

The current subspace-distance is: 0.00018495249969419092 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.86; acc: 0.75
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.77
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.53; acc: 0.92
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.8; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.79; acc: 0.73
Batch: 760; loss: 0.84; acc: 0.75
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00019722027354873717
0.000186504126759246
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.6000901217673235; val_accuracy: 0.8543988853503185 

The current subspace-distance is: 0.000186504126759246 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.94; acc: 0.73
Batch: 320; loss: 0.47; acc: 0.94
Batch: 340; loss: 1.0; acc: 0.7
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.92
Batch: 600; loss: 0.74; acc: 0.78
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.67; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.88; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00019635568605735898
0.0001884225057438016
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.6013382969388537; val_accuracy: 0.8558917197452229 

The current subspace-distance is: 0.0001884225057438016 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.88; acc: 0.8
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.8
Batch: 240; loss: 0.76; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.78
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.49; acc: 0.94
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.96; acc: 0.69
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.77
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.0001968652504729107
0.00018880311108659953
Batch: 0; loss: 0.61; acc: 0.92
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.5965750483190937; val_accuracy: 0.8550955414012739 

The current subspace-distance is: 0.00018880311108659953 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.76; acc: 0.73
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.66; acc: 0.78
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.82; acc: 0.78
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.6; acc: 0.89
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.75
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00019728658662643284
0.00019169511506333947
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.5956904819816541; val_accuracy: 0.8539012738853503 

The current subspace-distance is: 0.00019169511506333947 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.78
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.72
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.83
Batch: 720; loss: 1.0; acc: 0.73
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00019840338791254908
0.00018750058370642364
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.5966638343729032; val_accuracy: 0.8525079617834395 

The current subspace-distance is: 0.00018750058370642364 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.71; acc: 0.78
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.94
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 0.69; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00019720257841981947
0.0001886331883724779
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.92
Val Epoch over. val_loss: 0.5880020198169028; val_accuracy: 0.8565883757961783 

The current subspace-distance is: 0.0001886331883724779 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.82; acc: 0.75
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.78
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.81; acc: 0.78
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.66; acc: 0.75
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00019817615975625813
0.00019100721692666411
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.5860553019365687; val_accuracy: 0.8565883757961783 

The current subspace-distance is: 0.00019100721692666411 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.91
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.75
Batch: 360; loss: 0.87; acc: 0.78
Batch: 380; loss: 0.76; acc: 0.73
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.85; acc: 0.81
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.54; acc: 0.92
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00019844851340167224
0.0001914391468744725
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.5906479337792487; val_accuracy: 0.8532046178343949 

The current subspace-distance is: 0.0001914391468744725 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.92
Batch: 560; loss: 0.72; acc: 0.75
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.71; acc: 0.78
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00020071763719897717
0.00019343881285749376
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.5921421935603877; val_accuracy: 0.8541003184713376 

The current subspace-distance is: 0.00019343881285749376 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.92
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.95
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00020112135098315775
0.00019268611504230648
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.5850813863383737; val_accuracy: 0.8577826433121019 

The current subspace-distance is: 0.00019268611504230648 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.72
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.73
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.88
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.81; acc: 0.77
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.77; acc: 0.72
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.74; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00019934293231926858
0.00019287201575934887
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.5910766212044248; val_accuracy: 0.8540007961783439 

The current subspace-distance is: 0.00019287201575934887 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.0473763698939185

The number of parameters is: 252462

The number of individual parameters is:

17
306
17
17
25
38250
25
25
50
112500
50
50
64
96000
64
64
4096
64
640
10
64
64

nonzero elements in E: 75738592
elements in E: 75738600
fraction nonzero: 0.9999998943735426
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.47; acc: 0.09
Batch: 20; loss: 2.12; acc: 0.14
Batch: 40; loss: 1.98; acc: 0.38
Batch: 60; loss: 1.86; acc: 0.44
Batch: 80; loss: 1.77; acc: 0.52
Batch: 100; loss: 1.79; acc: 0.47
Batch: 120; loss: 1.66; acc: 0.59
Batch: 140; loss: 1.52; acc: 0.72
Batch: 160; loss: 1.55; acc: 0.59
Batch: 180; loss: 1.58; acc: 0.66
Batch: 200; loss: 1.61; acc: 0.61
Batch: 220; loss: 1.47; acc: 0.69
Batch: 240; loss: 1.49; acc: 0.66
Batch: 260; loss: 1.43; acc: 0.73
Batch: 280; loss: 1.44; acc: 0.62
Batch: 300; loss: 1.3; acc: 0.73
Batch: 320; loss: 1.47; acc: 0.61
Batch: 340; loss: 1.36; acc: 0.72
Batch: 360; loss: 1.34; acc: 0.7
Batch: 380; loss: 1.46; acc: 0.64
Batch: 400; loss: 1.3; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.81
Batch: 440; loss: 1.37; acc: 0.72
Batch: 460; loss: 1.3; acc: 0.77
Batch: 480; loss: 1.42; acc: 0.64
Batch: 500; loss: 1.24; acc: 0.8
Batch: 520; loss: 1.41; acc: 0.64
Batch: 540; loss: 1.32; acc: 0.78
Batch: 560; loss: 1.37; acc: 0.69
Batch: 580; loss: 1.32; acc: 0.73
Batch: 600; loss: 1.22; acc: 0.8
Batch: 620; loss: 1.24; acc: 0.75
Batch: 640; loss: 1.42; acc: 0.67
Batch: 660; loss: 1.23; acc: 0.8
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.31; acc: 0.73
Batch: 720; loss: 1.19; acc: 0.83
Batch: 740; loss: 1.19; acc: 0.83
Batch: 760; loss: 1.23; acc: 0.78
Batch: 780; loss: 1.14; acc: 0.77
Train Epoch over. train_loss: 1.45; train_accuracy: 0.65 

5.666696961270645e-05
5.2113147830823436e-05
Batch: 0; loss: 1.19; acc: 0.81
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 0.91; acc: 0.88
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.06; acc: 0.83
Batch: 100; loss: 1.17; acc: 0.81
Batch: 120; loss: 1.35; acc: 0.69
Batch: 140; loss: 1.04; acc: 0.84
Val Epoch over. val_loss: 1.150457161626998; val_accuracy: 0.7952826433121019 

The current subspace-distance is: 5.2113147830823436e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.62
Batch: 20; loss: 0.97; acc: 0.89
Batch: 40; loss: 1.11; acc: 0.86
Batch: 60; loss: 1.15; acc: 0.8
Batch: 80; loss: 1.29; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.72
Batch: 140; loss: 1.01; acc: 0.83
Batch: 160; loss: 1.23; acc: 0.7
Batch: 180; loss: 1.05; acc: 0.81
Batch: 200; loss: 1.25; acc: 0.7
Batch: 220; loss: 1.07; acc: 0.81
Batch: 240; loss: 1.22; acc: 0.72
Batch: 260; loss: 1.16; acc: 0.81
Batch: 280; loss: 1.2; acc: 0.73
Batch: 300; loss: 1.12; acc: 0.81
Batch: 320; loss: 1.13; acc: 0.81
Batch: 340; loss: 1.01; acc: 0.83
Batch: 360; loss: 1.01; acc: 0.8
Batch: 380; loss: 1.17; acc: 0.81
Batch: 400; loss: 1.12; acc: 0.73
Batch: 420; loss: 1.11; acc: 0.78
Batch: 440; loss: 1.17; acc: 0.77
Batch: 460; loss: 1.14; acc: 0.72
Batch: 480; loss: 0.98; acc: 0.86
Batch: 500; loss: 1.01; acc: 0.81
Batch: 520; loss: 1.04; acc: 0.88
Batch: 540; loss: 1.02; acc: 0.84
Batch: 560; loss: 1.14; acc: 0.77
Batch: 580; loss: 1.16; acc: 0.7
Batch: 600; loss: 1.04; acc: 0.83
Batch: 620; loss: 1.09; acc: 0.77
Batch: 640; loss: 1.11; acc: 0.8
Batch: 660; loss: 0.97; acc: 0.83
Batch: 680; loss: 1.06; acc: 0.8
Batch: 700; loss: 0.97; acc: 0.88
Batch: 720; loss: 1.14; acc: 0.72
Batch: 740; loss: 1.07; acc: 0.78
Batch: 760; loss: 0.82; acc: 0.92
Batch: 780; loss: 0.95; acc: 0.86
Train Epoch over. train_loss: 1.09; train_accuracy: 0.79 

7.633768836967647e-05
7.184880814747885e-05
Batch: 0; loss: 0.95; acc: 0.86
Batch: 20; loss: 1.2; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.92
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.91
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 1.23; acc: 0.7
Batch: 140; loss: 0.8; acc: 0.94
Val Epoch over. val_loss: 0.9443429670516094; val_accuracy: 0.8363853503184714 

The current subspace-distance is: 7.184880814747885e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.86
Batch: 40; loss: 1.02; acc: 0.77
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 0.94; acc: 0.88
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.8
Batch: 140; loss: 0.93; acc: 0.84
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 0.89; acc: 0.88
Batch: 200; loss: 1.01; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.81
Batch: 240; loss: 1.01; acc: 0.8
Batch: 260; loss: 0.99; acc: 0.83
Batch: 280; loss: 0.91; acc: 0.84
Batch: 300; loss: 0.98; acc: 0.84
Batch: 320; loss: 0.96; acc: 0.8
Batch: 340; loss: 0.98; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.88
Batch: 380; loss: 1.04; acc: 0.77
Batch: 400; loss: 1.0; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.84
Batch: 440; loss: 1.0; acc: 0.78
Batch: 460; loss: 0.99; acc: 0.81
Batch: 480; loss: 0.93; acc: 0.86
Batch: 500; loss: 0.86; acc: 0.89
Batch: 520; loss: 0.84; acc: 0.91
Batch: 540; loss: 0.92; acc: 0.77
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.91; acc: 0.84
Batch: 600; loss: 0.91; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.91; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.88
Batch: 680; loss: 0.97; acc: 0.8
Batch: 700; loss: 0.94; acc: 0.83
Batch: 720; loss: 0.83; acc: 0.83
Batch: 740; loss: 0.88; acc: 0.84
Batch: 760; loss: 0.91; acc: 0.81
Batch: 780; loss: 0.88; acc: 0.86
Train Epoch over. train_loss: 0.93; train_accuracy: 0.83 

9.427848999621347e-05
8.941827400121838e-05
Batch: 0; loss: 0.84; acc: 0.86
Batch: 20; loss: 1.01; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.89; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.94
Batch: 100; loss: 0.81; acc: 0.86
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 0.62; acc: 0.94
Val Epoch over. val_loss: 0.8080216885372332; val_accuracy: 0.8589769108280255 

The current subspace-distance is: 8.941827400121838e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.81
Batch: 40; loss: 1.03; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.89
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.91
Batch: 140; loss: 0.85; acc: 0.83
Batch: 160; loss: 0.94; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.91
Batch: 220; loss: 0.77; acc: 0.88
Batch: 240; loss: 0.85; acc: 0.83
Batch: 260; loss: 0.83; acc: 0.84
Batch: 280; loss: 0.96; acc: 0.81
Batch: 300; loss: 0.78; acc: 0.89
Batch: 320; loss: 0.86; acc: 0.81
Batch: 340; loss: 0.9; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.83
Batch: 380; loss: 0.83; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.89
Batch: 420; loss: 0.9; acc: 0.83
Batch: 440; loss: 0.8; acc: 0.88
Batch: 460; loss: 0.87; acc: 0.81
Batch: 480; loss: 0.87; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.84
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.97; acc: 0.77
Batch: 560; loss: 0.88; acc: 0.84
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.84
Batch: 640; loss: 0.86; acc: 0.8
Batch: 660; loss: 0.73; acc: 0.92
Batch: 680; loss: 0.87; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.88
Batch: 740; loss: 0.82; acc: 0.84
Batch: 760; loss: 0.8; acc: 0.83
Batch: 780; loss: 0.74; acc: 0.86
Train Epoch over. train_loss: 0.83; train_accuracy: 0.84 

0.00010921846842393279
0.00010455617302795872
Batch: 0; loss: 0.76; acc: 0.92
Batch: 20; loss: 0.93; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.95
Batch: 100; loss: 0.74; acc: 0.88
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.7385102094738347; val_accuracy: 0.8694267515923567 

The current subspace-distance is: 0.00010455617302795872 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.91
Batch: 60; loss: 0.87; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.95; acc: 0.7
Batch: 120; loss: 0.81; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.91
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.89
Batch: 200; loss: 0.77; acc: 0.88
Batch: 220; loss: 0.73; acc: 0.86
Batch: 240; loss: 0.85; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.84
Batch: 280; loss: 0.89; acc: 0.78
Batch: 300; loss: 0.68; acc: 0.89
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.61; acc: 0.92
Batch: 360; loss: 0.73; acc: 0.89
Batch: 380; loss: 0.88; acc: 0.78
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 0.97; acc: 0.78
Batch: 440; loss: 0.64; acc: 0.91
Batch: 460; loss: 0.74; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.84
Batch: 500; loss: 0.83; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.78; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.91
Batch: 640; loss: 0.82; acc: 0.81
Batch: 660; loss: 0.69; acc: 0.92
Batch: 680; loss: 0.69; acc: 0.92
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.85 

0.00012453156523406506
0.00011928827007068321
Batch: 0; loss: 0.7; acc: 0.92
Batch: 20; loss: 0.89; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.97
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.48; acc: 0.95
Val Epoch over. val_loss: 0.6728450574313; val_accuracy: 0.8738057324840764 

The current subspace-distance is: 0.00011928827007068321 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.88
Batch: 140; loss: 0.75; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.83; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.67; acc: 0.89
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.89; acc: 0.8
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.92
Batch: 400; loss: 0.57; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.92
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.89
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.8; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.54; acc: 0.92
Train Epoch over. train_loss: 0.7; train_accuracy: 0.86 

0.00013716696412302554
0.00013082101941108704
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.95
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6172014535612361; val_accuracy: 0.8798765923566879 

The current subspace-distance is: 0.00013082101941108704 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.75; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.92
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.95
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.86
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.86 

0.00014854110486339778
0.00014257025031838566
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5713704292941245; val_accuracy: 0.8855493630573248 

The current subspace-distance is: 0.00014257025031838566 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.95
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.92
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.94
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.91
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.51; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.0001594315835973248
0.00015333210467360914
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.95
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.5473780136579162; val_accuracy: 0.8875398089171974 

The current subspace-distance is: 0.00015333210467360914 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.94
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.94
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.62; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.00017167192709166557
0.00016342161688953638
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.5133884864248288; val_accuracy: 0.8931130573248408 

The current subspace-distance is: 0.00016342161688953638 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.68; acc: 0.75
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0001813536073314026
0.00017337639292236418
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.48886373183529847; val_accuracy: 0.8986863057324841 

The current subspace-distance is: 0.00017337639292236418 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.95
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.82; acc: 0.73
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00018218277546111494
0.00017438312352169305
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.47561670403191997; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 0.00017438312352169305 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00018353876657783985
0.00017559691332280636
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.46652864479714895; val_accuracy: 0.9002786624203821 

The current subspace-distance is: 0.00017559691332280636 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.97
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.77; acc: 0.78
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00018656844622455537
0.00017903286789078265
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4707424084472049; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.00017903286789078265 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.47; acc: 0.94
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.95
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00018945448391605169
0.00018301985983271152
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4572396111336483; val_accuracy: 0.9025676751592356 

The current subspace-distance is: 0.00018301985983271152 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00019241779227741063
0.00018393404025118798
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.45297563939717167; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 0.00018393404025118798 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.78
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.64; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019391367095522583
0.00018615748558659106
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4439306755544274; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.00018615748558659106 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.43; acc: 0.97
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019616000645328313
0.00018807966262102127
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4402304895364555; val_accuracy: 0.9049562101910829 

The current subspace-distance is: 0.00018807966262102127 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.66; acc: 0.77
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.58; acc: 0.81
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00020219304133206606
0.00019140119547955692
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.43637265492776395; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 0.00019140119547955692 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.78
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00020222688908688724
0.00019440529285930097
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4320529413640879; val_accuracy: 0.9061504777070064 

The current subspace-distance is: 0.00019440529285930097 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00020521831174846739
0.000197574554476887
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4309673520979608; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.000197574554476887 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00020717427833005786
0.0001989890297409147
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.423743591566754; val_accuracy: 0.9070461783439491 

The current subspace-distance is: 0.0001989890297409147 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.57; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00020394778402987868
0.00019829584925901145
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.42629878365310137; val_accuracy: 0.9059514331210191 

The current subspace-distance is: 0.00019829584925901145 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00020765428780578077
0.0001975132036022842
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.42182341540694995; val_accuracy: 0.9069466560509554 

The current subspace-distance is: 0.0001975132036022842 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00020490172028075904
0.00019785226322710514
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.41966996697863196; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 0.00019785226322710514 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.51; acc: 0.78
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00020548320026136935
0.0001981224922928959
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.41420193120932125; val_accuracy: 0.908937101910828 

The current subspace-distance is: 0.0001981224922928959 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.33; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.97
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.000206085984245874
0.00019743091252166778
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.42234061620417673; val_accuracy: 0.9058519108280255 

The current subspace-distance is: 0.00019743091252166778 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.95
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.95
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00020775415759999305
0.0002004623820539564
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4151640700496686; val_accuracy: 0.9075437898089171 

The current subspace-distance is: 0.0002004623820539564 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.98
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002115073148161173
0.00020208947535138577
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4136703655028799; val_accuracy: 0.9074442675159236 

The current subspace-distance is: 0.00020208947535138577 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.31; acc: 0.97
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.0002080857229884714
0.0001986180286621675
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4075164790176282; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 0.0001986180286621675 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.78
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021154904970899224
0.00020345201482996345
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4131734732799469; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 0.00020345201482996345 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.0473763698939185

The number of parameters is: 252462

The number of individual parameters is:

17
306
17
17
25
38250
25
25
50
112500
50
50
64
96000
64
64
4096
64
640
10
64
64

nonzero elements in E: 100984789
elements in E: 100984800
fraction nonzero: 0.9999998910727159
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.06
Batch: 20; loss: 2.08; acc: 0.31
Batch: 40; loss: 2.0; acc: 0.34
Batch: 60; loss: 1.85; acc: 0.47
Batch: 80; loss: 1.71; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.75
Batch: 140; loss: 1.55; acc: 0.67
Batch: 160; loss: 1.57; acc: 0.61
Batch: 180; loss: 1.35; acc: 0.78
Batch: 200; loss: 1.43; acc: 0.7
Batch: 220; loss: 1.53; acc: 0.66
Batch: 240; loss: 1.48; acc: 0.66
Batch: 260; loss: 1.36; acc: 0.7
Batch: 280; loss: 1.29; acc: 0.8
Batch: 300; loss: 1.32; acc: 0.73
Batch: 320; loss: 1.33; acc: 0.77
Batch: 340; loss: 1.32; acc: 0.73
Batch: 360; loss: 1.23; acc: 0.75
Batch: 380; loss: 1.18; acc: 0.83
Batch: 400; loss: 1.28; acc: 0.8
Batch: 420; loss: 1.12; acc: 0.83
Batch: 440; loss: 1.25; acc: 0.78
Batch: 460; loss: 1.19; acc: 0.75
Batch: 480; loss: 1.3; acc: 0.7
Batch: 500; loss: 1.02; acc: 0.86
Batch: 520; loss: 1.01; acc: 0.88
Batch: 540; loss: 0.96; acc: 0.89
Batch: 560; loss: 1.22; acc: 0.73
Batch: 580; loss: 1.06; acc: 0.86
Batch: 600; loss: 1.21; acc: 0.78
Batch: 620; loss: 1.07; acc: 0.78
Batch: 640; loss: 1.1; acc: 0.8
Batch: 660; loss: 1.02; acc: 0.8
Batch: 680; loss: 1.02; acc: 0.81
Batch: 700; loss: 1.02; acc: 0.78
Batch: 720; loss: 0.97; acc: 0.88
Batch: 740; loss: 1.05; acc: 0.81
Batch: 760; loss: 1.12; acc: 0.77
Batch: 780; loss: 0.96; acc: 0.81
Train Epoch over. train_loss: 1.32; train_accuracy: 0.73 

2.4892136934795417e-05
9.033104106492829e-06
Batch: 0; loss: 1.09; acc: 0.81
Batch: 20; loss: 1.06; acc: 0.81
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 0.95; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.88
Batch: 100; loss: 0.97; acc: 0.88
Batch: 120; loss: 1.1; acc: 0.77
Batch: 140; loss: 0.76; acc: 0.91
Val Epoch over. val_loss: 0.9545252604089725; val_accuracy: 0.8461385350318471 

The current subspace-distance is: 9.033104106492829e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.91
Batch: 20; loss: 1.11; acc: 0.77
Batch: 40; loss: 1.15; acc: 0.7
Batch: 60; loss: 0.93; acc: 0.86
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.94; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.84
Batch: 140; loss: 1.02; acc: 0.73
Batch: 160; loss: 0.97; acc: 0.78
Batch: 180; loss: 0.98; acc: 0.84
Batch: 200; loss: 1.06; acc: 0.8
Batch: 220; loss: 1.08; acc: 0.75
Batch: 240; loss: 0.94; acc: 0.81
Batch: 260; loss: 1.07; acc: 0.73
Batch: 280; loss: 0.91; acc: 0.86
Batch: 300; loss: 0.92; acc: 0.81
Batch: 320; loss: 0.9; acc: 0.83
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 1.01; acc: 0.75
Batch: 380; loss: 0.87; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 0.99; acc: 0.78
Batch: 440; loss: 0.86; acc: 0.86
Batch: 460; loss: 0.86; acc: 0.86
Batch: 480; loss: 0.86; acc: 0.84
Batch: 500; loss: 0.83; acc: 0.86
Batch: 520; loss: 0.86; acc: 0.84
Batch: 540; loss: 0.9; acc: 0.8
Batch: 560; loss: 0.86; acc: 0.8
Batch: 580; loss: 0.79; acc: 0.86
Batch: 600; loss: 0.85; acc: 0.88
Batch: 620; loss: 0.85; acc: 0.88
Batch: 640; loss: 0.86; acc: 0.89
Batch: 660; loss: 0.81; acc: 0.84
Batch: 680; loss: 0.91; acc: 0.83
Batch: 700; loss: 1.01; acc: 0.8
Batch: 720; loss: 0.78; acc: 0.91
Batch: 740; loss: 0.77; acc: 0.86
Batch: 760; loss: 0.84; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.84
Train Epoch over. train_loss: 0.91; train_accuracy: 0.84 

3.0166509532136843e-05
1.2017868357361294e-05
Batch: 0; loss: 0.86; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7559867406346995; val_accuracy: 0.8751990445859873 

The current subspace-distance is: 1.2017868357361294e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.89
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.88; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.84
Batch: 180; loss: 0.89; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.78; acc: 0.86
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.83; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.92
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.92
Batch: 380; loss: 0.69; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.91
Batch: 420; loss: 0.79; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.97
Batch: 480; loss: 0.73; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.95
Batch: 520; loss: 0.85; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.91
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.64; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.73; acc: 0.89
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.84
Train Epoch over. train_loss: 0.75; train_accuracy: 0.86 

3.5013410524697974e-05
1.4840556104900315e-05
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6258325848230131; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 1.4840556104900315e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.91
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.88; acc: 0.73
Batch: 280; loss: 0.69; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.94
Batch: 340; loss: 0.64; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.94
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.92
Batch: 640; loss: 0.57; acc: 0.92
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.94
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

3.98159863834735e-05
1.8092488971888088e-05
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.5370277244194298; val_accuracy: 0.8985867834394905 

The current subspace-distance is: 1.8092488971888088e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.84; acc: 0.7
Batch: 200; loss: 0.54; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.91
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.92
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.92
Batch: 420; loss: 0.68; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.92
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.95
Batch: 740; loss: 0.65; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.88 

4.3157535401405767e-05
1.9242621419834904e-05
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.48292197524362307; val_accuracy: 0.9076433121019108 

The current subspace-distance is: 1.9242621419834904e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.57; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.92
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.95
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.607258233590983e-05
1.996465834963601e-05
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.45615757489280334; val_accuracy: 0.9103304140127388 

The current subspace-distance is: 1.996465834963601e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.95
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.95
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.8788180720293894e-05
2.1824082068633288e-05
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.41651195895140336; val_accuracy: 0.915406050955414 

The current subspace-distance is: 2.1824082068633288e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.181968299439177e-05
2.520807356631849e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3994160097116118; val_accuracy: 0.9202826433121019 

The current subspace-distance is: 2.520807356631849e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.375727414502762e-05
2.5195327907567844e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3764223761049805; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 2.5195327907567844e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.97
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.5505130148958415e-05
2.6009714929386973e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3627654387123266; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.6009714929386973e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.97
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.5650736612733454e-05
2.493926695024129e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3578905081673033; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.493926695024129e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.723584399675019e-05
2.6476489438209683e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.36082874893382855; val_accuracy: 0.924562101910828 

The current subspace-distance is: 2.6476489438209683e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.758177212555893e-05
2.6622436053003184e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3490286745653031; val_accuracy: 0.926453025477707 

The current subspace-distance is: 2.6622436053003184e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.75988415221218e-05
2.51529327215394e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.34534599845576436; val_accuracy: 0.9261544585987261 

The current subspace-distance is: 2.51529327215394e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.98
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.98
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.8623536460800096e-05
2.855435013771057e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3422562530750682; val_accuracy: 0.9254578025477707 

The current subspace-distance is: 2.855435013771057e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.98
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.937049718340859e-05
2.7510703148436733e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.33756238867522803; val_accuracy: 0.926453025477707 

The current subspace-distance is: 2.7510703148436733e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.24; acc: 1.0
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9990008594468236e-05
2.8489104806794785e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.33481966822769993; val_accuracy: 0.9251592356687898 

The current subspace-distance is: 2.8489104806794785e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9863894421141595e-05
2.791838051052764e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.334236418935144; val_accuracy: 0.9259554140127388 

The current subspace-distance is: 2.791838051052764e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.98
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.97
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.066766582080163e-05
2.8622829631785862e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3288519937711157; val_accuracy: 0.9272492038216561 

The current subspace-distance is: 2.8622829631785862e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.081920219003223e-05
2.6507485017646104e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.32958402527365716; val_accuracy: 0.9248606687898089 

The current subspace-distance is: 2.6507485017646104e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.137045420473441e-05
2.910361399699468e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3272050341508191; val_accuracy: 0.927547770700637 

The current subspace-distance is: 2.910361399699468e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.98
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.102292536525056e-05
2.903499625972472e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.32009102816984153; val_accuracy: 0.927547770700637 

The current subspace-distance is: 2.903499625972472e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.100236714701168e-05
2.7752583264373243e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3199389906256062; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.7752583264373243e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.089980888646096e-05
2.8149048375780694e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.31792438651915567; val_accuracy: 0.927547770700637 

The current subspace-distance is: 2.8149048375780694e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.233549356693402e-05
2.9340289984247647e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.32364499094379934; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.9340289984247647e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.97
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.207535625435412e-05
2.965009116451256e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.32156117469262163; val_accuracy: 0.9266520700636943 

The current subspace-distance is: 2.965009116451256e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.291038152994588e-05
3.04683817375917e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.31672565624782234; val_accuracy: 0.929140127388535 

The current subspace-distance is: 3.04683817375917e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.24657332082279e-05
2.8788346753572114e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3173453840100841; val_accuracy: 0.9279458598726115 

The current subspace-distance is: 2.8788346753572114e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.98
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.276396743487567e-05
2.8428245059330948e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.31759531705812283; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 2.8428245059330948e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.180723721627146e-05
2.8870512323919684e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.31605497629019863; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.8870512323919684e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.0473763698939185

The number of parameters is: 252462

The number of individual parameters is:

17
306
17
17
25
38250
25
25
50
112500
50
50
64
96000
64
64
4096
64
640
10
64
64

nonzero elements in E: 126230989
elements in E: 126231000
fraction nonzero: 0.9999999128581727
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.06
Batch: 20; loss: 2.09; acc: 0.27
Batch: 40; loss: 1.8; acc: 0.47
Batch: 60; loss: 1.6; acc: 0.62
Batch: 80; loss: 1.53; acc: 0.72
Batch: 100; loss: 1.41; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.77
Batch: 140; loss: 1.27; acc: 0.86
Batch: 160; loss: 1.39; acc: 0.77
Batch: 180; loss: 1.28; acc: 0.75
Batch: 200; loss: 1.29; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.81
Batch: 240; loss: 1.19; acc: 0.77
Batch: 260; loss: 1.22; acc: 0.78
Batch: 280; loss: 1.1; acc: 0.88
Batch: 300; loss: 1.14; acc: 0.81
Batch: 320; loss: 1.1; acc: 0.84
Batch: 340; loss: 1.04; acc: 0.88
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.13; acc: 0.8
Batch: 400; loss: 1.11; acc: 0.8
Batch: 420; loss: 0.96; acc: 0.84
Batch: 440; loss: 0.99; acc: 0.83
Batch: 460; loss: 0.91; acc: 0.89
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 0.89; acc: 0.88
Batch: 520; loss: 0.94; acc: 0.83
Batch: 540; loss: 0.84; acc: 0.91
Batch: 560; loss: 0.95; acc: 0.88
Batch: 580; loss: 0.9; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.94
Batch: 620; loss: 0.83; acc: 0.84
Batch: 640; loss: 0.95; acc: 0.78
Batch: 660; loss: 0.87; acc: 0.88
Batch: 680; loss: 0.74; acc: 0.88
Batch: 700; loss: 0.94; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.91
Batch: 740; loss: 0.8; acc: 0.94
Batch: 760; loss: 0.78; acc: 0.91
Batch: 780; loss: 0.83; acc: 0.84
Train Epoch over. train_loss: 1.15; train_accuracy: 0.78 

2.5405121050425805e-05
8.849979167280253e-06
Batch: 0; loss: 0.84; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.84; acc: 0.88
Batch: 120; loss: 0.97; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.92
Val Epoch over. val_loss: 0.7679216878808988; val_accuracy: 0.8794785031847133 

The current subspace-distance is: 8.849979167280253e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.8
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.92
Batch: 160; loss: 0.91; acc: 0.8
Batch: 180; loss: 0.79; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.92
Batch: 240; loss: 0.87; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.88
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.86
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.92
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.81
Batch: 420; loss: 0.74; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.61; acc: 0.92
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.59; acc: 0.91
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.69; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.91
Batch: 660; loss: 0.63; acc: 0.92
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.89
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.87 

3.134668804705143e-05
1.1895695024577435e-05
Batch: 0; loss: 0.67; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.98
Val Epoch over. val_loss: 0.5881776933077794; val_accuracy: 0.9036624203821656 

The current subspace-distance is: 1.1895695024577435e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.6; acc: 0.92
Batch: 180; loss: 0.66; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.92
Batch: 220; loss: 0.72; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.95
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.95
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.92
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.98
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.91
Batch: 600; loss: 0.57; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.92
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.97
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.89 

3.566248415154405e-05
1.4510804248857312e-05
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.98
Val Epoch over. val_loss: 0.5000092344489068; val_accuracy: 0.9139132165605095 

The current subspace-distance is: 1.4510804248857312e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.94
Batch: 140; loss: 0.6; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.97
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.62; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.95
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.92
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.9 

3.8950307498453185e-05
1.7036240024026483e-05
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.44480591879528797; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 1.7036240024026483e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.48; acc: 0.97
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.95
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

4.2220362956868485e-05
1.806590262276586e-05
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4076052128699175; val_accuracy: 0.921875 

The current subspace-distance is: 1.806590262276586e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.26; acc: 1.0
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.517946217674762e-05
1.9487722966005094e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3833627306921467; val_accuracy: 0.9253582802547771 

The current subspace-distance is: 1.9487722966005094e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.796664870809764e-05
2.1326239220798016e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3581851981817537; val_accuracy: 0.9296377388535032 

The current subspace-distance is: 2.1326239220798016e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.29; acc: 1.0
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

4.968177745467983e-05
2.2090584025136195e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.32825532949464337; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.2090584025136195e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.2464551117736846e-05
2.4222897991421632e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3163533081674272; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.4222897991421632e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.98
Batch: 700; loss: 0.37; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.457459337776527e-05
2.492728890501894e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2992831268792699; val_accuracy: 0.9367038216560509 

The current subspace-distance is: 2.492728890501894e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.25; acc: 1.0
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.98
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.551137655857019e-05
2.483689968357794e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.29575491658631403; val_accuracy: 0.9377985668789809 

The current subspace-distance is: 2.483689968357794e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.98
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.552982838707976e-05
2.410392335150391e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.29311636783135164; val_accuracy: 0.9380971337579618 

The current subspace-distance is: 2.410392335150391e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.21; acc: 1.0
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.662558032781817e-05
2.4334496629307978e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.28799835312518346; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.4334496629307978e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.6691707868594676e-05
2.5176497729262337e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2848899164681981; val_accuracy: 0.9384952229299363 

The current subspace-distance is: 2.5176497729262337e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.733086436521262e-05
2.555312130425591e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2827065547180783; val_accuracy: 0.9394904458598726 

The current subspace-distance is: 2.555312130425591e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.14; acc: 1.0
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.98
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.25; acc: 1.0
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.7842815294861794e-05
2.5450370230828412e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2770579167327304; val_accuracy: 0.9399880573248408 

The current subspace-distance is: 2.5450370230828412e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.98
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.863963087904267e-05
2.502285315131303e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2744196327818427; val_accuracy: 0.9411823248407644 

The current subspace-distance is: 2.502285315131303e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.98
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.9646175941452384e-05
2.7635904189082794e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27149061996276214; val_accuracy: 0.9405851910828026 

The current subspace-distance is: 2.7635904189082794e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.21; acc: 1.0
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.9916957980021834e-05
2.693335773074068e-05
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27182003499785806; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.693335773074068e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.047999340808019e-05
2.7376976504456252e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.26012809522402514; val_accuracy: 0.9410828025477707 

The current subspace-distance is: 2.7376976504456252e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.0545207816176116e-05
2.7856161977979355e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2641826177098949; val_accuracy: 0.9407842356687898 

The current subspace-distance is: 2.7856161977979355e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.48; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.02481268288102e-05
2.60424058069475e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.26394256503339025; val_accuracy: 0.9418789808917197 

The current subspace-distance is: 2.60424058069475e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.043747998774052e-05
2.8203934562043287e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.25821127078145933; val_accuracy: 0.9425756369426752 

The current subspace-distance is: 2.8203934562043287e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.14225136814639e-05
2.8192123863846064e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2546398926312756; val_accuracy: 0.9426751592356688 

The current subspace-distance is: 2.8192123863846064e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.98
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.16; acc: 1.0
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.0937116359127685e-05
2.7562376999412663e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2552521234959554; val_accuracy: 0.9426751592356688 

The current subspace-distance is: 2.7562376999412663e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.145708175608888e-05
2.9323609851417132e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.25869390743363435; val_accuracy: 0.9427746815286624 

The current subspace-distance is: 2.9323609851417132e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.98
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.205860699992627e-05
2.8276410375838168e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2590304447966776; val_accuracy: 0.9415804140127388 

The current subspace-distance is: 2.8276410375838168e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.98
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.98
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.0866990679642186e-05
2.748502629401628e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.25404106612038463; val_accuracy: 0.9437699044585988 

The current subspace-distance is: 2.748502629401628e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.0890710301464424e-05
2.7637315724859945e-05
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2552253563123144; val_accuracy: 0.9422770700636943 

The current subspace-distance is: 2.7637315724859945e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.137302261777222e-05
2.7711104849004187e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2575285225442261; val_accuracy: 0.942078025477707 

The current subspace-distance is: 2.7711104849004187e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_10_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
