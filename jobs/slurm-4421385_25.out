model : table13slim
N : 9
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:57

Channel scaling factor: 1.45

The number of parameters is: 273705

The number of individual parameters is:

12
216
12
12
18
37584
18
18
35
109620
35
35
64
120960
64
64
4096
64
640
10
64
64

nonzero elements in E: 13685249
elements in E: 13685250
fraction nonzero: 0.9999999269286275
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.39; acc: 0.11
Batch: 40; loss: 2.25; acc: 0.19
Batch: 60; loss: 2.26; acc: 0.17
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.19; acc: 0.25
Batch: 120; loss: 2.27; acc: 0.22
Batch: 140; loss: 2.12; acc: 0.27
Batch: 160; loss: 2.21; acc: 0.27
Batch: 180; loss: 2.09; acc: 0.3
Batch: 200; loss: 2.07; acc: 0.27
Batch: 220; loss: 2.04; acc: 0.33
Batch: 240; loss: 2.16; acc: 0.22
Batch: 260; loss: 2.03; acc: 0.3
Batch: 280; loss: 2.09; acc: 0.28
Batch: 300; loss: 2.05; acc: 0.28
Batch: 320; loss: 2.1; acc: 0.23
Batch: 340; loss: 2.06; acc: 0.28
Batch: 360; loss: 2.03; acc: 0.33
Batch: 380; loss: 1.92; acc: 0.39
Batch: 400; loss: 2.05; acc: 0.27
Batch: 420; loss: 2.0; acc: 0.39
Batch: 440; loss: 2.06; acc: 0.3
Batch: 460; loss: 2.02; acc: 0.31
Batch: 480; loss: 1.99; acc: 0.38
Batch: 500; loss: 2.03; acc: 0.3
Batch: 520; loss: 1.97; acc: 0.3
Batch: 540; loss: 2.0; acc: 0.27
Batch: 560; loss: 2.06; acc: 0.39
Batch: 580; loss: 1.94; acc: 0.36
Batch: 600; loss: 1.97; acc: 0.34
Batch: 620; loss: 1.97; acc: 0.33
Batch: 640; loss: 1.92; acc: 0.45
Batch: 660; loss: 2.01; acc: 0.34
Batch: 680; loss: 1.87; acc: 0.41
Batch: 700; loss: 1.98; acc: 0.36
Batch: 720; loss: 1.82; acc: 0.39
Batch: 740; loss: 1.92; acc: 0.33
Batch: 760; loss: 1.98; acc: 0.34
Batch: 780; loss: 1.93; acc: 0.33
Train Epoch over. train_loss: 2.05; train_accuracy: 0.3 

2.3964541469467804e-05
3.466308726274292e-06
Batch: 0; loss: 1.92; acc: 0.44
Batch: 20; loss: 2.0; acc: 0.39
Batch: 40; loss: 1.72; acc: 0.55
Batch: 60; loss: 1.88; acc: 0.41
Batch: 80; loss: 1.74; acc: 0.58
Batch: 100; loss: 1.93; acc: 0.38
Batch: 120; loss: 2.0; acc: 0.41
Batch: 140; loss: 2.01; acc: 0.3
Val Epoch over. val_loss: 1.9162785141331375; val_accuracy: 0.3909235668789809 

The current subspace-distance is: 3.466308726274292e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.0; acc: 0.33
Batch: 20; loss: 1.98; acc: 0.33
Batch: 40; loss: 1.95; acc: 0.31
Batch: 60; loss: 1.87; acc: 0.44
Batch: 80; loss: 1.98; acc: 0.33
Batch: 100; loss: 1.95; acc: 0.38
Batch: 120; loss: 1.97; acc: 0.42
Batch: 140; loss: 1.92; acc: 0.41
Batch: 160; loss: 1.9; acc: 0.38
Batch: 180; loss: 1.87; acc: 0.45
Batch: 200; loss: 1.92; acc: 0.41
Batch: 220; loss: 1.96; acc: 0.34
Batch: 240; loss: 1.94; acc: 0.44
Batch: 260; loss: 1.89; acc: 0.44
Batch: 280; loss: 1.83; acc: 0.44
Batch: 300; loss: 1.92; acc: 0.33
Batch: 320; loss: 1.88; acc: 0.44
Batch: 340; loss: 1.83; acc: 0.48
Batch: 360; loss: 1.87; acc: 0.34
Batch: 380; loss: 1.98; acc: 0.31
Batch: 400; loss: 1.97; acc: 0.34
Batch: 420; loss: 1.93; acc: 0.44
Batch: 440; loss: 1.84; acc: 0.39
Batch: 460; loss: 1.93; acc: 0.34
Batch: 480; loss: 1.85; acc: 0.42
Batch: 500; loss: 1.91; acc: 0.42
Batch: 520; loss: 1.78; acc: 0.5
Batch: 540; loss: 1.86; acc: 0.45
Batch: 560; loss: 1.87; acc: 0.47
Batch: 580; loss: 1.84; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.52
Batch: 620; loss: 1.9; acc: 0.38
Batch: 640; loss: 1.81; acc: 0.47
Batch: 660; loss: 1.75; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.31
Batch: 700; loss: 1.84; acc: 0.47
Batch: 720; loss: 1.83; acc: 0.5
Batch: 740; loss: 1.92; acc: 0.39
Batch: 760; loss: 1.79; acc: 0.45
Batch: 780; loss: 1.88; acc: 0.36
Train Epoch over. train_loss: 1.9; train_accuracy: 0.4 

2.5619900043238886e-05
5.467673418024788e-06
Batch: 0; loss: 1.79; acc: 0.44
Batch: 20; loss: 1.91; acc: 0.41
Batch: 40; loss: 1.63; acc: 0.52
Batch: 60; loss: 1.8; acc: 0.48
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.84; acc: 0.38
Batch: 120; loss: 1.94; acc: 0.39
Batch: 140; loss: 1.97; acc: 0.39
Val Epoch over. val_loss: 1.8407788952444768; val_accuracy: 0.4325238853503185 

The current subspace-distance is: 5.467673418024788e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.86; acc: 0.36
Batch: 20; loss: 1.87; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.38
Batch: 60; loss: 1.98; acc: 0.34
Batch: 80; loss: 1.85; acc: 0.39
Batch: 100; loss: 1.88; acc: 0.38
Batch: 120; loss: 1.84; acc: 0.41
Batch: 140; loss: 1.85; acc: 0.41
Batch: 160; loss: 1.77; acc: 0.44
Batch: 180; loss: 1.79; acc: 0.44
Batch: 200; loss: 1.82; acc: 0.44
Batch: 220; loss: 1.79; acc: 0.45
Batch: 240; loss: 1.82; acc: 0.39
Batch: 260; loss: 1.78; acc: 0.47
Batch: 280; loss: 1.9; acc: 0.38
Batch: 300; loss: 1.73; acc: 0.52
Batch: 320; loss: 1.76; acc: 0.48
Batch: 340; loss: 1.76; acc: 0.47
Batch: 360; loss: 1.72; acc: 0.53
Batch: 380; loss: 1.89; acc: 0.42
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.79; acc: 0.53
Batch: 440; loss: 1.89; acc: 0.42
Batch: 460; loss: 1.72; acc: 0.5
Batch: 480; loss: 1.78; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.44
Batch: 520; loss: 1.8; acc: 0.42
Batch: 540; loss: 1.79; acc: 0.47
Batch: 560; loss: 1.8; acc: 0.38
Batch: 580; loss: 1.8; acc: 0.41
Batch: 600; loss: 1.79; acc: 0.48
Batch: 620; loss: 1.79; acc: 0.45
Batch: 640; loss: 1.77; acc: 0.5
Batch: 660; loss: 1.72; acc: 0.58
Batch: 680; loss: 1.84; acc: 0.39
Batch: 700; loss: 1.81; acc: 0.38
Batch: 720; loss: 1.81; acc: 0.45
Batch: 740; loss: 1.79; acc: 0.42
Batch: 760; loss: 1.69; acc: 0.47
Batch: 780; loss: 1.77; acc: 0.48
Train Epoch over. train_loss: 1.81; train_accuracy: 0.44 

2.817821405187715e-05
9.151965969067533e-06
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.81; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.64
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.64
Batch: 100; loss: 1.67; acc: 0.52
Batch: 120; loss: 1.85; acc: 0.42
Batch: 140; loss: 1.82; acc: 0.56
Val Epoch over. val_loss: 1.7273287006244538; val_accuracy: 0.48736066878980894 

The current subspace-distance is: 9.151965969067533e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.59
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.76; acc: 0.45
Batch: 60; loss: 1.77; acc: 0.44
Batch: 80; loss: 1.8; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.36
Batch: 120; loss: 1.69; acc: 0.55
Batch: 140; loss: 1.73; acc: 0.48
Batch: 160; loss: 1.83; acc: 0.45
Batch: 180; loss: 1.67; acc: 0.59
Batch: 200; loss: 1.74; acc: 0.47
Batch: 220; loss: 1.63; acc: 0.55
Batch: 240; loss: 1.74; acc: 0.45
Batch: 260; loss: 1.67; acc: 0.42
Batch: 280; loss: 1.77; acc: 0.39
Batch: 300; loss: 1.67; acc: 0.58
Batch: 320; loss: 1.72; acc: 0.44
Batch: 340; loss: 1.75; acc: 0.47
Batch: 360; loss: 1.66; acc: 0.53
Batch: 380; loss: 1.71; acc: 0.45
Batch: 400; loss: 1.9; acc: 0.42
Batch: 420; loss: 1.61; acc: 0.53
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.69; acc: 0.44
Batch: 480; loss: 1.7; acc: 0.52
Batch: 500; loss: 1.75; acc: 0.5
Batch: 520; loss: 1.68; acc: 0.52
Batch: 540; loss: 1.65; acc: 0.5
Batch: 560; loss: 1.67; acc: 0.48
Batch: 580; loss: 1.73; acc: 0.47
Batch: 600; loss: 1.77; acc: 0.45
Batch: 620; loss: 1.82; acc: 0.34
Batch: 640; loss: 1.69; acc: 0.47
Batch: 660; loss: 1.65; acc: 0.52
Batch: 680; loss: 1.76; acc: 0.41
Batch: 700; loss: 1.63; acc: 0.47
Batch: 720; loss: 1.7; acc: 0.47
Batch: 740; loss: 1.76; acc: 0.34
Batch: 760; loss: 1.75; acc: 0.44
Batch: 780; loss: 1.7; acc: 0.53
Train Epoch over. train_loss: 1.72; train_accuracy: 0.47 

3.07522204820998e-05
9.680097718955949e-06
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.75; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.52
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.77; acc: 0.47
Batch: 140; loss: 1.73; acc: 0.52
Val Epoch over. val_loss: 1.6606897875002236; val_accuracy: 0.5024880573248408 

The current subspace-distance is: 9.680097718955949e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 1.73; acc: 0.48
Batch: 40; loss: 1.71; acc: 0.47
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.68; acc: 0.48
Batch: 100; loss: 1.69; acc: 0.47
Batch: 120; loss: 1.65; acc: 0.41
Batch: 140; loss: 1.75; acc: 0.41
Batch: 160; loss: 1.68; acc: 0.47
Batch: 180; loss: 1.69; acc: 0.44
Batch: 200; loss: 1.73; acc: 0.42
Batch: 220; loss: 1.75; acc: 0.39
Batch: 240; loss: 1.69; acc: 0.48
Batch: 260; loss: 1.55; acc: 0.58
Batch: 280; loss: 1.72; acc: 0.47
Batch: 300; loss: 1.76; acc: 0.45
Batch: 320; loss: 1.69; acc: 0.42
Batch: 340; loss: 1.63; acc: 0.45
Batch: 360; loss: 1.58; acc: 0.58
Batch: 380; loss: 1.7; acc: 0.52
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.65; acc: 0.53
Batch: 440; loss: 1.69; acc: 0.47
Batch: 460; loss: 1.78; acc: 0.38
Batch: 480; loss: 1.67; acc: 0.52
Batch: 500; loss: 1.71; acc: 0.5
Batch: 520; loss: 1.74; acc: 0.42
Batch: 540; loss: 1.44; acc: 0.69
Batch: 560; loss: 1.72; acc: 0.39
Batch: 580; loss: 1.72; acc: 0.42
Batch: 600; loss: 1.5; acc: 0.56
Batch: 620; loss: 1.74; acc: 0.45
Batch: 640; loss: 1.67; acc: 0.44
Batch: 660; loss: 1.67; acc: 0.5
Batch: 680; loss: 1.67; acc: 0.47
Batch: 700; loss: 1.58; acc: 0.52
Batch: 720; loss: 1.78; acc: 0.42
Batch: 740; loss: 1.62; acc: 0.55
Batch: 760; loss: 1.67; acc: 0.45
Batch: 780; loss: 1.53; acc: 0.45
Train Epoch over. train_loss: 1.66; train_accuracy: 0.49 

3.2636260584695265e-05
9.413377483724616e-06
Batch: 0; loss: 1.66; acc: 0.5
Batch: 20; loss: 1.67; acc: 0.55
Batch: 40; loss: 1.43; acc: 0.64
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.46; acc: 0.58
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.63; acc: 0.48
Val Epoch over. val_loss: 1.6030378751694017; val_accuracy: 0.5130374203821656 

The current subspace-distance is: 9.413377483724616e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.52
Batch: 40; loss: 1.59; acc: 0.5
Batch: 60; loss: 1.63; acc: 0.55
Batch: 80; loss: 1.74; acc: 0.41
Batch: 100; loss: 1.71; acc: 0.42
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.62; acc: 0.55
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.41; acc: 0.62
Batch: 200; loss: 1.56; acc: 0.52
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.59; acc: 0.47
Batch: 280; loss: 1.62; acc: 0.5
Batch: 300; loss: 1.62; acc: 0.44
Batch: 320; loss: 1.69; acc: 0.47
Batch: 340; loss: 1.56; acc: 0.48
Batch: 360; loss: 1.7; acc: 0.53
Batch: 380; loss: 1.57; acc: 0.52
Batch: 400; loss: 1.74; acc: 0.47
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.71; acc: 0.48
Batch: 480; loss: 1.52; acc: 0.55
Batch: 500; loss: 1.67; acc: 0.45
Batch: 520; loss: 1.61; acc: 0.52
Batch: 540; loss: 1.57; acc: 0.55
Batch: 560; loss: 1.53; acc: 0.52
Batch: 580; loss: 1.8; acc: 0.33
Batch: 600; loss: 1.53; acc: 0.5
Batch: 620; loss: 1.57; acc: 0.64
Batch: 640; loss: 1.61; acc: 0.44
Batch: 660; loss: 1.56; acc: 0.45
Batch: 680; loss: 1.52; acc: 0.52
Batch: 700; loss: 1.64; acc: 0.55
Batch: 720; loss: 1.6; acc: 0.55
Batch: 740; loss: 1.56; acc: 0.42
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.53
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.6243069189367816e-05
1.3142479474481661e-05
Batch: 0; loss: 1.67; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.61
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.54; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.44
Batch: 140; loss: 1.59; acc: 0.52
Val Epoch over. val_loss: 1.5693177888347845; val_accuracy: 0.5172173566878981 

The current subspace-distance is: 1.3142479474481661e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.68; acc: 0.41
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.65; acc: 0.52
Batch: 100; loss: 1.74; acc: 0.42
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.67; acc: 0.47
Batch: 160; loss: 1.72; acc: 0.48
Batch: 180; loss: 1.57; acc: 0.53
Batch: 200; loss: 1.62; acc: 0.45
Batch: 220; loss: 1.69; acc: 0.44
Batch: 240; loss: 1.67; acc: 0.48
Batch: 260; loss: 1.61; acc: 0.52
Batch: 280; loss: 1.56; acc: 0.58
Batch: 300; loss: 1.67; acc: 0.45
Batch: 320; loss: 1.68; acc: 0.55
Batch: 340; loss: 1.49; acc: 0.64
Batch: 360; loss: 1.47; acc: 0.56
Batch: 380; loss: 1.57; acc: 0.56
Batch: 400; loss: 1.76; acc: 0.42
Batch: 420; loss: 1.5; acc: 0.52
Batch: 440; loss: 1.47; acc: 0.56
Batch: 460; loss: 1.71; acc: 0.36
Batch: 480; loss: 1.47; acc: 0.5
Batch: 500; loss: 1.55; acc: 0.5
Batch: 520; loss: 1.58; acc: 0.52
Batch: 540; loss: 1.57; acc: 0.53
Batch: 560; loss: 1.58; acc: 0.53
Batch: 580; loss: 1.48; acc: 0.56
Batch: 600; loss: 1.57; acc: 0.47
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.51; acc: 0.58
Batch: 660; loss: 1.8; acc: 0.36
Batch: 680; loss: 1.54; acc: 0.52
Batch: 700; loss: 1.62; acc: 0.47
Batch: 720; loss: 1.54; acc: 0.53
Batch: 740; loss: 1.59; acc: 0.53
Batch: 760; loss: 1.74; acc: 0.41
Batch: 780; loss: 1.62; acc: 0.48
Train Epoch over. train_loss: 1.59; train_accuracy: 0.5 

3.90649183827918e-05
1.6459163816762157e-05
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.61; acc: 0.59
Batch: 40; loss: 1.3; acc: 0.66
Batch: 60; loss: 1.53; acc: 0.52
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.51; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.5
Val Epoch over. val_loss: 1.5429397365849489; val_accuracy: 0.5263734076433121 

The current subspace-distance is: 1.6459163816762157e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.48
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.44; acc: 0.59
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.57; acc: 0.53
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.41; acc: 0.67
Batch: 200; loss: 1.69; acc: 0.44
Batch: 220; loss: 1.65; acc: 0.42
Batch: 240; loss: 1.63; acc: 0.47
Batch: 260; loss: 1.55; acc: 0.42
Batch: 280; loss: 1.51; acc: 0.56
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.52; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.46; acc: 0.53
Batch: 380; loss: 1.56; acc: 0.5
Batch: 400; loss: 1.62; acc: 0.47
Batch: 420; loss: 1.63; acc: 0.5
Batch: 440; loss: 1.64; acc: 0.47
Batch: 460; loss: 1.64; acc: 0.52
Batch: 480; loss: 1.59; acc: 0.5
Batch: 500; loss: 1.71; acc: 0.42
Batch: 520; loss: 1.54; acc: 0.55
Batch: 540; loss: 1.53; acc: 0.44
Batch: 560; loss: 1.43; acc: 0.61
Batch: 580; loss: 1.5; acc: 0.55
Batch: 600; loss: 1.51; acc: 0.55
Batch: 620; loss: 1.69; acc: 0.45
Batch: 640; loss: 1.56; acc: 0.52
Batch: 660; loss: 1.52; acc: 0.52
Batch: 680; loss: 1.53; acc: 0.53
Batch: 700; loss: 1.6; acc: 0.47
Batch: 720; loss: 1.61; acc: 0.45
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.46; acc: 0.59
Batch: 780; loss: 1.7; acc: 0.5
Train Epoch over. train_loss: 1.57; train_accuracy: 0.51 

3.9471171476179734e-05
1.6126858099596575e-05
Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.58; acc: 0.58
Batch: 40; loss: 1.26; acc: 0.66
Batch: 60; loss: 1.52; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.48; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.48
Batch: 140; loss: 1.51; acc: 0.56
Val Epoch over. val_loss: 1.521316431889868; val_accuracy: 0.5298566878980892 

The current subspace-distance is: 1.6126858099596575e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.42; acc: 0.58
Batch: 40; loss: 1.68; acc: 0.42
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.43; acc: 0.67
Batch: 100; loss: 1.57; acc: 0.45
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 1.74; acc: 0.38
Batch: 160; loss: 1.44; acc: 0.56
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.64; acc: 0.44
Batch: 220; loss: 1.43; acc: 0.64
Batch: 240; loss: 1.44; acc: 0.61
Batch: 260; loss: 1.6; acc: 0.45
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.55
Batch: 320; loss: 1.49; acc: 0.61
Batch: 340; loss: 1.66; acc: 0.47
Batch: 360; loss: 1.52; acc: 0.56
Batch: 380; loss: 1.51; acc: 0.59
Batch: 400; loss: 1.67; acc: 0.44
Batch: 420; loss: 1.7; acc: 0.39
Batch: 440; loss: 1.45; acc: 0.64
Batch: 460; loss: 1.48; acc: 0.62
Batch: 480; loss: 1.52; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.65; acc: 0.47
Batch: 540; loss: 1.76; acc: 0.45
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.74; acc: 0.38
Batch: 600; loss: 1.4; acc: 0.58
Batch: 620; loss: 1.43; acc: 0.62
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.59; acc: 0.52
Batch: 680; loss: 1.54; acc: 0.56
Batch: 700; loss: 1.61; acc: 0.52
Batch: 720; loss: 1.38; acc: 0.62
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.67; acc: 0.41
Batch: 780; loss: 1.56; acc: 0.55
Train Epoch over. train_loss: 1.55; train_accuracy: 0.52 

4.050279676448554e-05
1.588061059010215e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.25; acc: 0.64
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.47; acc: 0.56
Batch: 120; loss: 1.59; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.58
Val Epoch over. val_loss: 1.509234877149011; val_accuracy: 0.533937101910828 

The current subspace-distance is: 1.588061059010215e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.48; acc: 0.59
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.37; acc: 0.64
Batch: 80; loss: 1.49; acc: 0.62
Batch: 100; loss: 1.38; acc: 0.59
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.39; acc: 0.55
Batch: 160; loss: 1.6; acc: 0.47
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.65; acc: 0.45
Batch: 220; loss: 1.51; acc: 0.56
Batch: 240; loss: 1.6; acc: 0.53
Batch: 260; loss: 1.47; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.58
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.53; acc: 0.59
Batch: 340; loss: 1.57; acc: 0.47
Batch: 360; loss: 1.47; acc: 0.58
Batch: 380; loss: 1.75; acc: 0.41
Batch: 400; loss: 1.53; acc: 0.53
Batch: 420; loss: 1.59; acc: 0.52
Batch: 440; loss: 1.39; acc: 0.67
Batch: 460; loss: 1.59; acc: 0.5
Batch: 480; loss: 1.57; acc: 0.47
Batch: 500; loss: 1.45; acc: 0.56
Batch: 520; loss: 1.45; acc: 0.53
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.54; acc: 0.55
Batch: 580; loss: 1.39; acc: 0.62
Batch: 600; loss: 1.45; acc: 0.55
Batch: 620; loss: 1.49; acc: 0.56
Batch: 640; loss: 1.47; acc: 0.62
Batch: 660; loss: 1.39; acc: 0.64
Batch: 680; loss: 1.67; acc: 0.42
Batch: 700; loss: 1.53; acc: 0.47
Batch: 720; loss: 1.52; acc: 0.52
Batch: 740; loss: 1.46; acc: 0.58
Batch: 760; loss: 1.38; acc: 0.69
Batch: 780; loss: 1.52; acc: 0.55
Train Epoch over. train_loss: 1.53; train_accuracy: 0.52 

4.205367440590635e-05
1.3467298231262248e-05
Batch: 0; loss: 1.66; acc: 0.42
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 1.48; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.62
Batch: 100; loss: 1.44; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.43; acc: 0.58
Val Epoch over. val_loss: 1.481166471341613; val_accuracy: 0.543093152866242 

The current subspace-distance is: 1.3467298231262248e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.55; acc: 0.53
Batch: 60; loss: 1.5; acc: 0.58
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.43; acc: 0.56
Batch: 160; loss: 1.54; acc: 0.53
Batch: 180; loss: 1.52; acc: 0.45
Batch: 200; loss: 1.32; acc: 0.7
Batch: 220; loss: 1.41; acc: 0.62
Batch: 240; loss: 1.46; acc: 0.56
Batch: 260; loss: 1.51; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.44; acc: 0.53
Batch: 320; loss: 1.46; acc: 0.58
Batch: 340; loss: 1.59; acc: 0.52
Batch: 360; loss: 1.43; acc: 0.59
Batch: 380; loss: 1.39; acc: 0.56
Batch: 400; loss: 1.53; acc: 0.55
Batch: 420; loss: 1.42; acc: 0.58
Batch: 440; loss: 1.41; acc: 0.58
Batch: 460; loss: 1.49; acc: 0.55
Batch: 480; loss: 1.62; acc: 0.47
Batch: 500; loss: 1.54; acc: 0.48
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.57; acc: 0.52
Batch: 560; loss: 1.41; acc: 0.69
Batch: 580; loss: 1.55; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.52
Batch: 620; loss: 1.54; acc: 0.53
Batch: 640; loss: 1.49; acc: 0.5
Batch: 660; loss: 1.58; acc: 0.47
Batch: 680; loss: 1.64; acc: 0.52
Batch: 700; loss: 1.69; acc: 0.47
Batch: 720; loss: 1.48; acc: 0.52
Batch: 740; loss: 1.45; acc: 0.52
Batch: 760; loss: 1.62; acc: 0.47
Batch: 780; loss: 1.56; acc: 0.52
Train Epoch over. train_loss: 1.51; train_accuracy: 0.53 

4.287819319870323e-05
1.395427079842193e-05
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 1.48; acc: 0.55
Batch: 80; loss: 1.35; acc: 0.62
Batch: 100; loss: 1.44; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.42; acc: 0.56
Val Epoch over. val_loss: 1.4711861283915817; val_accuracy: 0.5496616242038217 

The current subspace-distance is: 1.395427079842193e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.55
Batch: 20; loss: 1.53; acc: 0.45
Batch: 40; loss: 1.43; acc: 0.56
Batch: 60; loss: 1.5; acc: 0.47
Batch: 80; loss: 1.39; acc: 0.69
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.6; acc: 0.5
Batch: 140; loss: 1.38; acc: 0.58
Batch: 160; loss: 1.5; acc: 0.47
Batch: 180; loss: 1.45; acc: 0.53
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.5
Batch: 240; loss: 1.54; acc: 0.53
Batch: 260; loss: 1.6; acc: 0.55
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.43; acc: 0.56
Batch: 320; loss: 1.52; acc: 0.53
Batch: 340; loss: 1.62; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.53
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.43; acc: 0.61
Batch: 420; loss: 1.39; acc: 0.55
Batch: 440; loss: 1.59; acc: 0.48
Batch: 460; loss: 1.41; acc: 0.62
Batch: 480; loss: 1.46; acc: 0.59
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.56; acc: 0.53
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.42; acc: 0.58
Batch: 580; loss: 1.38; acc: 0.66
Batch: 600; loss: 1.53; acc: 0.53
Batch: 620; loss: 1.56; acc: 0.55
Batch: 640; loss: 1.39; acc: 0.64
Batch: 660; loss: 1.54; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.58; acc: 0.52
Batch: 720; loss: 1.56; acc: 0.55
Batch: 740; loss: 1.41; acc: 0.61
Batch: 760; loss: 1.44; acc: 0.66
Batch: 780; loss: 1.45; acc: 0.55
Train Epoch over. train_loss: 1.51; train_accuracy: 0.53 

4.4785661884816363e-05
1.7218120774487033e-05
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.57; acc: 0.47
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 1.48; acc: 0.56
Batch: 80; loss: 1.36; acc: 0.62
Batch: 100; loss: 1.44; acc: 0.58
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.41; acc: 0.55
Val Epoch over. val_loss: 1.4751712053444734; val_accuracy: 0.5466759554140127 

The current subspace-distance is: 1.7218120774487033e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.44; acc: 0.61
Batch: 20; loss: 1.44; acc: 0.55
Batch: 40; loss: 1.58; acc: 0.53
Batch: 60; loss: 1.48; acc: 0.55
Batch: 80; loss: 1.48; acc: 0.58
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.4; acc: 0.62
Batch: 140; loss: 1.51; acc: 0.64
Batch: 160; loss: 1.55; acc: 0.48
Batch: 180; loss: 1.53; acc: 0.53
Batch: 200; loss: 1.52; acc: 0.5
Batch: 220; loss: 1.47; acc: 0.5
Batch: 240; loss: 1.45; acc: 0.56
Batch: 260; loss: 1.41; acc: 0.55
Batch: 280; loss: 1.55; acc: 0.5
Batch: 300; loss: 1.52; acc: 0.56
Batch: 320; loss: 1.62; acc: 0.44
Batch: 340; loss: 1.42; acc: 0.61
Batch: 360; loss: 1.7; acc: 0.42
Batch: 380; loss: 1.43; acc: 0.56
Batch: 400; loss: 1.45; acc: 0.56
Batch: 420; loss: 1.63; acc: 0.45
Batch: 440; loss: 1.37; acc: 0.61
Batch: 460; loss: 1.41; acc: 0.55
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.53; acc: 0.58
Batch: 520; loss: 1.4; acc: 0.62
Batch: 540; loss: 1.54; acc: 0.48
Batch: 560; loss: 1.53; acc: 0.55
Batch: 580; loss: 1.54; acc: 0.52
Batch: 600; loss: 1.59; acc: 0.52
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.49; acc: 0.53
Batch: 660; loss: 1.47; acc: 0.55
Batch: 680; loss: 1.53; acc: 0.53
Batch: 700; loss: 1.49; acc: 0.48
Batch: 720; loss: 1.42; acc: 0.62
Batch: 740; loss: 1.39; acc: 0.58
Batch: 760; loss: 1.57; acc: 0.47
Batch: 780; loss: 1.54; acc: 0.47
Train Epoch over. train_loss: 1.5; train_accuracy: 0.54 

4.36761329183355e-05
1.4391113836609293e-05
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.57; acc: 0.47
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.48; acc: 0.56
Batch: 80; loss: 1.35; acc: 0.66
Batch: 100; loss: 1.43; acc: 0.64
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.55
Val Epoch over. val_loss: 1.4675239392906239; val_accuracy: 0.5512539808917197 

The current subspace-distance is: 1.4391113836609293e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.39; acc: 0.59
Batch: 20; loss: 1.48; acc: 0.53
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.41; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.58
Batch: 100; loss: 1.51; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.39; acc: 0.58
Batch: 160; loss: 1.52; acc: 0.56
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.41; acc: 0.64
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.46; acc: 0.53
Batch: 260; loss: 1.38; acc: 0.58
Batch: 280; loss: 1.42; acc: 0.64
Batch: 300; loss: 1.56; acc: 0.47
Batch: 320; loss: 1.52; acc: 0.45
Batch: 340; loss: 1.47; acc: 0.55
Batch: 360; loss: 1.55; acc: 0.44
Batch: 380; loss: 1.36; acc: 0.64
Batch: 400; loss: 1.49; acc: 0.52
Batch: 420; loss: 1.49; acc: 0.5
Batch: 440; loss: 1.39; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.59
Batch: 480; loss: 1.59; acc: 0.48
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 1.37; acc: 0.56
Batch: 540; loss: 1.48; acc: 0.56
Batch: 560; loss: 1.47; acc: 0.56
Batch: 580; loss: 1.27; acc: 0.67
Batch: 600; loss: 1.47; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.56
Batch: 640; loss: 1.5; acc: 0.62
Batch: 660; loss: 1.52; acc: 0.47
Batch: 680; loss: 1.59; acc: 0.52
Batch: 700; loss: 1.55; acc: 0.52
Batch: 720; loss: 1.39; acc: 0.61
Batch: 740; loss: 1.39; acc: 0.64
Batch: 760; loss: 1.51; acc: 0.5
Batch: 780; loss: 1.59; acc: 0.47
Train Epoch over. train_loss: 1.49; train_accuracy: 0.54 

4.40789335698355e-05
1.4174272109812591e-05
Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.56; acc: 0.45
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.36; acc: 0.64
Batch: 100; loss: 1.44; acc: 0.58
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.39; acc: 0.55
Val Epoch over. val_loss: 1.467765848348095; val_accuracy: 0.5499601910828026 

The current subspace-distance is: 1.4174272109812591e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.44; acc: 0.61
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 1.42; acc: 0.55
Batch: 60; loss: 1.6; acc: 0.45
Batch: 80; loss: 1.36; acc: 0.61
Batch: 100; loss: 1.24; acc: 0.66
Batch: 120; loss: 1.45; acc: 0.61
Batch: 140; loss: 1.52; acc: 0.5
Batch: 160; loss: 1.4; acc: 0.58
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.31; acc: 0.61
Batch: 220; loss: 1.7; acc: 0.47
Batch: 240; loss: 1.65; acc: 0.41
Batch: 260; loss: 1.52; acc: 0.52
Batch: 280; loss: 1.41; acc: 0.55
Batch: 300; loss: 1.46; acc: 0.53
Batch: 320; loss: 1.44; acc: 0.56
Batch: 340; loss: 1.64; acc: 0.56
Batch: 360; loss: 1.45; acc: 0.58
Batch: 380; loss: 1.47; acc: 0.58
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.53; acc: 0.48
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.38; acc: 0.58
Batch: 480; loss: 1.69; acc: 0.36
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.47
Batch: 540; loss: 1.47; acc: 0.56
Batch: 560; loss: 1.51; acc: 0.53
Batch: 580; loss: 1.33; acc: 0.62
Batch: 600; loss: 1.49; acc: 0.5
Batch: 620; loss: 1.49; acc: 0.48
Batch: 640; loss: 1.6; acc: 0.48
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.43; acc: 0.53
Batch: 700; loss: 1.45; acc: 0.56
Batch: 720; loss: 1.45; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.45
Batch: 760; loss: 1.37; acc: 0.62
Batch: 780; loss: 1.48; acc: 0.61
Train Epoch over. train_loss: 1.49; train_accuracy: 0.54 

4.44189427071251e-05
1.4552096217812505e-05
Batch: 0; loss: 1.62; acc: 0.47
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 1.48; acc: 0.56
Batch: 80; loss: 1.34; acc: 0.66
Batch: 100; loss: 1.43; acc: 0.58
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.55
Val Epoch over. val_loss: 1.4580812203656337; val_accuracy: 0.5617038216560509 

The current subspace-distance is: 1.4552096217812505e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.44
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.48; acc: 0.53
Batch: 80; loss: 1.48; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.41
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.46; acc: 0.61
Batch: 160; loss: 1.43; acc: 0.56
Batch: 180; loss: 1.51; acc: 0.48
Batch: 200; loss: 1.48; acc: 0.55
Batch: 220; loss: 1.59; acc: 0.44
Batch: 240; loss: 1.59; acc: 0.52
Batch: 260; loss: 1.52; acc: 0.48
Batch: 280; loss: 1.61; acc: 0.48
Batch: 300; loss: 1.43; acc: 0.58
Batch: 320; loss: 1.42; acc: 0.62
Batch: 340; loss: 1.53; acc: 0.47
Batch: 360; loss: 1.53; acc: 0.56
Batch: 380; loss: 1.51; acc: 0.58
Batch: 400; loss: 1.34; acc: 0.66
Batch: 420; loss: 1.53; acc: 0.5
Batch: 440; loss: 1.49; acc: 0.59
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.46; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.52
Batch: 520; loss: 1.28; acc: 0.67
Batch: 540; loss: 1.56; acc: 0.55
Batch: 560; loss: 1.62; acc: 0.5
Batch: 580; loss: 1.46; acc: 0.53
Batch: 600; loss: 1.39; acc: 0.59
Batch: 620; loss: 1.39; acc: 0.56
Batch: 640; loss: 1.43; acc: 0.59
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.49; acc: 0.53
Batch: 700; loss: 1.62; acc: 0.44
Batch: 720; loss: 1.4; acc: 0.58
Batch: 740; loss: 1.4; acc: 0.55
Batch: 760; loss: 1.52; acc: 0.53
Batch: 780; loss: 1.55; acc: 0.53
Train Epoch over. train_loss: 1.49; train_accuracy: 0.54 

4.465130768949166e-05
1.316128054895671e-05
Batch: 0; loss: 1.6; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.33; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.58
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.56
Val Epoch over. val_loss: 1.4478795460075329; val_accuracy: 0.56359474522293 

The current subspace-distance is: 1.316128054895671e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.39
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.38; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.35; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.53
Batch: 160; loss: 1.4; acc: 0.59
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.47; acc: 0.55
Batch: 220; loss: 1.42; acc: 0.56
Batch: 240; loss: 1.52; acc: 0.47
Batch: 260; loss: 1.66; acc: 0.41
Batch: 280; loss: 1.63; acc: 0.52
Batch: 300; loss: 1.51; acc: 0.58
Batch: 320; loss: 1.38; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.55
Batch: 360; loss: 1.51; acc: 0.58
Batch: 380; loss: 1.51; acc: 0.56
Batch: 400; loss: 1.48; acc: 0.48
Batch: 420; loss: 1.47; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.5
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.42; acc: 0.64
Batch: 500; loss: 1.45; acc: 0.61
Batch: 520; loss: 1.41; acc: 0.53
Batch: 540; loss: 1.48; acc: 0.53
Batch: 560; loss: 1.52; acc: 0.5
Batch: 580; loss: 1.51; acc: 0.58
Batch: 600; loss: 1.56; acc: 0.48
Batch: 620; loss: 1.36; acc: 0.58
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.35; acc: 0.67
Batch: 680; loss: 1.5; acc: 0.52
Batch: 700; loss: 1.34; acc: 0.64
Batch: 720; loss: 1.45; acc: 0.59
Batch: 740; loss: 1.46; acc: 0.56
Batch: 760; loss: 1.36; acc: 0.62
Batch: 780; loss: 1.47; acc: 0.61
Train Epoch over. train_loss: 1.48; train_accuracy: 0.54 

4.526866541709751e-05
1.5737878129584715e-05
Batch: 0; loss: 1.6; acc: 0.47
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.33; acc: 0.64
Batch: 100; loss: 1.4; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.58
Val Epoch over. val_loss: 1.4434806966477898; val_accuracy: 0.5625995222929936 

The current subspace-distance is: 1.5737878129584715e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.44
Batch: 20; loss: 1.49; acc: 0.59
Batch: 40; loss: 1.7; acc: 0.45
Batch: 60; loss: 1.42; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.45
Batch: 100; loss: 1.34; acc: 0.59
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.29; acc: 0.66
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.45; acc: 0.62
Batch: 200; loss: 1.62; acc: 0.42
Batch: 220; loss: 1.5; acc: 0.55
Batch: 240; loss: 1.41; acc: 0.69
Batch: 260; loss: 1.63; acc: 0.47
Batch: 280; loss: 1.47; acc: 0.58
Batch: 300; loss: 1.54; acc: 0.45
Batch: 320; loss: 1.64; acc: 0.44
Batch: 340; loss: 1.45; acc: 0.56
Batch: 360; loss: 1.37; acc: 0.64
Batch: 380; loss: 1.44; acc: 0.59
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.53; acc: 0.42
Batch: 440; loss: 1.47; acc: 0.55
Batch: 460; loss: 1.46; acc: 0.58
Batch: 480; loss: 1.42; acc: 0.59
Batch: 500; loss: 1.41; acc: 0.58
Batch: 520; loss: 1.54; acc: 0.48
Batch: 540; loss: 1.44; acc: 0.56
Batch: 560; loss: 1.48; acc: 0.48
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.62; acc: 0.47
Batch: 620; loss: 1.59; acc: 0.5
Batch: 640; loss: 1.55; acc: 0.48
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.55; acc: 0.52
Batch: 700; loss: 1.52; acc: 0.47
Batch: 720; loss: 1.36; acc: 0.61
Batch: 740; loss: 1.44; acc: 0.53
Batch: 760; loss: 1.48; acc: 0.52
Batch: 780; loss: 1.61; acc: 0.39
Train Epoch over. train_loss: 1.48; train_accuracy: 0.54 

4.593488483806141e-05
1.659272311371751e-05
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.33; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.58
Batch: 120; loss: 1.49; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.56
Val Epoch over. val_loss: 1.4459374140781962; val_accuracy: 0.5671775477707006 

The current subspace-distance is: 1.659272311371751e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.55
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.42; acc: 0.53
Batch: 60; loss: 1.61; acc: 0.47
Batch: 80; loss: 1.35; acc: 0.66
Batch: 100; loss: 1.64; acc: 0.48
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 1.44; acc: 0.55
Batch: 160; loss: 1.5; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.5
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.64; acc: 0.42
Batch: 240; loss: 1.58; acc: 0.52
Batch: 260; loss: 1.5; acc: 0.58
Batch: 280; loss: 1.61; acc: 0.47
Batch: 300; loss: 1.52; acc: 0.55
Batch: 320; loss: 1.39; acc: 0.55
Batch: 340; loss: 1.47; acc: 0.55
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.42; acc: 0.52
Batch: 400; loss: 1.41; acc: 0.56
Batch: 420; loss: 1.3; acc: 0.62
Batch: 440; loss: 1.45; acc: 0.61
Batch: 460; loss: 1.41; acc: 0.47
Batch: 480; loss: 1.37; acc: 0.61
Batch: 500; loss: 1.37; acc: 0.59
Batch: 520; loss: 1.28; acc: 0.59
Batch: 540; loss: 1.63; acc: 0.44
Batch: 560; loss: 1.49; acc: 0.45
Batch: 580; loss: 1.51; acc: 0.55
Batch: 600; loss: 1.68; acc: 0.45
Batch: 620; loss: 1.5; acc: 0.55
Batch: 640; loss: 1.61; acc: 0.42
Batch: 660; loss: 1.58; acc: 0.48
Batch: 680; loss: 1.53; acc: 0.52
Batch: 700; loss: 1.35; acc: 0.59
Batch: 720; loss: 1.34; acc: 0.62
Batch: 740; loss: 1.43; acc: 0.56
Batch: 760; loss: 1.49; acc: 0.56
Batch: 780; loss: 1.68; acc: 0.39
Train Epoch over. train_loss: 1.48; train_accuracy: 0.54 

4.6373203076655045e-05
1.794517447706312e-05
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.44
Batch: 40; loss: 1.23; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.34; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.62
Batch: 140; loss: 1.38; acc: 0.55
Val Epoch over. val_loss: 1.4525741308358064; val_accuracy: 0.5564291401273885 

The current subspace-distance is: 1.794517447706312e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.64
Batch: 20; loss: 1.4; acc: 0.58
Batch: 40; loss: 1.28; acc: 0.66
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.47; acc: 0.53
Batch: 100; loss: 1.5; acc: 0.53
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 1.52; acc: 0.53
Batch: 160; loss: 1.58; acc: 0.53
Batch: 180; loss: 1.58; acc: 0.41
Batch: 200; loss: 1.6; acc: 0.41
Batch: 220; loss: 1.64; acc: 0.42
Batch: 240; loss: 1.52; acc: 0.58
Batch: 260; loss: 1.52; acc: 0.59
Batch: 280; loss: 1.41; acc: 0.56
Batch: 300; loss: 1.44; acc: 0.56
Batch: 320; loss: 1.35; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.58
Batch: 360; loss: 1.49; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.52
Batch: 400; loss: 1.45; acc: 0.64
Batch: 420; loss: 1.32; acc: 0.69
Batch: 440; loss: 1.43; acc: 0.53
Batch: 460; loss: 1.44; acc: 0.58
Batch: 480; loss: 1.31; acc: 0.72
Batch: 500; loss: 1.38; acc: 0.61
Batch: 520; loss: 1.56; acc: 0.55
Batch: 540; loss: 1.32; acc: 0.59
Batch: 560; loss: 1.54; acc: 0.44
Batch: 580; loss: 1.54; acc: 0.44
Batch: 600; loss: 1.4; acc: 0.59
Batch: 620; loss: 1.5; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.61
Batch: 660; loss: 1.53; acc: 0.44
Batch: 680; loss: 1.55; acc: 0.52
Batch: 700; loss: 1.62; acc: 0.44
Batch: 720; loss: 1.58; acc: 0.53
Batch: 740; loss: 1.37; acc: 0.64
Batch: 760; loss: 1.66; acc: 0.45
Batch: 780; loss: 1.48; acc: 0.53
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.60346054751426e-05
1.3801684872305486e-05
Batch: 0; loss: 1.59; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.32; acc: 0.59
Batch: 100; loss: 1.41; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.4368339746621004; val_accuracy: 0.5733479299363057 

The current subspace-distance is: 1.3801684872305486e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.48; acc: 0.53
Batch: 40; loss: 1.4; acc: 0.58
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 1.5; acc: 0.42
Batch: 100; loss: 1.38; acc: 0.55
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.58; acc: 0.5
Batch: 160; loss: 1.45; acc: 0.47
Batch: 180; loss: 1.43; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.52
Batch: 220; loss: 1.29; acc: 0.72
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.44; acc: 0.61
Batch: 280; loss: 1.72; acc: 0.38
Batch: 300; loss: 1.46; acc: 0.61
Batch: 320; loss: 1.29; acc: 0.67
Batch: 340; loss: 1.47; acc: 0.56
Batch: 360; loss: 1.51; acc: 0.5
Batch: 380; loss: 1.56; acc: 0.47
Batch: 400; loss: 1.62; acc: 0.47
Batch: 420; loss: 1.44; acc: 0.56
Batch: 440; loss: 1.51; acc: 0.55
Batch: 460; loss: 1.43; acc: 0.56
Batch: 480; loss: 1.47; acc: 0.59
Batch: 500; loss: 1.42; acc: 0.55
Batch: 520; loss: 1.37; acc: 0.56
Batch: 540; loss: 1.53; acc: 0.53
Batch: 560; loss: 1.41; acc: 0.56
Batch: 580; loss: 1.41; acc: 0.56
Batch: 600; loss: 1.41; acc: 0.53
Batch: 620; loss: 1.39; acc: 0.55
Batch: 640; loss: 1.4; acc: 0.55
Batch: 660; loss: 1.39; acc: 0.62
Batch: 680; loss: 1.38; acc: 0.59
Batch: 700; loss: 1.41; acc: 0.58
Batch: 720; loss: 1.31; acc: 0.69
Batch: 740; loss: 1.37; acc: 0.64
Batch: 760; loss: 1.52; acc: 0.56
Batch: 780; loss: 1.5; acc: 0.52
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.603786874213256e-05
1.7699430827633478e-05
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.55; acc: 0.44
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.48; acc: 0.58
Batch: 80; loss: 1.34; acc: 0.59
Batch: 100; loss: 1.42; acc: 0.56
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 1.37; acc: 0.55
Val Epoch over. val_loss: 1.44467249797408; val_accuracy: 0.5641918789808917 

The current subspace-distance is: 1.7699430827633478e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.54; acc: 0.42
Batch: 40; loss: 1.46; acc: 0.53
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.33; acc: 0.62
Batch: 120; loss: 1.35; acc: 0.66
Batch: 140; loss: 1.36; acc: 0.62
Batch: 160; loss: 1.51; acc: 0.53
Batch: 180; loss: 1.5; acc: 0.5
Batch: 200; loss: 1.42; acc: 0.59
Batch: 220; loss: 1.53; acc: 0.52
Batch: 240; loss: 1.41; acc: 0.59
Batch: 260; loss: 1.48; acc: 0.5
Batch: 280; loss: 1.61; acc: 0.5
Batch: 300; loss: 1.32; acc: 0.67
Batch: 320; loss: 1.65; acc: 0.5
Batch: 340; loss: 1.34; acc: 0.59
Batch: 360; loss: 1.51; acc: 0.58
Batch: 380; loss: 1.51; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.45
Batch: 420; loss: 1.64; acc: 0.45
Batch: 440; loss: 1.4; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.64
Batch: 480; loss: 1.37; acc: 0.66
Batch: 500; loss: 1.43; acc: 0.61
Batch: 520; loss: 1.45; acc: 0.55
Batch: 540; loss: 1.56; acc: 0.5
Batch: 560; loss: 1.39; acc: 0.61
Batch: 580; loss: 1.51; acc: 0.5
Batch: 600; loss: 1.47; acc: 0.53
Batch: 620; loss: 1.48; acc: 0.53
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.6; acc: 0.5
Batch: 700; loss: 1.26; acc: 0.62
Batch: 720; loss: 1.46; acc: 0.55
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.57; acc: 0.53
Batch: 780; loss: 1.39; acc: 0.55
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.583198096952401e-05
1.3859383216185961e-05
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.55; acc: 0.44
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.32; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.55
Batch: 120; loss: 1.47; acc: 0.64
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.4336696741687265; val_accuracy: 0.5695660828025477 

The current subspace-distance is: 1.3859383216185961e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.42; acc: 0.52
Batch: 40; loss: 1.44; acc: 0.59
Batch: 60; loss: 1.41; acc: 0.62
Batch: 80; loss: 1.38; acc: 0.62
Batch: 100; loss: 1.45; acc: 0.59
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 1.56; acc: 0.45
Batch: 160; loss: 1.64; acc: 0.41
Batch: 180; loss: 1.45; acc: 0.52
Batch: 200; loss: 1.33; acc: 0.69
Batch: 220; loss: 1.51; acc: 0.48
Batch: 240; loss: 1.27; acc: 0.67
Batch: 260; loss: 1.35; acc: 0.62
Batch: 280; loss: 1.5; acc: 0.58
Batch: 300; loss: 1.58; acc: 0.52
Batch: 320; loss: 1.42; acc: 0.52
Batch: 340; loss: 1.62; acc: 0.53
Batch: 360; loss: 1.57; acc: 0.45
Batch: 380; loss: 1.49; acc: 0.52
Batch: 400; loss: 1.37; acc: 0.61
Batch: 420; loss: 1.48; acc: 0.56
Batch: 440; loss: 1.47; acc: 0.48
Batch: 460; loss: 1.6; acc: 0.48
Batch: 480; loss: 1.35; acc: 0.64
Batch: 500; loss: 1.58; acc: 0.47
Batch: 520; loss: 1.44; acc: 0.45
Batch: 540; loss: 1.45; acc: 0.56
Batch: 560; loss: 1.34; acc: 0.56
Batch: 580; loss: 1.36; acc: 0.61
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.52; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.3; acc: 0.64
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 1.56; acc: 0.45
Batch: 740; loss: 1.56; acc: 0.48
Batch: 760; loss: 1.4; acc: 0.61
Batch: 780; loss: 1.28; acc: 0.66
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.563789116218686e-05
1.6656224033795297e-05
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.55; acc: 0.44
Batch: 40; loss: 1.22; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.58
Batch: 120; loss: 1.47; acc: 0.61
Batch: 140; loss: 1.36; acc: 0.55
Val Epoch over. val_loss: 1.439807213795413; val_accuracy: 0.5599124203821656 

The current subspace-distance is: 1.6656224033795297e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.56
Batch: 20; loss: 1.36; acc: 0.61
Batch: 40; loss: 1.41; acc: 0.55
Batch: 60; loss: 1.41; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.41; acc: 0.55
Batch: 120; loss: 1.51; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.64
Batch: 160; loss: 1.44; acc: 0.55
Batch: 180; loss: 1.44; acc: 0.56
Batch: 200; loss: 1.44; acc: 0.61
Batch: 220; loss: 1.48; acc: 0.61
Batch: 240; loss: 1.44; acc: 0.5
Batch: 260; loss: 1.48; acc: 0.55
Batch: 280; loss: 1.49; acc: 0.53
Batch: 300; loss: 1.38; acc: 0.64
Batch: 320; loss: 1.5; acc: 0.55
Batch: 340; loss: 1.32; acc: 0.62
Batch: 360; loss: 1.44; acc: 0.52
Batch: 380; loss: 1.41; acc: 0.52
Batch: 400; loss: 1.52; acc: 0.55
Batch: 420; loss: 1.4; acc: 0.62
Batch: 440; loss: 1.51; acc: 0.52
Batch: 460; loss: 1.48; acc: 0.5
Batch: 480; loss: 1.45; acc: 0.56
Batch: 500; loss: 1.69; acc: 0.5
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.35; acc: 0.59
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.38; acc: 0.58
Batch: 600; loss: 1.63; acc: 0.58
Batch: 620; loss: 1.53; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.48
Batch: 660; loss: 1.4; acc: 0.58
Batch: 680; loss: 1.46; acc: 0.53
Batch: 700; loss: 1.38; acc: 0.56
Batch: 720; loss: 1.46; acc: 0.52
Batch: 740; loss: 1.51; acc: 0.53
Batch: 760; loss: 1.33; acc: 0.61
Batch: 780; loss: 1.48; acc: 0.52
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.600953616318293e-05
1.3466218661051244e-05
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.53; acc: 0.45
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.33; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.64
Batch: 140; loss: 1.34; acc: 0.59
Val Epoch over. val_loss: 1.433894840775022; val_accuracy: 0.5739450636942676 

The current subspace-distance is: 1.3466218661051244e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.54; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.49; acc: 0.59
Batch: 80; loss: 1.47; acc: 0.53
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.41; acc: 0.52
Batch: 140; loss: 1.43; acc: 0.53
Batch: 160; loss: 1.38; acc: 0.64
Batch: 180; loss: 1.59; acc: 0.47
Batch: 200; loss: 1.37; acc: 0.62
Batch: 220; loss: 1.43; acc: 0.56
Batch: 240; loss: 1.53; acc: 0.52
Batch: 260; loss: 1.38; acc: 0.56
Batch: 280; loss: 1.55; acc: 0.52
Batch: 300; loss: 1.44; acc: 0.61
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.49; acc: 0.59
Batch: 360; loss: 1.5; acc: 0.52
Batch: 380; loss: 1.32; acc: 0.67
Batch: 400; loss: 1.38; acc: 0.62
Batch: 420; loss: 1.45; acc: 0.56
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.51; acc: 0.61
Batch: 480; loss: 1.59; acc: 0.48
Batch: 500; loss: 1.66; acc: 0.45
Batch: 520; loss: 1.43; acc: 0.56
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.44; acc: 0.55
Batch: 580; loss: 1.5; acc: 0.52
Batch: 600; loss: 1.43; acc: 0.53
Batch: 620; loss: 1.5; acc: 0.55
Batch: 640; loss: 1.44; acc: 0.58
Batch: 660; loss: 1.6; acc: 0.45
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.49; acc: 0.48
Batch: 720; loss: 1.46; acc: 0.53
Batch: 740; loss: 1.44; acc: 0.56
Batch: 760; loss: 1.46; acc: 0.61
Batch: 780; loss: 1.36; acc: 0.64
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.550783705781214e-05
1.4380951142811682e-05
Batch: 0; loss: 1.57; acc: 0.5
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.31; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.42883263272085; val_accuracy: 0.5668789808917197 

The current subspace-distance is: 1.4380951142811682e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.45
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 1.42; acc: 0.59
Batch: 60; loss: 1.6; acc: 0.42
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.38; acc: 0.62
Batch: 120; loss: 1.47; acc: 0.66
Batch: 140; loss: 1.48; acc: 0.55
Batch: 160; loss: 1.44; acc: 0.58
Batch: 180; loss: 1.42; acc: 0.56
Batch: 200; loss: 1.37; acc: 0.58
Batch: 220; loss: 1.37; acc: 0.66
Batch: 240; loss: 1.41; acc: 0.58
Batch: 260; loss: 1.49; acc: 0.44
Batch: 280; loss: 1.55; acc: 0.52
Batch: 300; loss: 1.37; acc: 0.66
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.43; acc: 0.55
Batch: 360; loss: 1.46; acc: 0.58
Batch: 380; loss: 1.37; acc: 0.59
Batch: 400; loss: 1.39; acc: 0.59
Batch: 420; loss: 1.41; acc: 0.53
Batch: 440; loss: 1.45; acc: 0.52
Batch: 460; loss: 1.38; acc: 0.61
Batch: 480; loss: 1.28; acc: 0.64
Batch: 500; loss: 1.31; acc: 0.67
Batch: 520; loss: 1.43; acc: 0.5
Batch: 540; loss: 1.52; acc: 0.53
Batch: 560; loss: 1.53; acc: 0.47
Batch: 580; loss: 1.4; acc: 0.56
Batch: 600; loss: 1.46; acc: 0.59
Batch: 620; loss: 1.49; acc: 0.58
Batch: 640; loss: 1.49; acc: 0.56
Batch: 660; loss: 1.52; acc: 0.52
Batch: 680; loss: 1.5; acc: 0.52
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.51; acc: 0.55
Batch: 740; loss: 1.45; acc: 0.58
Batch: 760; loss: 1.59; acc: 0.47
Batch: 780; loss: 1.48; acc: 0.59
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.653280848287977e-05
1.5293377146008424e-05
Batch: 0; loss: 1.58; acc: 0.47
Batch: 20; loss: 1.55; acc: 0.47
Batch: 40; loss: 1.22; acc: 0.61
Batch: 60; loss: 1.48; acc: 0.55
Batch: 80; loss: 1.33; acc: 0.62
Batch: 100; loss: 1.43; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.4406350281587832; val_accuracy: 0.5625 

The current subspace-distance is: 1.5293377146008424e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.59
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 1.28; acc: 0.72
Batch: 60; loss: 1.43; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.5; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.47
Batch: 160; loss: 1.54; acc: 0.44
Batch: 180; loss: 1.52; acc: 0.47
Batch: 200; loss: 1.4; acc: 0.56
Batch: 220; loss: 1.34; acc: 0.64
Batch: 240; loss: 1.43; acc: 0.66
Batch: 260; loss: 1.38; acc: 0.58
Batch: 280; loss: 1.51; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.47
Batch: 320; loss: 1.43; acc: 0.58
Batch: 340; loss: 1.55; acc: 0.52
Batch: 360; loss: 1.43; acc: 0.55
Batch: 380; loss: 1.32; acc: 0.62
Batch: 400; loss: 1.52; acc: 0.47
Batch: 420; loss: 1.32; acc: 0.61
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.61; acc: 0.47
Batch: 480; loss: 1.49; acc: 0.59
Batch: 500; loss: 1.37; acc: 0.59
Batch: 520; loss: 1.45; acc: 0.48
Batch: 540; loss: 1.49; acc: 0.5
Batch: 560; loss: 1.47; acc: 0.59
Batch: 580; loss: 1.5; acc: 0.5
Batch: 600; loss: 1.53; acc: 0.55
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.57; acc: 0.48
Batch: 660; loss: 1.6; acc: 0.47
Batch: 680; loss: 1.33; acc: 0.7
Batch: 700; loss: 1.37; acc: 0.59
Batch: 720; loss: 1.33; acc: 0.56
Batch: 740; loss: 1.51; acc: 0.48
Batch: 760; loss: 1.35; acc: 0.66
Batch: 780; loss: 1.37; acc: 0.53
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.6879431465640664e-05
1.7097652744269e-05
Batch: 0; loss: 1.57; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.56
Val Epoch over. val_loss: 1.4359439656992627; val_accuracy: 0.5654856687898089 

The current subspace-distance is: 1.7097652744269e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.52
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 1.57; acc: 0.5
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 1.46; acc: 0.5
Batch: 100; loss: 1.38; acc: 0.61
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 1.53; acc: 0.45
Batch: 160; loss: 1.48; acc: 0.5
Batch: 180; loss: 1.71; acc: 0.38
Batch: 200; loss: 1.57; acc: 0.45
Batch: 220; loss: 1.46; acc: 0.56
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.58; acc: 0.47
Batch: 280; loss: 1.51; acc: 0.47
Batch: 300; loss: 1.41; acc: 0.56
Batch: 320; loss: 1.43; acc: 0.56
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.45; acc: 0.56
Batch: 380; loss: 1.34; acc: 0.62
Batch: 400; loss: 1.54; acc: 0.5
Batch: 420; loss: 1.47; acc: 0.56
Batch: 440; loss: 1.46; acc: 0.45
Batch: 460; loss: 1.41; acc: 0.58
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.58; acc: 0.5
Batch: 520; loss: 1.28; acc: 0.69
Batch: 540; loss: 1.49; acc: 0.56
Batch: 560; loss: 1.53; acc: 0.53
Batch: 580; loss: 1.37; acc: 0.59
Batch: 600; loss: 1.47; acc: 0.48
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 1.42; acc: 0.61
Batch: 660; loss: 1.41; acc: 0.61
Batch: 680; loss: 1.65; acc: 0.47
Batch: 700; loss: 1.53; acc: 0.52
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.36; acc: 0.55
Batch: 760; loss: 1.4; acc: 0.64
Batch: 780; loss: 1.49; acc: 0.61
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.653492214856669e-05
1.630197948543355e-05
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.44
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.32; acc: 0.61
Batch: 100; loss: 1.4; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.4320408500683535; val_accuracy: 0.5677746815286624 

The current subspace-distance is: 1.630197948543355e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.41
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 1.53; acc: 0.52
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.35; acc: 0.58
Batch: 100; loss: 1.48; acc: 0.55
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.5; acc: 0.47
Batch: 160; loss: 1.54; acc: 0.44
Batch: 180; loss: 1.47; acc: 0.62
Batch: 200; loss: 1.45; acc: 0.5
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.32; acc: 0.64
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.45; acc: 0.55
Batch: 300; loss: 1.54; acc: 0.45
Batch: 320; loss: 1.35; acc: 0.64
Batch: 340; loss: 1.54; acc: 0.5
Batch: 360; loss: 1.38; acc: 0.64
Batch: 380; loss: 1.48; acc: 0.5
Batch: 400; loss: 1.59; acc: 0.48
Batch: 420; loss: 1.39; acc: 0.58
Batch: 440; loss: 1.45; acc: 0.59
Batch: 460; loss: 1.46; acc: 0.5
Batch: 480; loss: 1.44; acc: 0.53
Batch: 500; loss: 1.43; acc: 0.56
Batch: 520; loss: 1.45; acc: 0.56
Batch: 540; loss: 1.43; acc: 0.58
Batch: 560; loss: 1.45; acc: 0.5
Batch: 580; loss: 1.24; acc: 0.77
Batch: 600; loss: 1.47; acc: 0.59
Batch: 620; loss: 1.5; acc: 0.55
Batch: 640; loss: 1.32; acc: 0.64
Batch: 660; loss: 1.32; acc: 0.62
Batch: 680; loss: 1.37; acc: 0.66
Batch: 700; loss: 1.38; acc: 0.58
Batch: 720; loss: 1.35; acc: 0.61
Batch: 740; loss: 1.54; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.52
Batch: 780; loss: 1.41; acc: 0.55
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.686087049776688e-05
1.621467890799977e-05
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.53; acc: 0.44
Batch: 40; loss: 1.2; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.33; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.59
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.56
Val Epoch over. val_loss: 1.4342509447389347; val_accuracy: 0.5666799363057324 

The current subspace-distance is: 1.621467890799977e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.5
Batch: 20; loss: 1.38; acc: 0.58
Batch: 40; loss: 1.52; acc: 0.42
Batch: 60; loss: 1.41; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.61
Batch: 120; loss: 1.3; acc: 0.69
Batch: 140; loss: 1.5; acc: 0.48
Batch: 160; loss: 1.29; acc: 0.61
Batch: 180; loss: 1.47; acc: 0.47
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.46; acc: 0.58
Batch: 240; loss: 1.35; acc: 0.61
Batch: 260; loss: 1.49; acc: 0.56
Batch: 280; loss: 1.53; acc: 0.5
Batch: 300; loss: 1.57; acc: 0.45
Batch: 320; loss: 1.57; acc: 0.55
Batch: 340; loss: 1.44; acc: 0.56
Batch: 360; loss: 1.38; acc: 0.61
Batch: 380; loss: 1.42; acc: 0.61
Batch: 400; loss: 1.41; acc: 0.58
Batch: 420; loss: 1.31; acc: 0.69
Batch: 440; loss: 1.47; acc: 0.53
Batch: 460; loss: 1.58; acc: 0.45
Batch: 480; loss: 1.51; acc: 0.5
Batch: 500; loss: 1.49; acc: 0.64
Batch: 520; loss: 1.38; acc: 0.59
Batch: 540; loss: 1.38; acc: 0.55
Batch: 560; loss: 1.45; acc: 0.55
Batch: 580; loss: 1.26; acc: 0.69
Batch: 600; loss: 1.26; acc: 0.66
Batch: 620; loss: 1.46; acc: 0.58
Batch: 640; loss: 1.44; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.64
Batch: 680; loss: 1.61; acc: 0.45
Batch: 700; loss: 1.49; acc: 0.52
Batch: 720; loss: 1.58; acc: 0.5
Batch: 740; loss: 1.54; acc: 0.52
Batch: 760; loss: 1.53; acc: 0.5
Batch: 780; loss: 1.52; acc: 0.48
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.617330705514178e-05
1.4404362445930019e-05
Batch: 0; loss: 1.57; acc: 0.5
Batch: 20; loss: 1.55; acc: 0.44
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.41; acc: 0.58
Batch: 120; loss: 1.46; acc: 0.64
Batch: 140; loss: 1.34; acc: 0.55
Val Epoch over. val_loss: 1.4319079013387108; val_accuracy: 0.5654856687898089 

The current subspace-distance is: 1.4404362445930019e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.45

The number of parameters is: 273705

The number of individual parameters is:

12
216
12
12
18
37584
18
18
35
109620
35
35
64
120960
64
64
4096
64
640
10
64
64

nonzero elements in E: 27370497
elements in E: 27370500
fraction nonzero: 0.9999998903929413
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.42; acc: 0.08
Batch: 20; loss: 2.35; acc: 0.05
Batch: 40; loss: 2.3; acc: 0.19
Batch: 60; loss: 2.1; acc: 0.25
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.15; acc: 0.25
Batch: 120; loss: 2.07; acc: 0.28
Batch: 140; loss: 2.01; acc: 0.41
Batch: 160; loss: 2.01; acc: 0.33
Batch: 180; loss: 1.95; acc: 0.36
Batch: 200; loss: 1.85; acc: 0.5
Batch: 220; loss: 1.95; acc: 0.39
Batch: 240; loss: 1.79; acc: 0.48
Batch: 260; loss: 1.91; acc: 0.41
Batch: 280; loss: 1.88; acc: 0.42
Batch: 300; loss: 1.76; acc: 0.48
Batch: 320; loss: 1.77; acc: 0.52
Batch: 340; loss: 1.76; acc: 0.55
Batch: 360; loss: 1.82; acc: 0.47
Batch: 380; loss: 1.83; acc: 0.41
Batch: 400; loss: 1.63; acc: 0.59
Batch: 420; loss: 1.78; acc: 0.52
Batch: 440; loss: 1.81; acc: 0.47
Batch: 460; loss: 1.85; acc: 0.45
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.65; acc: 0.59
Batch: 520; loss: 1.69; acc: 0.48
Batch: 540; loss: 1.85; acc: 0.53
Batch: 560; loss: 1.8; acc: 0.42
Batch: 580; loss: 1.75; acc: 0.48
Batch: 600; loss: 1.7; acc: 0.52
Batch: 620; loss: 1.69; acc: 0.5
Batch: 640; loss: 1.75; acc: 0.45
Batch: 660; loss: 1.89; acc: 0.38
Batch: 680; loss: 1.86; acc: 0.34
Batch: 700; loss: 1.62; acc: 0.59
Batch: 720; loss: 1.67; acc: 0.53
Batch: 740; loss: 1.57; acc: 0.59
Batch: 760; loss: 1.63; acc: 0.56
Batch: 780; loss: 1.67; acc: 0.48
Train Epoch over. train_loss: 1.85; train_accuracy: 0.44 

5.6824719649739563e-05
5.1879156671930104e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.48
Batch: 40; loss: 1.47; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.45; acc: 0.62
Batch: 100; loss: 1.8; acc: 0.52
Batch: 120; loss: 1.71; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.62298485579764; val_accuracy: 0.5712579617834395 

The current subspace-distance is: 5.1879156671930104e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.86; acc: 0.45
Batch: 60; loss: 1.73; acc: 0.5
Batch: 80; loss: 1.71; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.44
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.67; acc: 0.52
Batch: 160; loss: 1.52; acc: 0.55
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.62; acc: 0.53
Batch: 220; loss: 1.69; acc: 0.48
Batch: 240; loss: 1.71; acc: 0.48
Batch: 260; loss: 1.73; acc: 0.45
Batch: 280; loss: 1.61; acc: 0.59
Batch: 300; loss: 1.47; acc: 0.61
Batch: 320; loss: 1.48; acc: 0.62
Batch: 340; loss: 1.5; acc: 0.64
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.68; acc: 0.53
Batch: 400; loss: 1.58; acc: 0.58
Batch: 420; loss: 1.61; acc: 0.53
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.62; acc: 0.56
Batch: 480; loss: 1.72; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.62
Batch: 520; loss: 1.34; acc: 0.66
Batch: 540; loss: 1.51; acc: 0.69
Batch: 560; loss: 1.39; acc: 0.7
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.65; acc: 0.56
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.45; acc: 0.61
Batch: 660; loss: 1.69; acc: 0.39
Batch: 680; loss: 1.51; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.53
Batch: 720; loss: 1.48; acc: 0.58
Batch: 740; loss: 1.57; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.58
Batch: 780; loss: 1.44; acc: 0.62
Train Epoch over. train_loss: 1.58; train_accuracy: 0.57 

7.709398050792515e-05
7.198751700343564e-05
Batch: 0; loss: 1.56; acc: 0.52
Batch: 20; loss: 1.74; acc: 0.48
Batch: 40; loss: 1.38; acc: 0.62
Batch: 60; loss: 1.45; acc: 0.66
Batch: 80; loss: 1.3; acc: 0.69
Batch: 100; loss: 1.66; acc: 0.56
Batch: 120; loss: 1.58; acc: 0.59
Batch: 140; loss: 1.44; acc: 0.58
Val Epoch over. val_loss: 1.494395369936706; val_accuracy: 0.5949442675159236 

The current subspace-distance is: 7.198751700343564e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.45
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.58; acc: 0.53
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.72; acc: 0.5
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.74; acc: 0.5
Batch: 140; loss: 1.54; acc: 0.59
Batch: 160; loss: 1.52; acc: 0.56
Batch: 180; loss: 1.36; acc: 0.69
Batch: 200; loss: 1.45; acc: 0.59
Batch: 220; loss: 1.38; acc: 0.62
Batch: 240; loss: 1.52; acc: 0.59
Batch: 260; loss: 1.62; acc: 0.53
Batch: 280; loss: 1.7; acc: 0.39
Batch: 300; loss: 1.46; acc: 0.66
Batch: 320; loss: 1.41; acc: 0.58
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.4; acc: 0.62
Batch: 380; loss: 1.56; acc: 0.58
Batch: 400; loss: 1.47; acc: 0.55
Batch: 420; loss: 1.52; acc: 0.55
Batch: 440; loss: 1.38; acc: 0.64
Batch: 460; loss: 1.57; acc: 0.52
Batch: 480; loss: 1.37; acc: 0.69
Batch: 500; loss: 1.44; acc: 0.67
Batch: 520; loss: 1.49; acc: 0.59
Batch: 540; loss: 1.55; acc: 0.52
Batch: 560; loss: 1.57; acc: 0.56
Batch: 580; loss: 1.47; acc: 0.66
Batch: 600; loss: 1.52; acc: 0.59
Batch: 620; loss: 1.53; acc: 0.5
Batch: 640; loss: 1.39; acc: 0.64
Batch: 660; loss: 1.62; acc: 0.48
Batch: 680; loss: 1.45; acc: 0.59
Batch: 700; loss: 1.37; acc: 0.67
Batch: 720; loss: 1.52; acc: 0.59
Batch: 740; loss: 1.51; acc: 0.5
Batch: 760; loss: 1.52; acc: 0.59
Batch: 780; loss: 1.42; acc: 0.67
Train Epoch over. train_loss: 1.5; train_accuracy: 0.58 

8.768640691414475e-05
8.349127892870456e-05
Batch: 0; loss: 1.51; acc: 0.56
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.31; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.55; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.52
Val Epoch over. val_loss: 1.4297765941376899; val_accuracy: 0.5981289808917197 

The current subspace-distance is: 8.349127892870456e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.58
Batch: 100; loss: 1.44; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.58
Batch: 160; loss: 1.62; acc: 0.52
Batch: 180; loss: 1.44; acc: 0.61
Batch: 200; loss: 1.52; acc: 0.56
Batch: 220; loss: 1.49; acc: 0.62
Batch: 240; loss: 1.4; acc: 0.62
Batch: 260; loss: 1.54; acc: 0.5
Batch: 280; loss: 1.44; acc: 0.66
Batch: 300; loss: 1.53; acc: 0.58
Batch: 320; loss: 1.41; acc: 0.59
Batch: 340; loss: 1.38; acc: 0.66
Batch: 360; loss: 1.52; acc: 0.55
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.5; acc: 0.59
Batch: 420; loss: 1.48; acc: 0.58
Batch: 440; loss: 1.45; acc: 0.59
Batch: 460; loss: 1.32; acc: 0.67
Batch: 480; loss: 1.43; acc: 0.66
Batch: 500; loss: 1.44; acc: 0.62
Batch: 520; loss: 1.64; acc: 0.48
Batch: 540; loss: 1.46; acc: 0.52
Batch: 560; loss: 1.58; acc: 0.55
Batch: 580; loss: 1.52; acc: 0.59
Batch: 600; loss: 1.63; acc: 0.48
Batch: 620; loss: 1.47; acc: 0.56
Batch: 640; loss: 1.46; acc: 0.58
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.32; acc: 0.61
Batch: 700; loss: 1.46; acc: 0.58
Batch: 720; loss: 1.37; acc: 0.58
Batch: 740; loss: 1.33; acc: 0.66
Batch: 760; loss: 1.44; acc: 0.59
Batch: 780; loss: 1.49; acc: 0.55
Train Epoch over. train_loss: 1.46; train_accuracy: 0.59 

9.875802788883448e-05
9.364346624352038e-05
Batch: 0; loss: 1.47; acc: 0.53
Batch: 20; loss: 1.58; acc: 0.45
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.35; acc: 0.62
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.44; acc: 0.58
Val Epoch over. val_loss: 1.3946902152079685; val_accuracy: 0.6161425159235668 

The current subspace-distance is: 9.364346624352038e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.56
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 1.49; acc: 0.55
Batch: 60; loss: 1.36; acc: 0.58
Batch: 80; loss: 1.36; acc: 0.62
Batch: 100; loss: 1.45; acc: 0.61
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.52; acc: 0.56
Batch: 160; loss: 1.38; acc: 0.64
Batch: 180; loss: 1.59; acc: 0.5
Batch: 200; loss: 1.46; acc: 0.56
Batch: 220; loss: 1.38; acc: 0.64
Batch: 240; loss: 1.47; acc: 0.5
Batch: 260; loss: 1.45; acc: 0.56
Batch: 280; loss: 1.26; acc: 0.72
Batch: 300; loss: 1.56; acc: 0.58
Batch: 320; loss: 1.48; acc: 0.58
Batch: 340; loss: 1.42; acc: 0.61
Batch: 360; loss: 1.19; acc: 0.75
Batch: 380; loss: 1.28; acc: 0.75
Batch: 400; loss: 1.68; acc: 0.5
Batch: 420; loss: 1.36; acc: 0.64
Batch: 440; loss: 1.42; acc: 0.61
Batch: 460; loss: 1.32; acc: 0.69
Batch: 480; loss: 1.42; acc: 0.62
Batch: 500; loss: 1.36; acc: 0.62
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.33; acc: 0.69
Batch: 560; loss: 1.61; acc: 0.45
Batch: 580; loss: 1.55; acc: 0.56
Batch: 600; loss: 1.47; acc: 0.58
Batch: 620; loss: 1.34; acc: 0.62
Batch: 640; loss: 1.39; acc: 0.59
Batch: 660; loss: 1.4; acc: 0.64
Batch: 680; loss: 1.22; acc: 0.67
Batch: 700; loss: 1.24; acc: 0.77
Batch: 720; loss: 1.37; acc: 0.67
Batch: 740; loss: 1.58; acc: 0.53
Batch: 760; loss: 1.39; acc: 0.62
Batch: 780; loss: 1.43; acc: 0.62
Train Epoch over. train_loss: 1.41; train_accuracy: 0.61 

0.00010879226465476677
0.00010331910016248003
Batch: 0; loss: 1.41; acc: 0.56
Batch: 20; loss: 1.49; acc: 0.52
Batch: 40; loss: 1.14; acc: 0.75
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.8
Batch: 100; loss: 1.44; acc: 0.56
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.41; acc: 0.59
Val Epoch over. val_loss: 1.344145000360574; val_accuracy: 0.6447054140127388 

The current subspace-distance is: 0.00010331910016248003 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.45; acc: 0.59
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.38; acc: 0.56
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.25; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.66
Batch: 140; loss: 1.38; acc: 0.62
Batch: 160; loss: 1.2; acc: 0.73
Batch: 180; loss: 1.32; acc: 0.64
Batch: 200; loss: 1.52; acc: 0.5
Batch: 220; loss: 1.36; acc: 0.62
Batch: 240; loss: 1.37; acc: 0.66
Batch: 260; loss: 1.5; acc: 0.58
Batch: 280; loss: 1.28; acc: 0.66
Batch: 300; loss: 1.52; acc: 0.53
Batch: 320; loss: 1.27; acc: 0.69
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.23; acc: 0.75
Batch: 380; loss: 1.36; acc: 0.67
Batch: 400; loss: 1.33; acc: 0.66
Batch: 420; loss: 1.24; acc: 0.73
Batch: 440; loss: 1.42; acc: 0.64
Batch: 460; loss: 1.39; acc: 0.56
Batch: 480; loss: 1.33; acc: 0.64
Batch: 500; loss: 1.46; acc: 0.53
Batch: 520; loss: 1.38; acc: 0.58
Batch: 540; loss: 1.31; acc: 0.66
Batch: 560; loss: 1.42; acc: 0.56
Batch: 580; loss: 1.41; acc: 0.62
Batch: 600; loss: 1.4; acc: 0.69
Batch: 620; loss: 1.36; acc: 0.67
Batch: 640; loss: 1.3; acc: 0.64
Batch: 660; loss: 1.22; acc: 0.69
Batch: 680; loss: 1.42; acc: 0.56
Batch: 700; loss: 1.31; acc: 0.59
Batch: 720; loss: 1.42; acc: 0.59
Batch: 740; loss: 1.28; acc: 0.77
Batch: 760; loss: 1.41; acc: 0.61
Batch: 780; loss: 1.36; acc: 0.61
Train Epoch over. train_loss: 1.35; train_accuracy: 0.63 

0.00012051520025124773
0.00011388387065380812
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.43; acc: 0.55
Batch: 40; loss: 1.05; acc: 0.67
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.84
Batch: 100; loss: 1.36; acc: 0.56
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.59
Val Epoch over. val_loss: 1.2818018258756894; val_accuracy: 0.6570461783439491 

The current subspace-distance is: 0.00011388387065380812 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.67
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.24; acc: 0.7
Batch: 80; loss: 1.24; acc: 0.61
Batch: 100; loss: 1.58; acc: 0.42
Batch: 120; loss: 1.39; acc: 0.56
Batch: 140; loss: 1.2; acc: 0.7
Batch: 160; loss: 1.25; acc: 0.58
Batch: 180; loss: 1.51; acc: 0.53
Batch: 200; loss: 1.24; acc: 0.62
Batch: 220; loss: 1.28; acc: 0.66
Batch: 240; loss: 1.39; acc: 0.59
Batch: 260; loss: 1.3; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.75
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.15; acc: 0.72
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.41; acc: 0.55
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.38; acc: 0.56
Batch: 420; loss: 1.38; acc: 0.66
Batch: 440; loss: 1.38; acc: 0.66
Batch: 460; loss: 1.28; acc: 0.66
Batch: 480; loss: 1.23; acc: 0.7
Batch: 500; loss: 1.4; acc: 0.61
Batch: 520; loss: 1.26; acc: 0.64
Batch: 540; loss: 1.28; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.59
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 1.27; acc: 0.66
Batch: 620; loss: 1.31; acc: 0.56
Batch: 640; loss: 1.3; acc: 0.66
Batch: 660; loss: 1.2; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.67
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 1.46; acc: 0.53
Train Epoch over. train_loss: 1.3; train_accuracy: 0.64 

0.00013112550368532538
0.00012574622815009207
Batch: 0; loss: 1.33; acc: 0.58
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 0.98; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.84
Batch: 100; loss: 1.31; acc: 0.58
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.28; acc: 0.59
Val Epoch over. val_loss: 1.2356079721906383; val_accuracy: 0.6740644904458599 

The current subspace-distance is: 0.00012574622815009207 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.75
Batch: 20; loss: 1.31; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.42; acc: 0.56
Batch: 80; loss: 1.17; acc: 0.67
Batch: 100; loss: 1.25; acc: 0.62
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 1.25; acc: 0.75
Batch: 160; loss: 1.25; acc: 0.67
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.24; acc: 0.67
Batch: 220; loss: 1.25; acc: 0.72
Batch: 240; loss: 1.35; acc: 0.55
Batch: 260; loss: 1.34; acc: 0.59
Batch: 280; loss: 1.25; acc: 0.62
Batch: 300; loss: 1.31; acc: 0.61
Batch: 320; loss: 1.25; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.55
Batch: 380; loss: 1.37; acc: 0.58
Batch: 400; loss: 1.32; acc: 0.64
Batch: 420; loss: 1.22; acc: 0.69
Batch: 440; loss: 1.31; acc: 0.61
Batch: 460; loss: 1.27; acc: 0.64
Batch: 480; loss: 1.28; acc: 0.59
Batch: 500; loss: 1.38; acc: 0.59
Batch: 520; loss: 1.16; acc: 0.72
Batch: 540; loss: 1.38; acc: 0.56
Batch: 560; loss: 1.42; acc: 0.62
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.24; acc: 0.64
Batch: 640; loss: 1.22; acc: 0.67
Batch: 660; loss: 1.34; acc: 0.66
Batch: 680; loss: 1.36; acc: 0.53
Batch: 700; loss: 1.23; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.69
Batch: 740; loss: 1.3; acc: 0.58
Batch: 760; loss: 1.18; acc: 0.64
Batch: 780; loss: 1.44; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

0.0001437316823285073
0.00013966165715828538
Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.94; acc: 0.73
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 0.98; acc: 0.84
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.21; acc: 0.62
Val Epoch over. val_loss: 1.1911759163923323; val_accuracy: 0.6886942675159236 

The current subspace-distance is: 0.00013966165715828538 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 1.1; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.2; acc: 0.69
Batch: 100; loss: 1.04; acc: 0.81
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 1.17; acc: 0.62
Batch: 160; loss: 1.39; acc: 0.53
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.32; acc: 0.59
Batch: 220; loss: 1.22; acc: 0.64
Batch: 240; loss: 1.35; acc: 0.59
Batch: 260; loss: 1.26; acc: 0.66
Batch: 280; loss: 1.29; acc: 0.64
Batch: 300; loss: 1.27; acc: 0.61
Batch: 320; loss: 1.29; acc: 0.69
Batch: 340; loss: 1.14; acc: 0.73
Batch: 360; loss: 1.42; acc: 0.67
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.28; acc: 0.58
Batch: 420; loss: 1.28; acc: 0.53
Batch: 440; loss: 1.42; acc: 0.58
Batch: 460; loss: 1.18; acc: 0.66
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 1.28; acc: 0.67
Batch: 520; loss: 1.17; acc: 0.67
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.26; acc: 0.69
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.31; acc: 0.64
Batch: 640; loss: 1.22; acc: 0.64
Batch: 660; loss: 1.28; acc: 0.67
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.09; acc: 0.67
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.7
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.000153136279550381
0.00014838487550150603
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.37; acc: 0.58
Batch: 40; loss: 0.91; acc: 0.77
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.16; acc: 0.62
Val Epoch over. val_loss: 1.1557584294847623; val_accuracy: 0.6979498407643312 

The current subspace-distance is: 0.00014838487550150603 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.19; acc: 0.75
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.77
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.14; acc: 0.69
Batch: 120; loss: 1.11; acc: 0.77
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.2; acc: 0.7
Batch: 180; loss: 1.16; acc: 0.61
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.26; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.72
Batch: 260; loss: 1.16; acc: 0.72
Batch: 280; loss: 1.37; acc: 0.56
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 1.09; acc: 0.75
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.17; acc: 0.73
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.25; acc: 0.62
Batch: 420; loss: 1.36; acc: 0.62
Batch: 440; loss: 1.04; acc: 0.77
Batch: 460; loss: 1.25; acc: 0.69
Batch: 480; loss: 1.34; acc: 0.55
Batch: 500; loss: 1.26; acc: 0.69
Batch: 520; loss: 1.2; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.66
Batch: 560; loss: 1.26; acc: 0.69
Batch: 580; loss: 1.29; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.66
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 1.1; acc: 0.72
Batch: 660; loss: 1.09; acc: 0.75
Batch: 680; loss: 1.03; acc: 0.72
Batch: 700; loss: 1.18; acc: 0.73
Batch: 720; loss: 1.21; acc: 0.69
Batch: 740; loss: 1.3; acc: 0.67
Batch: 760; loss: 1.16; acc: 0.72
Batch: 780; loss: 1.27; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.67 

0.00016272220818791538
0.00015763059491291642
Batch: 0; loss: 1.22; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.67
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 1.08; acc: 0.66
Val Epoch over. val_loss: 1.1081036340658832; val_accuracy: 0.7109872611464968 

The current subspace-distance is: 0.00015763059491291642 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.16; acc: 0.72
Batch: 200; loss: 1.13; acc: 0.73
Batch: 220; loss: 1.14; acc: 0.64
Batch: 240; loss: 1.19; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.73
Batch: 280; loss: 1.14; acc: 0.73
Batch: 300; loss: 1.15; acc: 0.72
Batch: 320; loss: 1.03; acc: 0.78
Batch: 340; loss: 1.15; acc: 0.67
Batch: 360; loss: 1.27; acc: 0.67
Batch: 380; loss: 1.2; acc: 0.66
Batch: 400; loss: 1.13; acc: 0.69
Batch: 420; loss: 1.26; acc: 0.64
Batch: 440; loss: 1.09; acc: 0.73
Batch: 460; loss: 1.0; acc: 0.75
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.17; acc: 0.69
Batch: 520; loss: 1.02; acc: 0.7
Batch: 540; loss: 1.25; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.73
Batch: 580; loss: 0.99; acc: 0.73
Batch: 600; loss: 1.13; acc: 0.72
Batch: 620; loss: 1.11; acc: 0.67
Batch: 640; loss: 1.0; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.73
Batch: 680; loss: 1.26; acc: 0.66
Batch: 700; loss: 1.14; acc: 0.73
Batch: 720; loss: 1.26; acc: 0.67
Batch: 740; loss: 1.27; acc: 0.59
Batch: 760; loss: 1.23; acc: 0.67
Batch: 780; loss: 1.25; acc: 0.69
Train Epoch over. train_loss: 1.16; train_accuracy: 0.68 

0.00016780110308900476
0.0001605467841727659
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.72
Val Epoch over. val_loss: 1.0999591536582656; val_accuracy: 0.711484872611465 

The current subspace-distance is: 0.0001605467841727659 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.2; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.64
Batch: 200; loss: 1.11; acc: 0.77
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 1.49; acc: 0.48
Batch: 260; loss: 1.13; acc: 0.75
Batch: 280; loss: 1.29; acc: 0.56
Batch: 300; loss: 1.28; acc: 0.66
Batch: 320; loss: 1.23; acc: 0.66
Batch: 340; loss: 1.27; acc: 0.61
Batch: 360; loss: 0.99; acc: 0.8
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.19; acc: 0.66
Batch: 440; loss: 1.17; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.62
Batch: 480; loss: 1.3; acc: 0.61
Batch: 500; loss: 1.31; acc: 0.5
Batch: 520; loss: 1.16; acc: 0.66
Batch: 540; loss: 1.29; acc: 0.61
Batch: 560; loss: 1.24; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.7
Batch: 600; loss: 1.09; acc: 0.72
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 1.11; acc: 0.62
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.19; acc: 0.66
Batch: 700; loss: 1.16; acc: 0.62
Batch: 720; loss: 1.23; acc: 0.67
Batch: 740; loss: 1.37; acc: 0.5
Batch: 760; loss: 1.19; acc: 0.72
Batch: 780; loss: 1.23; acc: 0.69
Train Epoch over. train_loss: 1.15; train_accuracy: 0.69 

0.00016919340123422444
0.00016359565779566765
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.67
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.03; acc: 0.73
Val Epoch over. val_loss: 1.0885871037556107; val_accuracy: 0.7153662420382165 

The current subspace-distance is: 0.00016359565779566765 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 1.17; acc: 0.69
Batch: 60; loss: 1.1; acc: 0.59
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 0.98; acc: 0.75
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 0.98; acc: 0.77
Batch: 240; loss: 1.35; acc: 0.62
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 1.27; acc: 0.62
Batch: 300; loss: 1.37; acc: 0.61
Batch: 320; loss: 1.11; acc: 0.72
Batch: 340; loss: 1.31; acc: 0.59
Batch: 360; loss: 1.03; acc: 0.78
Batch: 380; loss: 1.15; acc: 0.72
Batch: 400; loss: 1.02; acc: 0.73
Batch: 420; loss: 1.17; acc: 0.62
Batch: 440; loss: 1.26; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.7
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.17; acc: 0.75
Batch: 560; loss: 1.06; acc: 0.75
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 1.1; acc: 0.77
Batch: 620; loss: 1.29; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.56
Batch: 660; loss: 0.83; acc: 0.81
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 1.16; acc: 0.66
Batch: 720; loss: 1.23; acc: 0.72
Batch: 740; loss: 1.02; acc: 0.78
Batch: 760; loss: 1.19; acc: 0.62
Batch: 780; loss: 1.07; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.69 

0.0001770279777701944
0.00016986523405648768
Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.69
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.75
Val Epoch over. val_loss: 1.0601227522655656; val_accuracy: 0.7286027070063694 

The current subspace-distance is: 0.00016986523405648768 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.04; acc: 0.75
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 1.22; acc: 0.58
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.16; acc: 0.72
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 1.17; acc: 0.61
Batch: 160; loss: 1.14; acc: 0.75
Batch: 180; loss: 1.12; acc: 0.64
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 1.24; acc: 0.61
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.23; acc: 0.59
Batch: 320; loss: 1.01; acc: 0.75
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 0.94; acc: 0.77
Batch: 420; loss: 1.13; acc: 0.62
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.07; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 1.2; acc: 0.67
Batch: 520; loss: 1.13; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.58
Batch: 560; loss: 1.26; acc: 0.56
Batch: 580; loss: 0.96; acc: 0.77
Batch: 600; loss: 1.13; acc: 0.66
Batch: 620; loss: 1.22; acc: 0.66
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 1.17; acc: 0.73
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.7
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.1; acc: 0.64
Batch: 780; loss: 1.13; acc: 0.72
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.00017749039398040622
0.0001696280960459262
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 0.88; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.75
Val Epoch over. val_loss: 1.0633537404856104; val_accuracy: 0.7241242038216561 

The current subspace-distance is: 0.0001696280960459262 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.72
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.66
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.1; acc: 0.72
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 0.87; acc: 0.78
Batch: 200; loss: 1.33; acc: 0.61
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.2; acc: 0.58
Batch: 280; loss: 1.12; acc: 0.72
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.69
Batch: 340; loss: 1.21; acc: 0.67
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.4; acc: 0.58
Batch: 400; loss: 1.01; acc: 0.73
Batch: 420; loss: 1.25; acc: 0.67
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 1.08; acc: 0.7
Batch: 480; loss: 1.2; acc: 0.64
Batch: 500; loss: 1.09; acc: 0.73
Batch: 520; loss: 0.95; acc: 0.81
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.04; acc: 0.67
Batch: 580; loss: 1.12; acc: 0.72
Batch: 600; loss: 1.17; acc: 0.66
Batch: 620; loss: 1.22; acc: 0.72
Batch: 640; loss: 0.96; acc: 0.77
Batch: 660; loss: 1.09; acc: 0.7
Batch: 680; loss: 1.05; acc: 0.75
Batch: 700; loss: 1.12; acc: 0.72
Batch: 720; loss: 1.01; acc: 0.75
Batch: 740; loss: 1.27; acc: 0.64
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.04; acc: 0.77
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00017948602908290923
0.00017330606351606548
Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.28; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 0.86; acc: 0.78
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 1.0393936892223965; val_accuracy: 0.7331807324840764 

The current subspace-distance is: 0.00017330606351606548 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 1.27; acc: 0.58
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.01; acc: 0.75
Batch: 200; loss: 0.93; acc: 0.77
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.31; acc: 0.58
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.05; acc: 0.73
Batch: 300; loss: 1.12; acc: 0.69
Batch: 320; loss: 0.92; acc: 0.8
Batch: 340; loss: 1.32; acc: 0.58
Batch: 360; loss: 1.09; acc: 0.67
Batch: 380; loss: 0.94; acc: 0.78
Batch: 400; loss: 1.08; acc: 0.66
Batch: 420; loss: 1.18; acc: 0.61
Batch: 440; loss: 1.18; acc: 0.64
Batch: 460; loss: 1.08; acc: 0.7
Batch: 480; loss: 1.28; acc: 0.59
Batch: 500; loss: 0.99; acc: 0.75
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 1.22; acc: 0.67
Batch: 560; loss: 1.02; acc: 0.75
Batch: 580; loss: 1.02; acc: 0.83
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 1.02; acc: 0.7
Batch: 640; loss: 1.13; acc: 0.73
Batch: 660; loss: 1.07; acc: 0.75
Batch: 680; loss: 1.16; acc: 0.73
Batch: 700; loss: 1.08; acc: 0.62
Batch: 720; loss: 1.29; acc: 0.62
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 1.08; acc: 0.7
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.00018232045113109052
0.00017458428919781
Batch: 0; loss: 1.16; acc: 0.67
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.67
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.59
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.040807276774364; val_accuracy: 0.7284036624203821 

The current subspace-distance is: 0.00017458428919781 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.66
Batch: 80; loss: 1.14; acc: 0.64
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 1.0; acc: 0.75
Batch: 160; loss: 1.05; acc: 0.72
Batch: 180; loss: 1.17; acc: 0.78
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.01; acc: 0.8
Batch: 240; loss: 1.13; acc: 0.62
Batch: 260; loss: 1.13; acc: 0.69
Batch: 280; loss: 1.19; acc: 0.62
Batch: 300; loss: 1.13; acc: 0.7
Batch: 320; loss: 0.98; acc: 0.81
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 1.08; acc: 0.73
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 1.09; acc: 0.69
Batch: 420; loss: 0.98; acc: 0.77
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 0.96; acc: 0.77
Batch: 500; loss: 1.2; acc: 0.66
Batch: 520; loss: 0.89; acc: 0.77
Batch: 540; loss: 1.31; acc: 0.61
Batch: 560; loss: 1.01; acc: 0.69
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.33; acc: 0.58
Batch: 640; loss: 1.06; acc: 0.77
Batch: 660; loss: 1.16; acc: 0.67
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 1.17; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.62
Batch: 740; loss: 0.92; acc: 0.8
Batch: 760; loss: 1.19; acc: 0.64
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.7 

0.00018503966566640884
0.00018035212997347116
Batch: 0; loss: 1.14; acc: 0.69
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.59
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.019045104646379; val_accuracy: 0.7407444267515924 

The current subspace-distance is: 0.00018035212997347116 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 0.98; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.66
Batch: 140; loss: 1.12; acc: 0.64
Batch: 160; loss: 1.03; acc: 0.72
Batch: 180; loss: 1.1; acc: 0.62
Batch: 200; loss: 1.09; acc: 0.7
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 0.91; acc: 0.84
Batch: 260; loss: 1.21; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 1.01; acc: 0.81
Batch: 360; loss: 1.17; acc: 0.64
Batch: 380; loss: 1.12; acc: 0.7
Batch: 400; loss: 0.89; acc: 0.81
Batch: 420; loss: 1.19; acc: 0.66
Batch: 440; loss: 1.22; acc: 0.61
Batch: 460; loss: 1.08; acc: 0.72
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.21; acc: 0.56
Batch: 540; loss: 1.09; acc: 0.78
Batch: 560; loss: 1.03; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.73
Batch: 600; loss: 1.11; acc: 0.67
Batch: 620; loss: 1.04; acc: 0.69
Batch: 640; loss: 0.95; acc: 0.8
Batch: 660; loss: 1.32; acc: 0.67
Batch: 680; loss: 1.23; acc: 0.58
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 1.14; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 0.9; acc: 0.84
Batch: 780; loss: 1.05; acc: 0.73
Train Epoch over. train_loss: 1.08; train_accuracy: 0.7 

0.0001933476742124185
0.00018448702758178115
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 0.82; acc: 0.81
Batch: 100; loss: 1.01; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 0.88; acc: 0.83
Val Epoch over. val_loss: 1.0085616760952458; val_accuracy: 0.7384554140127388 

The current subspace-distance is: 0.00018448702758178115 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 1.11; acc: 0.69
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.69
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 0.98; acc: 0.73
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 0.99; acc: 0.69
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.01; acc: 0.8
Batch: 240; loss: 0.95; acc: 0.73
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.07; acc: 0.75
Batch: 300; loss: 1.31; acc: 0.56
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.09; acc: 0.72
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 0.97; acc: 0.75
Batch: 400; loss: 0.93; acc: 0.83
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 0.84; acc: 0.78
Batch: 480; loss: 1.24; acc: 0.66
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 0.85; acc: 0.77
Batch: 540; loss: 1.15; acc: 0.72
Batch: 560; loss: 1.12; acc: 0.67
Batch: 580; loss: 1.09; acc: 0.69
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.67
Batch: 640; loss: 1.16; acc: 0.66
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 1.0; acc: 0.75
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 1.0; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 1.27; acc: 0.59
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.00019763810269068927
0.0001878973562270403
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.84
Val Epoch over. val_loss: 0.9938640059179561; val_accuracy: 0.7500995222929936 

The current subspace-distance is: 0.0001878973562270403 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.01; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.72
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 1.2; acc: 0.55
Batch: 160; loss: 1.13; acc: 0.75
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 1.25; acc: 0.62
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 1.06; acc: 0.7
Batch: 260; loss: 0.99; acc: 0.75
Batch: 280; loss: 1.06; acc: 0.75
Batch: 300; loss: 1.08; acc: 0.75
Batch: 320; loss: 0.93; acc: 0.78
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 1.03; acc: 0.75
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.81
Batch: 420; loss: 1.01; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.08; acc: 0.73
Batch: 480; loss: 0.89; acc: 0.78
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.73
Batch: 540; loss: 1.32; acc: 0.56
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 1.04; acc: 0.8
Batch: 600; loss: 0.98; acc: 0.7
Batch: 620; loss: 1.21; acc: 0.61
Batch: 640; loss: 0.99; acc: 0.72
Batch: 660; loss: 1.01; acc: 0.7
Batch: 680; loss: 1.07; acc: 0.78
Batch: 700; loss: 1.0; acc: 0.75
Batch: 720; loss: 1.09; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.73
Batch: 760; loss: 1.03; acc: 0.73
Batch: 780; loss: 1.29; acc: 0.67
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00019799252913799137
0.00019179728406015784
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.83
Val Epoch over. val_loss: 0.9950660619006795; val_accuracy: 0.7429339171974523 

The current subspace-distance is: 0.00019179728406015784 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.89; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.78
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 1.1; acc: 0.7
Batch: 160; loss: 1.16; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.67
Batch: 200; loss: 1.15; acc: 0.58
Batch: 220; loss: 1.04; acc: 0.77
Batch: 240; loss: 0.95; acc: 0.78
Batch: 260; loss: 1.02; acc: 0.67
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 1.24; acc: 0.69
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.0; acc: 0.73
Batch: 360; loss: 0.93; acc: 0.8
Batch: 380; loss: 1.08; acc: 0.69
Batch: 400; loss: 1.06; acc: 0.67
Batch: 420; loss: 1.16; acc: 0.67
Batch: 440; loss: 1.0; acc: 0.72
Batch: 460; loss: 0.84; acc: 0.8
Batch: 480; loss: 1.08; acc: 0.66
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 1.11; acc: 0.73
Batch: 540; loss: 1.21; acc: 0.62
Batch: 560; loss: 1.05; acc: 0.77
Batch: 580; loss: 1.21; acc: 0.62
Batch: 600; loss: 0.96; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.23; acc: 0.58
Batch: 680; loss: 1.2; acc: 0.72
Batch: 700; loss: 0.92; acc: 0.81
Batch: 720; loss: 0.92; acc: 0.73
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.75
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00020178957493044436
0.0001932179438881576
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 0.9846817080382329; val_accuracy: 0.7500995222929936 

The current subspace-distance is: 0.0001932179438881576 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 1.12; acc: 0.66
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 1.21; acc: 0.64
Batch: 280; loss: 1.09; acc: 0.7
Batch: 300; loss: 1.15; acc: 0.72
Batch: 320; loss: 1.08; acc: 0.64
Batch: 340; loss: 0.88; acc: 0.78
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 0.96; acc: 0.75
Batch: 400; loss: 0.92; acc: 0.8
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 1.02; acc: 0.75
Batch: 460; loss: 1.05; acc: 0.75
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 0.96; acc: 0.77
Batch: 520; loss: 1.19; acc: 0.67
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.01; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.67
Batch: 600; loss: 1.28; acc: 0.56
Batch: 620; loss: 1.0; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.67
Batch: 680; loss: 1.15; acc: 0.72
Batch: 700; loss: 0.97; acc: 0.75
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.01; acc: 0.72
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.0001959218643605709
0.00018711324082687497
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.69
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.59
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 0.9833458699997822; val_accuracy: 0.742734872611465 

The current subspace-distance is: 0.00018711324082687497 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.0; acc: 0.73
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 1.05; acc: 0.69
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 1.0; acc: 0.73
Batch: 220; loss: 1.0; acc: 0.67
Batch: 240; loss: 1.15; acc: 0.62
Batch: 260; loss: 1.03; acc: 0.7
Batch: 280; loss: 0.95; acc: 0.7
Batch: 300; loss: 1.14; acc: 0.62
Batch: 320; loss: 0.97; acc: 0.77
Batch: 340; loss: 0.83; acc: 0.83
Batch: 360; loss: 1.04; acc: 0.7
Batch: 380; loss: 1.1; acc: 0.64
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 0.94; acc: 0.8
Batch: 440; loss: 1.0; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.03; acc: 0.73
Batch: 520; loss: 0.89; acc: 0.84
Batch: 540; loss: 1.08; acc: 0.72
Batch: 560; loss: 1.17; acc: 0.67
Batch: 580; loss: 1.17; acc: 0.66
Batch: 600; loss: 1.1; acc: 0.64
Batch: 620; loss: 0.95; acc: 0.8
Batch: 640; loss: 1.13; acc: 0.64
Batch: 660; loss: 1.16; acc: 0.62
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 1.18; acc: 0.64
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 1.2; acc: 0.64
Batch: 760; loss: 0.96; acc: 0.77
Batch: 780; loss: 0.94; acc: 0.8
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00020006147678941488
0.00019216412329114974
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.84
Val Epoch over. val_loss: 0.9721729622525015; val_accuracy: 0.7507961783439491 

The current subspace-distance is: 0.00019216412329114974 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 1.25; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.81
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.75
Batch: 160; loss: 0.99; acc: 0.77
Batch: 180; loss: 1.2; acc: 0.69
Batch: 200; loss: 1.28; acc: 0.58
Batch: 220; loss: 1.18; acc: 0.62
Batch: 240; loss: 1.06; acc: 0.72
Batch: 260; loss: 0.97; acc: 0.69
Batch: 280; loss: 1.22; acc: 0.66
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.01; acc: 0.73
Batch: 340; loss: 0.94; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.06; acc: 0.72
Batch: 400; loss: 0.99; acc: 0.72
Batch: 420; loss: 0.99; acc: 0.75
Batch: 440; loss: 1.01; acc: 0.73
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 1.04; acc: 0.73
Batch: 500; loss: 1.11; acc: 0.7
Batch: 520; loss: 1.07; acc: 0.77
Batch: 540; loss: 1.03; acc: 0.7
Batch: 560; loss: 1.11; acc: 0.7
Batch: 580; loss: 0.88; acc: 0.77
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 1.02; acc: 0.69
Batch: 640; loss: 0.92; acc: 0.81
Batch: 660; loss: 1.16; acc: 0.62
Batch: 680; loss: 0.95; acc: 0.75
Batch: 700; loss: 1.13; acc: 0.62
Batch: 720; loss: 1.02; acc: 0.73
Batch: 740; loss: 0.94; acc: 0.78
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00020230119116604328
0.00019268944743089378
Batch: 0; loss: 1.11; acc: 0.69
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.79; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.86
Val Epoch over. val_loss: 0.9752723058317877; val_accuracy: 0.7463176751592356 

The current subspace-distance is: 0.00019268944743089378 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 0.92; acc: 0.83
Batch: 140; loss: 1.21; acc: 0.69
Batch: 160; loss: 1.02; acc: 0.67
Batch: 180; loss: 1.05; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.64
Batch: 220; loss: 1.01; acc: 0.8
Batch: 240; loss: 1.0; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.72
Batch: 280; loss: 1.1; acc: 0.59
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.97; acc: 0.73
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 0.96; acc: 0.77
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 1.26; acc: 0.64
Batch: 460; loss: 0.85; acc: 0.84
Batch: 480; loss: 1.0; acc: 0.78
Batch: 500; loss: 0.91; acc: 0.78
Batch: 520; loss: 1.04; acc: 0.73
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 1.15; acc: 0.64
Batch: 580; loss: 0.97; acc: 0.73
Batch: 600; loss: 0.94; acc: 0.81
Batch: 620; loss: 1.07; acc: 0.7
Batch: 640; loss: 1.06; acc: 0.64
Batch: 660; loss: 1.03; acc: 0.73
Batch: 680; loss: 1.09; acc: 0.67
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 1.05; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.66
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00020155159290879965
0.00019753589003812522
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.84
Batch: 100; loss: 0.96; acc: 0.78
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.84
Val Epoch over. val_loss: 0.9621150892251616; val_accuracy: 0.7517914012738853 

The current subspace-distance is: 0.00019753589003812522 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.98; acc: 0.75
Batch: 20; loss: 1.03; acc: 0.7
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.03; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.58
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 0.98; acc: 0.72
Batch: 200; loss: 1.05; acc: 0.75
Batch: 220; loss: 1.04; acc: 0.7
Batch: 240; loss: 1.24; acc: 0.61
Batch: 260; loss: 1.07; acc: 0.73
Batch: 280; loss: 1.02; acc: 0.83
Batch: 300; loss: 0.87; acc: 0.77
Batch: 320; loss: 1.01; acc: 0.73
Batch: 340; loss: 0.95; acc: 0.77
Batch: 360; loss: 0.98; acc: 0.7
Batch: 380; loss: 1.03; acc: 0.72
Batch: 400; loss: 0.97; acc: 0.7
Batch: 420; loss: 0.88; acc: 0.75
Batch: 440; loss: 0.99; acc: 0.72
Batch: 460; loss: 1.19; acc: 0.66
Batch: 480; loss: 0.93; acc: 0.77
Batch: 500; loss: 1.18; acc: 0.64
Batch: 520; loss: 0.99; acc: 0.8
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.05; acc: 0.75
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 1.05; acc: 0.78
Batch: 640; loss: 0.95; acc: 0.72
Batch: 660; loss: 1.09; acc: 0.7
Batch: 680; loss: 1.04; acc: 0.75
Batch: 700; loss: 1.06; acc: 0.67
Batch: 720; loss: 1.21; acc: 0.52
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 1.05; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.62
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.00020431142183952034
0.00019641667313408107
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 0.85; acc: 0.84
Val Epoch over. val_loss: 0.9818526703840608; val_accuracy: 0.7442277070063694 

The current subspace-distance is: 0.00019641667313408107 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.67
Batch: 20; loss: 1.03; acc: 0.66
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.14; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 1.1; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.53
Batch: 180; loss: 1.07; acc: 0.73
Batch: 200; loss: 1.15; acc: 0.64
Batch: 220; loss: 0.96; acc: 0.73
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 1.05; acc: 0.69
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 1.04; acc: 0.75
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 0.99; acc: 0.73
Batch: 380; loss: 0.98; acc: 0.67
Batch: 400; loss: 1.03; acc: 0.72
Batch: 420; loss: 1.2; acc: 0.66
Batch: 440; loss: 0.99; acc: 0.73
Batch: 460; loss: 0.97; acc: 0.78
Batch: 480; loss: 1.21; acc: 0.59
Batch: 500; loss: 0.98; acc: 0.77
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 1.0; acc: 0.72
Batch: 560; loss: 0.94; acc: 0.8
Batch: 580; loss: 1.03; acc: 0.77
Batch: 600; loss: 1.08; acc: 0.72
Batch: 620; loss: 1.03; acc: 0.73
Batch: 640; loss: 1.02; acc: 0.72
Batch: 660; loss: 0.88; acc: 0.81
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 0.92; acc: 0.73
Batch: 720; loss: 1.02; acc: 0.7
Batch: 740; loss: 0.96; acc: 0.75
Batch: 760; loss: 1.0; acc: 0.75
Batch: 780; loss: 1.02; acc: 0.7
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.00020598183618858457
0.00020016348571516573
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.95; acc: 0.78
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.86
Val Epoch over. val_loss: 0.9665690288422214; val_accuracy: 0.7509952229299363 

The current subspace-distance is: 0.00020016348571516573 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.89; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.73
Batch: 80; loss: 1.12; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 1.18; acc: 0.66
Batch: 160; loss: 1.02; acc: 0.72
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.24; acc: 0.64
Batch: 220; loss: 0.94; acc: 0.8
Batch: 240; loss: 0.95; acc: 0.77
Batch: 260; loss: 1.39; acc: 0.56
Batch: 280; loss: 1.16; acc: 0.58
Batch: 300; loss: 1.22; acc: 0.67
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 0.92; acc: 0.75
Batch: 380; loss: 1.02; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.14; acc: 0.72
Batch: 440; loss: 1.18; acc: 0.69
Batch: 460; loss: 1.0; acc: 0.66
Batch: 480; loss: 0.95; acc: 0.75
Batch: 500; loss: 0.92; acc: 0.78
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 1.08; acc: 0.64
Batch: 600; loss: 1.21; acc: 0.61
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 0.89; acc: 0.78
Batch: 660; loss: 1.11; acc: 0.64
Batch: 680; loss: 1.09; acc: 0.69
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 1.09; acc: 0.67
Batch: 780; loss: 0.96; acc: 0.8
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.000203507297555916
0.00019464455544948578
Batch: 0; loss: 1.11; acc: 0.7
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 0.84; acc: 0.81
Val Epoch over. val_loss: 0.9635561021270266; val_accuracy: 0.7479100318471338 

The current subspace-distance is: 0.00019464455544948578 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 1.04; acc: 0.72
Batch: 180; loss: 0.92; acc: 0.83
Batch: 200; loss: 1.13; acc: 0.67
Batch: 220; loss: 0.92; acc: 0.78
Batch: 240; loss: 1.05; acc: 0.7
Batch: 260; loss: 1.13; acc: 0.72
Batch: 280; loss: 1.24; acc: 0.62
Batch: 300; loss: 1.05; acc: 0.64
Batch: 320; loss: 0.92; acc: 0.8
Batch: 340; loss: 0.82; acc: 0.83
Batch: 360; loss: 0.97; acc: 0.75
Batch: 380; loss: 0.87; acc: 0.81
Batch: 400; loss: 1.1; acc: 0.62
Batch: 420; loss: 1.13; acc: 0.72
Batch: 440; loss: 1.14; acc: 0.62
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 0.94; acc: 0.8
Batch: 540; loss: 1.13; acc: 0.69
Batch: 560; loss: 1.04; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.7
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 0.95; acc: 0.73
Batch: 640; loss: 0.98; acc: 0.7
Batch: 660; loss: 1.02; acc: 0.72
Batch: 680; loss: 1.05; acc: 0.77
Batch: 700; loss: 1.07; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.13; acc: 0.77
Batch: 780; loss: 1.21; acc: 0.62
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.0002098226686939597
0.00020168867195025086
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.91
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 0.82; acc: 0.86
Val Epoch over. val_loss: 0.9566788639232611; val_accuracy: 0.7536823248407644 

The current subspace-distance is: 0.00020168867195025086 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 1.09; acc: 0.67
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.77
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 1.08; acc: 0.67
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.87; acc: 0.8
Batch: 260; loss: 1.06; acc: 0.7
Batch: 280; loss: 1.05; acc: 0.72
Batch: 300; loss: 1.12; acc: 0.66
Batch: 320; loss: 1.07; acc: 0.67
Batch: 340; loss: 1.12; acc: 0.62
Batch: 360; loss: 1.17; acc: 0.67
Batch: 380; loss: 0.97; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.72
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.0; acc: 0.81
Batch: 460; loss: 0.89; acc: 0.81
Batch: 480; loss: 1.08; acc: 0.75
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 0.93; acc: 0.78
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 0.97; acc: 0.78
Batch: 580; loss: 1.06; acc: 0.69
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 1.07; acc: 0.66
Batch: 640; loss: 1.22; acc: 0.62
Batch: 660; loss: 0.95; acc: 0.77
Batch: 680; loss: 0.94; acc: 0.77
Batch: 700; loss: 0.93; acc: 0.75
Batch: 720; loss: 1.01; acc: 0.72
Batch: 740; loss: 1.0; acc: 0.75
Batch: 760; loss: 1.06; acc: 0.7
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.00020773362484760582
0.00020018522627651691
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.73; acc: 0.88
Batch: 60; loss: 0.97; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.95; acc: 0.78
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.83; acc: 0.84
Val Epoch over. val_loss: 0.9531970016515938; val_accuracy: 0.7541799363057324 

The current subspace-distance is: 0.00020018522627651691 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.45

The number of parameters is: 273705

The number of individual parameters is:

12
216
12
12
18
37584
18
18
35
109620
35
35
64
120960
64
64
4096
64
640
10
64
64

nonzero elements in E: 54740996
elements in E: 54741000
fraction nonzero: 0.9999999269286275
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.51; acc: 0.09
Batch: 20; loss: 2.18; acc: 0.27
Batch: 40; loss: 2.06; acc: 0.27
Batch: 60; loss: 1.94; acc: 0.44
Batch: 80; loss: 1.89; acc: 0.41
Batch: 100; loss: 1.83; acc: 0.42
Batch: 120; loss: 1.88; acc: 0.39
Batch: 140; loss: 1.64; acc: 0.61
Batch: 160; loss: 1.7; acc: 0.53
Batch: 180; loss: 1.68; acc: 0.45
Batch: 200; loss: 1.69; acc: 0.55
Batch: 220; loss: 1.71; acc: 0.53
Batch: 240; loss: 1.63; acc: 0.58
Batch: 260; loss: 1.69; acc: 0.53
Batch: 280; loss: 1.65; acc: 0.55
Batch: 300; loss: 1.46; acc: 0.69
Batch: 320; loss: 1.52; acc: 0.7
Batch: 340; loss: 1.5; acc: 0.62
Batch: 360; loss: 1.54; acc: 0.59
Batch: 380; loss: 1.43; acc: 0.73
Batch: 400; loss: 1.38; acc: 0.8
Batch: 420; loss: 1.62; acc: 0.58
Batch: 440; loss: 1.51; acc: 0.67
Batch: 460; loss: 1.4; acc: 0.75
Batch: 480; loss: 1.39; acc: 0.75
Batch: 500; loss: 1.45; acc: 0.67
Batch: 520; loss: 1.57; acc: 0.58
Batch: 540; loss: 1.41; acc: 0.8
Batch: 560; loss: 1.38; acc: 0.78
Batch: 580; loss: 1.5; acc: 0.64
Batch: 600; loss: 1.47; acc: 0.69
Batch: 620; loss: 1.42; acc: 0.69
Batch: 640; loss: 1.42; acc: 0.72
Batch: 660; loss: 1.5; acc: 0.62
Batch: 680; loss: 1.37; acc: 0.66
Batch: 700; loss: 1.41; acc: 0.64
Batch: 720; loss: 1.51; acc: 0.64
Batch: 740; loss: 1.45; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.88
Batch: 780; loss: 1.43; acc: 0.73
Train Epoch over. train_loss: 1.6; train_accuracy: 0.6 

6.176333408802748e-05
5.69175390410237e-05
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.41; acc: 0.7
Batch: 40; loss: 1.07; acc: 0.83
Batch: 60; loss: 1.25; acc: 0.78
Batch: 80; loss: 1.18; acc: 0.84
Batch: 100; loss: 1.41; acc: 0.78
Batch: 120; loss: 1.45; acc: 0.66
Batch: 140; loss: 1.15; acc: 0.88
Val Epoch over. val_loss: 1.354482088878656; val_accuracy: 0.7357683121019108 

The current subspace-distance is: 5.69175390410237e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.66
Batch: 20; loss: 1.44; acc: 0.67
Batch: 40; loss: 1.36; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.73
Batch: 80; loss: 1.33; acc: 0.72
Batch: 100; loss: 1.39; acc: 0.77
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 1.39; acc: 0.66
Batch: 160; loss: 1.47; acc: 0.62
Batch: 180; loss: 1.3; acc: 0.72
Batch: 200; loss: 1.31; acc: 0.73
Batch: 220; loss: 1.27; acc: 0.77
Batch: 240; loss: 1.45; acc: 0.69
Batch: 260; loss: 1.29; acc: 0.78
Batch: 280; loss: 1.26; acc: 0.78
Batch: 300; loss: 1.41; acc: 0.62
Batch: 320; loss: 1.33; acc: 0.72
Batch: 340; loss: 1.36; acc: 0.77
Batch: 360; loss: 1.23; acc: 0.8
Batch: 380; loss: 1.14; acc: 0.86
Batch: 400; loss: 1.38; acc: 0.69
Batch: 420; loss: 1.28; acc: 0.75
Batch: 440; loss: 1.24; acc: 0.77
Batch: 460; loss: 1.28; acc: 0.7
Batch: 480; loss: 1.27; acc: 0.75
Batch: 500; loss: 1.24; acc: 0.8
Batch: 520; loss: 1.32; acc: 0.72
Batch: 540; loss: 1.34; acc: 0.75
Batch: 560; loss: 1.3; acc: 0.75
Batch: 580; loss: 1.26; acc: 0.75
Batch: 600; loss: 1.26; acc: 0.81
Batch: 620; loss: 1.19; acc: 0.81
Batch: 640; loss: 1.27; acc: 0.64
Batch: 660; loss: 1.18; acc: 0.83
Batch: 680; loss: 1.33; acc: 0.7
Batch: 700; loss: 1.25; acc: 0.73
Batch: 720; loss: 1.22; acc: 0.73
Batch: 740; loss: 1.28; acc: 0.73
Batch: 760; loss: 1.22; acc: 0.73
Batch: 780; loss: 1.41; acc: 0.64
Train Epoch over. train_loss: 1.33; train_accuracy: 0.72 

8.208358485717326e-05
7.796670979587361e-05
Batch: 0; loss: 1.22; acc: 0.75
Batch: 20; loss: 1.31; acc: 0.7
Batch: 40; loss: 0.97; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.81
Batch: 80; loss: 1.04; acc: 0.8
Batch: 100; loss: 1.3; acc: 0.72
Batch: 120; loss: 1.37; acc: 0.64
Batch: 140; loss: 1.05; acc: 0.84
Val Epoch over. val_loss: 1.2242415490423797; val_accuracy: 0.759952229299363 

The current subspace-distance is: 7.796670979587361e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.75
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.75
Batch: 80; loss: 1.18; acc: 0.81
Batch: 100; loss: 1.21; acc: 0.81
Batch: 120; loss: 1.27; acc: 0.73
Batch: 140; loss: 1.02; acc: 0.86
Batch: 160; loss: 1.19; acc: 0.8
Batch: 180; loss: 1.25; acc: 0.73
Batch: 200; loss: 1.31; acc: 0.64
Batch: 220; loss: 1.24; acc: 0.69
Batch: 240; loss: 1.31; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.73
Batch: 280; loss: 1.19; acc: 0.8
Batch: 300; loss: 1.31; acc: 0.69
Batch: 320; loss: 1.22; acc: 0.78
Batch: 340; loss: 1.16; acc: 0.8
Batch: 360; loss: 1.26; acc: 0.7
Batch: 380; loss: 1.09; acc: 0.86
Batch: 400; loss: 1.22; acc: 0.75
Batch: 420; loss: 1.13; acc: 0.78
Batch: 440; loss: 1.17; acc: 0.73
Batch: 460; loss: 1.2; acc: 0.77
Batch: 480; loss: 1.23; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.78
Batch: 520; loss: 1.19; acc: 0.81
Batch: 540; loss: 1.19; acc: 0.73
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.01; acc: 0.86
Batch: 600; loss: 1.12; acc: 0.75
Batch: 620; loss: 1.23; acc: 0.75
Batch: 640; loss: 1.2; acc: 0.72
Batch: 660; loss: 1.47; acc: 0.56
Batch: 680; loss: 1.16; acc: 0.77
Batch: 700; loss: 1.06; acc: 0.81
Batch: 720; loss: 1.08; acc: 0.78
Batch: 740; loss: 1.15; acc: 0.75
Batch: 760; loss: 1.18; acc: 0.72
Batch: 780; loss: 1.04; acc: 0.75
Train Epoch over. train_loss: 1.21; train_accuracy: 0.74 

9.976112778531387e-05
9.615632734494284e-05
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.72
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.94; acc: 0.86
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 0.97; acc: 0.88
Val Epoch over. val_loss: 1.113086334079694; val_accuracy: 0.7791600318471338 

The current subspace-distance is: 9.615632734494284e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 1.15; acc: 0.75
Batch: 40; loss: 1.07; acc: 0.81
Batch: 60; loss: 1.12; acc: 0.78
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.1; acc: 0.8
Batch: 140; loss: 1.16; acc: 0.69
Batch: 160; loss: 1.05; acc: 0.77
Batch: 180; loss: 1.07; acc: 0.83
Batch: 200; loss: 1.01; acc: 0.83
Batch: 220; loss: 1.13; acc: 0.73
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.77
Batch: 300; loss: 1.12; acc: 0.78
Batch: 320; loss: 1.11; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.72
Batch: 360; loss: 1.13; acc: 0.8
Batch: 380; loss: 1.13; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.8
Batch: 420; loss: 0.98; acc: 0.81
Batch: 440; loss: 1.16; acc: 0.73
Batch: 460; loss: 1.17; acc: 0.72
Batch: 480; loss: 1.16; acc: 0.72
Batch: 500; loss: 1.12; acc: 0.77
Batch: 520; loss: 1.02; acc: 0.86
Batch: 540; loss: 1.08; acc: 0.78
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.12; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.84
Batch: 640; loss: 1.12; acc: 0.72
Batch: 660; loss: 1.08; acc: 0.77
Batch: 680; loss: 1.15; acc: 0.77
Batch: 700; loss: 1.15; acc: 0.73
Batch: 720; loss: 1.09; acc: 0.73
Batch: 740; loss: 1.07; acc: 0.77
Batch: 760; loss: 0.97; acc: 0.88
Batch: 780; loss: 1.06; acc: 0.8
Train Epoch over. train_loss: 1.11; train_accuracy: 0.76 

0.00011509315663715824
0.00011038607772206888
Batch: 0; loss: 1.05; acc: 0.81
Batch: 20; loss: 1.21; acc: 0.7
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 0.81; acc: 0.91
Batch: 100; loss: 1.08; acc: 0.78
Batch: 120; loss: 1.18; acc: 0.72
Batch: 140; loss: 0.92; acc: 0.88
Val Epoch over. val_loss: 1.0255571839156423; val_accuracy: 0.7895103503184714 

The current subspace-distance is: 0.00011038607772206888 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.84
Batch: 20; loss: 0.98; acc: 0.83
Batch: 40; loss: 1.04; acc: 0.78
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.05; acc: 0.78
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.03; acc: 0.8
Batch: 160; loss: 0.94; acc: 0.86
Batch: 180; loss: 1.05; acc: 0.77
Batch: 200; loss: 1.19; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 0.97; acc: 0.83
Batch: 260; loss: 0.95; acc: 0.84
Batch: 280; loss: 1.06; acc: 0.75
Batch: 300; loss: 1.02; acc: 0.8
Batch: 320; loss: 1.01; acc: 0.77
Batch: 340; loss: 1.05; acc: 0.72
Batch: 360; loss: 0.93; acc: 0.78
Batch: 380; loss: 0.98; acc: 0.8
Batch: 400; loss: 1.08; acc: 0.75
Batch: 420; loss: 1.06; acc: 0.78
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 0.9; acc: 0.83
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 0.91; acc: 0.83
Batch: 520; loss: 1.02; acc: 0.78
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 1.22; acc: 0.7
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 1.02; acc: 0.77
Batch: 620; loss: 0.97; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.78
Batch: 660; loss: 1.08; acc: 0.72
Batch: 680; loss: 0.98; acc: 0.77
Batch: 700; loss: 1.03; acc: 0.81
Batch: 720; loss: 1.0; acc: 0.83
Batch: 740; loss: 0.89; acc: 0.86
Batch: 760; loss: 1.12; acc: 0.73
Batch: 780; loss: 1.06; acc: 0.8
Train Epoch over. train_loss: 1.04; train_accuracy: 0.77 

0.00012952191173098981
0.00012510627857409418
Batch: 0; loss: 1.02; acc: 0.81
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.94
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.84
Val Epoch over. val_loss: 0.9704039654913982; val_accuracy: 0.7926950636942676 

The current subspace-distance is: 0.00012510627857409418 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.04; acc: 0.77
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 1.13; acc: 0.64
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 0.91; acc: 0.83
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 0.92; acc: 0.8
Batch: 220; loss: 1.18; acc: 0.7
Batch: 240; loss: 0.9; acc: 0.86
Batch: 260; loss: 1.03; acc: 0.8
Batch: 280; loss: 0.95; acc: 0.81
Batch: 300; loss: 1.02; acc: 0.77
Batch: 320; loss: 0.85; acc: 0.88
Batch: 340; loss: 1.01; acc: 0.81
Batch: 360; loss: 0.97; acc: 0.77
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.86; acc: 0.81
Batch: 420; loss: 0.94; acc: 0.83
Batch: 440; loss: 0.95; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.84
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 0.95; acc: 0.77
Batch: 520; loss: 0.96; acc: 0.8
Batch: 540; loss: 0.96; acc: 0.81
Batch: 560; loss: 1.01; acc: 0.75
Batch: 580; loss: 0.97; acc: 0.75
Batch: 600; loss: 0.96; acc: 0.77
Batch: 620; loss: 0.92; acc: 0.78
Batch: 640; loss: 0.93; acc: 0.83
Batch: 660; loss: 0.97; acc: 0.78
Batch: 680; loss: 0.96; acc: 0.8
Batch: 700; loss: 0.94; acc: 0.84
Batch: 720; loss: 1.09; acc: 0.73
Batch: 740; loss: 0.95; acc: 0.81
Batch: 760; loss: 0.9; acc: 0.78
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 0.99; train_accuracy: 0.78 

0.00014350644778460264
0.0001372634433209896
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.94
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.88
Val Epoch over. val_loss: 0.9132273508484956; val_accuracy: 0.7967754777070064 

The current subspace-distance is: 0.0001372634433209896 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.73
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.03; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.01; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.8
Batch: 160; loss: 1.03; acc: 0.72
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 0.92; acc: 0.8
Batch: 220; loss: 0.92; acc: 0.78
Batch: 240; loss: 0.76; acc: 0.88
Batch: 260; loss: 0.89; acc: 0.83
Batch: 280; loss: 0.89; acc: 0.83
Batch: 300; loss: 0.87; acc: 0.86
Batch: 320; loss: 1.0; acc: 0.75
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 0.93; acc: 0.77
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 1.0; acc: 0.72
Batch: 420; loss: 1.01; acc: 0.72
Batch: 440; loss: 0.93; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 0.86; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.88
Batch: 540; loss: 1.05; acc: 0.75
Batch: 560; loss: 0.89; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.8
Batch: 600; loss: 0.85; acc: 0.78
Batch: 620; loss: 0.94; acc: 0.78
Batch: 640; loss: 0.88; acc: 0.84
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 0.85; acc: 0.8
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 1.08; acc: 0.72
Batch: 740; loss: 0.97; acc: 0.75
Batch: 760; loss: 0.97; acc: 0.69
Batch: 780; loss: 0.89; acc: 0.81
Train Epoch over. train_loss: 0.94; train_accuracy: 0.78 

0.00015270827861968428
0.00014790242130402476
Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.92
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 1.06; acc: 0.77
Batch: 140; loss: 0.82; acc: 0.84
Val Epoch over. val_loss: 0.858506673080906; val_accuracy: 0.8082205414012739 

The current subspace-distance is: 0.00014790242130402476 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.92; acc: 0.78
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 1.01; acc: 0.73
Batch: 160; loss: 0.94; acc: 0.73
Batch: 180; loss: 0.81; acc: 0.86
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.84; acc: 0.83
Batch: 240; loss: 0.83; acc: 0.83
Batch: 260; loss: 0.96; acc: 0.8
Batch: 280; loss: 0.77; acc: 0.88
Batch: 300; loss: 0.88; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.81
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.88; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.84
Batch: 400; loss: 0.9; acc: 0.75
Batch: 420; loss: 0.72; acc: 0.89
Batch: 440; loss: 1.05; acc: 0.75
Batch: 460; loss: 0.9; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.86; acc: 0.84
Batch: 520; loss: 1.0; acc: 0.75
Batch: 540; loss: 0.89; acc: 0.77
Batch: 560; loss: 0.89; acc: 0.8
Batch: 580; loss: 0.94; acc: 0.73
Batch: 600; loss: 0.87; acc: 0.78
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 0.86; acc: 0.75
Batch: 660; loss: 0.83; acc: 0.77
Batch: 680; loss: 0.88; acc: 0.81
Batch: 700; loss: 1.05; acc: 0.72
Batch: 720; loss: 0.81; acc: 0.86
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.84; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.88
Train Epoch over. train_loss: 0.89; train_accuracy: 0.79 

0.0001684908347669989
0.00016220039105974138
Batch: 0; loss: 0.91; acc: 0.75
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 0.62; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 1.04; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.84
Val Epoch over. val_loss: 0.822242229987102; val_accuracy: 0.8139928343949044 

The current subspace-distance is: 0.00016220039105974138 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.11; acc: 0.73
Batch: 140; loss: 0.92; acc: 0.73
Batch: 160; loss: 0.87; acc: 0.8
Batch: 180; loss: 0.87; acc: 0.78
Batch: 200; loss: 0.87; acc: 0.83
Batch: 220; loss: 0.81; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.89
Batch: 260; loss: 0.79; acc: 0.89
Batch: 280; loss: 0.85; acc: 0.8
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.96; acc: 0.75
Batch: 340; loss: 1.02; acc: 0.77
Batch: 360; loss: 0.96; acc: 0.73
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.92; acc: 0.77
Batch: 440; loss: 0.85; acc: 0.77
Batch: 460; loss: 0.86; acc: 0.81
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 0.87; acc: 0.84
Batch: 540; loss: 0.84; acc: 0.78
Batch: 560; loss: 0.81; acc: 0.81
Batch: 580; loss: 0.85; acc: 0.78
Batch: 600; loss: 0.9; acc: 0.78
Batch: 620; loss: 0.79; acc: 0.86
Batch: 640; loss: 0.85; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.89
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 0.87; acc: 0.73
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 0.93; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.79 

0.00017606778419576585
0.00017018546350300312
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.69; acc: 0.84
Val Epoch over. val_loss: 0.7887366201467575; val_accuracy: 0.8199641719745223 

The current subspace-distance is: 0.00017018546350300312 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.8
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.89
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.91; acc: 0.8
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.96; acc: 0.73
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.9; acc: 0.77
Batch: 380; loss: 0.76; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.88
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.84; acc: 0.72
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.99; acc: 0.67
Batch: 500; loss: 1.15; acc: 0.7
Batch: 520; loss: 0.73; acc: 0.81
Batch: 540; loss: 0.86; acc: 0.77
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.85; acc: 0.81
Batch: 720; loss: 0.99; acc: 0.7
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.8
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00018693352467380464
0.00018052748055197299
Batch: 0; loss: 0.83; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.69
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.86
Val Epoch over. val_loss: 0.7545490621761152; val_accuracy: 0.8256369426751592 

The current subspace-distance is: 0.00018052748055197299 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.89
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.84
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 0.75; acc: 0.83
Batch: 180; loss: 0.97; acc: 0.7
Batch: 200; loss: 0.82; acc: 0.83
Batch: 220; loss: 0.77; acc: 0.81
Batch: 240; loss: 0.8; acc: 0.78
Batch: 260; loss: 0.92; acc: 0.75
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.95; acc: 0.8
Batch: 360; loss: 0.93; acc: 0.72
Batch: 380; loss: 0.8; acc: 0.81
Batch: 400; loss: 0.62; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.89
Batch: 460; loss: 0.72; acc: 0.88
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.86
Batch: 520; loss: 0.79; acc: 0.8
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.69
Batch: 600; loss: 0.81; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.83; acc: 0.77
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.82; train_accuracy: 0.8 

0.0001877494214568287
0.00018331386672798544
Batch: 0; loss: 0.84; acc: 0.78
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.88
Val Epoch over. val_loss: 0.757030158285882; val_accuracy: 0.8227507961783439 

The current subspace-distance is: 0.00018331386672798544 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.86; acc: 0.77
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.85; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.88; acc: 0.77
Batch: 220; loss: 0.7; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.72; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.92
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.93; acc: 0.75
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.66; acc: 0.91
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.66; acc: 0.89
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 0.79; acc: 0.78
Batch: 560; loss: 0.78; acc: 0.78
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.83; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.8
Batch: 640; loss: 0.86; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.82; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.88
Batch: 740; loss: 0.87; acc: 0.8
Batch: 760; loss: 0.91; acc: 0.77
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.81; train_accuracy: 0.8 

0.00019262568093836308
0.00018544435442890972
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.84
Val Epoch over. val_loss: 0.7510434463145627; val_accuracy: 0.8249402866242038 

The current subspace-distance is: 0.00018544435442890972 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.69
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.93; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.83
Batch: 140; loss: 0.78; acc: 0.83
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.88; acc: 0.78
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 0.76; acc: 0.84
Batch: 280; loss: 0.88; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.83; acc: 0.8
Batch: 360; loss: 0.91; acc: 0.69
Batch: 380; loss: 0.7; acc: 0.89
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.89
Batch: 440; loss: 0.92; acc: 0.72
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 0.68; acc: 0.89
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.97; acc: 0.72
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.56; acc: 0.92
Batch: 640; loss: 0.8; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.89; acc: 0.77
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.81; acc: 0.83
Batch: 760; loss: 0.84; acc: 0.77
Batch: 780; loss: 0.74; acc: 0.83
Train Epoch over. train_loss: 0.8; train_accuracy: 0.8 

0.00019407153013162315
0.00018829002510756254
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.86
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.7390220814449772; val_accuracy: 0.8265326433121019 

The current subspace-distance is: 0.00018829002510756254 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.89; acc: 0.69
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.77; acc: 0.81
Batch: 200; loss: 0.88; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.84
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 0.82; acc: 0.83
Batch: 280; loss: 1.1; acc: 0.62
Batch: 300; loss: 0.81; acc: 0.8
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.9; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.85; acc: 0.8
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 0.86; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.77
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 0.83; acc: 0.8
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.93; acc: 0.72
Batch: 760; loss: 0.86; acc: 0.77
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.8; train_accuracy: 0.8 

0.00019791320664808154
0.00019088572298642248
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.88
Val Epoch over. val_loss: 0.7327857463602807; val_accuracy: 0.8287221337579618 

The current subspace-distance is: 0.00019088572298642248 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.86; acc: 0.77
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.79; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.84; acc: 0.8
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.86; acc: 0.69
Batch: 520; loss: 0.77; acc: 0.86
Batch: 540; loss: 0.84; acc: 0.8
Batch: 560; loss: 0.73; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.9; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.77
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.83; acc: 0.78
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 0.79; train_accuracy: 0.8 

0.00020356297318357974
0.0001949615980265662
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.7276675308206279; val_accuracy: 0.8320063694267515 

The current subspace-distance is: 0.0001949615980265662 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.96; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.75; acc: 0.83
Batch: 60; loss: 0.93; acc: 0.66
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.9; acc: 0.8
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.92; acc: 0.78
Batch: 240; loss: 0.74; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.8; acc: 0.81
Batch: 380; loss: 0.95; acc: 0.75
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.81; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.81; acc: 0.81
Batch: 520; loss: 0.78; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 0.75; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.83; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.88; acc: 0.75
Batch: 720; loss: 0.94; acc: 0.75
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.79; train_accuracy: 0.8 

0.00020267667423468083
0.00019695406081154943
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.89
Val Epoch over. val_loss: 0.719240776482661; val_accuracy: 0.8317078025477707 

The current subspace-distance is: 0.00019695406081154943 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.92; acc: 0.75
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.95; acc: 0.72
Batch: 260; loss: 0.86; acc: 0.75
Batch: 280; loss: 0.7; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.8; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.81; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.81
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.82; acc: 0.84
Batch: 480; loss: 0.85; acc: 0.67
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.79; acc: 0.78
Batch: 540; loss: 0.85; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.91; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.66; acc: 0.86
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.86; acc: 0.77
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.77; acc: 0.78
Batch: 780; loss: 0.83; acc: 0.69
Train Epoch over. train_loss: 0.78; train_accuracy: 0.8 

0.00020680198213085532
0.00019716711540240794
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7140767008635649; val_accuracy: 0.8315087579617835 

The current subspace-distance is: 0.00019716711540240794 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.85; acc: 0.75
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.83; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.77; acc: 0.77
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.83
Batch: 460; loss: 0.79; acc: 0.86
Batch: 480; loss: 1.01; acc: 0.62
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.9; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.83; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.85; acc: 0.78
Batch: 700; loss: 0.89; acc: 0.8
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.9; acc: 0.69
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.78; train_accuracy: 0.8 

0.00020969717297703028
0.00020245148334652185
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.86
Val Epoch over. val_loss: 0.714056577841947; val_accuracy: 0.8325039808917197 

The current subspace-distance is: 0.00020245148334652185 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.67; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.86
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.95; acc: 0.77
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.79; acc: 0.83
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.83; acc: 0.77
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.79; acc: 0.81
Batch: 560; loss: 0.76; acc: 0.78
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.89
Batch: 620; loss: 0.77; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 0.61; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.87; acc: 0.73
Batch: 740; loss: 0.81; acc: 0.77
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.81; acc: 0.78
Train Epoch over. train_loss: 0.77; train_accuracy: 0.8 

0.00021023412409704179
0.00020346847304608673
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.89
Val Epoch over. val_loss: 0.7079942947740008; val_accuracy: 0.8354896496815286 

The current subspace-distance is: 0.00020346847304608673 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.88; acc: 0.69
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.87; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.92; acc: 0.8
Batch: 200; loss: 0.7; acc: 0.86
Batch: 220; loss: 0.93; acc: 0.77
Batch: 240; loss: 0.87; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.8
Batch: 280; loss: 0.72; acc: 0.86
Batch: 300; loss: 0.74; acc: 0.73
Batch: 320; loss: 0.89; acc: 0.75
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.84; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.92
Batch: 400; loss: 0.81; acc: 0.75
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.84; acc: 0.69
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.85; acc: 0.81
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.89; acc: 0.78
Batch: 660; loss: 0.77; acc: 0.8
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.77
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.71; acc: 0.88
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00021095767442602664
0.00020405117538757622
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.88
Val Epoch over. val_loss: 0.7105170740823078; val_accuracy: 0.834593949044586 

The current subspace-distance is: 0.00020405117538757622 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.92
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.87; acc: 0.73
Batch: 300; loss: 0.67; acc: 0.88
Batch: 320; loss: 0.71; acc: 0.89
Batch: 340; loss: 0.89; acc: 0.75
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.84; acc: 0.78
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.89; acc: 0.77
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.92; acc: 0.73
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.87; acc: 0.77
Batch: 660; loss: 0.81; acc: 0.78
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.84; acc: 0.67
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00021215165907051414
0.00020670561934821308
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.88
Val Epoch over. val_loss: 0.6981985901191736; val_accuracy: 0.8379777070063694 

The current subspace-distance is: 0.00020670561934821308 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.78; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.91
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.89
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.72; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.78; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.95; acc: 0.73
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.89; acc: 0.75
Batch: 720; loss: 0.81; acc: 0.75
Batch: 740; loss: 0.94; acc: 0.72
Batch: 760; loss: 0.76; acc: 0.75
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00020984731963835657
0.00020400200446601957
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.88
Val Epoch over. val_loss: 0.6891215451204094; val_accuracy: 0.841062898089172 

The current subspace-distance is: 0.00020400200446601957 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.84
Batch: 40; loss: 0.82; acc: 0.8
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 1.05; acc: 0.62
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.88
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.82; acc: 0.78
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.78; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.73; acc: 0.83
Batch: 380; loss: 0.77; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.85; acc: 0.69
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 0.69; acc: 0.86
Batch: 500; loss: 0.77; acc: 0.83
Batch: 520; loss: 0.89; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.75
Batch: 560; loss: 0.84; acc: 0.72
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.86; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.83
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 1.18; acc: 0.67
Batch: 740; loss: 0.87; acc: 0.78
Batch: 760; loss: 0.6; acc: 0.91
Batch: 780; loss: 0.79; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00021631385607179254
0.00020723578927572817
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.89
Val Epoch over. val_loss: 0.6941388625248223; val_accuracy: 0.8385748407643312 

The current subspace-distance is: 0.00020723578927572817 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.81; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.69
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.83
Batch: 180; loss: 0.77; acc: 0.81
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.77
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.75; acc: 0.84
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.8
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.91
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.91; acc: 0.72
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.93; acc: 0.73
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.89; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.85; acc: 0.77
Batch: 780; loss: 0.52; acc: 0.92
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.0002142108860425651
0.00020770187256857753
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.6992532705805105; val_accuracy: 0.8390724522292994 

The current subspace-distance is: 0.00020770187256857753 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.85; acc: 0.78
Batch: 160; loss: 0.63; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.91
Batch: 200; loss: 0.61; acc: 0.88
Batch: 220; loss: 0.89; acc: 0.7
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.83; acc: 0.81
Batch: 280; loss: 0.77; acc: 0.77
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.85; acc: 0.77
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.81
Batch: 420; loss: 0.96; acc: 0.73
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.85; acc: 0.77
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.93; acc: 0.73
Batch: 520; loss: 0.54; acc: 0.94
Batch: 540; loss: 0.79; acc: 0.77
Batch: 560; loss: 0.75; acc: 0.78
Batch: 580; loss: 0.83; acc: 0.75
Batch: 600; loss: 0.97; acc: 0.72
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.83
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.64; acc: 0.89
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021414922957774252
0.0002058210811810568
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.89
Val Epoch over. val_loss: 0.6942244798514494; val_accuracy: 0.8395700636942676 

The current subspace-distance is: 0.0002058210811810568 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.72
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.84; acc: 0.73
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.86; acc: 0.8
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.83; acc: 0.73
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 0.77; acc: 0.81
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.82; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.75; acc: 0.83
Batch: 640; loss: 0.78; acc: 0.78
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.95
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021340529201552272
0.00020785628294106573
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.88
Val Epoch over. val_loss: 0.6851805344129064; val_accuracy: 0.8401671974522293 

The current subspace-distance is: 0.00020785628294106573 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.71; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.83; acc: 0.8
Batch: 160; loss: 0.97; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.82; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.93; acc: 0.67
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.89; acc: 0.7
Batch: 300; loss: 0.81; acc: 0.75
Batch: 320; loss: 0.78; acc: 0.83
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.91; acc: 0.73
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.86; acc: 0.72
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.78
Batch: 560; loss: 0.79; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 0.74; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.81
Batch: 680; loss: 0.92; acc: 0.72
Batch: 700; loss: 0.94; acc: 0.69
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.89
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021785330318380147
0.00021183128410484642
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.88
Val Epoch over. val_loss: 0.6856620412343627; val_accuracy: 0.8404657643312102 

The current subspace-distance is: 0.00021183128410484642 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.88; acc: 0.77
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.89; acc: 0.75
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.82; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.74; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.77
Batch: 360; loss: 0.8; acc: 0.73
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.69; acc: 0.86
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.84; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.89; acc: 0.8
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.75; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.89
Batch: 700; loss: 0.65; acc: 0.89
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021920310973655432
0.00021183361241128296
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.86
Val Epoch over. val_loss: 0.6906336823086829; val_accuracy: 0.8403662420382165 

The current subspace-distance is: 0.00021183361241128296 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.67
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.81; acc: 0.83
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.72; acc: 0.8
Batch: 220; loss: 0.86; acc: 0.73
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.83
Batch: 360; loss: 0.88; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.94
Batch: 400; loss: 0.67; acc: 0.86
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.72; acc: 0.8
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 0.78; acc: 0.88
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.75; acc: 0.83
Batch: 580; loss: 0.87; acc: 0.75
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.92
Batch: 680; loss: 0.89; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 0.8; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021931339870207012
0.0002134628448402509
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6808429664107645; val_accuracy: 0.8428542993630573 

The current subspace-distance is: 0.0002134628448402509 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 0.76; acc: 0.7
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.83; acc: 0.78
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.9; acc: 0.67
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.89; acc: 0.8
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.75
Batch: 560; loss: 0.88; acc: 0.75
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.75; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 0.91; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021903112065047026
0.0002097773103741929
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6796119774982428; val_accuracy: 0.8438495222929936 

The current subspace-distance is: 0.0002097773103741929 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.45

The number of parameters is: 273705

The number of individual parameters is:

12
216
12
12
18
37584
18
18
35
109620
35
35
64
120960
64
64
4096
64
640
10
64
64

nonzero elements in E: 82111494
elements in E: 82111500
fraction nonzero: 0.9999999269286275
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.47; acc: 0.06
Batch: 20; loss: 2.07; acc: 0.28
Batch: 40; loss: 1.86; acc: 0.44
Batch: 60; loss: 1.83; acc: 0.47
Batch: 80; loss: 1.82; acc: 0.42
Batch: 100; loss: 1.77; acc: 0.45
Batch: 120; loss: 1.74; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.69; acc: 0.64
Batch: 180; loss: 1.48; acc: 0.7
Batch: 200; loss: 1.49; acc: 0.67
Batch: 220; loss: 1.5; acc: 0.67
Batch: 240; loss: 1.49; acc: 0.66
Batch: 260; loss: 1.43; acc: 0.7
Batch: 280; loss: 1.37; acc: 0.75
Batch: 300; loss: 1.46; acc: 0.59
Batch: 320; loss: 1.44; acc: 0.7
Batch: 340; loss: 1.45; acc: 0.69
Batch: 360; loss: 1.45; acc: 0.61
Batch: 380; loss: 1.3; acc: 0.72
Batch: 400; loss: 1.28; acc: 0.77
Batch: 420; loss: 1.32; acc: 0.81
Batch: 440; loss: 1.21; acc: 0.8
Batch: 460; loss: 1.34; acc: 0.67
Batch: 480; loss: 1.24; acc: 0.78
Batch: 500; loss: 1.25; acc: 0.75
Batch: 520; loss: 1.26; acc: 0.69
Batch: 540; loss: 1.28; acc: 0.7
Batch: 560; loss: 1.3; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.78
Batch: 600; loss: 1.23; acc: 0.73
Batch: 620; loss: 1.28; acc: 0.77
Batch: 640; loss: 1.15; acc: 0.84
Batch: 660; loss: 1.18; acc: 0.77
Batch: 680; loss: 1.32; acc: 0.73
Batch: 700; loss: 1.23; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.77
Batch: 740; loss: 1.09; acc: 0.77
Batch: 760; loss: 1.05; acc: 0.84
Batch: 780; loss: 1.06; acc: 0.86
Train Epoch over. train_loss: 1.42; train_accuracy: 0.68 

6.951027171453461e-05
6.516360735986382e-05
Batch: 0; loss: 1.18; acc: 0.8
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 0.84; acc: 0.97
Batch: 60; loss: 1.05; acc: 0.81
Batch: 80; loss: 0.94; acc: 0.89
Batch: 100; loss: 1.12; acc: 0.84
Batch: 120; loss: 1.2; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.83
Val Epoch over. val_loss: 1.0690216805524886; val_accuracy: 0.8233479299363057 

The current subspace-distance is: 6.516360735986382e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.75
Batch: 40; loss: 1.13; acc: 0.81
Batch: 60; loss: 1.12; acc: 0.78
Batch: 80; loss: 1.18; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.81
Batch: 120; loss: 1.08; acc: 0.86
Batch: 140; loss: 1.0; acc: 0.83
Batch: 160; loss: 1.04; acc: 0.83
Batch: 180; loss: 1.09; acc: 0.8
Batch: 200; loss: 1.03; acc: 0.77
Batch: 220; loss: 1.16; acc: 0.73
Batch: 240; loss: 1.04; acc: 0.86
Batch: 260; loss: 1.1; acc: 0.75
Batch: 280; loss: 1.02; acc: 0.83
Batch: 300; loss: 0.94; acc: 0.88
Batch: 320; loss: 0.95; acc: 0.83
Batch: 340; loss: 1.02; acc: 0.89
Batch: 360; loss: 1.04; acc: 0.77
Batch: 380; loss: 1.09; acc: 0.77
Batch: 400; loss: 0.99; acc: 0.75
Batch: 420; loss: 0.99; acc: 0.81
Batch: 440; loss: 1.05; acc: 0.75
Batch: 460; loss: 1.17; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.73
Batch: 500; loss: 1.03; acc: 0.8
Batch: 520; loss: 1.03; acc: 0.72
Batch: 540; loss: 1.07; acc: 0.77
Batch: 560; loss: 1.03; acc: 0.8
Batch: 580; loss: 1.0; acc: 0.78
Batch: 600; loss: 1.03; acc: 0.78
Batch: 620; loss: 0.91; acc: 0.84
Batch: 640; loss: 1.0; acc: 0.83
Batch: 660; loss: 1.17; acc: 0.72
Batch: 680; loss: 1.04; acc: 0.73
Batch: 700; loss: 0.8; acc: 0.94
Batch: 720; loss: 0.87; acc: 0.84
Batch: 740; loss: 0.95; acc: 0.78
Batch: 760; loss: 1.01; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.75
Train Epoch over. train_loss: 1.02; train_accuracy: 0.81 

9.719633817439899e-05
9.230346040567383e-05
Batch: 0; loss: 1.02; acc: 0.81
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 0.63; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.91
Batch: 100; loss: 0.95; acc: 0.84
Batch: 120; loss: 1.04; acc: 0.78
Batch: 140; loss: 0.76; acc: 0.84
Val Epoch over. val_loss: 0.8868740967884186; val_accuracy: 0.8404657643312102 

The current subspace-distance is: 9.230346040567383e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.84
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.89
Batch: 140; loss: 0.99; acc: 0.81
Batch: 160; loss: 0.9; acc: 0.83
Batch: 180; loss: 0.99; acc: 0.75
Batch: 200; loss: 0.91; acc: 0.8
Batch: 220; loss: 0.87; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.89
Batch: 260; loss: 0.91; acc: 0.84
Batch: 280; loss: 1.02; acc: 0.75
Batch: 300; loss: 0.81; acc: 0.89
Batch: 320; loss: 0.93; acc: 0.78
Batch: 340; loss: 0.9; acc: 0.8
Batch: 360; loss: 0.81; acc: 0.84
Batch: 380; loss: 0.88; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.89
Batch: 420; loss: 1.03; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.88
Batch: 460; loss: 0.83; acc: 0.86
Batch: 480; loss: 0.95; acc: 0.73
Batch: 500; loss: 0.75; acc: 0.94
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.84; acc: 0.84
Batch: 560; loss: 0.86; acc: 0.86
Batch: 580; loss: 0.77; acc: 0.86
Batch: 600; loss: 0.88; acc: 0.83
Batch: 620; loss: 1.0; acc: 0.73
Batch: 640; loss: 0.74; acc: 0.91
Batch: 660; loss: 0.82; acc: 0.83
Batch: 680; loss: 0.83; acc: 0.86
Batch: 700; loss: 0.9; acc: 0.83
Batch: 720; loss: 0.87; acc: 0.84
Batch: 740; loss: 0.94; acc: 0.72
Batch: 760; loss: 0.8; acc: 0.91
Batch: 780; loss: 0.91; acc: 0.78
Train Epoch over. train_loss: 0.87; train_accuracy: 0.83 

0.00011625819752225652
0.00011091003398178145
Batch: 0; loss: 0.86; acc: 0.84
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.98
Batch: 100; loss: 0.83; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.92
Val Epoch over. val_loss: 0.757612813620051; val_accuracy: 0.8645501592356688 

The current subspace-distance is: 0.00011091003398178145 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.84
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.92
Batch: 80; loss: 0.75; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.92
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.79; acc: 0.89
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.95; acc: 0.78
Batch: 260; loss: 0.75; acc: 0.88
Batch: 280; loss: 0.67; acc: 0.91
Batch: 300; loss: 0.77; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.83; acc: 0.77
Batch: 360; loss: 0.92; acc: 0.78
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.91
Batch: 420; loss: 0.79; acc: 0.89
Batch: 440; loss: 0.78; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.79; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.71; acc: 0.89
Batch: 580; loss: 0.67; acc: 0.91
Batch: 600; loss: 0.78; acc: 0.81
Batch: 620; loss: 0.76; acc: 0.88
Batch: 640; loss: 0.82; acc: 0.83
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.7; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.73; acc: 0.89
Train Epoch over. train_loss: 0.77; train_accuracy: 0.84 

0.00013489146658685058
0.00012999451428186148
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.97
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.94
Val Epoch over. val_loss: 0.6756673692517979; val_accuracy: 0.8736066878980892 

The current subspace-distance is: 0.00012999451428186148 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.94
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.84; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.58; acc: 0.92
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.94
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.59; acc: 0.92
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.81
Batch: 500; loss: 0.84; acc: 0.84
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.74; acc: 0.89
Batch: 580; loss: 0.69; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.81; acc: 0.83
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.7; train_accuracy: 0.85 

0.00014985630696173757
0.00014370742428582162
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.97
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.37; acc: 0.98
Val Epoch over. val_loss: 0.6070099788106931; val_accuracy: 0.8800756369426752 

The current subspace-distance is: 0.00014370742428582162 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.77; acc: 0.78
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.92
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.63; acc: 0.92
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.97
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.89
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.81; acc: 0.77
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.0001639372785575688
0.00015725834236945957
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.97
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.98
Val Epoch over. val_loss: 0.5671340289768899; val_accuracy: 0.8845541401273885 

The current subspace-distance is: 0.00015725834236945957 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.92
Batch: 240; loss: 0.63; acc: 0.91
Batch: 260; loss: 0.8; acc: 0.78
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.94
Batch: 360; loss: 0.55; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.76; acc: 0.75
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.95
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.74; acc: 0.77
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00017415828187949955
0.00016739757847972214
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.5318044815093849; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.00016739757847972214 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.77
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

0.00018315197667106986
0.00017778728215489537
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.5064849675081338; val_accuracy: 0.8851512738853503 

The current subspace-distance is: 0.00017778728215489537 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.95
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.8; acc: 0.72
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.95
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.66; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.94
Batch: 700; loss: 0.64; acc: 0.77
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00019261441775597632
0.0001881335920188576
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4858320027020327; val_accuracy: 0.887937898089172 

The current subspace-distance is: 0.0001881335920188576 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.33; acc: 0.97
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.95
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00020088659948669374
0.00019406338105909526
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.21; acc: 1.0
Val Epoch over. val_loss: 0.4681786898594753; val_accuracy: 0.8924164012738853 

The current subspace-distance is: 0.00019406338105909526 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.98
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.0002044113352894783
0.00019587765564210713
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.46021224311582604; val_accuracy: 0.8943073248407644 

The current subspace-distance is: 0.00019587765564210713 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.78
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00020808016415685415
0.00020074051280971617
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.4621779421332535; val_accuracy: 0.8918192675159236 

The current subspace-distance is: 0.00020074051280971617 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

0.00020723829220514745
0.00019831358804367483
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.4495629916904838; val_accuracy: 0.8951035031847133 

The current subspace-distance is: 0.00019831358804367483 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.5; acc: 0.94
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

0.0002093849325319752
0.0002016248181462288
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.4479413859213993; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.0002016248181462288 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

0.00021096326236147434
0.0002050750917987898
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.4467317405969474; val_accuracy: 0.8958001592356688 

The current subspace-distance is: 0.0002050750917987898 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

0.00021401961566880345
0.0002052636700682342
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.44365962770334477; val_accuracy: 0.8947054140127388 

The current subspace-distance is: 0.0002052636700682342 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

0.00021235605527181178
0.00020714139100164175
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.43428607398916963; val_accuracy: 0.895203025477707 

The current subspace-distance is: 0.00020714139100164175 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 1.0
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

0.00021529789955820888
0.0002093017246806994
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.43396209702370275; val_accuracy: 0.897093949044586 

The current subspace-distance is: 0.0002093017246806994 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.68; acc: 0.77
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021774058404844254
0.0002105889725498855
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4335231248550354; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.0002105889725498855 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.95
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.62; acc: 0.77
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021879617997910827
0.00021230497804936022
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.43265086820550785; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00021230497804936022 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.6; acc: 0.75
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022168594296090305
0.0002136524417437613
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4297776395917698; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.0002136524417437613 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.95
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.97
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.000221084279473871
0.00021383410785347223
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.4340416621060888; val_accuracy: 0.8946058917197452 

The current subspace-distance is: 0.00021383410785347223 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022039332543499768
0.00021231165737845004
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4293004769808168; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.00021231165737845004 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.66; acc: 0.75
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002208780642831698
0.00021424306032713503
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.41903688374218667; val_accuracy: 0.8960987261146497 

The current subspace-distance is: 0.00021424306032713503 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.77
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.45; acc: 0.97
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.95
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002226127835456282
0.00021661764185409993
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4246898451048857; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 0.00021661764185409993 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.97
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.79; acc: 0.77
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 0.5; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002237020235043019
0.0002167514176107943
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4245277039564339; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.0002167514176107943 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022214831551536918
0.00021698937052860856
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4269510745812374; val_accuracy: 0.8986863057324841 

The current subspace-distance is: 0.00021698937052860856 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.95
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022417282161768526
0.00021752470638602972
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.41912599069297696; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 0.00021752470638602972 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022254356008488685
0.00021572672994807363
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.42676550396688423; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.00021572672994807363 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.81
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022477276797872037
0.00021549790108110756
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.42468780506947995; val_accuracy: 0.897890127388535 

The current subspace-distance is: 0.00021549790108110756 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.45

The number of parameters is: 273705

The number of individual parameters is:

12
216
12
12
18
37584
18
18
35
109620
35
35
64
120960
64
64
4096
64
640
10
64
64

nonzero elements in E: 109481990
elements in E: 109482000
fraction nonzero: 0.9999999086607845
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.11
Batch: 20; loss: 2.04; acc: 0.28
Batch: 40; loss: 1.9; acc: 0.42
Batch: 60; loss: 1.78; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.62
Batch: 100; loss: 1.58; acc: 0.61
Batch: 120; loss: 1.45; acc: 0.7
Batch: 140; loss: 1.39; acc: 0.8
Batch: 160; loss: 1.56; acc: 0.64
Batch: 180; loss: 1.52; acc: 0.66
Batch: 200; loss: 1.45; acc: 0.7
Batch: 220; loss: 1.35; acc: 0.73
Batch: 240; loss: 1.32; acc: 0.67
Batch: 260; loss: 1.23; acc: 0.83
Batch: 280; loss: 1.27; acc: 0.77
Batch: 300; loss: 1.29; acc: 0.8
Batch: 320; loss: 1.14; acc: 0.8
Batch: 340; loss: 1.36; acc: 0.67
Batch: 360; loss: 1.23; acc: 0.84
Batch: 380; loss: 1.21; acc: 0.78
Batch: 400; loss: 1.12; acc: 0.8
Batch: 420; loss: 1.1; acc: 0.86
Batch: 440; loss: 1.23; acc: 0.73
Batch: 460; loss: 1.0; acc: 0.91
Batch: 480; loss: 1.08; acc: 0.81
Batch: 500; loss: 1.1; acc: 0.73
Batch: 520; loss: 1.02; acc: 0.88
Batch: 540; loss: 1.12; acc: 0.78
Batch: 560; loss: 1.1; acc: 0.81
Batch: 580; loss: 0.96; acc: 0.88
Batch: 600; loss: 0.99; acc: 0.86
Batch: 620; loss: 0.98; acc: 0.88
Batch: 640; loss: 1.1; acc: 0.75
Batch: 660; loss: 1.05; acc: 0.77
Batch: 680; loss: 0.98; acc: 0.78
Batch: 700; loss: 0.99; acc: 0.86
Batch: 720; loss: 0.94; acc: 0.89
Batch: 740; loss: 1.05; acc: 0.81
Batch: 760; loss: 0.95; acc: 0.89
Batch: 780; loss: 0.96; acc: 0.86
Train Epoch over. train_loss: 1.25; train_accuracy: 0.74 

2.6190808057435788e-05
9.524250344838947e-06
Batch: 0; loss: 0.93; acc: 0.88
Batch: 20; loss: 1.07; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.94
Batch: 60; loss: 0.87; acc: 0.91
Batch: 80; loss: 0.73; acc: 0.97
Batch: 100; loss: 0.92; acc: 0.84
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 0.8905210445640953; val_accuracy: 0.8672372611464968 

The current subspace-distance is: 9.524250344838947e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.89
Batch: 20; loss: 1.0; acc: 0.81
Batch: 40; loss: 1.01; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.83
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 0.94; acc: 0.88
Batch: 120; loss: 0.94; acc: 0.81
Batch: 140; loss: 0.86; acc: 0.86
Batch: 160; loss: 0.9; acc: 0.86
Batch: 180; loss: 0.97; acc: 0.78
Batch: 200; loss: 1.0; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.89
Batch: 240; loss: 0.82; acc: 0.92
Batch: 260; loss: 0.94; acc: 0.84
Batch: 280; loss: 0.85; acc: 0.83
Batch: 300; loss: 0.84; acc: 0.91
Batch: 320; loss: 0.73; acc: 0.89
Batch: 340; loss: 0.76; acc: 0.88
Batch: 360; loss: 0.79; acc: 0.83
Batch: 380; loss: 0.89; acc: 0.84
Batch: 400; loss: 0.9; acc: 0.84
Batch: 420; loss: 0.86; acc: 0.86
Batch: 440; loss: 0.8; acc: 0.86
Batch: 460; loss: 0.93; acc: 0.83
Batch: 480; loss: 0.82; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.94
Batch: 520; loss: 0.9; acc: 0.83
Batch: 540; loss: 0.87; acc: 0.81
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.95
Batch: 600; loss: 0.75; acc: 0.88
Batch: 620; loss: 0.83; acc: 0.89
Batch: 640; loss: 0.83; acc: 0.86
Batch: 660; loss: 0.79; acc: 0.83
Batch: 680; loss: 0.74; acc: 0.88
Batch: 700; loss: 0.77; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.97
Batch: 740; loss: 0.77; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.88
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.86; train_accuracy: 0.85 

3.2301108149113134e-05
1.281720233237138e-05
Batch: 0; loss: 0.73; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.97
Batch: 60; loss: 0.71; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.97
Batch: 100; loss: 0.74; acc: 0.88
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.57; acc: 0.94
Val Epoch over. val_loss: 0.7171008784300202; val_accuracy: 0.884156050955414 

The current subspace-distance is: 1.281720233237138e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.88
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.94
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.89
Batch: 140; loss: 0.79; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.91
Batch: 180; loss: 0.79; acc: 0.86
Batch: 200; loss: 0.88; acc: 0.84
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.87; acc: 0.81
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.92
Batch: 320; loss: 0.71; acc: 0.88
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.91
Batch: 380; loss: 0.77; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 0.76; acc: 0.84
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.94
Batch: 660; loss: 0.86; acc: 0.81
Batch: 680; loss: 0.75; acc: 0.84
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.8; acc: 0.86
Batch: 780; loss: 0.7; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.87 

3.623841985245235e-05
1.5858302504057065e-05
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.97
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.49; acc: 0.92
Val Epoch over. val_loss: 0.630340662351839; val_accuracy: 0.8921178343949044 

The current subspace-distance is: 1.5858302504057065e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.94
Batch: 180; loss: 0.64; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.95
Batch: 240; loss: 0.82; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.94
Batch: 280; loss: 0.65; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.92
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.74; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.89
Batch: 440; loss: 0.73; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.94
Batch: 560; loss: 0.56; acc: 0.92
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.94
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.94
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.92
Train Epoch over. train_loss: 0.65; train_accuracy: 0.88 

4.017537503386848e-05
1.6859346942510456e-05
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.98
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.5560216499362022; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 1.6859346942510456e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.92
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.92
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.59; train_accuracy: 0.88 

4.3474727135617286e-05
1.9826256902888417e-05
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.98
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.4984556846557909; val_accuracy: 0.9054538216560509 

The current subspace-distance is: 1.9826256902888417e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.95
Batch: 280; loss: 0.49; acc: 0.94
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.95
Batch: 660; loss: 0.46; acc: 0.94
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.6744331484660506e-05
2.2128868295112625e-05
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.98
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.4651960688791457; val_accuracy: 0.9118232484076433 

The current subspace-distance is: 2.2128868295112625e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.94
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

4.92213002871722e-05
2.150681393686682e-05
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.98
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.43630209887862964; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.150681393686682e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.94
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.97
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

5.2493673138087615e-05
2.5646029826020822e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4205890090032748; val_accuracy: 0.9150079617834395 

The current subspace-distance is: 2.5646029826020822e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.97
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.98
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.405405681813136e-05
2.3684073312324472e-05
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.39515864184707594; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 2.3684073312324472e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.97
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.97
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.782906009699218e-05
2.7781761673395522e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.24; acc: 1.0
Val Epoch over. val_loss: 0.3885069292062407; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 2.7781761673395522e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.78
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.867175423190929e-05
2.7739350116462447e-05
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.3810669770286341; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 2.7739350116462447e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.98
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.8527661167318e-05
2.7165822757524438e-05
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3732598207558796; val_accuracy: 0.9205812101910829 

The current subspace-distance is: 2.7165822757524438e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.71; acc: 0.77
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.98
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.97
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.9896541642956436e-05
2.8550490242196247e-05
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3721782928629286; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 2.8550490242196247e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.98
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.98
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.98
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.9423589846119285e-05
2.8175463739898987e-05
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3646565721293164; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 2.8175463739898987e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.98
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.97
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.9751087974291295e-05
2.7413769203121774e-05
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.368795585005906; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.7413769203121774e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.97
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.0089379985583946e-05
2.7734531613532454e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3663321318709926; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.7734531613532454e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.131206464488059e-05
2.88302508124616e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.36150937437251873; val_accuracy: 0.924562101910828 

The current subspace-distance is: 2.88302508124616e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.98
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

6.134940485935658e-05
2.9289169106050394e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.3553019154603314; val_accuracy: 0.9242635350318471 

The current subspace-distance is: 2.9289169106050394e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.267538265092298e-05
3.0093551686150022e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3537886822299593; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 3.0093551686150022e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.98
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.98
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.32551746093668e-05
2.9947745133540593e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.35360898124943874; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.9947745133540593e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.98
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.97
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.24; acc: 1.0
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.357929669320583e-05
3.0461322239716537e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3487029611874538; val_accuracy: 0.924562101910828 

The current subspace-distance is: 3.0461322239716537e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.77
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.35; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.39242134639062e-05
2.9593993531307206e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.34953752245492997; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 2.9593993531307206e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.303023110376671e-05
2.971087633341085e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3442646529833982; val_accuracy: 0.9266520700636943 

The current subspace-distance is: 2.971087633341085e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.97
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.333522469503805e-05
3.0000466722412966e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.34067517329173486; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 3.0000466722412966e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.97
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.442984158638865e-05
3.05266585201025e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3415279245110834; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 3.05266585201025e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.387896428350359e-05
2.981617217301391e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3488897043428603; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 2.981617217301391e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.357769598253071e-05
2.9338461899897084e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3406525463055653; val_accuracy: 0.9269506369426752 

The current subspace-distance is: 2.9338461899897084e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.97
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.439666321966797e-05
3.0392753615160473e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3432121129742094; val_accuracy: 0.923765923566879 

The current subspace-distance is: 3.0392753615160473e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.19; acc: 1.0
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.414462404791266e-05
2.9207460102043115e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3437754740570761; val_accuracy: 0.9254578025477707 

The current subspace-distance is: 2.9207460102043115e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.95
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.405723979696631e-05
3.0270017305156216e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3342570840932761; val_accuracy: 0.9266520700636943 

The current subspace-distance is: 3.0270017305156216e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.45

The number of parameters is: 273705

The number of individual parameters is:

12
216
12
12
18
37584
18
18
35
109620
35
35
64
120960
64
64
4096
64
640
10
64
64

nonzero elements in E: 136852490
elements in E: 136852500
fraction nonzero: 0.9999999269286275
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.2
Batch: 20; loss: 2.11; acc: 0.25
Batch: 40; loss: 1.69; acc: 0.61
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.7
Batch: 140; loss: 1.37; acc: 0.69
Batch: 160; loss: 1.29; acc: 0.77
Batch: 180; loss: 1.16; acc: 0.81
Batch: 200; loss: 1.15; acc: 0.77
Batch: 220; loss: 1.15; acc: 0.83
Batch: 240; loss: 1.16; acc: 0.77
Batch: 260; loss: 1.2; acc: 0.77
Batch: 280; loss: 1.06; acc: 0.88
Batch: 300; loss: 1.14; acc: 0.78
Batch: 320; loss: 0.98; acc: 0.86
Batch: 340; loss: 1.01; acc: 0.89
Batch: 360; loss: 1.04; acc: 0.84
Batch: 380; loss: 0.98; acc: 0.83
Batch: 400; loss: 1.01; acc: 0.83
Batch: 420; loss: 1.09; acc: 0.73
Batch: 440; loss: 1.02; acc: 0.84
Batch: 460; loss: 0.99; acc: 0.91
Batch: 480; loss: 0.89; acc: 0.84
Batch: 500; loss: 1.0; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.91
Batch: 540; loss: 0.84; acc: 0.89
Batch: 560; loss: 1.0; acc: 0.77
Batch: 580; loss: 0.92; acc: 0.81
Batch: 600; loss: 0.85; acc: 0.89
Batch: 620; loss: 0.98; acc: 0.73
Batch: 640; loss: 0.87; acc: 0.83
Batch: 660; loss: 0.97; acc: 0.81
Batch: 680; loss: 0.89; acc: 0.89
Batch: 700; loss: 0.9; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.91
Batch: 740; loss: 0.95; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.91
Train Epoch over. train_loss: 1.11; train_accuracy: 0.78 

2.617143400129862e-05
9.38163520913804e-06
Batch: 0; loss: 0.74; acc: 0.95
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.98
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.59; acc: 0.97
Batch: 100; loss: 0.72; acc: 0.94
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.61; acc: 0.92
Val Epoch over. val_loss: 0.7661122041902725; val_accuracy: 0.8798765923566879 

The current subspace-distance is: 9.38163520913804e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.84; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.86
Batch: 140; loss: 0.69; acc: 0.94
Batch: 160; loss: 0.73; acc: 0.88
Batch: 180; loss: 0.84; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.79; acc: 0.89
Batch: 280; loss: 0.63; acc: 0.95
Batch: 300; loss: 0.68; acc: 0.92
Batch: 320; loss: 0.85; acc: 0.83
Batch: 340; loss: 0.82; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.92
Batch: 380; loss: 0.93; acc: 0.81
Batch: 400; loss: 0.77; acc: 0.86
Batch: 420; loss: 0.67; acc: 0.92
Batch: 440; loss: 0.8; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.89
Batch: 500; loss: 0.73; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.81; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 0.89; acc: 0.77
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.94
Train Epoch over. train_loss: 0.74; train_accuracy: 0.87 

3.2328727684216574e-05
1.2487023013818543e-05
Batch: 0; loss: 0.56; acc: 0.97
Batch: 20; loss: 0.75; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.97
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6058089624924264; val_accuracy: 0.9037619426751592 

The current subspace-distance is: 1.2487023013818543e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.95
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.92
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.92
Batch: 240; loss: 0.67; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.94
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.89
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.7; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.95
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.82; acc: 0.81
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.94
Batch: 520; loss: 0.6; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.94
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.94
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.82; acc: 0.73
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.62; acc: 0.89
Train Epoch over. train_loss: 0.61; train_accuracy: 0.89 

3.583791112760082e-05
1.5062756574479863e-05
Batch: 0; loss: 0.44; acc: 0.98
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.49718464341512913; val_accuracy: 0.911922770700637 

The current subspace-distance is: 1.5062756574479863e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.97
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.95
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.94
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.92
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.95
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.95
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.9 

4.0511520637664944e-05
1.7598591512069106e-05
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4455444091444562; val_accuracy: 0.9197850318471338 

The current subspace-distance is: 1.7598591512069106e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.92
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.97
Batch: 240; loss: 0.54; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.91 

4.3643412936944515e-05
2.066056003968697e-05
Batch: 0; loss: 0.35; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3966907627263646; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 2.066056003968697e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.97
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

4.7150762839009985e-05
2.2852753318147734e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.35870044768615894; val_accuracy: 0.932921974522293 

The current subspace-distance is: 2.2852753318147734e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.98
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.92 

4.995457493350841e-05
2.423332080070395e-05
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3257374213949131; val_accuracy: 0.9369028662420382 

The current subspace-distance is: 2.423332080070395e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.97
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.98
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.171597877051681e-05
2.4430992198176682e-05
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.3118632929814849; val_accuracy: 0.9407842356687898 

The current subspace-distance is: 2.4430992198176682e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.98
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.54; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.98
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.58; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.451602191897109e-05
2.6677664209273644e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.29598860371454505; val_accuracy: 0.9404856687898089 

The current subspace-distance is: 2.6677664209273644e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.98
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.651680476148613e-05
2.7277736080577597e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2838568484802155; val_accuracy: 0.9400875796178344 

The current subspace-distance is: 2.7277736080577597e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.663583942805417e-05
2.6142866772715934e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2824186422642629; val_accuracy: 0.9417794585987261 

The current subspace-distance is: 2.6142866772715934e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.98
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.7321023632539436e-05
2.7256721295998432e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2823921676938701; val_accuracy: 0.9415804140127388 

The current subspace-distance is: 2.7256721295998432e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.7590306823840365e-05
2.634385964483954e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2770068271050028; val_accuracy: 0.9421775477707006 

The current subspace-distance is: 2.634385964483954e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.14; acc: 1.0
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.97
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.842486280016601e-05
2.6823394364328124e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.27779089982152744; val_accuracy: 0.9411823248407644 

The current subspace-distance is: 2.6823394364328124e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.98
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.98
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.9056423197034746e-05
2.950757152575534e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.27279749303866346; val_accuracy: 0.9424761146496815 

The current subspace-distance is: 2.950757152575534e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.15; acc: 1.0
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.940224218647927e-05
2.8324793674983084e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2662215173054653; val_accuracy: 0.9429737261146497 

The current subspace-distance is: 2.8324793674983084e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.970972415525466e-05
2.7111927920486778e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.26939596202532956; val_accuracy: 0.9421775477707006 

The current subspace-distance is: 2.7111927920486778e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.115903670433909e-05
2.930078517238144e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.26470050884849705; val_accuracy: 0.941281847133758 

The current subspace-distance is: 2.930078517238144e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.19; acc: 1.0
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.0837021010229364e-05
3.048800863325596e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2644454185750074; val_accuracy: 0.940187101910828 

The current subspace-distance is: 3.048800863325596e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.19; acc: 1.0
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.98
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.122171180322766e-05
2.913534626713954e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2611253141977225; val_accuracy: 0.9424761146496815 

The current subspace-distance is: 2.913534626713954e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.98
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.119101453805342e-05
2.8232752811163664e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2668023365698043; val_accuracy: 0.9426751592356688 

The current subspace-distance is: 2.8232752811163664e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.100545942899771e-05
2.9125978471711278e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2619371309306971; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.9125978471711278e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.157941970741376e-05
3.126530282315798e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.25905892575622363; val_accuracy: 0.9405851910828026 

The current subspace-distance is: 3.126530282315798e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.052713069948368e-05
2.8377620765240863e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.26059813859166614; val_accuracy: 0.9416799363057324 

The current subspace-distance is: 2.8377620765240863e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.140813638921827e-05
2.840586421370972e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.25510182492672256; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.840586421370972e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.179360207170248e-05
2.990787834278308e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.26372677059310257; val_accuracy: 0.9402866242038217 

The current subspace-distance is: 2.990787834278308e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.98
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.155581650091335e-05
2.9026881747995503e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.25789237511195956; val_accuracy: 0.9426751592356688 

The current subspace-distance is: 2.9026881747995503e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.156189920147881e-05
2.9673958124476485e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2540695543406875; val_accuracy: 0.9425756369426752 

The current subspace-distance is: 2.9673958124476485e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.17; acc: 1.0
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.314274651231244e-05
3.211518924217671e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.255155400153558; val_accuracy: 0.9428742038216561 

The current subspace-distance is: 3.211518924217671e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.338756065815687e-05
3.202037623850629e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2580054036940739; val_accuracy: 0.9400875796178344 

The current subspace-distance is: 3.202037623850629e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:57/N_9_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
