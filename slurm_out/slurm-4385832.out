Namespace(batch_size=64, chunked=False, ddim_vs_acc=True, dense=False, device=device(type='cuda'), lr=1.0, model='reg_lenet', n_epochs=50, non_wrapped=False, optimizer='SGD', parameter_correction=False, print_freq=20, print_prec=2, schedule=True, schedule_freq=10, schedule_gamma=0.4, seed=1, subspace_training=True, timestamp='2020-01-19 18:09:26')
nonzero elements in E: 10637
elements in E: 2197600
fraction nonzero: 0.004840280305788132
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.11
Batch: 40; loss: 2.37; acc: 0.08
Batch: 60; loss: 2.34; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.2
Batch: 100; loss: 2.26; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.28; acc: 0.17
Batch: 160; loss: 2.3; acc: 0.16
Batch: 180; loss: 2.33; acc: 0.08
Batch: 200; loss: 2.29; acc: 0.09
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.2
Batch: 260; loss: 2.32; acc: 0.19
Batch: 280; loss: 2.3; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.12
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.23; acc: 0.36
Batch: 360; loss: 2.28; acc: 0.2
Batch: 380; loss: 2.28; acc: 0.09
Batch: 400; loss: 2.27; acc: 0.22
Batch: 420; loss: 2.29; acc: 0.22
Batch: 440; loss: 2.29; acc: 0.2
Batch: 460; loss: 2.27; acc: 0.22
Batch: 480; loss: 2.23; acc: 0.25
Batch: 500; loss: 2.24; acc: 0.23
Batch: 520; loss: 2.19; acc: 0.28
Batch: 540; loss: 2.26; acc: 0.19
Batch: 560; loss: 2.27; acc: 0.2
Batch: 580; loss: 2.22; acc: 0.23
Batch: 600; loss: 2.17; acc: 0.28
Batch: 620; loss: 2.18; acc: 0.27
Batch: 640; loss: 2.2; acc: 0.28
Batch: 660; loss: 2.19; acc: 0.22
Batch: 680; loss: 2.19; acc: 0.25
Batch: 700; loss: 2.17; acc: 0.27
Batch: 720; loss: 2.15; acc: 0.27
Batch: 740; loss: 2.25; acc: 0.2
Batch: 760; loss: 2.2; acc: 0.27
Batch: 780; loss: 2.11; acc: 0.28
Train Epoch over. train_loss: 2.25; train_accuracy: 0.21 

Batch: 0; loss: 2.13; acc: 0.31
Batch: 20; loss: 2.21; acc: 0.2
Batch: 40; loss: 2.06; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.27
Batch: 80; loss: 2.11; acc: 0.27
Batch: 100; loss: 2.17; acc: 0.23
Batch: 120; loss: 2.11; acc: 0.3
Batch: 140; loss: 2.16; acc: 0.2
Val Epoch over. val_loss: 2.1348039906495697; val_accuracy: 0.2721934713375796 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.13; acc: 0.34
Batch: 20; loss: 2.08; acc: 0.33
Batch: 40; loss: 2.14; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.36
Batch: 80; loss: 2.06; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.33
Batch: 120; loss: 2.08; acc: 0.28
Batch: 140; loss: 2.04; acc: 0.34
Batch: 160; loss: 2.14; acc: 0.23
Batch: 180; loss: 2.06; acc: 0.33
Batch: 200; loss: 2.03; acc: 0.28
Batch: 220; loss: 2.02; acc: 0.36
Batch: 240; loss: 2.02; acc: 0.31
Batch: 260; loss: 2.01; acc: 0.33
Batch: 280; loss: 2.06; acc: 0.28
Batch: 300; loss: 1.92; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.3
Batch: 340; loss: 1.93; acc: 0.31
Batch: 360; loss: 2.01; acc: 0.25
Batch: 380; loss: 1.9; acc: 0.41
Batch: 400; loss: 1.99; acc: 0.3
Batch: 420; loss: 1.93; acc: 0.28
Batch: 440; loss: 1.88; acc: 0.3
Batch: 460; loss: 1.93; acc: 0.27
Batch: 480; loss: 1.93; acc: 0.25
Batch: 500; loss: 1.86; acc: 0.31
Batch: 520; loss: 1.8; acc: 0.34
Batch: 540; loss: 2.1; acc: 0.17
Batch: 560; loss: 1.93; acc: 0.3
Batch: 580; loss: 1.91; acc: 0.28
Batch: 600; loss: 1.9; acc: 0.31
Batch: 620; loss: 1.91; acc: 0.3
Batch: 640; loss: 1.81; acc: 0.41
Batch: 660; loss: 1.83; acc: 0.36
Batch: 680; loss: 1.83; acc: 0.39
Batch: 700; loss: 1.69; acc: 0.33
Batch: 720; loss: 1.91; acc: 0.31
Batch: 740; loss: 1.65; acc: 0.45
Batch: 760; loss: 1.77; acc: 0.36
Batch: 780; loss: 1.8; acc: 0.41
Train Epoch over. train_loss: 1.94; train_accuracy: 0.32 

Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.59; acc: 0.39
Batch: 60; loss: 1.62; acc: 0.34
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.71; acc: 0.38
Batch: 120; loss: 1.68; acc: 0.33
Batch: 140; loss: 1.85; acc: 0.28
Val Epoch over. val_loss: 1.7447628344699835; val_accuracy: 0.36285828025477707 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.34
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.98; acc: 0.33
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.38
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.73; acc: 0.42
Batch: 160; loss: 1.86; acc: 0.41
Batch: 180; loss: 1.68; acc: 0.42
Batch: 200; loss: 1.68; acc: 0.41
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.77; acc: 0.28
Batch: 260; loss: 1.75; acc: 0.38
Batch: 280; loss: 1.57; acc: 0.5
Batch: 300; loss: 1.59; acc: 0.48
Batch: 320; loss: 1.67; acc: 0.42
Batch: 340; loss: 1.73; acc: 0.38
Batch: 360; loss: 1.57; acc: 0.41
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.67; acc: 0.39
Batch: 420; loss: 1.81; acc: 0.34
Batch: 440; loss: 1.39; acc: 0.55
Batch: 460; loss: 1.7; acc: 0.38
Batch: 480; loss: 1.56; acc: 0.47
Batch: 500; loss: 1.6; acc: 0.42
Batch: 520; loss: 1.47; acc: 0.53
Batch: 540; loss: 1.63; acc: 0.38
Batch: 560; loss: 1.45; acc: 0.53
Batch: 580; loss: 1.44; acc: 0.5
Batch: 600; loss: 1.64; acc: 0.44
Batch: 620; loss: 1.38; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.5
Batch: 660; loss: 1.42; acc: 0.55
Batch: 680; loss: 1.53; acc: 0.44
Batch: 700; loss: 1.39; acc: 0.53
Batch: 720; loss: 1.52; acc: 0.5
Batch: 740; loss: 1.5; acc: 0.52
Batch: 760; loss: 1.57; acc: 0.45
Batch: 780; loss: 1.6; acc: 0.45
Train Epoch over. train_loss: 1.64; train_accuracy: 0.43 

Batch: 0; loss: 1.47; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.44
Batch: 40; loss: 1.42; acc: 0.52
Batch: 60; loss: 1.46; acc: 0.5
Batch: 80; loss: 1.47; acc: 0.47
Batch: 100; loss: 1.38; acc: 0.55
Batch: 120; loss: 1.49; acc: 0.48
Batch: 140; loss: 1.47; acc: 0.5
Val Epoch over. val_loss: 1.5437839874036752; val_accuracy: 0.4628781847133758 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.45
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.63; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 1.65; acc: 0.45
Batch: 120; loss: 1.31; acc: 0.55
Batch: 140; loss: 1.41; acc: 0.47
Batch: 160; loss: 1.54; acc: 0.5
Batch: 180; loss: 1.43; acc: 0.52
Batch: 200; loss: 1.44; acc: 0.52
Batch: 220; loss: 1.7; acc: 0.36
Batch: 240; loss: 1.28; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.44
Batch: 280; loss: 1.37; acc: 0.55
Batch: 300; loss: 1.42; acc: 0.55
Batch: 320; loss: 1.35; acc: 0.55
Batch: 340; loss: 1.35; acc: 0.56
Batch: 360; loss: 1.44; acc: 0.55
Batch: 380; loss: 1.51; acc: 0.44
Batch: 400; loss: 1.44; acc: 0.42
Batch: 420; loss: 1.53; acc: 0.5
Batch: 440; loss: 1.52; acc: 0.53
Batch: 460; loss: 1.43; acc: 0.55
Batch: 480; loss: 1.51; acc: 0.48
Batch: 500; loss: 1.56; acc: 0.48
Batch: 520; loss: 1.68; acc: 0.39
Batch: 540; loss: 1.6; acc: 0.44
Batch: 560; loss: 1.19; acc: 0.61
Batch: 580; loss: 1.64; acc: 0.5
Batch: 600; loss: 1.53; acc: 0.48
Batch: 620; loss: 1.55; acc: 0.55
Batch: 640; loss: 1.45; acc: 0.55
Batch: 660; loss: 1.42; acc: 0.53
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.27; acc: 0.56
Batch: 720; loss: 1.52; acc: 0.47
Batch: 740; loss: 1.3; acc: 0.53
Batch: 760; loss: 1.46; acc: 0.52
Batch: 780; loss: 1.52; acc: 0.48
Train Epoch over. train_loss: 1.45; train_accuracy: 0.51 

Batch: 0; loss: 1.41; acc: 0.56
Batch: 20; loss: 1.47; acc: 0.45
Batch: 40; loss: 1.28; acc: 0.56
Batch: 60; loss: 1.31; acc: 0.56
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.23; acc: 0.55
Batch: 140; loss: 1.14; acc: 0.67
Val Epoch over. val_loss: 1.3939633612420148; val_accuracy: 0.5254777070063694 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.45
Batch: 20; loss: 1.3; acc: 0.53
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 1.37; acc: 0.52
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.55
Batch: 160; loss: 1.4; acc: 0.58
Batch: 180; loss: 1.42; acc: 0.53
Batch: 200; loss: 1.5; acc: 0.56
Batch: 220; loss: 1.3; acc: 0.56
Batch: 240; loss: 1.23; acc: 0.55
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 1.39; acc: 0.55
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.29; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.48
Batch: 360; loss: 1.29; acc: 0.62
Batch: 380; loss: 1.29; acc: 0.56
Batch: 400; loss: 1.35; acc: 0.61
Batch: 420; loss: 1.32; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.48
Batch: 460; loss: 1.51; acc: 0.52
Batch: 480; loss: 1.28; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.48
Batch: 520; loss: 1.41; acc: 0.47
Batch: 540; loss: 1.18; acc: 0.59
Batch: 560; loss: 1.28; acc: 0.56
Batch: 580; loss: 1.46; acc: 0.5
Batch: 600; loss: 1.15; acc: 0.64
Batch: 620; loss: 1.27; acc: 0.62
Batch: 640; loss: 1.35; acc: 0.55
Batch: 660; loss: 1.3; acc: 0.58
Batch: 680; loss: 1.46; acc: 0.41
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 1.13; acc: 0.58
Batch: 740; loss: 1.21; acc: 0.56
Batch: 760; loss: 1.24; acc: 0.62
Batch: 780; loss: 1.23; acc: 0.61
Train Epoch over. train_loss: 1.35; train_accuracy: 0.55 

Batch: 0; loss: 1.64; acc: 0.42
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.28; acc: 0.58
Batch: 60; loss: 1.31; acc: 0.56
Batch: 80; loss: 1.26; acc: 0.61
Batch: 100; loss: 1.44; acc: 0.56
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.33; acc: 0.56
Val Epoch over. val_loss: 1.5150863228330187; val_accuracy: 0.5002985668789809 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.47
Batch: 20; loss: 1.23; acc: 0.55
Batch: 40; loss: 1.3; acc: 0.55
Batch: 60; loss: 1.49; acc: 0.5
Batch: 80; loss: 1.0; acc: 0.73
Batch: 100; loss: 1.42; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.5
Batch: 140; loss: 1.34; acc: 0.53
Batch: 160; loss: 1.49; acc: 0.48
Batch: 180; loss: 1.46; acc: 0.47
Batch: 200; loss: 1.35; acc: 0.56
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 1.14; acc: 0.64
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 1.4; acc: 0.56
Batch: 300; loss: 1.28; acc: 0.58
Batch: 320; loss: 1.2; acc: 0.62
Batch: 340; loss: 1.35; acc: 0.53
Batch: 360; loss: 1.33; acc: 0.58
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 1.3; acc: 0.62
Batch: 420; loss: 1.05; acc: 0.56
Batch: 440; loss: 1.29; acc: 0.53
Batch: 460; loss: 1.24; acc: 0.55
Batch: 480; loss: 1.35; acc: 0.52
Batch: 500; loss: 1.5; acc: 0.62
Batch: 520; loss: 1.2; acc: 0.53
Batch: 540; loss: 1.31; acc: 0.56
Batch: 560; loss: 1.3; acc: 0.56
Batch: 580; loss: 1.3; acc: 0.56
Batch: 600; loss: 1.28; acc: 0.56
Batch: 620; loss: 1.07; acc: 0.67
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.32; acc: 0.52
Batch: 680; loss: 1.39; acc: 0.53
Batch: 700; loss: 1.26; acc: 0.59
Batch: 720; loss: 1.53; acc: 0.45
Batch: 740; loss: 1.54; acc: 0.44
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.58; acc: 0.42
Train Epoch over. train_loss: 1.32; train_accuracy: 0.56 

Batch: 0; loss: 1.35; acc: 0.56
Batch: 20; loss: 1.37; acc: 0.5
Batch: 40; loss: 1.13; acc: 0.61
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.56
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 1.08; acc: 0.62
Val Epoch over. val_loss: 1.2885222567874155; val_accuracy: 0.5592157643312102 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.26; acc: 0.59
Batch: 80; loss: 1.21; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.48
Batch: 120; loss: 1.4; acc: 0.5
Batch: 140; loss: 1.42; acc: 0.52
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.12; acc: 0.59
Batch: 200; loss: 1.55; acc: 0.48
Batch: 220; loss: 1.83; acc: 0.41
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.26; acc: 0.61
Batch: 300; loss: 1.42; acc: 0.56
Batch: 320; loss: 1.67; acc: 0.48
Batch: 340; loss: 1.54; acc: 0.44
Batch: 360; loss: 1.39; acc: 0.5
Batch: 380; loss: 1.26; acc: 0.56
Batch: 400; loss: 1.31; acc: 0.55
Batch: 420; loss: 1.27; acc: 0.55
Batch: 440; loss: 1.29; acc: 0.55
Batch: 460; loss: 1.16; acc: 0.59
Batch: 480; loss: 1.27; acc: 0.56
Batch: 500; loss: 1.7; acc: 0.5
Batch: 520; loss: 1.57; acc: 0.44
Batch: 540; loss: 1.42; acc: 0.56
Batch: 560; loss: 1.26; acc: 0.56
Batch: 580; loss: 1.18; acc: 0.66
Batch: 600; loss: 1.3; acc: 0.55
Batch: 620; loss: 1.42; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.61
Batch: 660; loss: 1.46; acc: 0.48
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 1.13; acc: 0.61
Batch: 720; loss: 1.27; acc: 0.55
Batch: 740; loss: 1.44; acc: 0.56
Batch: 760; loss: 1.51; acc: 0.47
Batch: 780; loss: 1.34; acc: 0.55
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 1.54; acc: 0.52
Batch: 20; loss: 1.23; acc: 0.48
Batch: 40; loss: 1.21; acc: 0.55
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 1.29; acc: 0.59
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 1.05; acc: 0.67
Val Epoch over. val_loss: 1.345543950985951; val_accuracy: 0.5380175159235668 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.56
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.44; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.52; acc: 0.48
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 1.06; acc: 0.64
Batch: 160; loss: 1.39; acc: 0.52
Batch: 180; loss: 1.46; acc: 0.53
Batch: 200; loss: 1.34; acc: 0.52
Batch: 220; loss: 1.61; acc: 0.45
Batch: 240; loss: 1.37; acc: 0.59
Batch: 260; loss: 1.42; acc: 0.53
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 1.36; acc: 0.53
Batch: 320; loss: 1.24; acc: 0.58
Batch: 340; loss: 1.15; acc: 0.59
Batch: 360; loss: 1.29; acc: 0.55
Batch: 380; loss: 1.65; acc: 0.48
Batch: 400; loss: 1.4; acc: 0.47
Batch: 420; loss: 1.61; acc: 0.47
Batch: 440; loss: 1.72; acc: 0.48
Batch: 460; loss: 1.15; acc: 0.53
Batch: 480; loss: 1.25; acc: 0.62
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.5
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.24; acc: 0.59
Batch: 580; loss: 1.0; acc: 0.67
Batch: 600; loss: 1.45; acc: 0.56
Batch: 620; loss: 1.25; acc: 0.61
Batch: 640; loss: 1.37; acc: 0.5
Batch: 660; loss: 1.11; acc: 0.64
Batch: 680; loss: 1.26; acc: 0.61
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 1.14; acc: 0.58
Batch: 740; loss: 1.27; acc: 0.52
Batch: 760; loss: 1.53; acc: 0.42
Batch: 780; loss: 1.25; acc: 0.56
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 2.02; acc: 0.28
Batch: 20; loss: 1.44; acc: 0.55
Batch: 40; loss: 1.62; acc: 0.45
Batch: 60; loss: 1.65; acc: 0.45
Batch: 80; loss: 1.79; acc: 0.42
Batch: 100; loss: 1.36; acc: 0.5
Batch: 120; loss: 1.6; acc: 0.39
Batch: 140; loss: 1.36; acc: 0.53
Val Epoch over. val_loss: 1.728936154371614; val_accuracy: 0.4099323248407643 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.38
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 1.3; acc: 0.59
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 1.22; acc: 0.55
Batch: 120; loss: 1.36; acc: 0.48
Batch: 140; loss: 1.23; acc: 0.66
Batch: 160; loss: 1.25; acc: 0.61
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.46; acc: 0.53
Batch: 240; loss: 1.37; acc: 0.55
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 1.14; acc: 0.58
Batch: 300; loss: 1.45; acc: 0.48
Batch: 320; loss: 1.28; acc: 0.59
Batch: 340; loss: 1.32; acc: 0.53
Batch: 360; loss: 1.39; acc: 0.59
Batch: 380; loss: 1.42; acc: 0.52
Batch: 400; loss: 1.23; acc: 0.62
Batch: 420; loss: 1.49; acc: 0.55
Batch: 440; loss: 1.38; acc: 0.53
Batch: 460; loss: 1.32; acc: 0.61
Batch: 480; loss: 1.57; acc: 0.44
Batch: 500; loss: 1.19; acc: 0.59
Batch: 520; loss: 1.34; acc: 0.59
Batch: 540; loss: 1.16; acc: 0.56
Batch: 560; loss: 1.27; acc: 0.59
Batch: 580; loss: 1.59; acc: 0.45
Batch: 600; loss: 1.17; acc: 0.59
Batch: 620; loss: 1.53; acc: 0.53
Batch: 640; loss: 1.16; acc: 0.66
Batch: 660; loss: 1.07; acc: 0.64
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.1; acc: 0.67
Batch: 720; loss: 1.38; acc: 0.56
Batch: 740; loss: 1.4; acc: 0.56
Batch: 760; loss: 1.03; acc: 0.67
Batch: 780; loss: 1.08; acc: 0.58
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 1.41; acc: 0.52
Batch: 20; loss: 1.36; acc: 0.53
Batch: 40; loss: 1.19; acc: 0.64
Batch: 60; loss: 1.23; acc: 0.59
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.67
Val Epoch over. val_loss: 1.31462300848809; val_accuracy: 0.5559315286624203 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.56
Batch: 20; loss: 1.26; acc: 0.56
Batch: 40; loss: 1.31; acc: 0.58
Batch: 60; loss: 1.17; acc: 0.56
Batch: 80; loss: 1.43; acc: 0.5
Batch: 100; loss: 0.99; acc: 0.67
Batch: 120; loss: 1.27; acc: 0.56
Batch: 140; loss: 1.22; acc: 0.61
Batch: 160; loss: 1.34; acc: 0.53
Batch: 180; loss: 1.23; acc: 0.56
Batch: 200; loss: 1.48; acc: 0.61
Batch: 220; loss: 1.24; acc: 0.56
Batch: 240; loss: 1.36; acc: 0.59
Batch: 260; loss: 1.25; acc: 0.55
Batch: 280; loss: 1.62; acc: 0.47
Batch: 300; loss: 1.11; acc: 0.62
Batch: 320; loss: 1.08; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.62
Batch: 360; loss: 1.46; acc: 0.53
Batch: 380; loss: 1.39; acc: 0.48
Batch: 400; loss: 1.41; acc: 0.59
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.33; acc: 0.56
Batch: 460; loss: 1.44; acc: 0.55
Batch: 480; loss: 1.15; acc: 0.58
Batch: 500; loss: 1.48; acc: 0.52
Batch: 520; loss: 1.37; acc: 0.52
Batch: 540; loss: 1.44; acc: 0.45
Batch: 560; loss: 1.5; acc: 0.48
Batch: 580; loss: 1.34; acc: 0.47
Batch: 600; loss: 1.32; acc: 0.58
Batch: 620; loss: 1.26; acc: 0.59
Batch: 640; loss: 1.15; acc: 0.61
Batch: 660; loss: 1.05; acc: 0.7
Batch: 680; loss: 1.31; acc: 0.53
Batch: 700; loss: 1.29; acc: 0.56
Batch: 720; loss: 1.21; acc: 0.59
Batch: 740; loss: 0.99; acc: 0.67
Batch: 760; loss: 1.43; acc: 0.52
Batch: 780; loss: 1.46; acc: 0.56
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 1.53; acc: 0.47
Batch: 20; loss: 1.9; acc: 0.36
Batch: 40; loss: 1.4; acc: 0.55
Batch: 60; loss: 1.4; acc: 0.55
Batch: 80; loss: 1.33; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.53
Val Epoch over. val_loss: 1.5708869088227582; val_accuracy: 0.48387738853503187 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.52; acc: 0.48
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 1.32; acc: 0.62
Batch: 60; loss: 1.15; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.41
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.5
Batch: 140; loss: 1.29; acc: 0.55
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.27; acc: 0.56
Batch: 200; loss: 1.55; acc: 0.44
Batch: 220; loss: 1.33; acc: 0.53
Batch: 240; loss: 1.17; acc: 0.64
Batch: 260; loss: 1.29; acc: 0.58
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.38; acc: 0.44
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 1.33; acc: 0.48
Batch: 380; loss: 1.37; acc: 0.56
Batch: 400; loss: 1.24; acc: 0.55
Batch: 420; loss: 1.35; acc: 0.52
Batch: 440; loss: 1.28; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.55
Batch: 480; loss: 1.51; acc: 0.47
Batch: 500; loss: 1.17; acc: 0.55
Batch: 520; loss: 1.47; acc: 0.59
Batch: 540; loss: 1.49; acc: 0.58
Batch: 560; loss: 1.21; acc: 0.59
Batch: 580; loss: 1.54; acc: 0.53
Batch: 600; loss: 1.39; acc: 0.5
Batch: 620; loss: 1.23; acc: 0.56
Batch: 640; loss: 1.19; acc: 0.62
Batch: 660; loss: 1.15; acc: 0.66
Batch: 680; loss: 1.28; acc: 0.58
Batch: 700; loss: 1.18; acc: 0.59
Batch: 720; loss: 1.28; acc: 0.55
Batch: 740; loss: 1.19; acc: 0.66
Batch: 760; loss: 1.14; acc: 0.59
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.36; acc: 0.5
Batch: 20; loss: 1.35; acc: 0.52
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 1.14; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.66
Val Epoch over. val_loss: 1.255988194684314; val_accuracy: 0.5838972929936306 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.62
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.21; acc: 0.59
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.5
Batch: 100; loss: 1.22; acc: 0.56
Batch: 120; loss: 1.03; acc: 0.66
Batch: 140; loss: 1.43; acc: 0.5
Batch: 160; loss: 1.22; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.58
Batch: 200; loss: 1.34; acc: 0.52
Batch: 220; loss: 1.29; acc: 0.53
Batch: 240; loss: 1.32; acc: 0.47
Batch: 260; loss: 1.33; acc: 0.48
Batch: 280; loss: 1.4; acc: 0.5
Batch: 300; loss: 1.24; acc: 0.61
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.55; acc: 0.52
Batch: 360; loss: 1.31; acc: 0.59
Batch: 380; loss: 1.61; acc: 0.45
Batch: 400; loss: 1.44; acc: 0.53
Batch: 420; loss: 1.22; acc: 0.58
Batch: 440; loss: 1.35; acc: 0.53
Batch: 460; loss: 1.31; acc: 0.53
Batch: 480; loss: 1.24; acc: 0.67
Batch: 500; loss: 1.36; acc: 0.52
Batch: 520; loss: 1.33; acc: 0.53
Batch: 540; loss: 1.39; acc: 0.56
Batch: 560; loss: 1.25; acc: 0.55
Batch: 580; loss: 1.62; acc: 0.52
Batch: 600; loss: 1.67; acc: 0.44
Batch: 620; loss: 1.26; acc: 0.56
Batch: 640; loss: 1.44; acc: 0.5
Batch: 660; loss: 1.21; acc: 0.62
Batch: 680; loss: 0.96; acc: 0.7
Batch: 700; loss: 1.4; acc: 0.53
Batch: 720; loss: 1.35; acc: 0.56
Batch: 740; loss: 1.07; acc: 0.66
Batch: 760; loss: 1.45; acc: 0.53
Batch: 780; loss: 1.25; acc: 0.62
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.35; acc: 0.53
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.69
Val Epoch over. val_loss: 1.2403116712144986; val_accuracy: 0.5920581210191083 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.42; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.53
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 1.5; acc: 0.44
Batch: 180; loss: 1.18; acc: 0.58
Batch: 200; loss: 1.27; acc: 0.62
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.46; acc: 0.5
Batch: 260; loss: 1.13; acc: 0.56
Batch: 280; loss: 1.3; acc: 0.59
Batch: 300; loss: 1.23; acc: 0.56
Batch: 320; loss: 1.24; acc: 0.55
Batch: 340; loss: 1.21; acc: 0.58
Batch: 360; loss: 1.49; acc: 0.52
Batch: 380; loss: 1.39; acc: 0.52
Batch: 400; loss: 1.35; acc: 0.64
Batch: 420; loss: 1.5; acc: 0.5
Batch: 440; loss: 1.14; acc: 0.62
Batch: 460; loss: 1.03; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.62
Batch: 500; loss: 1.41; acc: 0.48
Batch: 520; loss: 1.14; acc: 0.59
Batch: 540; loss: 1.39; acc: 0.48
Batch: 560; loss: 1.49; acc: 0.45
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.41; acc: 0.5
Batch: 620; loss: 1.23; acc: 0.59
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 1.38; acc: 0.56
Batch: 680; loss: 1.59; acc: 0.48
Batch: 700; loss: 1.14; acc: 0.59
Batch: 720; loss: 1.3; acc: 0.59
Batch: 740; loss: 1.18; acc: 0.61
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.07; acc: 0.66
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.36; acc: 0.48
Batch: 20; loss: 1.3; acc: 0.55
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.11; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 1.01; acc: 0.7
Val Epoch over. val_loss: 1.2474740291856656; val_accuracy: 0.5911624203821656 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.53; acc: 0.44
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.15; acc: 0.58
Batch: 60; loss: 1.7; acc: 0.44
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 1.33; acc: 0.56
Batch: 120; loss: 1.5; acc: 0.5
Batch: 140; loss: 1.1; acc: 0.56
Batch: 160; loss: 1.37; acc: 0.53
Batch: 180; loss: 1.39; acc: 0.52
Batch: 200; loss: 1.59; acc: 0.48
Batch: 220; loss: 1.34; acc: 0.59
Batch: 240; loss: 1.25; acc: 0.56
Batch: 260; loss: 1.22; acc: 0.56
Batch: 280; loss: 1.06; acc: 0.64
Batch: 300; loss: 1.42; acc: 0.53
Batch: 320; loss: 1.31; acc: 0.48
Batch: 340; loss: 1.42; acc: 0.5
Batch: 360; loss: 1.14; acc: 0.59
Batch: 380; loss: 1.14; acc: 0.58
Batch: 400; loss: 1.14; acc: 0.61
Batch: 420; loss: 1.3; acc: 0.66
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 1.25; acc: 0.62
Batch: 480; loss: 1.42; acc: 0.48
Batch: 500; loss: 1.01; acc: 0.69
Batch: 520; loss: 1.31; acc: 0.56
Batch: 540; loss: 1.4; acc: 0.56
Batch: 560; loss: 1.04; acc: 0.64
Batch: 580; loss: 1.27; acc: 0.55
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 1.34; acc: 0.58
Batch: 640; loss: 1.42; acc: 0.56
Batch: 660; loss: 1.19; acc: 0.58
Batch: 680; loss: 1.43; acc: 0.58
Batch: 700; loss: 1.33; acc: 0.56
Batch: 720; loss: 1.26; acc: 0.61
Batch: 740; loss: 1.09; acc: 0.58
Batch: 760; loss: 1.37; acc: 0.52
Batch: 780; loss: 1.35; acc: 0.52
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.43; acc: 0.5
Batch: 20; loss: 1.27; acc: 0.53
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.58
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.55
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.257750968264926; val_accuracy: 0.5779259554140127 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.42; acc: 0.56
Batch: 20; loss: 1.33; acc: 0.53
Batch: 40; loss: 1.27; acc: 0.56
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.56
Batch: 160; loss: 1.39; acc: 0.53
Batch: 180; loss: 1.28; acc: 0.55
Batch: 200; loss: 1.11; acc: 0.53
Batch: 220; loss: 1.23; acc: 0.5
Batch: 240; loss: 1.07; acc: 0.67
Batch: 260; loss: 1.48; acc: 0.48
Batch: 280; loss: 1.24; acc: 0.64
Batch: 300; loss: 1.23; acc: 0.52
Batch: 320; loss: 1.21; acc: 0.59
Batch: 340; loss: 1.3; acc: 0.58
Batch: 360; loss: 1.21; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.59
Batch: 400; loss: 1.19; acc: 0.53
Batch: 420; loss: 1.35; acc: 0.53
Batch: 440; loss: 1.28; acc: 0.52
Batch: 460; loss: 1.26; acc: 0.66
Batch: 480; loss: 1.54; acc: 0.47
Batch: 500; loss: 1.11; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.56
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 1.34; acc: 0.5
Batch: 580; loss: 1.15; acc: 0.61
Batch: 600; loss: 1.1; acc: 0.61
Batch: 620; loss: 1.38; acc: 0.55
Batch: 640; loss: 1.35; acc: 0.55
Batch: 660; loss: 1.16; acc: 0.66
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.28; acc: 0.58
Batch: 720; loss: 1.28; acc: 0.53
Batch: 740; loss: 1.32; acc: 0.53
Batch: 760; loss: 1.26; acc: 0.56
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.25; acc: 0.58
Batch: 60; loss: 1.31; acc: 0.55
Batch: 80; loss: 1.39; acc: 0.48
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.47
Batch: 140; loss: 1.11; acc: 0.59
Val Epoch over. val_loss: 1.4101472060391858; val_accuracy: 0.5030851910828026 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.43; acc: 0.47
Batch: 20; loss: 1.18; acc: 0.55
Batch: 40; loss: 1.22; acc: 0.55
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 1.26; acc: 0.56
Batch: 100; loss: 1.2; acc: 0.62
Batch: 120; loss: 1.37; acc: 0.48
Batch: 140; loss: 1.34; acc: 0.62
Batch: 160; loss: 1.34; acc: 0.56
Batch: 180; loss: 1.31; acc: 0.64
Batch: 200; loss: 1.3; acc: 0.58
Batch: 220; loss: 1.21; acc: 0.56
Batch: 240; loss: 1.14; acc: 0.61
Batch: 260; loss: 1.17; acc: 0.66
Batch: 280; loss: 1.2; acc: 0.61
Batch: 300; loss: 1.23; acc: 0.53
Batch: 320; loss: 1.46; acc: 0.53
Batch: 340; loss: 1.27; acc: 0.55
Batch: 360; loss: 1.21; acc: 0.56
Batch: 380; loss: 1.3; acc: 0.53
Batch: 400; loss: 1.4; acc: 0.48
Batch: 420; loss: 1.29; acc: 0.55
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.41; acc: 0.5
Batch: 500; loss: 1.24; acc: 0.55
Batch: 520; loss: 1.2; acc: 0.53
Batch: 540; loss: 1.43; acc: 0.58
Batch: 560; loss: 1.3; acc: 0.58
Batch: 580; loss: 1.37; acc: 0.58
Batch: 600; loss: 1.31; acc: 0.53
Batch: 620; loss: 1.31; acc: 0.56
Batch: 640; loss: 1.36; acc: 0.56
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.39; acc: 0.53
Batch: 700; loss: 1.32; acc: 0.61
Batch: 720; loss: 1.2; acc: 0.69
Batch: 740; loss: 1.28; acc: 0.58
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.58
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.41; acc: 0.48
Batch: 20; loss: 1.31; acc: 0.47
Batch: 40; loss: 1.12; acc: 0.67
Batch: 60; loss: 1.16; acc: 0.59
Batch: 80; loss: 1.17; acc: 0.58
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.58
Batch: 140; loss: 1.04; acc: 0.64
Val Epoch over. val_loss: 1.268599674959851; val_accuracy: 0.5721536624203821 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.46; acc: 0.47
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 1.42; acc: 0.58
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 1.34; acc: 0.62
Batch: 100; loss: 1.26; acc: 0.52
Batch: 120; loss: 1.27; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.61
Batch: 160; loss: 1.42; acc: 0.55
Batch: 180; loss: 1.48; acc: 0.55
Batch: 200; loss: 1.3; acc: 0.53
Batch: 220; loss: 1.65; acc: 0.44
Batch: 240; loss: 1.33; acc: 0.59
Batch: 260; loss: 1.5; acc: 0.47
Batch: 280; loss: 1.04; acc: 0.66
Batch: 300; loss: 1.46; acc: 0.47
Batch: 320; loss: 1.37; acc: 0.53
Batch: 340; loss: 1.27; acc: 0.59
Batch: 360; loss: 1.23; acc: 0.55
Batch: 380; loss: 1.11; acc: 0.62
Batch: 400; loss: 1.25; acc: 0.62
Batch: 420; loss: 1.41; acc: 0.48
Batch: 440; loss: 1.24; acc: 0.58
Batch: 460; loss: 1.51; acc: 0.55
Batch: 480; loss: 1.04; acc: 0.66
Batch: 500; loss: 1.2; acc: 0.58
Batch: 520; loss: 1.28; acc: 0.56
Batch: 540; loss: 1.36; acc: 0.55
Batch: 560; loss: 1.35; acc: 0.56
Batch: 580; loss: 1.22; acc: 0.62
Batch: 600; loss: 1.47; acc: 0.52
Batch: 620; loss: 1.26; acc: 0.59
Batch: 640; loss: 1.18; acc: 0.53
Batch: 660; loss: 1.34; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.59
Batch: 700; loss: 1.13; acc: 0.67
Batch: 720; loss: 1.46; acc: 0.59
Batch: 740; loss: 1.22; acc: 0.62
Batch: 760; loss: 1.23; acc: 0.58
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.48
Batch: 20; loss: 1.35; acc: 0.55
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 1.07; acc: 0.66
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 1.01; acc: 0.67
Val Epoch over. val_loss: 1.2434904863879939; val_accuracy: 0.5958399681528662 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.17; acc: 0.58
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 1.26; acc: 0.58
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 1.34; acc: 0.59
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.62
Batch: 160; loss: 1.18; acc: 0.58
Batch: 180; loss: 1.32; acc: 0.55
Batch: 200; loss: 1.38; acc: 0.56
Batch: 220; loss: 0.94; acc: 0.69
Batch: 240; loss: 1.4; acc: 0.52
Batch: 260; loss: 1.32; acc: 0.55
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.18; acc: 0.58
Batch: 320; loss: 1.48; acc: 0.56
Batch: 340; loss: 1.4; acc: 0.52
Batch: 360; loss: 1.41; acc: 0.58
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.42; acc: 0.48
Batch: 420; loss: 1.37; acc: 0.5
Batch: 440; loss: 1.43; acc: 0.5
Batch: 460; loss: 1.27; acc: 0.53
Batch: 480; loss: 1.38; acc: 0.56
Batch: 500; loss: 1.08; acc: 0.67
Batch: 520; loss: 1.4; acc: 0.53
Batch: 540; loss: 1.07; acc: 0.62
Batch: 560; loss: 1.6; acc: 0.44
Batch: 580; loss: 1.36; acc: 0.52
Batch: 600; loss: 1.22; acc: 0.66
Batch: 620; loss: 1.34; acc: 0.48
Batch: 640; loss: 1.33; acc: 0.59
Batch: 660; loss: 1.27; acc: 0.56
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.57; acc: 0.5
Batch: 720; loss: 1.02; acc: 0.64
Batch: 740; loss: 1.36; acc: 0.53
Batch: 760; loss: 1.5; acc: 0.45
Batch: 780; loss: 1.22; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.36; acc: 0.5
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 1.09; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.15; acc: 0.59
Batch: 100; loss: 1.04; acc: 0.7
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.72
Val Epoch over. val_loss: 1.243734005530169; val_accuracy: 0.5850915605095541 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.59
Batch: 20; loss: 1.44; acc: 0.53
Batch: 40; loss: 1.49; acc: 0.44
Batch: 60; loss: 1.37; acc: 0.55
Batch: 80; loss: 1.2; acc: 0.67
Batch: 100; loss: 1.23; acc: 0.58
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.33; acc: 0.52
Batch: 160; loss: 1.36; acc: 0.47
Batch: 180; loss: 1.3; acc: 0.58
Batch: 200; loss: 1.37; acc: 0.55
Batch: 220; loss: 1.4; acc: 0.53
Batch: 240; loss: 1.1; acc: 0.59
Batch: 260; loss: 1.36; acc: 0.58
Batch: 280; loss: 1.28; acc: 0.59
Batch: 300; loss: 1.4; acc: 0.56
Batch: 320; loss: 1.15; acc: 0.58
Batch: 340; loss: 1.3; acc: 0.5
Batch: 360; loss: 1.18; acc: 0.55
Batch: 380; loss: 1.49; acc: 0.56
Batch: 400; loss: 1.46; acc: 0.56
Batch: 420; loss: 1.21; acc: 0.61
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.36; acc: 0.61
Batch: 480; loss: 1.3; acc: 0.56
Batch: 500; loss: 1.28; acc: 0.61
Batch: 520; loss: 0.99; acc: 0.66
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.25; acc: 0.5
Batch: 580; loss: 1.2; acc: 0.61
Batch: 600; loss: 1.07; acc: 0.69
Batch: 620; loss: 1.7; acc: 0.47
Batch: 640; loss: 1.16; acc: 0.67
Batch: 660; loss: 1.17; acc: 0.61
Batch: 680; loss: 1.48; acc: 0.47
Batch: 700; loss: 1.21; acc: 0.61
Batch: 720; loss: 1.18; acc: 0.62
Batch: 740; loss: 1.13; acc: 0.58
Batch: 760; loss: 1.47; acc: 0.5
Batch: 780; loss: 1.27; acc: 0.58
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.48
Batch: 20; loss: 1.31; acc: 0.52
Batch: 40; loss: 1.11; acc: 0.62
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 1.15; acc: 0.61
Batch: 100; loss: 1.03; acc: 0.67
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.97; acc: 0.69
Val Epoch over. val_loss: 1.2573825549927486; val_accuracy: 0.5838972929936306 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 1.23; acc: 0.58
Batch: 60; loss: 1.49; acc: 0.58
Batch: 80; loss: 1.42; acc: 0.47
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.32; acc: 0.56
Batch: 140; loss: 1.06; acc: 0.61
Batch: 160; loss: 1.16; acc: 0.67
Batch: 180; loss: 1.22; acc: 0.66
Batch: 200; loss: 1.06; acc: 0.67
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.4; acc: 0.52
Batch: 260; loss: 1.48; acc: 0.52
Batch: 280; loss: 1.1; acc: 0.67
Batch: 300; loss: 1.43; acc: 0.41
Batch: 320; loss: 1.29; acc: 0.59
Batch: 340; loss: 1.28; acc: 0.53
Batch: 360; loss: 1.31; acc: 0.58
Batch: 380; loss: 1.24; acc: 0.59
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.3; acc: 0.56
Batch: 440; loss: 1.31; acc: 0.62
Batch: 460; loss: 1.22; acc: 0.59
Batch: 480; loss: 1.31; acc: 0.53
Batch: 500; loss: 1.08; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.53
Batch: 540; loss: 1.24; acc: 0.59
Batch: 560; loss: 1.57; acc: 0.45
Batch: 580; loss: 1.42; acc: 0.58
Batch: 600; loss: 1.15; acc: 0.59
Batch: 620; loss: 1.32; acc: 0.59
Batch: 640; loss: 1.38; acc: 0.55
Batch: 660; loss: 1.22; acc: 0.59
Batch: 680; loss: 1.25; acc: 0.58
Batch: 700; loss: 1.48; acc: 0.55
Batch: 720; loss: 1.57; acc: 0.5
Batch: 740; loss: 1.21; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.58
Batch: 780; loss: 1.48; acc: 0.55
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.31; acc: 0.52
Batch: 20; loss: 1.38; acc: 0.5
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 1.03; acc: 0.67
Val Epoch over. val_loss: 1.2499929977830049; val_accuracy: 0.5901671974522293 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 1.27; acc: 0.61
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 1.4; acc: 0.48
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.61
Batch: 160; loss: 1.22; acc: 0.61
Batch: 180; loss: 1.45; acc: 0.48
Batch: 200; loss: 1.13; acc: 0.62
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.31; acc: 0.53
Batch: 260; loss: 0.92; acc: 0.69
Batch: 280; loss: 1.5; acc: 0.48
Batch: 300; loss: 1.15; acc: 0.58
Batch: 320; loss: 1.52; acc: 0.48
Batch: 340; loss: 1.14; acc: 0.61
Batch: 360; loss: 1.23; acc: 0.53
Batch: 380; loss: 1.44; acc: 0.53
Batch: 400; loss: 1.07; acc: 0.69
Batch: 420; loss: 1.5; acc: 0.5
Batch: 440; loss: 1.25; acc: 0.58
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 1.28; acc: 0.52
Batch: 500; loss: 1.12; acc: 0.73
Batch: 520; loss: 1.52; acc: 0.5
Batch: 540; loss: 1.28; acc: 0.61
Batch: 560; loss: 1.21; acc: 0.66
Batch: 580; loss: 1.23; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.61
Batch: 620; loss: 1.44; acc: 0.5
Batch: 640; loss: 1.39; acc: 0.59
Batch: 660; loss: 1.28; acc: 0.55
Batch: 680; loss: 1.0; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.67
Batch: 720; loss: 1.09; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.66
Batch: 760; loss: 1.36; acc: 0.56
Batch: 780; loss: 1.35; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.48
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.12; acc: 0.66
Batch: 80; loss: 1.09; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 1.02; acc: 0.69
Val Epoch over. val_loss: 1.2424076396948214; val_accuracy: 0.5919585987261147 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.22; acc: 0.56
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 1.6; acc: 0.45
Batch: 100; loss: 1.36; acc: 0.56
Batch: 120; loss: 1.64; acc: 0.45
Batch: 140; loss: 1.12; acc: 0.59
Batch: 160; loss: 1.39; acc: 0.56
Batch: 180; loss: 1.24; acc: 0.53
Batch: 200; loss: 1.59; acc: 0.48
Batch: 220; loss: 1.29; acc: 0.55
Batch: 240; loss: 1.22; acc: 0.64
Batch: 260; loss: 1.43; acc: 0.55
Batch: 280; loss: 1.32; acc: 0.52
Batch: 300; loss: 1.22; acc: 0.66
Batch: 320; loss: 1.33; acc: 0.66
Batch: 340; loss: 1.35; acc: 0.55
Batch: 360; loss: 1.39; acc: 0.55
Batch: 380; loss: 1.18; acc: 0.62
Batch: 400; loss: 1.37; acc: 0.61
Batch: 420; loss: 1.39; acc: 0.52
Batch: 440; loss: 1.19; acc: 0.56
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 1.2; acc: 0.62
Batch: 520; loss: 1.05; acc: 0.67
Batch: 540; loss: 1.36; acc: 0.52
Batch: 560; loss: 1.44; acc: 0.55
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.28; acc: 0.58
Batch: 640; loss: 1.07; acc: 0.66
Batch: 660; loss: 1.32; acc: 0.58
Batch: 680; loss: 1.13; acc: 0.59
Batch: 700; loss: 1.24; acc: 0.55
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 1.52; acc: 0.58
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.12; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.0; acc: 0.72
Val Epoch over. val_loss: 1.23174732581825; val_accuracy: 0.5960390127388535 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.33; acc: 0.58
Batch: 40; loss: 1.18; acc: 0.61
Batch: 60; loss: 1.26; acc: 0.56
Batch: 80; loss: 1.18; acc: 0.48
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.38; acc: 0.56
Batch: 160; loss: 1.36; acc: 0.53
Batch: 180; loss: 1.06; acc: 0.66
Batch: 200; loss: 1.21; acc: 0.58
Batch: 220; loss: 1.27; acc: 0.61
Batch: 240; loss: 1.35; acc: 0.52
Batch: 260; loss: 1.05; acc: 0.59
Batch: 280; loss: 1.27; acc: 0.64
Batch: 300; loss: 1.23; acc: 0.61
Batch: 320; loss: 1.32; acc: 0.52
Batch: 340; loss: 1.41; acc: 0.53
Batch: 360; loss: 1.29; acc: 0.52
Batch: 380; loss: 1.33; acc: 0.58
Batch: 400; loss: 1.46; acc: 0.56
Batch: 420; loss: 1.3; acc: 0.61
Batch: 440; loss: 1.07; acc: 0.67
Batch: 460; loss: 1.12; acc: 0.66
Batch: 480; loss: 1.27; acc: 0.56
Batch: 500; loss: 1.38; acc: 0.62
Batch: 520; loss: 1.12; acc: 0.62
Batch: 540; loss: 1.2; acc: 0.56
Batch: 560; loss: 1.49; acc: 0.45
Batch: 580; loss: 1.25; acc: 0.58
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 1.17; acc: 0.59
Batch: 640; loss: 1.41; acc: 0.56
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.15; acc: 0.59
Batch: 700; loss: 1.26; acc: 0.59
Batch: 720; loss: 1.3; acc: 0.53
Batch: 740; loss: 1.32; acc: 0.58
Batch: 760; loss: 1.33; acc: 0.58
Batch: 780; loss: 1.53; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.53
Batch: 20; loss: 1.36; acc: 0.5
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.09; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 1.0; acc: 0.69
Val Epoch over. val_loss: 1.2345673912649702; val_accuracy: 0.5955414012738853 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.35; acc: 0.55
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 1.06; acc: 0.66
Batch: 60; loss: 1.37; acc: 0.55
Batch: 80; loss: 1.2; acc: 0.59
Batch: 100; loss: 1.25; acc: 0.55
Batch: 120; loss: 1.17; acc: 0.61
Batch: 140; loss: 1.5; acc: 0.47
Batch: 160; loss: 1.36; acc: 0.53
Batch: 180; loss: 1.2; acc: 0.52
Batch: 200; loss: 1.13; acc: 0.64
Batch: 220; loss: 1.19; acc: 0.72
Batch: 240; loss: 1.12; acc: 0.53
Batch: 260; loss: 1.15; acc: 0.59
Batch: 280; loss: 1.21; acc: 0.62
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.29; acc: 0.58
Batch: 340; loss: 1.31; acc: 0.48
Batch: 360; loss: 1.2; acc: 0.61
Batch: 380; loss: 1.14; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.59
Batch: 420; loss: 1.16; acc: 0.61
Batch: 440; loss: 1.42; acc: 0.53
Batch: 460; loss: 1.43; acc: 0.58
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.18; acc: 0.56
Batch: 520; loss: 1.09; acc: 0.59
Batch: 540; loss: 1.35; acc: 0.56
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 1.13; acc: 0.66
Batch: 600; loss: 1.22; acc: 0.61
Batch: 620; loss: 1.43; acc: 0.5
Batch: 640; loss: 1.22; acc: 0.58
Batch: 660; loss: 1.52; acc: 0.44
Batch: 680; loss: 1.05; acc: 0.66
Batch: 700; loss: 1.32; acc: 0.58
Batch: 720; loss: 1.09; acc: 0.64
Batch: 740; loss: 1.42; acc: 0.58
Batch: 760; loss: 1.33; acc: 0.48
Batch: 780; loss: 1.26; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.47
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.12; acc: 0.66
Batch: 80; loss: 1.11; acc: 0.61
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.2342350649985538; val_accuracy: 0.5913614649681529 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.21; acc: 0.58
Batch: 20; loss: 1.37; acc: 0.53
Batch: 40; loss: 1.18; acc: 0.62
Batch: 60; loss: 1.22; acc: 0.62
Batch: 80; loss: 1.2; acc: 0.55
Batch: 100; loss: 1.16; acc: 0.56
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 1.1; acc: 0.58
Batch: 160; loss: 1.15; acc: 0.59
Batch: 180; loss: 0.93; acc: 0.7
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.02; acc: 0.77
Batch: 240; loss: 1.22; acc: 0.62
Batch: 260; loss: 1.37; acc: 0.59
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.27; acc: 0.55
Batch: 320; loss: 1.38; acc: 0.53
Batch: 340; loss: 1.34; acc: 0.56
Batch: 360; loss: 1.05; acc: 0.61
Batch: 380; loss: 1.05; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.55
Batch: 420; loss: 1.22; acc: 0.55
Batch: 440; loss: 1.21; acc: 0.52
Batch: 460; loss: 1.21; acc: 0.59
Batch: 480; loss: 1.12; acc: 0.61
Batch: 500; loss: 1.48; acc: 0.55
Batch: 520; loss: 1.31; acc: 0.55
Batch: 540; loss: 1.08; acc: 0.62
Batch: 560; loss: 1.64; acc: 0.36
Batch: 580; loss: 1.19; acc: 0.59
Batch: 600; loss: 1.51; acc: 0.48
Batch: 620; loss: 1.38; acc: 0.5
Batch: 640; loss: 0.98; acc: 0.67
Batch: 660; loss: 1.07; acc: 0.67
Batch: 680; loss: 1.28; acc: 0.62
Batch: 700; loss: 1.17; acc: 0.59
Batch: 720; loss: 1.45; acc: 0.45
Batch: 740; loss: 1.32; acc: 0.56
Batch: 760; loss: 1.34; acc: 0.55
Batch: 780; loss: 1.32; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.36; acc: 0.47
Batch: 20; loss: 1.33; acc: 0.58
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 1.09; acc: 0.62
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.0; acc: 0.72
Val Epoch over. val_loss: 1.2343315770671626; val_accuracy: 0.5966361464968153 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.31; acc: 0.62
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.28; acc: 0.58
Batch: 60; loss: 1.18; acc: 0.58
Batch: 80; loss: 1.34; acc: 0.55
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 1.29; acc: 0.61
Batch: 160; loss: 1.39; acc: 0.5
Batch: 180; loss: 1.25; acc: 0.61
Batch: 200; loss: 1.11; acc: 0.61
Batch: 220; loss: 1.52; acc: 0.44
Batch: 240; loss: 1.2; acc: 0.55
Batch: 260; loss: 1.24; acc: 0.56
Batch: 280; loss: 1.29; acc: 0.56
Batch: 300; loss: 1.28; acc: 0.56
Batch: 320; loss: 1.39; acc: 0.52
Batch: 340; loss: 1.09; acc: 0.61
Batch: 360; loss: 1.28; acc: 0.58
Batch: 380; loss: 1.06; acc: 0.67
Batch: 400; loss: 1.35; acc: 0.56
Batch: 420; loss: 1.24; acc: 0.64
Batch: 440; loss: 1.33; acc: 0.48
Batch: 460; loss: 1.17; acc: 0.69
Batch: 480; loss: 1.24; acc: 0.61
Batch: 500; loss: 1.29; acc: 0.61
Batch: 520; loss: 1.14; acc: 0.61
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 1.23; acc: 0.64
Batch: 580; loss: 1.38; acc: 0.56
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 1.01; acc: 0.61
Batch: 640; loss: 1.31; acc: 0.53
Batch: 660; loss: 0.98; acc: 0.66
Batch: 680; loss: 1.08; acc: 0.69
Batch: 700; loss: 1.09; acc: 0.64
Batch: 720; loss: 1.4; acc: 0.55
Batch: 740; loss: 1.37; acc: 0.56
Batch: 760; loss: 1.42; acc: 0.5
Batch: 780; loss: 1.44; acc: 0.5
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.36; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 1.12; acc: 0.64
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.69
Val Epoch over. val_loss: 1.2398402930065324; val_accuracy: 0.5898686305732485 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.33; acc: 0.53
Batch: 20; loss: 1.39; acc: 0.48
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.42; acc: 0.53
Batch: 80; loss: 1.16; acc: 0.61
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.44; acc: 0.47
Batch: 140; loss: 1.28; acc: 0.5
Batch: 160; loss: 1.31; acc: 0.55
Batch: 180; loss: 1.32; acc: 0.53
Batch: 200; loss: 1.41; acc: 0.59
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 1.1; acc: 0.69
Batch: 260; loss: 1.23; acc: 0.58
Batch: 280; loss: 1.54; acc: 0.48
Batch: 300; loss: 1.54; acc: 0.55
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.04; acc: 0.64
Batch: 360; loss: 1.32; acc: 0.61
Batch: 380; loss: 1.27; acc: 0.53
Batch: 400; loss: 1.36; acc: 0.59
Batch: 420; loss: 1.12; acc: 0.67
Batch: 440; loss: 1.48; acc: 0.47
Batch: 460; loss: 1.41; acc: 0.52
Batch: 480; loss: 1.51; acc: 0.45
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.23; acc: 0.56
Batch: 540; loss: 1.08; acc: 0.61
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 1.24; acc: 0.61
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 1.42; acc: 0.52
Batch: 640; loss: 1.47; acc: 0.53
Batch: 660; loss: 1.52; acc: 0.5
Batch: 680; loss: 1.3; acc: 0.59
Batch: 700; loss: 1.22; acc: 0.62
Batch: 720; loss: 1.17; acc: 0.61
Batch: 740; loss: 1.48; acc: 0.48
Batch: 760; loss: 1.47; acc: 0.56
Batch: 780; loss: 1.55; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.33; acc: 0.52
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.7
Val Epoch over. val_loss: 1.231425787992538; val_accuracy: 0.5960390127388535 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.32; acc: 0.5
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 1.25; acc: 0.56
Batch: 60; loss: 1.45; acc: 0.53
Batch: 80; loss: 1.17; acc: 0.56
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.14; acc: 0.59
Batch: 140; loss: 1.21; acc: 0.58
Batch: 160; loss: 1.39; acc: 0.5
Batch: 180; loss: 1.24; acc: 0.66
Batch: 200; loss: 1.23; acc: 0.62
Batch: 220; loss: 1.51; acc: 0.47
Batch: 240; loss: 1.35; acc: 0.55
Batch: 260; loss: 1.19; acc: 0.59
Batch: 280; loss: 1.5; acc: 0.55
Batch: 300; loss: 1.27; acc: 0.61
Batch: 320; loss: 1.06; acc: 0.59
Batch: 340; loss: 1.22; acc: 0.56
Batch: 360; loss: 1.35; acc: 0.56
Batch: 380; loss: 1.4; acc: 0.5
Batch: 400; loss: 1.4; acc: 0.53
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.21; acc: 0.62
Batch: 460; loss: 1.14; acc: 0.59
Batch: 480; loss: 1.52; acc: 0.48
Batch: 500; loss: 1.05; acc: 0.66
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 1.2; acc: 0.55
Batch: 560; loss: 1.38; acc: 0.55
Batch: 580; loss: 1.35; acc: 0.56
Batch: 600; loss: 0.94; acc: 0.72
Batch: 620; loss: 1.37; acc: 0.48
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 1.54; acc: 0.5
Batch: 680; loss: 1.31; acc: 0.52
Batch: 700; loss: 1.3; acc: 0.59
Batch: 720; loss: 1.31; acc: 0.56
Batch: 740; loss: 1.45; acc: 0.55
Batch: 760; loss: 1.37; acc: 0.58
Batch: 780; loss: 1.57; acc: 0.5
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.52
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.67
Batch: 140; loss: 0.97; acc: 0.7
Val Epoch over. val_loss: 1.2334005396077588; val_accuracy: 0.5973328025477707 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.43; acc: 0.61
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 1.61; acc: 0.41
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.32; acc: 0.5
Batch: 100; loss: 1.51; acc: 0.45
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.16; acc: 0.58
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.04; acc: 0.69
Batch: 200; loss: 1.31; acc: 0.52
Batch: 220; loss: 1.32; acc: 0.47
Batch: 240; loss: 1.5; acc: 0.48
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.26; acc: 0.59
Batch: 320; loss: 1.3; acc: 0.55
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 1.1; acc: 0.61
Batch: 380; loss: 1.17; acc: 0.64
Batch: 400; loss: 1.21; acc: 0.59
Batch: 420; loss: 1.3; acc: 0.59
Batch: 440; loss: 1.25; acc: 0.53
Batch: 460; loss: 1.24; acc: 0.55
Batch: 480; loss: 1.25; acc: 0.59
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.45; acc: 0.55
Batch: 540; loss: 1.24; acc: 0.61
Batch: 560; loss: 1.26; acc: 0.58
Batch: 580; loss: 1.65; acc: 0.48
Batch: 600; loss: 1.08; acc: 0.58
Batch: 620; loss: 1.22; acc: 0.62
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.46; acc: 0.55
Batch: 680; loss: 1.49; acc: 0.52
Batch: 700; loss: 1.09; acc: 0.56
Batch: 720; loss: 1.24; acc: 0.53
Batch: 740; loss: 1.31; acc: 0.53
Batch: 760; loss: 1.6; acc: 0.5
Batch: 780; loss: 1.2; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.37; acc: 0.48
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.14; acc: 0.59
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.69
Val Epoch over. val_loss: 1.2440291867134676; val_accuracy: 0.5857882165605095 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.43; acc: 0.48
Batch: 40; loss: 1.2; acc: 0.61
Batch: 60; loss: 1.02; acc: 0.66
Batch: 80; loss: 1.14; acc: 0.58
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.43; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.53
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.25; acc: 0.66
Batch: 200; loss: 1.18; acc: 0.53
Batch: 220; loss: 1.32; acc: 0.5
Batch: 240; loss: 1.34; acc: 0.56
Batch: 260; loss: 1.48; acc: 0.55
Batch: 280; loss: 1.36; acc: 0.56
Batch: 300; loss: 1.42; acc: 0.55
Batch: 320; loss: 1.35; acc: 0.56
Batch: 340; loss: 1.41; acc: 0.56
Batch: 360; loss: 1.29; acc: 0.58
Batch: 380; loss: 1.35; acc: 0.48
Batch: 400; loss: 1.18; acc: 0.56
Batch: 420; loss: 1.27; acc: 0.64
Batch: 440; loss: 1.26; acc: 0.58
Batch: 460; loss: 1.32; acc: 0.58
Batch: 480; loss: 1.24; acc: 0.52
Batch: 500; loss: 1.38; acc: 0.5
Batch: 520; loss: 1.57; acc: 0.42
Batch: 540; loss: 1.47; acc: 0.5
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.18; acc: 0.61
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.4; acc: 0.52
Batch: 640; loss: 1.35; acc: 0.56
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.18; acc: 0.59
Batch: 700; loss: 1.16; acc: 0.61
Batch: 720; loss: 1.17; acc: 0.64
Batch: 740; loss: 1.03; acc: 0.64
Batch: 760; loss: 1.21; acc: 0.61
Batch: 780; loss: 1.07; acc: 0.64
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.5
Batch: 20; loss: 1.31; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2329931767882816; val_accuracy: 0.5963375796178344 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 1.16; acc: 0.52
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.53
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.32; acc: 0.62
Batch: 140; loss: 1.37; acc: 0.53
Batch: 160; loss: 1.38; acc: 0.56
Batch: 180; loss: 1.35; acc: 0.55
Batch: 200; loss: 1.13; acc: 0.67
Batch: 220; loss: 1.22; acc: 0.62
Batch: 240; loss: 1.33; acc: 0.56
Batch: 260; loss: 1.17; acc: 0.61
Batch: 280; loss: 1.41; acc: 0.5
Batch: 300; loss: 1.37; acc: 0.61
Batch: 320; loss: 1.02; acc: 0.64
Batch: 340; loss: 1.24; acc: 0.66
Batch: 360; loss: 1.23; acc: 0.67
Batch: 380; loss: 1.3; acc: 0.59
Batch: 400; loss: 1.57; acc: 0.56
Batch: 420; loss: 1.5; acc: 0.5
Batch: 440; loss: 1.23; acc: 0.66
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.54; acc: 0.45
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.2; acc: 0.61
Batch: 540; loss: 1.28; acc: 0.56
Batch: 560; loss: 1.12; acc: 0.62
Batch: 580; loss: 1.22; acc: 0.56
Batch: 600; loss: 1.37; acc: 0.61
Batch: 620; loss: 1.24; acc: 0.59
Batch: 640; loss: 1.1; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.64
Batch: 680; loss: 1.6; acc: 0.48
Batch: 700; loss: 1.14; acc: 0.62
Batch: 720; loss: 1.31; acc: 0.61
Batch: 740; loss: 1.3; acc: 0.59
Batch: 760; loss: 1.23; acc: 0.61
Batch: 780; loss: 1.34; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.72
Val Epoch over. val_loss: 1.2308476525507155; val_accuracy: 0.5963375796178344 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 1.18; acc: 0.59
Batch: 60; loss: 1.28; acc: 0.56
Batch: 80; loss: 1.47; acc: 0.44
Batch: 100; loss: 1.38; acc: 0.52
Batch: 120; loss: 1.14; acc: 0.55
Batch: 140; loss: 1.28; acc: 0.56
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 1.21; acc: 0.61
Batch: 200; loss: 1.28; acc: 0.58
Batch: 220; loss: 1.16; acc: 0.56
Batch: 240; loss: 1.09; acc: 0.64
Batch: 260; loss: 1.3; acc: 0.61
Batch: 280; loss: 1.37; acc: 0.5
Batch: 300; loss: 1.12; acc: 0.64
Batch: 320; loss: 1.43; acc: 0.5
Batch: 340; loss: 1.43; acc: 0.58
Batch: 360; loss: 1.39; acc: 0.47
Batch: 380; loss: 1.35; acc: 0.53
Batch: 400; loss: 1.04; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.28; acc: 0.55
Batch: 460; loss: 1.0; acc: 0.69
Batch: 480; loss: 1.44; acc: 0.55
Batch: 500; loss: 1.38; acc: 0.52
Batch: 520; loss: 1.23; acc: 0.61
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.32; acc: 0.5
Batch: 580; loss: 1.07; acc: 0.69
Batch: 600; loss: 1.2; acc: 0.64
Batch: 620; loss: 1.32; acc: 0.52
Batch: 640; loss: 1.43; acc: 0.53
Batch: 660; loss: 1.23; acc: 0.58
Batch: 680; loss: 1.13; acc: 0.61
Batch: 700; loss: 1.17; acc: 0.64
Batch: 720; loss: 1.2; acc: 0.62
Batch: 740; loss: 1.35; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.52
Batch: 780; loss: 1.36; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.33; acc: 0.52
Batch: 40; loss: 1.07; acc: 0.69
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.67
Val Epoch over. val_loss: 1.2307304906996952; val_accuracy: 0.5939490445859873 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.4; acc: 0.59
Batch: 20; loss: 1.46; acc: 0.52
Batch: 40; loss: 1.24; acc: 0.62
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 1.34; acc: 0.53
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 0.98; acc: 0.66
Batch: 160; loss: 1.33; acc: 0.52
Batch: 180; loss: 1.32; acc: 0.58
Batch: 200; loss: 1.42; acc: 0.5
Batch: 220; loss: 1.03; acc: 0.67
Batch: 240; loss: 1.17; acc: 0.58
Batch: 260; loss: 1.43; acc: 0.55
Batch: 280; loss: 1.36; acc: 0.55
Batch: 300; loss: 1.51; acc: 0.55
Batch: 320; loss: 1.35; acc: 0.62
Batch: 340; loss: 1.45; acc: 0.53
Batch: 360; loss: 1.55; acc: 0.45
Batch: 380; loss: 1.3; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.59
Batch: 420; loss: 1.11; acc: 0.61
Batch: 440; loss: 1.32; acc: 0.53
Batch: 460; loss: 1.1; acc: 0.59
Batch: 480; loss: 1.26; acc: 0.56
Batch: 500; loss: 1.29; acc: 0.67
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 0.97; acc: 0.69
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.36; acc: 0.59
Batch: 600; loss: 1.49; acc: 0.53
Batch: 620; loss: 1.52; acc: 0.5
Batch: 640; loss: 1.47; acc: 0.45
Batch: 660; loss: 1.3; acc: 0.53
Batch: 680; loss: 1.18; acc: 0.59
Batch: 700; loss: 1.22; acc: 0.58
Batch: 720; loss: 1.07; acc: 0.66
Batch: 740; loss: 1.34; acc: 0.53
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.36; acc: 0.47
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.12; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.62
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.2324250310089937; val_accuracy: 0.5944466560509554 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.11; acc: 0.58
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.46; acc: 0.53
Batch: 120; loss: 1.45; acc: 0.5
Batch: 140; loss: 1.17; acc: 0.58
Batch: 160; loss: 1.14; acc: 0.64
Batch: 180; loss: 1.19; acc: 0.66
Batch: 200; loss: 1.32; acc: 0.53
Batch: 220; loss: 1.14; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.61
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.65; acc: 0.52
Batch: 300; loss: 1.15; acc: 0.7
Batch: 320; loss: 1.16; acc: 0.59
Batch: 340; loss: 1.09; acc: 0.62
Batch: 360; loss: 1.34; acc: 0.59
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 1.29; acc: 0.48
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 1.33; acc: 0.53
Batch: 460; loss: 1.41; acc: 0.47
Batch: 480; loss: 1.38; acc: 0.5
Batch: 500; loss: 1.03; acc: 0.66
Batch: 520; loss: 1.32; acc: 0.62
Batch: 540; loss: 1.44; acc: 0.58
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.37; acc: 0.53
Batch: 600; loss: 1.02; acc: 0.67
Batch: 620; loss: 1.39; acc: 0.61
Batch: 640; loss: 1.33; acc: 0.56
Batch: 660; loss: 1.15; acc: 0.56
Batch: 680; loss: 1.47; acc: 0.5
Batch: 700; loss: 1.56; acc: 0.48
Batch: 720; loss: 1.28; acc: 0.62
Batch: 740; loss: 1.17; acc: 0.59
Batch: 760; loss: 1.31; acc: 0.56
Batch: 780; loss: 1.36; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.33; acc: 0.55
Batch: 40; loss: 1.06; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.72
Val Epoch over. val_loss: 1.2303605132801518; val_accuracy: 0.5959394904458599 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.29; acc: 0.56
Batch: 20; loss: 1.45; acc: 0.48
Batch: 40; loss: 1.24; acc: 0.58
Batch: 60; loss: 1.26; acc: 0.61
Batch: 80; loss: 1.36; acc: 0.5
Batch: 100; loss: 1.22; acc: 0.56
Batch: 120; loss: 1.25; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.56
Batch: 160; loss: 1.3; acc: 0.58
Batch: 180; loss: 1.08; acc: 0.58
Batch: 200; loss: 1.3; acc: 0.66
Batch: 220; loss: 1.42; acc: 0.56
Batch: 240; loss: 1.28; acc: 0.58
Batch: 260; loss: 1.22; acc: 0.56
Batch: 280; loss: 1.17; acc: 0.58
Batch: 300; loss: 1.09; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.56
Batch: 340; loss: 1.47; acc: 0.48
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.21; acc: 0.56
Batch: 400; loss: 0.95; acc: 0.67
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 0.98; acc: 0.64
Batch: 460; loss: 1.1; acc: 0.67
Batch: 480; loss: 1.57; acc: 0.53
Batch: 500; loss: 1.25; acc: 0.56
Batch: 520; loss: 1.28; acc: 0.61
Batch: 540; loss: 1.2; acc: 0.61
Batch: 560; loss: 1.25; acc: 0.62
Batch: 580; loss: 1.5; acc: 0.45
Batch: 600; loss: 1.16; acc: 0.53
Batch: 620; loss: 1.32; acc: 0.59
Batch: 640; loss: 1.01; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.59
Batch: 680; loss: 1.05; acc: 0.66
Batch: 700; loss: 1.31; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.53
Batch: 740; loss: 1.42; acc: 0.52
Batch: 760; loss: 1.17; acc: 0.69
Batch: 780; loss: 1.12; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.47
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.11; acc: 0.62
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.73
Val Epoch over. val_loss: 1.2298965123808308; val_accuracy: 0.5958399681528662 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 1.22; acc: 0.58
Batch: 60; loss: 1.26; acc: 0.59
Batch: 80; loss: 1.08; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.64
Batch: 160; loss: 1.24; acc: 0.52
Batch: 180; loss: 1.26; acc: 0.52
Batch: 200; loss: 1.34; acc: 0.61
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 0.96; acc: 0.7
Batch: 280; loss: 1.26; acc: 0.59
Batch: 300; loss: 1.52; acc: 0.55
Batch: 320; loss: 1.31; acc: 0.62
Batch: 340; loss: 1.39; acc: 0.56
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.4; acc: 0.53
Batch: 400; loss: 1.24; acc: 0.58
Batch: 420; loss: 1.15; acc: 0.62
Batch: 440; loss: 1.29; acc: 0.69
Batch: 460; loss: 1.4; acc: 0.59
Batch: 480; loss: 1.36; acc: 0.61
Batch: 500; loss: 1.26; acc: 0.59
Batch: 520; loss: 1.24; acc: 0.62
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 1.2; acc: 0.64
Batch: 580; loss: 1.21; acc: 0.58
Batch: 600; loss: 1.12; acc: 0.64
Batch: 620; loss: 1.35; acc: 0.53
Batch: 640; loss: 1.04; acc: 0.62
Batch: 660; loss: 1.4; acc: 0.58
Batch: 680; loss: 1.46; acc: 0.52
Batch: 700; loss: 1.13; acc: 0.59
Batch: 720; loss: 1.33; acc: 0.64
Batch: 740; loss: 1.17; acc: 0.61
Batch: 760; loss: 1.37; acc: 0.53
Batch: 780; loss: 1.4; acc: 0.48
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.7
Val Epoch over. val_loss: 1.2298924611632231; val_accuracy: 0.5979299363057324 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 1.34; acc: 0.52
Batch: 60; loss: 1.42; acc: 0.58
Batch: 80; loss: 1.41; acc: 0.45
Batch: 100; loss: 1.01; acc: 0.62
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 1.5; acc: 0.47
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 1.31; acc: 0.61
Batch: 200; loss: 1.41; acc: 0.53
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.18; acc: 0.61
Batch: 260; loss: 1.37; acc: 0.58
Batch: 280; loss: 1.03; acc: 0.7
Batch: 300; loss: 1.05; acc: 0.67
Batch: 320; loss: 1.24; acc: 0.62
Batch: 340; loss: 1.43; acc: 0.5
Batch: 360; loss: 1.08; acc: 0.73
Batch: 380; loss: 1.62; acc: 0.56
Batch: 400; loss: 0.98; acc: 0.7
Batch: 420; loss: 1.28; acc: 0.64
Batch: 440; loss: 1.3; acc: 0.61
Batch: 460; loss: 1.25; acc: 0.62
Batch: 480; loss: 1.29; acc: 0.58
Batch: 500; loss: 1.24; acc: 0.59
Batch: 520; loss: 1.23; acc: 0.55
Batch: 540; loss: 1.32; acc: 0.58
Batch: 560; loss: 1.14; acc: 0.56
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.29; acc: 0.59
Batch: 640; loss: 1.22; acc: 0.66
Batch: 660; loss: 1.28; acc: 0.53
Batch: 680; loss: 1.18; acc: 0.55
Batch: 700; loss: 1.29; acc: 0.59
Batch: 720; loss: 1.48; acc: 0.56
Batch: 740; loss: 1.24; acc: 0.52
Batch: 760; loss: 1.43; acc: 0.48
Batch: 780; loss: 1.21; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.33; acc: 0.53
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.7
Val Epoch over. val_loss: 1.2303318567336745; val_accuracy: 0.5979299363057324 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.18; acc: 0.62
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 1.41; acc: 0.56
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 1.31; acc: 0.61
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.38; acc: 0.56
Batch: 140; loss: 1.18; acc: 0.64
Batch: 160; loss: 1.17; acc: 0.59
Batch: 180; loss: 1.4; acc: 0.59
Batch: 200; loss: 1.32; acc: 0.53
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.27; acc: 0.56
Batch: 260; loss: 1.09; acc: 0.58
Batch: 280; loss: 1.27; acc: 0.55
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.42; acc: 0.58
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 1.32; acc: 0.56
Batch: 380; loss: 1.11; acc: 0.59
Batch: 400; loss: 1.69; acc: 0.44
Batch: 420; loss: 1.24; acc: 0.56
Batch: 440; loss: 1.15; acc: 0.62
Batch: 460; loss: 1.2; acc: 0.48
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 0.84; acc: 0.77
Batch: 520; loss: 1.21; acc: 0.62
Batch: 540; loss: 1.22; acc: 0.47
Batch: 560; loss: 1.37; acc: 0.48
Batch: 580; loss: 1.09; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.58
Batch: 620; loss: 1.28; acc: 0.52
Batch: 640; loss: 1.32; acc: 0.55
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 1.19; acc: 0.62
Batch: 700; loss: 1.32; acc: 0.52
Batch: 720; loss: 1.63; acc: 0.41
Batch: 740; loss: 1.49; acc: 0.48
Batch: 760; loss: 1.19; acc: 0.59
Batch: 780; loss: 1.17; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.53
Batch: 20; loss: 1.33; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.7
Val Epoch over. val_loss: 1.2318422353951035; val_accuracy: 0.5961385350318471 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.62
Batch: 60; loss: 1.03; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.67
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 1.5; acc: 0.45
Batch: 160; loss: 1.26; acc: 0.55
Batch: 180; loss: 1.38; acc: 0.5
Batch: 200; loss: 1.17; acc: 0.61
Batch: 220; loss: 1.38; acc: 0.56
Batch: 240; loss: 1.04; acc: 0.66
Batch: 260; loss: 1.16; acc: 0.61
Batch: 280; loss: 1.34; acc: 0.44
Batch: 300; loss: 1.48; acc: 0.53
Batch: 320; loss: 1.31; acc: 0.59
Batch: 340; loss: 1.27; acc: 0.59
Batch: 360; loss: 1.34; acc: 0.61
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.34; acc: 0.64
Batch: 420; loss: 1.26; acc: 0.52
Batch: 440; loss: 1.17; acc: 0.56
Batch: 460; loss: 1.07; acc: 0.64
Batch: 480; loss: 1.07; acc: 0.62
Batch: 500; loss: 1.34; acc: 0.55
Batch: 520; loss: 1.26; acc: 0.59
Batch: 540; loss: 1.38; acc: 0.53
Batch: 560; loss: 1.32; acc: 0.56
Batch: 580; loss: 1.35; acc: 0.59
Batch: 600; loss: 1.33; acc: 0.52
Batch: 620; loss: 1.42; acc: 0.5
Batch: 640; loss: 1.18; acc: 0.66
Batch: 660; loss: 1.63; acc: 0.48
Batch: 680; loss: 1.24; acc: 0.59
Batch: 700; loss: 1.33; acc: 0.55
Batch: 720; loss: 1.2; acc: 0.59
Batch: 740; loss: 1.35; acc: 0.55
Batch: 760; loss: 1.26; acc: 0.55
Batch: 780; loss: 1.19; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.53
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.2302900461634254; val_accuracy: 0.5965366242038217 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 1.42; acc: 0.5
Batch: 60; loss: 1.39; acc: 0.53
Batch: 80; loss: 1.28; acc: 0.62
Batch: 100; loss: 1.24; acc: 0.59
Batch: 120; loss: 1.13; acc: 0.59
Batch: 140; loss: 1.11; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.52
Batch: 180; loss: 1.34; acc: 0.52
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.29; acc: 0.55
Batch: 240; loss: 1.54; acc: 0.53
Batch: 260; loss: 1.24; acc: 0.55
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 1.21; acc: 0.64
Batch: 320; loss: 1.53; acc: 0.39
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.46; acc: 0.47
Batch: 380; loss: 1.16; acc: 0.61
Batch: 400; loss: 1.21; acc: 0.5
Batch: 420; loss: 1.02; acc: 0.73
Batch: 440; loss: 1.24; acc: 0.59
Batch: 460; loss: 1.48; acc: 0.5
Batch: 480; loss: 1.18; acc: 0.58
Batch: 500; loss: 1.39; acc: 0.56
Batch: 520; loss: 1.46; acc: 0.56
Batch: 540; loss: 1.3; acc: 0.59
Batch: 560; loss: 1.2; acc: 0.61
Batch: 580; loss: 1.54; acc: 0.48
Batch: 600; loss: 1.52; acc: 0.58
Batch: 620; loss: 1.12; acc: 0.67
Batch: 640; loss: 1.44; acc: 0.56
Batch: 660; loss: 1.22; acc: 0.58
Batch: 680; loss: 1.15; acc: 0.59
Batch: 700; loss: 1.13; acc: 0.61
Batch: 720; loss: 1.33; acc: 0.52
Batch: 740; loss: 1.39; acc: 0.5
Batch: 760; loss: 1.13; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.55
Batch: 20; loss: 1.34; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 1.09; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.7
Val Epoch over. val_loss: 1.2312562275844015; val_accuracy: 0.5988256369426752 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.17; acc: 0.58
Batch: 20; loss: 1.41; acc: 0.58
Batch: 40; loss: 1.19; acc: 0.64
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 1.28; acc: 0.5
Batch: 100; loss: 1.35; acc: 0.45
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.56
Batch: 160; loss: 1.33; acc: 0.52
Batch: 180; loss: 1.12; acc: 0.59
Batch: 200; loss: 1.34; acc: 0.55
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.53
Batch: 260; loss: 1.36; acc: 0.56
Batch: 280; loss: 1.23; acc: 0.55
Batch: 300; loss: 1.33; acc: 0.52
Batch: 320; loss: 1.4; acc: 0.61
Batch: 340; loss: 1.31; acc: 0.58
Batch: 360; loss: 1.0; acc: 0.66
Batch: 380; loss: 1.01; acc: 0.59
Batch: 400; loss: 1.42; acc: 0.52
Batch: 420; loss: 1.25; acc: 0.58
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.24; acc: 0.58
Batch: 480; loss: 1.37; acc: 0.55
Batch: 500; loss: 1.32; acc: 0.61
Batch: 520; loss: 1.21; acc: 0.55
Batch: 540; loss: 1.32; acc: 0.53
Batch: 560; loss: 1.51; acc: 0.41
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 1.58; acc: 0.5
Batch: 620; loss: 1.31; acc: 0.5
Batch: 640; loss: 1.42; acc: 0.53
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 1.45; acc: 0.47
Batch: 700; loss: 1.13; acc: 0.64
Batch: 720; loss: 1.26; acc: 0.55
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 1.19; acc: 0.69
Batch: 780; loss: 1.44; acc: 0.47
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.64
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.2295209260503197; val_accuracy: 0.5950437898089171 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.99; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.46; acc: 0.48
Batch: 60; loss: 1.33; acc: 0.52
Batch: 80; loss: 1.38; acc: 0.53
Batch: 100; loss: 1.06; acc: 0.61
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.26; acc: 0.52
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 1.26; acc: 0.56
Batch: 200; loss: 1.27; acc: 0.58
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.47; acc: 0.53
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 1.14; acc: 0.66
Batch: 300; loss: 1.01; acc: 0.67
Batch: 320; loss: 1.43; acc: 0.55
Batch: 340; loss: 1.16; acc: 0.67
Batch: 360; loss: 1.39; acc: 0.53
Batch: 380; loss: 1.38; acc: 0.53
Batch: 400; loss: 1.44; acc: 0.59
Batch: 420; loss: 1.19; acc: 0.66
Batch: 440; loss: 1.38; acc: 0.53
Batch: 460; loss: 1.36; acc: 0.64
Batch: 480; loss: 1.41; acc: 0.5
Batch: 500; loss: 1.1; acc: 0.66
Batch: 520; loss: 1.45; acc: 0.56
Batch: 540; loss: 1.07; acc: 0.58
Batch: 560; loss: 1.24; acc: 0.61
Batch: 580; loss: 1.07; acc: 0.61
Batch: 600; loss: 1.15; acc: 0.61
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.58
Batch: 680; loss: 1.19; acc: 0.53
Batch: 700; loss: 1.46; acc: 0.47
Batch: 720; loss: 1.22; acc: 0.58
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.17; acc: 0.62
Batch: 780; loss: 1.39; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2290786394647732; val_accuracy: 0.5970342356687898 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 1.15; acc: 0.58
Batch: 60; loss: 1.18; acc: 0.59
Batch: 80; loss: 1.47; acc: 0.58
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 1.26; acc: 0.56
Batch: 160; loss: 1.11; acc: 0.58
Batch: 180; loss: 1.39; acc: 0.52
Batch: 200; loss: 0.95; acc: 0.7
Batch: 220; loss: 1.39; acc: 0.56
Batch: 240; loss: 1.38; acc: 0.47
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.32; acc: 0.62
Batch: 320; loss: 1.35; acc: 0.59
Batch: 340; loss: 1.3; acc: 0.59
Batch: 360; loss: 1.22; acc: 0.62
Batch: 380; loss: 1.49; acc: 0.53
Batch: 400; loss: 1.14; acc: 0.62
Batch: 420; loss: 1.34; acc: 0.55
Batch: 440; loss: 1.2; acc: 0.59
Batch: 460; loss: 1.15; acc: 0.58
Batch: 480; loss: 1.33; acc: 0.58
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 1.21; acc: 0.67
Batch: 540; loss: 1.32; acc: 0.58
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.37; acc: 0.61
Batch: 600; loss: 1.38; acc: 0.59
Batch: 620; loss: 1.36; acc: 0.59
Batch: 640; loss: 1.19; acc: 0.61
Batch: 660; loss: 1.45; acc: 0.55
Batch: 680; loss: 1.2; acc: 0.61
Batch: 700; loss: 1.47; acc: 0.47
Batch: 720; loss: 1.33; acc: 0.5
Batch: 740; loss: 1.39; acc: 0.53
Batch: 760; loss: 1.24; acc: 0.61
Batch: 780; loss: 1.28; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.73
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2293601077832994; val_accuracy: 0.5951433121019108 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.25; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 1.24; acc: 0.5
Batch: 60; loss: 1.33; acc: 0.48
Batch: 80; loss: 1.37; acc: 0.58
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.32; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.59
Batch: 160; loss: 1.05; acc: 0.66
Batch: 180; loss: 1.0; acc: 0.72
Batch: 200; loss: 1.04; acc: 0.58
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 1.25; acc: 0.61
Batch: 280; loss: 1.32; acc: 0.59
Batch: 300; loss: 1.19; acc: 0.59
Batch: 320; loss: 1.13; acc: 0.56
Batch: 340; loss: 1.35; acc: 0.52
Batch: 360; loss: 1.31; acc: 0.5
Batch: 380; loss: 1.43; acc: 0.53
Batch: 400; loss: 1.47; acc: 0.55
Batch: 420; loss: 1.22; acc: 0.52
Batch: 440; loss: 1.2; acc: 0.59
Batch: 460; loss: 1.15; acc: 0.61
Batch: 480; loss: 1.38; acc: 0.53
Batch: 500; loss: 1.14; acc: 0.61
Batch: 520; loss: 1.35; acc: 0.59
Batch: 540; loss: 1.08; acc: 0.61
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.15; acc: 0.59
Batch: 600; loss: 1.22; acc: 0.59
Batch: 620; loss: 1.22; acc: 0.53
Batch: 640; loss: 1.39; acc: 0.59
Batch: 660; loss: 1.27; acc: 0.67
Batch: 680; loss: 1.34; acc: 0.56
Batch: 700; loss: 1.11; acc: 0.66
Batch: 720; loss: 1.26; acc: 0.64
Batch: 740; loss: 1.16; acc: 0.56
Batch: 760; loss: 1.22; acc: 0.64
Batch: 780; loss: 0.96; acc: 0.75
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.53
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.73
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.7
Val Epoch over. val_loss: 1.2293133678709625; val_accuracy: 0.5969347133757962 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 1.43; acc: 0.53
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 1.05; acc: 0.64
Batch: 100; loss: 1.26; acc: 0.55
Batch: 120; loss: 1.25; acc: 0.56
Batch: 140; loss: 1.22; acc: 0.59
Batch: 160; loss: 1.34; acc: 0.52
Batch: 180; loss: 1.42; acc: 0.53
Batch: 200; loss: 1.0; acc: 0.72
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.52; acc: 0.45
Batch: 280; loss: 1.32; acc: 0.52
Batch: 300; loss: 1.36; acc: 0.55
Batch: 320; loss: 1.12; acc: 0.61
Batch: 340; loss: 1.14; acc: 0.62
Batch: 360; loss: 1.14; acc: 0.64
Batch: 380; loss: 1.33; acc: 0.61
Batch: 400; loss: 1.22; acc: 0.59
Batch: 420; loss: 1.33; acc: 0.5
Batch: 440; loss: 1.19; acc: 0.59
Batch: 460; loss: 1.33; acc: 0.58
Batch: 480; loss: 1.26; acc: 0.64
Batch: 500; loss: 1.02; acc: 0.66
Batch: 520; loss: 1.28; acc: 0.55
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.34; acc: 0.62
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.3; acc: 0.55
Batch: 620; loss: 1.09; acc: 0.67
Batch: 640; loss: 0.93; acc: 0.66
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.44; acc: 0.47
Batch: 720; loss: 1.59; acc: 0.47
Batch: 740; loss: 1.16; acc: 0.55
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.07; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.52
Batch: 20; loss: 1.32; acc: 0.53
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2290484362347112; val_accuracy: 0.5978304140127388 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.25; acc: 0.58
Batch: 20; loss: 1.05; acc: 0.64
Batch: 40; loss: 1.46; acc: 0.52
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.28; acc: 0.52
Batch: 100; loss: 1.23; acc: 0.55
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.58
Batch: 160; loss: 1.27; acc: 0.64
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.35; acc: 0.52
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 1.25; acc: 0.59
Batch: 260; loss: 1.13; acc: 0.58
Batch: 280; loss: 1.09; acc: 0.64
Batch: 300; loss: 1.21; acc: 0.58
Batch: 320; loss: 1.38; acc: 0.53
Batch: 340; loss: 1.29; acc: 0.62
Batch: 360; loss: 1.27; acc: 0.64
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 1.56; acc: 0.52
Batch: 420; loss: 1.1; acc: 0.64
Batch: 440; loss: 1.28; acc: 0.58
Batch: 460; loss: 1.09; acc: 0.69
Batch: 480; loss: 1.33; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.56
Batch: 520; loss: 1.29; acc: 0.53
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.43; acc: 0.53
Batch: 580; loss: 1.54; acc: 0.53
Batch: 600; loss: 1.44; acc: 0.59
Batch: 620; loss: 1.33; acc: 0.59
Batch: 640; loss: 1.14; acc: 0.66
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.08; acc: 0.66
Batch: 700; loss: 1.19; acc: 0.53
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 1.24; acc: 0.53
Batch: 760; loss: 1.09; acc: 0.66
Batch: 780; loss: 1.2; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.31; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.12; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2293734679556196; val_accuracy: 0.5950437898089171 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.2; acc: 0.56
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.36; acc: 0.53
Batch: 80; loss: 1.38; acc: 0.47
Batch: 100; loss: 1.47; acc: 0.55
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.58
Batch: 160; loss: 1.36; acc: 0.48
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.21; acc: 0.59
Batch: 220; loss: 1.12; acc: 0.64
Batch: 240; loss: 1.3; acc: 0.56
Batch: 260; loss: 1.36; acc: 0.61
Batch: 280; loss: 1.33; acc: 0.59
Batch: 300; loss: 1.1; acc: 0.64
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.24; acc: 0.56
Batch: 360; loss: 1.14; acc: 0.64
Batch: 380; loss: 1.31; acc: 0.53
Batch: 400; loss: 1.32; acc: 0.58
Batch: 420; loss: 1.43; acc: 0.53
Batch: 440; loss: 1.53; acc: 0.5
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 1.17; acc: 0.58
Batch: 500; loss: 1.31; acc: 0.55
Batch: 520; loss: 1.29; acc: 0.56
Batch: 540; loss: 1.19; acc: 0.7
Batch: 560; loss: 1.43; acc: 0.52
Batch: 580; loss: 1.29; acc: 0.47
Batch: 600; loss: 1.18; acc: 0.61
Batch: 620; loss: 1.28; acc: 0.59
Batch: 640; loss: 1.09; acc: 0.59
Batch: 660; loss: 1.04; acc: 0.61
Batch: 680; loss: 1.43; acc: 0.5
Batch: 700; loss: 1.32; acc: 0.58
Batch: 720; loss: 1.69; acc: 0.45
Batch: 740; loss: 1.03; acc: 0.69
Batch: 760; loss: 1.24; acc: 0.52
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.31; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.73
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.229306564589215; val_accuracy: 0.5961385350318471 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.37; acc: 0.61
Batch: 40; loss: 1.23; acc: 0.61
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.29; acc: 0.56
Batch: 100; loss: 1.28; acc: 0.59
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 1.48; acc: 0.47
Batch: 160; loss: 1.28; acc: 0.52
Batch: 180; loss: 1.01; acc: 0.72
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.34; acc: 0.55
Batch: 240; loss: 1.19; acc: 0.59
Batch: 260; loss: 1.33; acc: 0.53
Batch: 280; loss: 1.12; acc: 0.58
Batch: 300; loss: 1.33; acc: 0.56
Batch: 320; loss: 1.27; acc: 0.61
Batch: 340; loss: 1.21; acc: 0.64
Batch: 360; loss: 1.33; acc: 0.61
Batch: 380; loss: 1.1; acc: 0.62
Batch: 400; loss: 1.46; acc: 0.47
Batch: 420; loss: 1.04; acc: 0.66
Batch: 440; loss: 1.16; acc: 0.64
Batch: 460; loss: 1.27; acc: 0.59
Batch: 480; loss: 1.48; acc: 0.52
Batch: 500; loss: 1.21; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.61
Batch: 540; loss: 1.21; acc: 0.56
Batch: 560; loss: 1.3; acc: 0.55
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 0.95; acc: 0.67
Batch: 620; loss: 1.33; acc: 0.52
Batch: 640; loss: 1.19; acc: 0.62
Batch: 660; loss: 1.35; acc: 0.52
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.31; acc: 0.58
Batch: 720; loss: 1.27; acc: 0.53
Batch: 740; loss: 1.41; acc: 0.53
Batch: 760; loss: 1.27; acc: 0.59
Batch: 780; loss: 1.1; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.32; acc: 0.53
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2294179261869687; val_accuracy: 0.5950437898089171 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.06; acc: 0.64
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 1.46; acc: 0.53
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.11; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.59
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 1.25; acc: 0.53
Batch: 180; loss: 1.36; acc: 0.53
Batch: 200; loss: 1.45; acc: 0.52
Batch: 220; loss: 1.0; acc: 0.69
Batch: 240; loss: 1.5; acc: 0.47
Batch: 260; loss: 1.39; acc: 0.5
Batch: 280; loss: 1.38; acc: 0.55
Batch: 300; loss: 1.35; acc: 0.55
Batch: 320; loss: 1.15; acc: 0.58
Batch: 340; loss: 1.32; acc: 0.59
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.29; acc: 0.53
Batch: 400; loss: 1.2; acc: 0.58
Batch: 420; loss: 1.41; acc: 0.5
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.15; acc: 0.58
Batch: 500; loss: 1.2; acc: 0.58
Batch: 520; loss: 1.34; acc: 0.48
Batch: 540; loss: 1.31; acc: 0.58
Batch: 560; loss: 1.39; acc: 0.59
Batch: 580; loss: 1.16; acc: 0.5
Batch: 600; loss: 1.48; acc: 0.48
Batch: 620; loss: 1.13; acc: 0.67
Batch: 640; loss: 1.33; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.48
Batch: 680; loss: 1.19; acc: 0.58
Batch: 700; loss: 1.39; acc: 0.56
Batch: 720; loss: 1.52; acc: 0.48
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 1.34; acc: 0.55
Batch: 780; loss: 1.36; acc: 0.48
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.62
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.2298096524682014; val_accuracy: 0.5959394904458599 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 1.38; acc: 0.59
Batch: 40; loss: 1.29; acc: 0.58
Batch: 60; loss: 1.43; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.48
Batch: 100; loss: 1.03; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.52
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.3; acc: 0.56
Batch: 180; loss: 1.21; acc: 0.56
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.5; acc: 0.48
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 0.97; acc: 0.7
Batch: 280; loss: 1.41; acc: 0.59
Batch: 300; loss: 1.55; acc: 0.52
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.58; acc: 0.52
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 1.38; acc: 0.58
Batch: 400; loss: 1.44; acc: 0.59
Batch: 420; loss: 1.31; acc: 0.53
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 1.32; acc: 0.62
Batch: 500; loss: 1.62; acc: 0.5
Batch: 520; loss: 1.34; acc: 0.52
Batch: 540; loss: 1.36; acc: 0.61
Batch: 560; loss: 1.49; acc: 0.53
Batch: 580; loss: 1.4; acc: 0.52
Batch: 600; loss: 1.27; acc: 0.55
Batch: 620; loss: 1.18; acc: 0.66
Batch: 640; loss: 1.21; acc: 0.62
Batch: 660; loss: 1.14; acc: 0.59
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.42; acc: 0.44
Batch: 720; loss: 1.09; acc: 0.56
Batch: 740; loss: 1.43; acc: 0.52
Batch: 760; loss: 1.37; acc: 0.55
Batch: 780; loss: 1.09; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.52
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.2289919386244124; val_accuracy: 0.5967356687898089 

plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 21189
elements in E: 4395200
fraction nonzero: 0.0048209410265744445
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.31; acc: 0.19
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.25; acc: 0.19
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.26; acc: 0.17
Batch: 140; loss: 2.26; acc: 0.17
Batch: 160; loss: 2.26; acc: 0.19
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.27; acc: 0.16
Batch: 220; loss: 2.27; acc: 0.22
Batch: 240; loss: 2.22; acc: 0.25
Batch: 260; loss: 2.23; acc: 0.2
Batch: 280; loss: 2.21; acc: 0.25
Batch: 300; loss: 2.22; acc: 0.22
Batch: 320; loss: 2.17; acc: 0.28
Batch: 340; loss: 2.12; acc: 0.38
Batch: 360; loss: 2.21; acc: 0.22
Batch: 380; loss: 2.26; acc: 0.12
Batch: 400; loss: 2.14; acc: 0.3
Batch: 420; loss: 2.18; acc: 0.25
Batch: 440; loss: 2.17; acc: 0.19
Batch: 460; loss: 2.16; acc: 0.23
Batch: 480; loss: 2.07; acc: 0.41
Batch: 500; loss: 2.1; acc: 0.39
Batch: 520; loss: 2.02; acc: 0.42
Batch: 540; loss: 2.15; acc: 0.17
Batch: 560; loss: 2.19; acc: 0.2
Batch: 580; loss: 2.01; acc: 0.34
Batch: 600; loss: 1.98; acc: 0.48
Batch: 620; loss: 2.05; acc: 0.33
Batch: 640; loss: 1.99; acc: 0.34
Batch: 660; loss: 1.93; acc: 0.39
Batch: 680; loss: 2.01; acc: 0.3
Batch: 700; loss: 1.89; acc: 0.42
Batch: 720; loss: 1.78; acc: 0.42
Batch: 740; loss: 2.1; acc: 0.23
Batch: 760; loss: 1.96; acc: 0.34
Batch: 780; loss: 1.73; acc: 0.44
Train Epoch over. train_loss: 2.13; train_accuracy: 0.28 

Batch: 0; loss: 1.81; acc: 0.36
Batch: 20; loss: 1.96; acc: 0.27
Batch: 40; loss: 1.66; acc: 0.39
Batch: 60; loss: 1.63; acc: 0.41
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.8; acc: 0.34
Batch: 120; loss: 1.79; acc: 0.33
Batch: 140; loss: 1.8; acc: 0.38
Val Epoch over. val_loss: 1.7757858372038338; val_accuracy: 0.3830613057324841 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.84; acc: 0.38
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.83; acc: 0.27
Batch: 60; loss: 1.66; acc: 0.45
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.64; acc: 0.39
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.51; acc: 0.5
Batch: 160; loss: 1.73; acc: 0.36
Batch: 180; loss: 1.51; acc: 0.53
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.25; acc: 0.55
Batch: 240; loss: 1.52; acc: 0.42
Batch: 260; loss: 1.43; acc: 0.44
Batch: 280; loss: 1.29; acc: 0.53
Batch: 300; loss: 1.31; acc: 0.53
Batch: 320; loss: 1.26; acc: 0.56
Batch: 340; loss: 1.24; acc: 0.61
Batch: 360; loss: 1.4; acc: 0.52
Batch: 380; loss: 1.19; acc: 0.53
Batch: 400; loss: 1.29; acc: 0.47
Batch: 420; loss: 1.25; acc: 0.5
Batch: 440; loss: 1.46; acc: 0.52
Batch: 460; loss: 1.19; acc: 0.56
Batch: 480; loss: 1.14; acc: 0.59
Batch: 500; loss: 1.27; acc: 0.66
Batch: 520; loss: 1.23; acc: 0.58
Batch: 540; loss: 1.43; acc: 0.5
Batch: 560; loss: 1.29; acc: 0.55
Batch: 580; loss: 1.38; acc: 0.53
Batch: 600; loss: 1.32; acc: 0.55
Batch: 620; loss: 1.09; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.58
Batch: 660; loss: 1.32; acc: 0.56
Batch: 680; loss: 1.12; acc: 0.62
Batch: 700; loss: 1.41; acc: 0.53
Batch: 720; loss: 1.34; acc: 0.52
Batch: 740; loss: 1.02; acc: 0.61
Batch: 760; loss: 1.36; acc: 0.48
Batch: 780; loss: 1.03; acc: 0.72
Train Epoch over. train_loss: 1.36; train_accuracy: 0.53 

Batch: 0; loss: 1.12; acc: 0.53
Batch: 20; loss: 1.28; acc: 0.45
Batch: 40; loss: 1.04; acc: 0.64
Batch: 60; loss: 1.12; acc: 0.56
Batch: 80; loss: 1.06; acc: 0.61
Batch: 100; loss: 1.02; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 0.83; acc: 0.7
Val Epoch over. val_loss: 1.204727970870437; val_accuracy: 0.5466759554140127 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.61
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 1.29; acc: 0.59
Batch: 60; loss: 0.98; acc: 0.7
Batch: 80; loss: 0.89; acc: 0.64
Batch: 100; loss: 1.18; acc: 0.58
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.67
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.49; acc: 0.48
Batch: 200; loss: 1.19; acc: 0.67
Batch: 220; loss: 0.99; acc: 0.67
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.38; acc: 0.55
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 1.1; acc: 0.64
Batch: 320; loss: 1.01; acc: 0.67
Batch: 340; loss: 1.31; acc: 0.52
Batch: 360; loss: 1.1; acc: 0.72
Batch: 380; loss: 0.91; acc: 0.72
Batch: 400; loss: 0.98; acc: 0.66
Batch: 420; loss: 1.27; acc: 0.61
Batch: 440; loss: 0.87; acc: 0.67
Batch: 460; loss: 1.47; acc: 0.48
Batch: 480; loss: 0.95; acc: 0.7
Batch: 500; loss: 1.03; acc: 0.66
Batch: 520; loss: 1.21; acc: 0.59
Batch: 540; loss: 1.12; acc: 0.64
Batch: 560; loss: 0.95; acc: 0.75
Batch: 580; loss: 1.0; acc: 0.67
Batch: 600; loss: 1.32; acc: 0.56
Batch: 620; loss: 1.13; acc: 0.59
Batch: 640; loss: 1.31; acc: 0.61
Batch: 660; loss: 1.13; acc: 0.55
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.06; acc: 0.59
Batch: 720; loss: 1.18; acc: 0.58
Batch: 740; loss: 1.15; acc: 0.59
Batch: 760; loss: 1.25; acc: 0.61
Batch: 780; loss: 1.2; acc: 0.62
Train Epoch over. train_loss: 1.13; train_accuracy: 0.62 

Batch: 0; loss: 0.89; acc: 0.7
Batch: 20; loss: 1.02; acc: 0.62
Batch: 40; loss: 0.86; acc: 0.66
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.83; acc: 0.73
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.67; acc: 0.78
Val Epoch over. val_loss: 1.027542498081353; val_accuracy: 0.6606289808917197 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.59
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 1.27; acc: 0.59
Batch: 60; loss: 1.38; acc: 0.56
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.53
Batch: 120; loss: 1.1; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.56
Batch: 160; loss: 1.25; acc: 0.62
Batch: 180; loss: 0.85; acc: 0.73
Batch: 200; loss: 1.23; acc: 0.58
Batch: 220; loss: 1.2; acc: 0.55
Batch: 240; loss: 0.91; acc: 0.67
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 1.27; acc: 0.56
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.05; acc: 0.58
Batch: 360; loss: 0.96; acc: 0.61
Batch: 380; loss: 1.05; acc: 0.67
Batch: 400; loss: 0.99; acc: 0.66
Batch: 420; loss: 1.34; acc: 0.58
Batch: 440; loss: 1.24; acc: 0.61
Batch: 460; loss: 0.98; acc: 0.62
Batch: 480; loss: 1.07; acc: 0.61
Batch: 500; loss: 1.35; acc: 0.53
Batch: 520; loss: 1.33; acc: 0.61
Batch: 540; loss: 1.31; acc: 0.5
Batch: 560; loss: 0.73; acc: 0.77
Batch: 580; loss: 1.12; acc: 0.61
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.34; acc: 0.59
Batch: 640; loss: 0.7; acc: 0.77
Batch: 660; loss: 1.04; acc: 0.69
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 1.02; acc: 0.64
Batch: 720; loss: 1.13; acc: 0.66
Batch: 740; loss: 0.85; acc: 0.61
Batch: 760; loss: 1.1; acc: 0.58
Batch: 780; loss: 1.09; acc: 0.64
Train Epoch over. train_loss: 1.06; train_accuracy: 0.65 

Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.9; acc: 0.69
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.7
Batch: 140; loss: 0.64; acc: 0.84
Val Epoch over. val_loss: 1.0589829949057026; val_accuracy: 0.6490843949044586 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.59
Batch: 120; loss: 0.9; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.72
Batch: 160; loss: 0.98; acc: 0.67
Batch: 180; loss: 1.04; acc: 0.62
Batch: 200; loss: 1.09; acc: 0.64
Batch: 220; loss: 0.99; acc: 0.64
Batch: 240; loss: 1.03; acc: 0.72
Batch: 260; loss: 0.94; acc: 0.69
Batch: 280; loss: 0.94; acc: 0.69
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.18; acc: 0.64
Batch: 340; loss: 1.16; acc: 0.61
Batch: 360; loss: 0.94; acc: 0.73
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.8; acc: 0.73
Batch: 420; loss: 1.03; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.66
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 0.9; acc: 0.72
Batch: 500; loss: 1.15; acc: 0.61
Batch: 520; loss: 1.08; acc: 0.67
Batch: 540; loss: 0.85; acc: 0.75
Batch: 560; loss: 0.9; acc: 0.72
Batch: 580; loss: 0.98; acc: 0.67
Batch: 600; loss: 0.97; acc: 0.61
Batch: 620; loss: 0.85; acc: 0.7
Batch: 640; loss: 1.11; acc: 0.62
Batch: 660; loss: 0.9; acc: 0.7
Batch: 680; loss: 1.03; acc: 0.66
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.66
Batch: 740; loss: 1.0; acc: 0.69
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 1.08; acc: 0.66
Train Epoch over. train_loss: 1.02; train_accuracy: 0.67 

Batch: 0; loss: 0.93; acc: 0.66
Batch: 20; loss: 1.37; acc: 0.5
Batch: 40; loss: 0.75; acc: 0.72
Batch: 60; loss: 1.18; acc: 0.61
Batch: 80; loss: 0.99; acc: 0.62
Batch: 100; loss: 1.14; acc: 0.61
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 1.1612710842661038; val_accuracy: 0.6026074840764332 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.62
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.98; acc: 0.64
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 0.87; acc: 0.75
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 1.03; acc: 0.59
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.04; acc: 0.67
Batch: 200; loss: 0.97; acc: 0.67
Batch: 220; loss: 0.81; acc: 0.73
Batch: 240; loss: 0.92; acc: 0.72
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.23; acc: 0.64
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.87; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.23; acc: 0.55
Batch: 380; loss: 0.97; acc: 0.61
Batch: 400; loss: 1.2; acc: 0.53
Batch: 420; loss: 0.76; acc: 0.72
Batch: 440; loss: 0.94; acc: 0.69
Batch: 460; loss: 0.81; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 0.94; acc: 0.73
Batch: 520; loss: 0.95; acc: 0.7
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 1.27; acc: 0.55
Batch: 580; loss: 1.3; acc: 0.59
Batch: 600; loss: 0.89; acc: 0.7
Batch: 620; loss: 0.61; acc: 0.78
Batch: 640; loss: 0.93; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.64
Batch: 680; loss: 1.04; acc: 0.69
Batch: 700; loss: 0.94; acc: 0.7
Batch: 720; loss: 0.97; acc: 0.66
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.88; acc: 0.73
Batch: 780; loss: 1.08; acc: 0.62
Train Epoch over. train_loss: 1.01; train_accuracy: 0.67 

Batch: 0; loss: 1.01; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 0.87; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 0.84; acc: 0.66
Val Epoch over. val_loss: 1.1048522158792824; val_accuracy: 0.6228105095541401 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 0.89; acc: 0.77
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 1.02; acc: 0.56
Batch: 120; loss: 0.85; acc: 0.64
Batch: 140; loss: 1.12; acc: 0.66
Batch: 160; loss: 0.82; acc: 0.72
Batch: 180; loss: 1.01; acc: 0.7
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.46; acc: 0.53
Batch: 240; loss: 1.15; acc: 0.59
Batch: 260; loss: 1.03; acc: 0.69
Batch: 280; loss: 0.93; acc: 0.69
Batch: 300; loss: 1.12; acc: 0.55
Batch: 320; loss: 1.27; acc: 0.59
Batch: 340; loss: 1.37; acc: 0.56
Batch: 360; loss: 1.26; acc: 0.59
Batch: 380; loss: 0.98; acc: 0.72
Batch: 400; loss: 0.94; acc: 0.67
Batch: 420; loss: 0.92; acc: 0.66
Batch: 440; loss: 1.12; acc: 0.64
Batch: 460; loss: 0.92; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.67
Batch: 500; loss: 1.04; acc: 0.61
Batch: 520; loss: 1.2; acc: 0.67
Batch: 540; loss: 1.09; acc: 0.66
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 1.18; acc: 0.61
Batch: 600; loss: 1.11; acc: 0.69
Batch: 620; loss: 1.11; acc: 0.58
Batch: 640; loss: 0.96; acc: 0.66
Batch: 660; loss: 0.93; acc: 0.7
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 1.08; acc: 0.64
Batch: 720; loss: 0.91; acc: 0.7
Batch: 740; loss: 1.15; acc: 0.62
Batch: 760; loss: 1.02; acc: 0.69
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 1.0; train_accuracy: 0.67 

Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.98; acc: 0.66
Batch: 80; loss: 0.84; acc: 0.69
Batch: 100; loss: 0.94; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.56; acc: 0.8
Val Epoch over. val_loss: 0.9360408217284331; val_accuracy: 0.6929737261146497 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.13; acc: 0.61
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.56
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.93; acc: 0.67
Batch: 160; loss: 1.16; acc: 0.64
Batch: 180; loss: 1.05; acc: 0.69
Batch: 200; loss: 0.83; acc: 0.78
Batch: 220; loss: 0.91; acc: 0.75
Batch: 240; loss: 1.25; acc: 0.66
Batch: 260; loss: 1.1; acc: 0.58
Batch: 280; loss: 0.8; acc: 0.73
Batch: 300; loss: 1.41; acc: 0.5
Batch: 320; loss: 0.97; acc: 0.66
Batch: 340; loss: 1.21; acc: 0.66
Batch: 360; loss: 0.97; acc: 0.75
Batch: 380; loss: 1.3; acc: 0.58
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 1.27; acc: 0.56
Batch: 440; loss: 1.12; acc: 0.64
Batch: 460; loss: 0.79; acc: 0.73
Batch: 480; loss: 0.84; acc: 0.73
Batch: 500; loss: 0.93; acc: 0.58
Batch: 520; loss: 1.03; acc: 0.72
Batch: 540; loss: 1.22; acc: 0.67
Batch: 560; loss: 1.21; acc: 0.59
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.66
Batch: 660; loss: 0.87; acc: 0.66
Batch: 680; loss: 1.01; acc: 0.64
Batch: 700; loss: 0.77; acc: 0.67
Batch: 720; loss: 0.63; acc: 0.73
Batch: 740; loss: 1.21; acc: 0.55
Batch: 760; loss: 1.06; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.95; acc: 0.66
Batch: 60; loss: 1.15; acc: 0.58
Batch: 80; loss: 1.11; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 0.79; acc: 0.72
Val Epoch over. val_loss: 1.099265824077995; val_accuracy: 0.6385350318471338 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 1.11; acc: 0.59
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.55
Batch: 120; loss: 0.8; acc: 0.66
Batch: 140; loss: 1.21; acc: 0.58
Batch: 160; loss: 1.06; acc: 0.67
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 1.03; acc: 0.75
Batch: 220; loss: 1.01; acc: 0.64
Batch: 240; loss: 1.25; acc: 0.61
Batch: 260; loss: 0.86; acc: 0.72
Batch: 280; loss: 1.04; acc: 0.67
Batch: 300; loss: 1.24; acc: 0.7
Batch: 320; loss: 0.99; acc: 0.72
Batch: 340; loss: 0.83; acc: 0.73
Batch: 360; loss: 0.86; acc: 0.67
Batch: 380; loss: 1.25; acc: 0.56
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 1.49; acc: 0.55
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 0.81; acc: 0.75
Batch: 480; loss: 0.88; acc: 0.7
Batch: 500; loss: 0.84; acc: 0.66
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.81; acc: 0.72
Batch: 580; loss: 1.14; acc: 0.61
Batch: 600; loss: 1.11; acc: 0.61
Batch: 620; loss: 0.93; acc: 0.66
Batch: 640; loss: 0.91; acc: 0.72
Batch: 660; loss: 1.07; acc: 0.59
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.95; acc: 0.66
Batch: 720; loss: 0.96; acc: 0.7
Batch: 740; loss: 1.02; acc: 0.69
Batch: 760; loss: 0.89; acc: 0.73
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.69; acc: 0.73
Val Epoch over. val_loss: 0.9544936965225609; val_accuracy: 0.6897890127388535 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.7
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.99; acc: 0.72
Batch: 60; loss: 0.94; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.73
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 1.04; acc: 0.67
Batch: 160; loss: 1.11; acc: 0.62
Batch: 180; loss: 1.13; acc: 0.59
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.08; acc: 0.59
Batch: 240; loss: 0.91; acc: 0.67
Batch: 260; loss: 0.77; acc: 0.75
Batch: 280; loss: 1.01; acc: 0.69
Batch: 300; loss: 0.96; acc: 0.67
Batch: 320; loss: 0.91; acc: 0.67
Batch: 340; loss: 1.02; acc: 0.66
Batch: 360; loss: 0.89; acc: 0.75
Batch: 380; loss: 0.94; acc: 0.64
Batch: 400; loss: 1.12; acc: 0.61
Batch: 420; loss: 0.93; acc: 0.66
Batch: 440; loss: 0.9; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.59
Batch: 480; loss: 0.86; acc: 0.72
Batch: 500; loss: 1.16; acc: 0.61
Batch: 520; loss: 0.95; acc: 0.67
Batch: 540; loss: 1.1; acc: 0.59
Batch: 560; loss: 1.14; acc: 0.59
Batch: 580; loss: 1.08; acc: 0.62
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 0.88; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.67
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 0.98; acc: 0.66
Batch: 720; loss: 1.16; acc: 0.61
Batch: 740; loss: 0.85; acc: 0.72
Batch: 760; loss: 1.18; acc: 0.67
Batch: 780; loss: 0.85; acc: 0.75
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 1.2; acc: 0.58
Batch: 40; loss: 0.75; acc: 0.7
Batch: 60; loss: 1.07; acc: 0.62
Batch: 80; loss: 0.94; acc: 0.73
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.72; acc: 0.77
Val Epoch over. val_loss: 1.0188523929589872; val_accuracy: 0.6602308917197452 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.92; acc: 0.67
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.77; acc: 0.72
Batch: 80; loss: 0.96; acc: 0.69
Batch: 100; loss: 0.71; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.66
Batch: 180; loss: 0.88; acc: 0.62
Batch: 200; loss: 0.86; acc: 0.7
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 0.81; acc: 0.75
Batch: 260; loss: 1.05; acc: 0.75
Batch: 280; loss: 0.8; acc: 0.72
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 0.8; acc: 0.75
Batch: 340; loss: 1.02; acc: 0.66
Batch: 360; loss: 0.96; acc: 0.66
Batch: 380; loss: 0.73; acc: 0.77
Batch: 400; loss: 0.88; acc: 0.69
Batch: 420; loss: 1.04; acc: 0.7
Batch: 440; loss: 1.02; acc: 0.64
Batch: 460; loss: 1.3; acc: 0.59
Batch: 480; loss: 1.08; acc: 0.62
Batch: 500; loss: 0.73; acc: 0.77
Batch: 520; loss: 1.0; acc: 0.73
Batch: 540; loss: 0.84; acc: 0.73
Batch: 560; loss: 0.7; acc: 0.77
Batch: 580; loss: 1.08; acc: 0.64
Batch: 600; loss: 0.94; acc: 0.64
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.82; acc: 0.66
Batch: 660; loss: 0.98; acc: 0.77
Batch: 680; loss: 0.97; acc: 0.78
Batch: 700; loss: 0.7; acc: 0.75
Batch: 720; loss: 0.73; acc: 0.75
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.88; acc: 0.7
Batch: 780; loss: 0.74; acc: 0.72
Train Epoch over. train_loss: 0.91; train_accuracy: 0.71 

Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 1.08; acc: 0.59
Batch: 40; loss: 0.75; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.69
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.61
Batch: 120; loss: 1.27; acc: 0.59
Batch: 140; loss: 0.59; acc: 0.81
Val Epoch over. val_loss: 0.93357494302616; val_accuracy: 0.6970541401273885 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.72
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.97; acc: 0.7
Batch: 200; loss: 1.12; acc: 0.62
Batch: 220; loss: 0.85; acc: 0.67
Batch: 240; loss: 0.77; acc: 0.75
Batch: 260; loss: 0.88; acc: 0.7
Batch: 280; loss: 1.05; acc: 0.66
Batch: 300; loss: 0.93; acc: 0.69
Batch: 320; loss: 0.93; acc: 0.7
Batch: 340; loss: 1.14; acc: 0.67
Batch: 360; loss: 1.02; acc: 0.58
Batch: 380; loss: 0.92; acc: 0.7
Batch: 400; loss: 0.96; acc: 0.75
Batch: 420; loss: 0.91; acc: 0.69
Batch: 440; loss: 0.84; acc: 0.75
Batch: 460; loss: 0.83; acc: 0.69
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 1.07; acc: 0.67
Batch: 560; loss: 0.83; acc: 0.7
Batch: 580; loss: 1.06; acc: 0.66
Batch: 600; loss: 0.96; acc: 0.75
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.95; acc: 0.75
Batch: 660; loss: 1.03; acc: 0.73
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 0.82; acc: 0.69
Batch: 720; loss: 0.93; acc: 0.69
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 1.22; acc: 0.58
Batch: 780; loss: 0.82; acc: 0.75
Train Epoch over. train_loss: 0.91; train_accuracy: 0.71 

Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.67
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 0.49; acc: 0.83
Val Epoch over. val_loss: 0.8416961477060986; val_accuracy: 0.7383558917197452 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.7
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 1.07; acc: 0.66
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.94; acc: 0.7
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 1.13; acc: 0.67
Batch: 260; loss: 0.72; acc: 0.75
Batch: 280; loss: 0.79; acc: 0.7
Batch: 300; loss: 0.92; acc: 0.72
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 0.98; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 0.91; acc: 0.67
Batch: 400; loss: 0.89; acc: 0.73
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 0.83; acc: 0.72
Batch: 460; loss: 0.79; acc: 0.75
Batch: 480; loss: 0.91; acc: 0.66
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.71; acc: 0.73
Batch: 540; loss: 0.96; acc: 0.59
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 1.02; acc: 0.66
Batch: 600; loss: 0.9; acc: 0.67
Batch: 620; loss: 1.01; acc: 0.64
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 1.0; acc: 0.75
Batch: 680; loss: 1.21; acc: 0.64
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.94; acc: 0.66
Batch: 760; loss: 1.0; acc: 0.67
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 0.91; train_accuracy: 0.71 

Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.67
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.83
Val Epoch over. val_loss: 0.8624255440797016; val_accuracy: 0.7298964968152867 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.69
Batch: 20; loss: 0.92; acc: 0.69
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 1.04; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.92; acc: 0.67
Batch: 180; loss: 0.99; acc: 0.67
Batch: 200; loss: 1.07; acc: 0.67
Batch: 220; loss: 0.9; acc: 0.7
Batch: 240; loss: 0.79; acc: 0.69
Batch: 260; loss: 0.84; acc: 0.7
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.98; acc: 0.61
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 1.06; acc: 0.67
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 0.86; acc: 0.72
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 1.14; acc: 0.64
Batch: 440; loss: 0.9; acc: 0.75
Batch: 460; loss: 0.8; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.7
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 0.99; acc: 0.66
Batch: 560; loss: 0.95; acc: 0.78
Batch: 580; loss: 0.94; acc: 0.67
Batch: 600; loss: 0.97; acc: 0.67
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 0.79; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.82; acc: 0.67
Batch: 700; loss: 1.21; acc: 0.67
Batch: 720; loss: 0.72; acc: 0.72
Batch: 740; loss: 0.82; acc: 0.69
Batch: 760; loss: 1.0; acc: 0.7
Batch: 780; loss: 0.85; acc: 0.7
Train Epoch over. train_loss: 0.91; train_accuracy: 0.71 

Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.73
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.82; acc: 0.77
Batch: 100; loss: 0.86; acc: 0.67
Batch: 120; loss: 1.28; acc: 0.55
Batch: 140; loss: 0.63; acc: 0.75
Val Epoch over. val_loss: 0.9171791568303563; val_accuracy: 0.7051154458598726 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.66
Batch: 20; loss: 1.14; acc: 0.61
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.69
Batch: 100; loss: 0.72; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 1.07; acc: 0.64
Batch: 160; loss: 0.94; acc: 0.66
Batch: 180; loss: 0.95; acc: 0.67
Batch: 200; loss: 0.87; acc: 0.7
Batch: 220; loss: 0.89; acc: 0.73
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 1.08; acc: 0.61
Batch: 280; loss: 0.77; acc: 0.77
Batch: 300; loss: 0.98; acc: 0.69
Batch: 320; loss: 1.13; acc: 0.61
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.94; acc: 0.67
Batch: 380; loss: 0.88; acc: 0.75
Batch: 400; loss: 0.81; acc: 0.77
Batch: 420; loss: 1.26; acc: 0.62
Batch: 440; loss: 0.89; acc: 0.69
Batch: 460; loss: 0.82; acc: 0.69
Batch: 480; loss: 0.94; acc: 0.7
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 0.72; acc: 0.8
Batch: 560; loss: 1.04; acc: 0.72
Batch: 580; loss: 0.94; acc: 0.66
Batch: 600; loss: 0.81; acc: 0.75
Batch: 620; loss: 1.11; acc: 0.61
Batch: 640; loss: 0.84; acc: 0.72
Batch: 660; loss: 0.9; acc: 0.72
Batch: 680; loss: 0.92; acc: 0.67
Batch: 700; loss: 0.9; acc: 0.73
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.94; acc: 0.66
Batch: 780; loss: 1.19; acc: 0.61
Train Epoch over. train_loss: 0.91; train_accuracy: 0.71 

Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 0.64; acc: 0.7
Val Epoch over. val_loss: 0.915592613493561; val_accuracy: 0.6971536624203821 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 0.93; acc: 0.7
Batch: 40; loss: 0.8; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.66
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 1.08; acc: 0.67
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.8; acc: 0.69
Batch: 240; loss: 0.88; acc: 0.72
Batch: 260; loss: 0.86; acc: 0.73
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.81; acc: 0.75
Batch: 320; loss: 1.09; acc: 0.62
Batch: 340; loss: 1.14; acc: 0.61
Batch: 360; loss: 0.75; acc: 0.73
Batch: 380; loss: 1.01; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.69
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.91; acc: 0.72
Batch: 460; loss: 0.91; acc: 0.67
Batch: 480; loss: 1.16; acc: 0.7
Batch: 500; loss: 0.68; acc: 0.72
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.73
Batch: 580; loss: 1.02; acc: 0.59
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.96; acc: 0.72
Batch: 640; loss: 1.11; acc: 0.62
Batch: 660; loss: 0.94; acc: 0.67
Batch: 680; loss: 0.83; acc: 0.73
Batch: 700; loss: 1.03; acc: 0.73
Batch: 720; loss: 0.77; acc: 0.83
Batch: 740; loss: 0.88; acc: 0.7
Batch: 760; loss: 0.76; acc: 0.67
Batch: 780; loss: 0.95; acc: 0.67
Train Epoch over. train_loss: 0.9; train_accuracy: 0.71 

Batch: 0; loss: 0.73; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.72
Batch: 60; loss: 0.96; acc: 0.67
Batch: 80; loss: 0.82; acc: 0.73
Batch: 100; loss: 0.85; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 0.53; acc: 0.8
Val Epoch over. val_loss: 0.9331765531734296; val_accuracy: 0.6919785031847133 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 1.22; acc: 0.55
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.98; acc: 0.69
Batch: 160; loss: 1.13; acc: 0.69
Batch: 180; loss: 0.99; acc: 0.62
Batch: 200; loss: 0.8; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.61
Batch: 240; loss: 1.2; acc: 0.67
Batch: 260; loss: 0.99; acc: 0.66
Batch: 280; loss: 1.05; acc: 0.73
Batch: 300; loss: 0.93; acc: 0.69
Batch: 320; loss: 0.99; acc: 0.7
Batch: 340; loss: 0.86; acc: 0.72
Batch: 360; loss: 1.09; acc: 0.62
Batch: 380; loss: 0.97; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.67
Batch: 420; loss: 1.13; acc: 0.64
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 1.06; acc: 0.64
Batch: 480; loss: 0.93; acc: 0.75
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.92; acc: 0.69
Batch: 540; loss: 1.31; acc: 0.58
Batch: 560; loss: 1.03; acc: 0.66
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.73
Batch: 620; loss: 1.09; acc: 0.64
Batch: 640; loss: 0.97; acc: 0.61
Batch: 660; loss: 0.86; acc: 0.72
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.85; acc: 0.75
Batch: 720; loss: 0.94; acc: 0.64
Batch: 740; loss: 0.74; acc: 0.75
Batch: 760; loss: 0.97; acc: 0.67
Batch: 780; loss: 1.01; acc: 0.66
Train Epoch over. train_loss: 0.91; train_accuracy: 0.71 

Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.83
Val Epoch over. val_loss: 0.8619453265408802; val_accuracy: 0.7291998407643312 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.32; acc: 0.56
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.93; acc: 0.61
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.67
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.89; acc: 0.69
Batch: 160; loss: 1.04; acc: 0.72
Batch: 180; loss: 0.94; acc: 0.72
Batch: 200; loss: 0.84; acc: 0.75
Batch: 220; loss: 0.79; acc: 0.72
Batch: 240; loss: 1.34; acc: 0.52
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 1.08; acc: 0.69
Batch: 340; loss: 1.02; acc: 0.66
Batch: 360; loss: 0.82; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.78
Batch: 400; loss: 1.06; acc: 0.66
Batch: 420; loss: 0.93; acc: 0.67
Batch: 440; loss: 0.91; acc: 0.69
Batch: 460; loss: 0.83; acc: 0.69
Batch: 480; loss: 0.74; acc: 0.72
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 1.22; acc: 0.56
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.0; acc: 0.73
Batch: 600; loss: 0.98; acc: 0.69
Batch: 620; loss: 1.12; acc: 0.67
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 1.23; acc: 0.64
Batch: 680; loss: 0.88; acc: 0.77
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 0.68; acc: 0.77
Batch: 740; loss: 0.83; acc: 0.7
Batch: 760; loss: 0.8; acc: 0.7
Batch: 780; loss: 0.9; acc: 0.75
Train Epoch over. train_loss: 0.9; train_accuracy: 0.71 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.71; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.83
Val Epoch over. val_loss: 0.847257997199988; val_accuracy: 0.7337778662420382 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 1.1; acc: 0.64
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 0.81; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 0.88; acc: 0.72
Batch: 180; loss: 1.03; acc: 0.66
Batch: 200; loss: 0.89; acc: 0.66
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.73; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.62
Batch: 280; loss: 0.9; acc: 0.73
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 0.87; acc: 0.73
Batch: 340; loss: 1.24; acc: 0.58
Batch: 360; loss: 0.88; acc: 0.7
Batch: 380; loss: 0.9; acc: 0.73
Batch: 400; loss: 0.88; acc: 0.75
Batch: 420; loss: 0.94; acc: 0.73
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.96; acc: 0.67
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 0.89; acc: 0.77
Batch: 520; loss: 0.83; acc: 0.7
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 1.01; acc: 0.7
Batch: 580; loss: 0.77; acc: 0.72
Batch: 600; loss: 0.9; acc: 0.78
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 0.77; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 0.8; acc: 0.75
Batch: 720; loss: 0.88; acc: 0.69
Batch: 740; loss: 0.7; acc: 0.75
Batch: 760; loss: 0.8; acc: 0.72
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.9; train_accuracy: 0.71 

Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.87; acc: 0.72
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.81
Val Epoch over. val_loss: 0.9596428244736543; val_accuracy: 0.6898885350318471 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.97; acc: 0.61
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.56
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.97; acc: 0.7
Batch: 200; loss: 0.77; acc: 0.73
Batch: 220; loss: 0.83; acc: 0.7
Batch: 240; loss: 1.06; acc: 0.62
Batch: 260; loss: 0.87; acc: 0.69
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 1.05; acc: 0.7
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 1.1; acc: 0.64
Batch: 360; loss: 0.99; acc: 0.72
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 0.88; acc: 0.75
Batch: 420; loss: 0.98; acc: 0.66
Batch: 440; loss: 1.18; acc: 0.66
Batch: 460; loss: 1.0; acc: 0.64
Batch: 480; loss: 0.91; acc: 0.72
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.87; acc: 0.7
Batch: 540; loss: 1.13; acc: 0.64
Batch: 560; loss: 1.15; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.7
Batch: 600; loss: 1.03; acc: 0.67
Batch: 620; loss: 0.82; acc: 0.75
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 1.02; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.62
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 1.11; acc: 0.62
Train Epoch over. train_loss: 0.9; train_accuracy: 0.71 

Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.6; acc: 0.77
Val Epoch over. val_loss: 0.8925709215698728; val_accuracy: 0.7124800955414012 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.21; acc: 0.58
Batch: 20; loss: 0.89; acc: 0.7
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 0.86; acc: 0.66
Batch: 120; loss: 0.81; acc: 0.69
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.95; acc: 0.73
Batch: 200; loss: 0.81; acc: 0.72
Batch: 220; loss: 0.89; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.93; acc: 0.72
Batch: 300; loss: 0.92; acc: 0.69
Batch: 320; loss: 0.81; acc: 0.75
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.96; acc: 0.64
Batch: 400; loss: 0.72; acc: 0.69
Batch: 420; loss: 0.95; acc: 0.73
Batch: 440; loss: 0.87; acc: 0.78
Batch: 460; loss: 0.97; acc: 0.73
Batch: 480; loss: 0.88; acc: 0.72
Batch: 500; loss: 0.86; acc: 0.77
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 1.01; acc: 0.7
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.91; acc: 0.69
Batch: 640; loss: 0.92; acc: 0.67
Batch: 660; loss: 0.87; acc: 0.67
Batch: 680; loss: 0.77; acc: 0.77
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 1.08; acc: 0.7
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 1.04; acc: 0.69
Batch: 140; loss: 0.5; acc: 0.84
Val Epoch over. val_loss: 0.8331526229336004; val_accuracy: 0.7392515923566879 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.01; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.67
Batch: 140; loss: 0.71; acc: 0.72
Batch: 160; loss: 0.84; acc: 0.75
Batch: 180; loss: 0.89; acc: 0.7
Batch: 200; loss: 1.19; acc: 0.59
Batch: 220; loss: 0.76; acc: 0.7
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 1.07; acc: 0.64
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.95; acc: 0.66
Batch: 340; loss: 0.94; acc: 0.73
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.99; acc: 0.69
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.92; acc: 0.72
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.94; acc: 0.7
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 1.1; acc: 0.61
Batch: 560; loss: 0.91; acc: 0.67
Batch: 580; loss: 0.78; acc: 0.84
Batch: 600; loss: 0.89; acc: 0.72
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.94; acc: 0.67
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.8; acc: 0.75
Batch: 740; loss: 1.12; acc: 0.69
Batch: 760; loss: 0.9; acc: 0.77
Batch: 780; loss: 0.68; acc: 0.75
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.67
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.49; acc: 0.84
Val Epoch over. val_loss: 0.8233152646927318; val_accuracy: 0.7442277070063694 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.7
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.75
Batch: 100; loss: 0.78; acc: 0.75
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.69
Batch: 180; loss: 0.9; acc: 0.67
Batch: 200; loss: 1.13; acc: 0.7
Batch: 220; loss: 1.15; acc: 0.7
Batch: 240; loss: 0.88; acc: 0.72
Batch: 260; loss: 0.86; acc: 0.69
Batch: 280; loss: 0.96; acc: 0.62
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 1.02; acc: 0.62
Batch: 340; loss: 0.88; acc: 0.73
Batch: 360; loss: 0.72; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.73
Batch: 400; loss: 1.28; acc: 0.58
Batch: 420; loss: 0.78; acc: 0.73
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.85; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.7
Batch: 520; loss: 0.91; acc: 0.75
Batch: 540; loss: 1.03; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 0.98; acc: 0.66
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.97; acc: 0.64
Batch: 640; loss: 0.76; acc: 0.69
Batch: 660; loss: 0.75; acc: 0.73
Batch: 680; loss: 0.63; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.77
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.7
Batch: 780; loss: 1.14; acc: 0.59
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.7
Batch: 120; loss: 1.1; acc: 0.62
Batch: 140; loss: 0.49; acc: 0.83
Val Epoch over. val_loss: 0.8420449704121632; val_accuracy: 0.736265923566879 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 1.03; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.97; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.69
Batch: 160; loss: 1.04; acc: 0.66
Batch: 180; loss: 0.76; acc: 0.7
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.72
Batch: 280; loss: 1.09; acc: 0.69
Batch: 300; loss: 0.91; acc: 0.73
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.66
Batch: 360; loss: 0.86; acc: 0.72
Batch: 380; loss: 0.83; acc: 0.73
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.94; acc: 0.77
Batch: 440; loss: 1.11; acc: 0.61
Batch: 460; loss: 0.93; acc: 0.66
Batch: 480; loss: 0.88; acc: 0.73
Batch: 500; loss: 0.93; acc: 0.77
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.78; acc: 0.73
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.95; acc: 0.67
Batch: 620; loss: 0.75; acc: 0.73
Batch: 640; loss: 0.74; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 0.76; acc: 0.77
Batch: 700; loss: 1.06; acc: 0.73
Batch: 720; loss: 0.64; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.7
Batch: 760; loss: 1.07; acc: 0.58
Batch: 780; loss: 0.83; acc: 0.72
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.72
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.72
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.51; acc: 0.84
Val Epoch over. val_loss: 0.837634582997887; val_accuracy: 0.7357683121019108 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.7
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.91; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.72
Batch: 160; loss: 0.89; acc: 0.72
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.63; acc: 0.75
Batch: 220; loss: 0.83; acc: 0.72
Batch: 240; loss: 0.79; acc: 0.84
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 1.1; acc: 0.72
Batch: 300; loss: 0.86; acc: 0.75
Batch: 320; loss: 1.34; acc: 0.62
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 0.88; acc: 0.72
Batch: 380; loss: 0.91; acc: 0.69
Batch: 400; loss: 0.84; acc: 0.78
Batch: 420; loss: 0.69; acc: 0.77
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 1.03; acc: 0.66
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.73
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 1.26; acc: 0.62
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 0.99; acc: 0.7
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.73
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.94; acc: 0.72
Batch: 720; loss: 0.86; acc: 0.77
Batch: 740; loss: 1.08; acc: 0.66
Batch: 760; loss: 0.94; acc: 0.66
Batch: 780; loss: 0.99; acc: 0.64
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.68; acc: 0.75
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.7
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 0.51; acc: 0.83
Val Epoch over. val_loss: 0.8359847594598296; val_accuracy: 0.7371616242038217 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.03; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 0.75; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.93; acc: 0.69
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 0.86; acc: 0.66
Batch: 200; loss: 0.82; acc: 0.7
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 0.96; acc: 0.67
Batch: 260; loss: 0.77; acc: 0.77
Batch: 280; loss: 0.94; acc: 0.67
Batch: 300; loss: 0.96; acc: 0.69
Batch: 320; loss: 1.08; acc: 0.67
Batch: 340; loss: 0.96; acc: 0.67
Batch: 360; loss: 0.77; acc: 0.72
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.85; acc: 0.73
Batch: 420; loss: 0.98; acc: 0.75
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.8; acc: 0.8
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.77
Batch: 580; loss: 0.94; acc: 0.73
Batch: 600; loss: 1.08; acc: 0.67
Batch: 620; loss: 0.93; acc: 0.62
Batch: 640; loss: 0.85; acc: 0.7
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.77; acc: 0.7
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 1.02; acc: 0.72
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 0.93; acc: 0.64
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 0.5; acc: 0.84
Val Epoch over. val_loss: 0.828318673334304; val_accuracy: 0.7394506369426752 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.82; acc: 0.7
Batch: 60; loss: 0.86; acc: 0.66
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.98; acc: 0.67
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.89; acc: 0.75
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.72
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 1.02; acc: 0.7
Batch: 320; loss: 1.17; acc: 0.64
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.86; acc: 0.84
Batch: 380; loss: 0.93; acc: 0.72
Batch: 400; loss: 0.97; acc: 0.64
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.97; acc: 0.73
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.68; acc: 0.77
Batch: 500; loss: 1.04; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.72
Batch: 540; loss: 0.81; acc: 0.73
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.89; acc: 0.69
Batch: 600; loss: 0.73; acc: 0.78
Batch: 620; loss: 1.0; acc: 0.7
Batch: 640; loss: 1.16; acc: 0.61
Batch: 660; loss: 0.85; acc: 0.75
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.87; acc: 0.66
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 1.04; acc: 0.67
Batch: 760; loss: 0.9; acc: 0.77
Batch: 780; loss: 1.07; acc: 0.67
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.69
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.5; acc: 0.83
Val Epoch over. val_loss: 0.8343527584698549; val_accuracy: 0.7398487261146497 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.91; acc: 0.77
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 0.69; acc: 0.73
Batch: 80; loss: 0.83; acc: 0.67
Batch: 100; loss: 0.85; acc: 0.73
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.62
Batch: 180; loss: 1.0; acc: 0.73
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 1.09; acc: 0.62
Batch: 240; loss: 0.97; acc: 0.67
Batch: 260; loss: 0.76; acc: 0.73
Batch: 280; loss: 1.26; acc: 0.62
Batch: 300; loss: 0.8; acc: 0.77
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.92; acc: 0.75
Batch: 380; loss: 1.09; acc: 0.67
Batch: 400; loss: 0.91; acc: 0.69
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 0.84; acc: 0.72
Batch: 460; loss: 0.88; acc: 0.7
Batch: 480; loss: 0.93; acc: 0.77
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.91; acc: 0.69
Batch: 540; loss: 0.85; acc: 0.69
Batch: 560; loss: 0.94; acc: 0.7
Batch: 580; loss: 0.96; acc: 0.72
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 0.77; acc: 0.72
Batch: 660; loss: 1.08; acc: 0.59
Batch: 680; loss: 1.03; acc: 0.7
Batch: 700; loss: 1.04; acc: 0.66
Batch: 720; loss: 0.9; acc: 0.69
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.92; acc: 0.75
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.97; acc: 0.69
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.82; acc: 0.73
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.64
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.8243028305138752; val_accuracy: 0.7424363057324841 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 0.95; acc: 0.7
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.67
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.99; acc: 0.67
Batch: 180; loss: 0.84; acc: 0.75
Batch: 200; loss: 0.99; acc: 0.64
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 1.04; acc: 0.61
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.7; acc: 0.72
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.7; acc: 0.75
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 0.86; acc: 0.7
Batch: 420; loss: 0.74; acc: 0.7
Batch: 440; loss: 0.89; acc: 0.67
Batch: 460; loss: 0.94; acc: 0.64
Batch: 480; loss: 0.97; acc: 0.67
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 1.03; acc: 0.64
Batch: 540; loss: 0.93; acc: 0.66
Batch: 560; loss: 1.01; acc: 0.66
Batch: 580; loss: 1.0; acc: 0.73
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.83; acc: 0.72
Batch: 660; loss: 1.0; acc: 0.73
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.8; acc: 0.73
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.75
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 0.82; acc: 0.69
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.73
Batch: 120; loss: 1.02; acc: 0.69
Batch: 140; loss: 0.49; acc: 0.84
Val Epoch over. val_loss: 0.8299737612532961; val_accuracy: 0.738953025477707 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.72
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.64
Batch: 160; loss: 0.8; acc: 0.72
Batch: 180; loss: 0.9; acc: 0.73
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.94; acc: 0.7
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 1.01; acc: 0.69
Batch: 320; loss: 0.94; acc: 0.66
Batch: 340; loss: 1.21; acc: 0.67
Batch: 360; loss: 0.81; acc: 0.78
Batch: 380; loss: 0.88; acc: 0.66
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.89; acc: 0.72
Batch: 440; loss: 0.95; acc: 0.67
Batch: 460; loss: 0.85; acc: 0.67
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 1.09; acc: 0.64
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.11; acc: 0.69
Batch: 560; loss: 0.82; acc: 0.72
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.7; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 0.7; acc: 0.75
Batch: 680; loss: 0.9; acc: 0.78
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.9; acc: 0.7
Batch: 740; loss: 0.95; acc: 0.73
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.77; acc: 0.67
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.7
Batch: 120; loss: 1.02; acc: 0.67
Batch: 140; loss: 0.48; acc: 0.83
Val Epoch over. val_loss: 0.8229263941193842; val_accuracy: 0.7441281847133758 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.83; acc: 0.72
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.95; acc: 0.7
Batch: 160; loss: 1.05; acc: 0.66
Batch: 180; loss: 0.84; acc: 0.69
Batch: 200; loss: 0.95; acc: 0.7
Batch: 220; loss: 0.91; acc: 0.69
Batch: 240; loss: 0.82; acc: 0.7
Batch: 260; loss: 1.0; acc: 0.7
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 1.05; acc: 0.7
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.75
Batch: 360; loss: 0.77; acc: 0.73
Batch: 380; loss: 0.88; acc: 0.7
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 1.02; acc: 0.66
Batch: 440; loss: 0.99; acc: 0.7
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 0.97; acc: 0.73
Batch: 520; loss: 0.75; acc: 0.72
Batch: 540; loss: 0.82; acc: 0.78
Batch: 560; loss: 0.78; acc: 0.73
Batch: 580; loss: 0.98; acc: 0.78
Batch: 600; loss: 1.05; acc: 0.64
Batch: 620; loss: 0.96; acc: 0.72
Batch: 640; loss: 0.92; acc: 0.72
Batch: 660; loss: 0.78; acc: 0.7
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.87; acc: 0.7
Batch: 720; loss: 1.05; acc: 0.62
Batch: 740; loss: 0.99; acc: 0.66
Batch: 760; loss: 1.09; acc: 0.64
Batch: 780; loss: 1.15; acc: 0.62
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 0.49; acc: 0.86
Val Epoch over. val_loss: 0.8217759310819541; val_accuracy: 0.7448248407643312 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.72
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 1.14; acc: 0.66
Batch: 100; loss: 0.83; acc: 0.7
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 1.03; acc: 0.7
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.85; acc: 0.8
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.77
Batch: 260; loss: 0.92; acc: 0.72
Batch: 280; loss: 1.09; acc: 0.61
Batch: 300; loss: 1.13; acc: 0.69
Batch: 320; loss: 0.86; acc: 0.73
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 0.77; acc: 0.7
Batch: 380; loss: 0.77; acc: 0.75
Batch: 400; loss: 0.77; acc: 0.7
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 0.76; acc: 0.73
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.98; acc: 0.7
Batch: 500; loss: 0.91; acc: 0.72
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 0.72; acc: 0.75
Batch: 580; loss: 0.85; acc: 0.72
Batch: 600; loss: 0.9; acc: 0.73
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 0.94; acc: 0.69
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 1.02; acc: 0.69
Batch: 740; loss: 1.05; acc: 0.69
Batch: 760; loss: 1.09; acc: 0.62
Batch: 780; loss: 0.71; acc: 0.75
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.75; acc: 0.7
Batch: 120; loss: 1.03; acc: 0.66
Batch: 140; loss: 0.5; acc: 0.84
Val Epoch over. val_loss: 0.8256523537028367; val_accuracy: 0.7384554140127388 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.04; acc: 0.59
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 0.96; acc: 0.72
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.95; acc: 0.66
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.73
Batch: 180; loss: 0.89; acc: 0.69
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.7; acc: 0.75
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 1.03; acc: 0.66
Batch: 280; loss: 0.9; acc: 0.75
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.03; acc: 0.59
Batch: 340; loss: 0.99; acc: 0.66
Batch: 360; loss: 1.09; acc: 0.69
Batch: 380; loss: 0.84; acc: 0.77
Batch: 400; loss: 0.84; acc: 0.75
Batch: 420; loss: 0.87; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.7
Batch: 460; loss: 0.65; acc: 0.75
Batch: 480; loss: 0.9; acc: 0.69
Batch: 500; loss: 0.81; acc: 0.73
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.72
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.72
Batch: 600; loss: 0.99; acc: 0.69
Batch: 620; loss: 0.85; acc: 0.77
Batch: 640; loss: 1.02; acc: 0.7
Batch: 660; loss: 0.65; acc: 0.77
Batch: 680; loss: 0.76; acc: 0.73
Batch: 700; loss: 0.83; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.75
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 1.17; acc: 0.61
Batch: 780; loss: 0.75; acc: 0.77
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.7
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 0.51; acc: 0.86
Val Epoch over. val_loss: 0.8312533998944956; val_accuracy: 0.7391520700636943 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.69
Batch: 100; loss: 1.01; acc: 0.61
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.75
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.81; acc: 0.77
Batch: 200; loss: 1.06; acc: 0.64
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.93; acc: 0.75
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 0.92; acc: 0.77
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.92; acc: 0.62
Batch: 360; loss: 0.96; acc: 0.75
Batch: 380; loss: 0.95; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.69
Batch: 420; loss: 0.79; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.97; acc: 0.7
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.85; acc: 0.7
Batch: 540; loss: 1.04; acc: 0.69
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.87; acc: 0.64
Batch: 600; loss: 0.99; acc: 0.66
Batch: 620; loss: 1.32; acc: 0.61
Batch: 640; loss: 0.92; acc: 0.66
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 0.73; acc: 0.72
Batch: 700; loss: 0.96; acc: 0.7
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 0.9; acc: 0.7
Batch: 760; loss: 1.01; acc: 0.73
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.7
Batch: 140; loss: 0.48; acc: 0.88
Val Epoch over. val_loss: 0.8201057732484902; val_accuracy: 0.744327229299363 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.69
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.75; acc: 0.69
Batch: 100; loss: 1.01; acc: 0.73
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.89; acc: 0.73
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.69
Batch: 240; loss: 0.8; acc: 0.69
Batch: 260; loss: 0.65; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.77
Batch: 320; loss: 0.83; acc: 0.67
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 0.74; acc: 0.73
Batch: 380; loss: 0.86; acc: 0.72
Batch: 400; loss: 0.69; acc: 0.75
Batch: 420; loss: 1.14; acc: 0.67
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.66
Batch: 600; loss: 0.79; acc: 0.69
Batch: 620; loss: 0.79; acc: 0.77
Batch: 640; loss: 0.76; acc: 0.75
Batch: 660; loss: 0.92; acc: 0.73
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 1.06; acc: 0.62
Batch: 720; loss: 0.86; acc: 0.77
Batch: 740; loss: 1.1; acc: 0.53
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.82; acc: 0.7
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 0.99; acc: 0.66
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.8192533362822928; val_accuracy: 0.7430334394904459 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.76; acc: 0.77
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.72
Batch: 100; loss: 0.93; acc: 0.66
Batch: 120; loss: 0.86; acc: 0.69
Batch: 140; loss: 0.76; acc: 0.72
Batch: 160; loss: 0.91; acc: 0.67
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.91; acc: 0.72
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.69
Batch: 280; loss: 0.86; acc: 0.72
Batch: 300; loss: 0.93; acc: 0.69
Batch: 320; loss: 0.99; acc: 0.66
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.02; acc: 0.62
Batch: 380; loss: 1.11; acc: 0.69
Batch: 400; loss: 0.93; acc: 0.7
Batch: 420; loss: 1.01; acc: 0.7
Batch: 440; loss: 0.96; acc: 0.72
Batch: 460; loss: 0.87; acc: 0.67
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 1.14; acc: 0.66
Batch: 540; loss: 0.78; acc: 0.72
Batch: 560; loss: 1.1; acc: 0.69
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.87; acc: 0.77
Batch: 620; loss: 0.97; acc: 0.7
Batch: 640; loss: 0.94; acc: 0.66
Batch: 660; loss: 0.92; acc: 0.72
Batch: 680; loss: 1.0; acc: 0.73
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 1.17; acc: 0.7
Batch: 740; loss: 0.85; acc: 0.72
Batch: 760; loss: 0.86; acc: 0.77
Batch: 780; loss: 1.05; acc: 0.7
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.8234618163792191; val_accuracy: 0.7411425159235668 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.98; acc: 0.69
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.89; acc: 0.7
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.95; acc: 0.75
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 1.12; acc: 0.72
Batch: 240; loss: 0.91; acc: 0.7
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.7
Batch: 340; loss: 0.96; acc: 0.66
Batch: 360; loss: 0.7; acc: 0.75
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 0.92; acc: 0.75
Batch: 420; loss: 1.0; acc: 0.67
Batch: 440; loss: 0.97; acc: 0.73
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 0.97; acc: 0.72
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.78; acc: 0.73
Batch: 580; loss: 0.82; acc: 0.75
Batch: 600; loss: 0.93; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.91; acc: 0.73
Batch: 660; loss: 0.99; acc: 0.66
Batch: 680; loss: 1.01; acc: 0.7
Batch: 700; loss: 0.86; acc: 0.69
Batch: 720; loss: 1.0; acc: 0.69
Batch: 740; loss: 0.82; acc: 0.75
Batch: 760; loss: 1.17; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.73
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.8193611906971902; val_accuracy: 0.7430334394904459 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.94; acc: 0.69
Batch: 180; loss: 0.96; acc: 0.7
Batch: 200; loss: 0.94; acc: 0.7
Batch: 220; loss: 0.85; acc: 0.77
Batch: 240; loss: 0.97; acc: 0.69
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 1.23; acc: 0.62
Batch: 320; loss: 1.06; acc: 0.62
Batch: 340; loss: 0.92; acc: 0.69
Batch: 360; loss: 0.97; acc: 0.69
Batch: 380; loss: 0.89; acc: 0.67
Batch: 400; loss: 0.98; acc: 0.66
Batch: 420; loss: 0.59; acc: 0.77
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.89; acc: 0.73
Batch: 560; loss: 1.17; acc: 0.62
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.72
Batch: 620; loss: 0.76; acc: 0.78
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.87; acc: 0.72
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 0.8; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 0.96; acc: 0.73
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.72
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.8184317542109519; val_accuracy: 0.7442277070063694 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.79; acc: 0.7
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 1.0; acc: 0.66
Batch: 160; loss: 0.93; acc: 0.67
Batch: 180; loss: 0.99; acc: 0.77
Batch: 200; loss: 0.86; acc: 0.77
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.82; acc: 0.72
Batch: 280; loss: 0.94; acc: 0.7
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.87; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.95; acc: 0.78
Batch: 400; loss: 0.86; acc: 0.72
Batch: 420; loss: 1.0; acc: 0.61
Batch: 440; loss: 0.95; acc: 0.7
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.71; acc: 0.72
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.82; acc: 0.73
Batch: 540; loss: 1.03; acc: 0.72
Batch: 560; loss: 1.03; acc: 0.7
Batch: 580; loss: 0.89; acc: 0.58
Batch: 600; loss: 0.9; acc: 0.62
Batch: 620; loss: 0.94; acc: 0.66
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.73
Batch: 680; loss: 1.41; acc: 0.55
Batch: 700; loss: 0.83; acc: 0.72
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 0.94; acc: 0.64
Batch: 760; loss: 0.85; acc: 0.69
Batch: 780; loss: 0.88; acc: 0.69
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 1.03; acc: 0.64
Batch: 140; loss: 0.5; acc: 0.86
Val Epoch over. val_loss: 0.8216625055310073; val_accuracy: 0.7413415605095541 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.8; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 1.22; acc: 0.61
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.88; acc: 0.66
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.75
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 1.34; acc: 0.66
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 1.18; acc: 0.59
Batch: 260; loss: 0.81; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 0.83; acc: 0.75
Batch: 360; loss: 1.15; acc: 0.58
Batch: 380; loss: 0.91; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.75
Batch: 420; loss: 0.67; acc: 0.75
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 1.22; acc: 0.61
Batch: 480; loss: 0.67; acc: 0.72
Batch: 500; loss: 0.89; acc: 0.75
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 1.0; acc: 0.7
Batch: 600; loss: 0.98; acc: 0.66
Batch: 620; loss: 0.87; acc: 0.7
Batch: 640; loss: 0.88; acc: 0.8
Batch: 660; loss: 1.02; acc: 0.69
Batch: 680; loss: 0.82; acc: 0.67
Batch: 700; loss: 1.02; acc: 0.73
Batch: 720; loss: 0.71; acc: 0.75
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 1.01; acc: 0.66
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 0.99; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.88
Val Epoch over. val_loss: 0.8193072160338141; val_accuracy: 0.7437300955414012 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.8; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 0.96; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.67
Batch: 160; loss: 0.72; acc: 0.72
Batch: 180; loss: 0.78; acc: 0.69
Batch: 200; loss: 0.87; acc: 0.7
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 0.98; acc: 0.69
Batch: 260; loss: 1.0; acc: 0.77
Batch: 280; loss: 0.63; acc: 0.73
Batch: 300; loss: 0.8; acc: 0.77
Batch: 320; loss: 0.88; acc: 0.72
Batch: 340; loss: 1.01; acc: 0.66
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.72
Batch: 400; loss: 0.9; acc: 0.75
Batch: 420; loss: 0.58; acc: 0.78
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.74; acc: 0.69
Batch: 500; loss: 1.02; acc: 0.73
Batch: 520; loss: 0.71; acc: 0.73
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 1.26; acc: 0.55
Batch: 580; loss: 0.83; acc: 0.72
Batch: 600; loss: 0.94; acc: 0.7
Batch: 620; loss: 0.83; acc: 0.69
Batch: 640; loss: 0.83; acc: 0.67
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 1.05; acc: 0.69
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.96; acc: 0.77
Batch: 760; loss: 0.96; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.75
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.48; acc: 0.86
Val Epoch over. val_loss: 0.8176948709093081; val_accuracy: 0.7426353503184714 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 1.04; acc: 0.59
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.67
Batch: 160; loss: 0.75; acc: 0.73
Batch: 180; loss: 0.79; acc: 0.77
Batch: 200; loss: 1.11; acc: 0.59
Batch: 220; loss: 1.21; acc: 0.61
Batch: 240; loss: 1.02; acc: 0.67
Batch: 260; loss: 0.78; acc: 0.77
Batch: 280; loss: 0.77; acc: 0.72
Batch: 300; loss: 0.59; acc: 0.78
Batch: 320; loss: 0.96; acc: 0.69
Batch: 340; loss: 0.93; acc: 0.77
Batch: 360; loss: 1.08; acc: 0.66
Batch: 380; loss: 0.92; acc: 0.8
Batch: 400; loss: 0.99; acc: 0.72
Batch: 420; loss: 0.9; acc: 0.73
Batch: 440; loss: 1.11; acc: 0.67
Batch: 460; loss: 1.03; acc: 0.66
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 0.91; acc: 0.69
Batch: 520; loss: 0.88; acc: 0.78
Batch: 540; loss: 0.85; acc: 0.72
Batch: 560; loss: 1.14; acc: 0.69
Batch: 580; loss: 0.8; acc: 0.77
Batch: 600; loss: 0.81; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.83; acc: 0.72
Batch: 680; loss: 0.87; acc: 0.67
Batch: 700; loss: 0.9; acc: 0.7
Batch: 720; loss: 0.86; acc: 0.7
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.63; acc: 0.75
Batch: 780; loss: 1.04; acc: 0.67
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.69
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.817445905155437; val_accuracy: 0.7414410828025477 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.72; acc: 0.75
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 1.02; acc: 0.66
Batch: 200; loss: 0.67; acc: 0.75
Batch: 220; loss: 0.83; acc: 0.78
Batch: 240; loss: 1.06; acc: 0.59
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 1.0; acc: 0.73
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.99; acc: 0.67
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 1.0; acc: 0.67
Batch: 400; loss: 0.93; acc: 0.73
Batch: 420; loss: 1.0; acc: 0.64
Batch: 440; loss: 0.96; acc: 0.73
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.87; acc: 0.73
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.86; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.84; acc: 0.73
Batch: 580; loss: 1.01; acc: 0.69
Batch: 600; loss: 1.06; acc: 0.64
Batch: 620; loss: 1.02; acc: 0.67
Batch: 640; loss: 0.8; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.73
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 0.87; acc: 0.72
Batch: 720; loss: 0.87; acc: 0.69
Batch: 740; loss: 0.92; acc: 0.7
Batch: 760; loss: 0.9; acc: 0.7
Batch: 780; loss: 0.83; acc: 0.69
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.8186064318866487; val_accuracy: 0.7426353503184714 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.92; acc: 0.7
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.74; acc: 0.75
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 1.0; acc: 0.67
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.91; acc: 0.73
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.82; acc: 0.81
Batch: 340; loss: 0.87; acc: 0.7
Batch: 360; loss: 0.89; acc: 0.75
Batch: 380; loss: 0.86; acc: 0.69
Batch: 400; loss: 1.0; acc: 0.7
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 1.05; acc: 0.67
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.8; acc: 0.73
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 0.89; acc: 0.73
Batch: 540; loss: 0.75; acc: 0.75
Batch: 560; loss: 0.88; acc: 0.69
Batch: 580; loss: 0.93; acc: 0.72
Batch: 600; loss: 0.94; acc: 0.67
Batch: 620; loss: 0.7; acc: 0.75
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 0.86; acc: 0.77
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.8; acc: 0.69
Batch: 780; loss: 0.81; acc: 0.73
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.8185885111997082; val_accuracy: 0.7421377388535032 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.9; acc: 0.73
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.96; acc: 0.69
Batch: 180; loss: 0.99; acc: 0.69
Batch: 200; loss: 0.75; acc: 0.77
Batch: 220; loss: 0.9; acc: 0.72
Batch: 240; loss: 0.91; acc: 0.77
Batch: 260; loss: 0.93; acc: 0.7
Batch: 280; loss: 0.8; acc: 0.73
Batch: 300; loss: 0.72; acc: 0.75
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.81; acc: 0.77
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.93; acc: 0.7
Batch: 400; loss: 1.13; acc: 0.62
Batch: 420; loss: 1.11; acc: 0.62
Batch: 440; loss: 1.11; acc: 0.62
Batch: 460; loss: 0.94; acc: 0.72
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.82; acc: 0.69
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 0.74; acc: 0.72
Batch: 560; loss: 0.85; acc: 0.75
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.85; acc: 0.8
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.85; acc: 0.67
Batch: 660; loss: 0.7; acc: 0.78
Batch: 680; loss: 0.74; acc: 0.77
Batch: 700; loss: 0.98; acc: 0.61
Batch: 720; loss: 1.05; acc: 0.59
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 1.06; acc: 0.67
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.67
Batch: 140; loss: 0.48; acc: 0.83
Val Epoch over. val_loss: 0.8181673654705096; val_accuracy: 0.741640127388535 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.72
Batch: 40; loss: 0.82; acc: 0.7
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 0.83; acc: 0.72
Batch: 120; loss: 0.92; acc: 0.7
Batch: 140; loss: 0.77; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.73
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.73
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 0.83; acc: 0.72
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.89; acc: 0.7
Batch: 320; loss: 0.9; acc: 0.7
Batch: 340; loss: 0.85; acc: 0.73
Batch: 360; loss: 0.8; acc: 0.77
Batch: 380; loss: 1.1; acc: 0.67
Batch: 400; loss: 1.43; acc: 0.62
Batch: 420; loss: 0.9; acc: 0.64
Batch: 440; loss: 1.04; acc: 0.77
Batch: 460; loss: 0.95; acc: 0.64
Batch: 480; loss: 1.1; acc: 0.66
Batch: 500; loss: 1.0; acc: 0.64
Batch: 520; loss: 0.85; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 0.88; acc: 0.67
Batch: 580; loss: 1.06; acc: 0.64
Batch: 600; loss: 0.93; acc: 0.66
Batch: 620; loss: 1.12; acc: 0.7
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.95; acc: 0.78
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.81; acc: 0.69
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 0.96; acc: 0.66
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.47; acc: 0.86
Val Epoch over. val_loss: 0.8172872210763822; val_accuracy: 0.7440286624203821 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.88; acc: 0.67
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.83; acc: 0.69
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 0.82; acc: 0.69
Batch: 260; loss: 0.98; acc: 0.64
Batch: 280; loss: 0.83; acc: 0.75
Batch: 300; loss: 0.73; acc: 0.72
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 1.06; acc: 0.58
Batch: 360; loss: 1.08; acc: 0.66
Batch: 380; loss: 0.94; acc: 0.7
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 1.0; acc: 0.7
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.89; acc: 0.66
Batch: 500; loss: 0.95; acc: 0.73
Batch: 520; loss: 1.02; acc: 0.66
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.93; acc: 0.69
Batch: 580; loss: 0.85; acc: 0.8
Batch: 600; loss: 0.91; acc: 0.69
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 1.05; acc: 0.64
Batch: 700; loss: 0.94; acc: 0.72
Batch: 720; loss: 0.83; acc: 0.78
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 0.85; acc: 0.69
Batch: 780; loss: 0.87; acc: 0.8
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.69
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 0.47; acc: 0.86
Val Epoch over. val_loss: 0.8164488513758228; val_accuracy: 0.7431329617834395 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.69
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.98; acc: 0.7
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.77; acc: 0.69
Batch: 200; loss: 0.87; acc: 0.72
Batch: 220; loss: 0.78; acc: 0.73
Batch: 240; loss: 1.07; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.67
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 1.03; acc: 0.7
Batch: 320; loss: 0.96; acc: 0.72
Batch: 340; loss: 0.73; acc: 0.78
Batch: 360; loss: 0.95; acc: 0.72
Batch: 380; loss: 0.83; acc: 0.7
Batch: 400; loss: 0.98; acc: 0.64
Batch: 420; loss: 0.83; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 1.14; acc: 0.64
Batch: 480; loss: 0.98; acc: 0.73
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 0.73; acc: 0.73
Batch: 540; loss: 0.87; acc: 0.7
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.86; acc: 0.72
Batch: 620; loss: 0.96; acc: 0.72
Batch: 640; loss: 1.21; acc: 0.58
Batch: 660; loss: 0.82; acc: 0.7
Batch: 680; loss: 0.79; acc: 0.67
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 0.85; acc: 0.84
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.8171770331586242; val_accuracy: 0.7413415605095541 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 1.13; acc: 0.66
Batch: 60; loss: 0.93; acc: 0.67
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.96; acc: 0.67
Batch: 160; loss: 1.09; acc: 0.62
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.89; acc: 0.66
Batch: 240; loss: 0.92; acc: 0.75
Batch: 260; loss: 1.01; acc: 0.7
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 1.05; acc: 0.59
Batch: 320; loss: 0.77; acc: 0.72
Batch: 340; loss: 1.0; acc: 0.66
Batch: 360; loss: 1.09; acc: 0.64
Batch: 380; loss: 0.86; acc: 0.67
Batch: 400; loss: 0.89; acc: 0.69
Batch: 420; loss: 1.02; acc: 0.61
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 1.03; acc: 0.64
Batch: 540; loss: 0.97; acc: 0.69
Batch: 560; loss: 0.64; acc: 0.73
Batch: 580; loss: 0.85; acc: 0.7
Batch: 600; loss: 0.94; acc: 0.66
Batch: 620; loss: 0.83; acc: 0.75
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 1.04; acc: 0.7
Batch: 680; loss: 0.74; acc: 0.73
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 1.1; acc: 0.73
Batch: 740; loss: 0.96; acc: 0.67
Batch: 760; loss: 1.06; acc: 0.64
Batch: 780; loss: 1.0; acc: 0.7
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 0.48; acc: 0.86
Val Epoch over. val_loss: 0.8164705000105937; val_accuracy: 0.7428343949044586 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.98; acc: 0.77
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.75
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.98; acc: 0.67
Batch: 160; loss: 0.92; acc: 0.72
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.1; acc: 0.67
Batch: 220; loss: 0.96; acc: 0.67
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 0.75; acc: 0.77
Batch: 280; loss: 0.81; acc: 0.73
Batch: 300; loss: 0.93; acc: 0.66
Batch: 320; loss: 0.73; acc: 0.7
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.93; acc: 0.69
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 0.95; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.61
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.89; acc: 0.72
Batch: 500; loss: 1.11; acc: 0.66
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 0.99; acc: 0.64
Batch: 560; loss: 0.95; acc: 0.67
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 0.98; acc: 0.64
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.87; acc: 0.7
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.85; acc: 0.77
Batch: 720; loss: 0.78; acc: 0.73
Batch: 740; loss: 0.81; acc: 0.7
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.86
Val Epoch over. val_loss: 0.8161779737016958; val_accuracy: 0.7419386942675159 

plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 41775
elements in E: 8790400
fraction nonzero: 0.004752343465598835
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.16
Batch: 100; loss: 2.23; acc: 0.16
Batch: 120; loss: 2.24; acc: 0.2
Batch: 140; loss: 2.22; acc: 0.31
Batch: 160; loss: 2.23; acc: 0.23
Batch: 180; loss: 2.25; acc: 0.2
Batch: 200; loss: 2.22; acc: 0.25
Batch: 220; loss: 2.23; acc: 0.17
Batch: 240; loss: 2.16; acc: 0.2
Batch: 260; loss: 2.18; acc: 0.3
Batch: 280; loss: 2.13; acc: 0.28
Batch: 300; loss: 2.11; acc: 0.28
Batch: 320; loss: 2.0; acc: 0.34
Batch: 340; loss: 1.94; acc: 0.42
Batch: 360; loss: 1.98; acc: 0.38
Batch: 380; loss: 2.08; acc: 0.23
Batch: 400; loss: 1.89; acc: 0.41
Batch: 420; loss: 1.88; acc: 0.42
Batch: 440; loss: 1.87; acc: 0.33
Batch: 460; loss: 1.84; acc: 0.42
Batch: 480; loss: 1.7; acc: 0.39
Batch: 500; loss: 1.55; acc: 0.56
Batch: 520; loss: 1.62; acc: 0.42
Batch: 540; loss: 1.72; acc: 0.42
Batch: 560; loss: 1.68; acc: 0.38
Batch: 580; loss: 1.55; acc: 0.45
Batch: 600; loss: 1.39; acc: 0.52
Batch: 620; loss: 1.44; acc: 0.45
Batch: 640; loss: 1.36; acc: 0.48
Batch: 660; loss: 1.44; acc: 0.45
Batch: 680; loss: 1.47; acc: 0.5
Batch: 700; loss: 1.35; acc: 0.56
Batch: 720; loss: 1.27; acc: 0.61
Batch: 740; loss: 1.53; acc: 0.47
Batch: 760; loss: 1.6; acc: 0.41
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 1.84; train_accuracy: 0.38 

Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 1.31; acc: 0.53
Batch: 40; loss: 1.11; acc: 0.62
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.06; acc: 0.66
Batch: 100; loss: 1.14; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 1.2; acc: 0.61
Val Epoch over. val_loss: 1.2418148984574968; val_accuracy: 0.5817078025477707 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 1.31; acc: 0.58
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.64
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 1.23; acc: 0.58
Batch: 160; loss: 1.35; acc: 0.53
Batch: 180; loss: 1.05; acc: 0.69
Batch: 200; loss: 0.95; acc: 0.72
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 1.08; acc: 0.62
Batch: 260; loss: 1.02; acc: 0.69
Batch: 280; loss: 1.31; acc: 0.55
Batch: 300; loss: 1.17; acc: 0.61
Batch: 320; loss: 0.84; acc: 0.78
Batch: 340; loss: 0.75; acc: 0.72
Batch: 360; loss: 1.05; acc: 0.59
Batch: 380; loss: 1.06; acc: 0.66
Batch: 400; loss: 0.82; acc: 0.77
Batch: 420; loss: 0.93; acc: 0.66
Batch: 440; loss: 0.92; acc: 0.66
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.89; acc: 0.66
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 1.17; acc: 0.58
Batch: 560; loss: 1.16; acc: 0.62
Batch: 580; loss: 0.8; acc: 0.73
Batch: 600; loss: 0.77; acc: 0.7
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.8; acc: 0.73
Batch: 660; loss: 0.93; acc: 0.69
Batch: 680; loss: 0.83; acc: 0.75
Batch: 700; loss: 0.61; acc: 0.73
Batch: 720; loss: 1.14; acc: 0.62
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.91; acc: 0.75
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.7; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.72
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.64
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.78
Val Epoch over. val_loss: 0.7397040700077251; val_accuracy: 0.7507961783439491 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.69
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.72
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.75
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.76; acc: 0.72
Batch: 200; loss: 0.66; acc: 0.75
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.73
Batch: 260; loss: 1.25; acc: 0.72
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.84; acc: 0.75
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.7
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.69
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.73
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.45; acc: 0.62
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.61; acc: 0.77
Train Epoch over. train_loss: 0.71; train_accuracy: 0.78 

Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.5684443002293824; val_accuracy: 0.8234474522292994 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.78
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.73
Batch: 280; loss: 1.04; acc: 0.58
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.82; acc: 0.73
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.93; acc: 0.67
Batch: 460; loss: 1.01; acc: 0.78
Batch: 480; loss: 0.77; acc: 0.69
Batch: 500; loss: 1.04; acc: 0.72
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.48; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.75; acc: 0.78
Batch: 700; loss: 0.96; acc: 0.73
Batch: 720; loss: 0.86; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.64
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 0.57; acc: 0.83
Val Epoch over. val_loss: 1.0058603045667054; val_accuracy: 0.6868033439490446 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.7
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.83; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.89; acc: 0.77
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.86; acc: 0.72
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.86; acc: 0.73
Batch: 440; loss: 1.14; acc: 0.72
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.93; acc: 0.67
Batch: 520; loss: 0.5; acc: 0.77
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.78
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.84; acc: 0.7
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.73; acc: 0.77
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6986011037021685; val_accuracy: 0.7860270700636943 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 0.56; acc: 0.77
Batch: 100; loss: 0.99; acc: 0.69
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.66; acc: 0.77
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.81
Batch: 340; loss: 0.95; acc: 0.77
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.78
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.78
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.75
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 1.69; acc: 0.58
Batch: 20; loss: 1.45; acc: 0.61
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.26; acc: 0.67
Batch: 80; loss: 1.53; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.7; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.7
Val Epoch over. val_loss: 1.3811279523904156; val_accuracy: 0.6079816878980892 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.79; acc: 0.8
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.74; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.8; acc: 0.72
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.95; acc: 0.7
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.77
Batch: 700; loss: 0.77; acc: 0.77
Batch: 720; loss: 0.51; acc: 0.81
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 1.02; acc: 0.67
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.5405856088088576; val_accuracy: 0.8307125796178344 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.78
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.77
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.57; acc: 0.77
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.81; acc: 0.67
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 1.06; acc: 0.7
Batch: 620; loss: 0.42; acc: 0.81
Batch: 640; loss: 0.86; acc: 0.7
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.66
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.51850867228713; val_accuracy: 0.8365843949044586 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.78
Batch: 380; loss: 0.99; acc: 0.69
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.8
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.69; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.75
Batch: 580; loss: 0.84; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.65; acc: 0.73
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.4555440234720327; val_accuracy: 0.862062101910828 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.8
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.82; acc: 0.73
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 1.01; acc: 0.73
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 0.93; acc: 0.75
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.96; acc: 0.75
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.5585449472734123; val_accuracy: 0.8217555732484076 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.46; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.75
Batch: 480; loss: 0.56; acc: 0.78
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.72
Batch: 540; loss: 0.59; acc: 0.78
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4309610792311134; val_accuracy: 0.8655453821656051 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.81
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.77
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.89; acc: 0.77
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.73
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.47910085510296424; val_accuracy: 0.8515127388535032 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.78
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.43199684247849096; val_accuracy: 0.8646496815286624 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.75
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.8; acc: 0.77
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.8
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.42064737666184737; val_accuracy: 0.872312898089172 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.8
Batch: 160; loss: 0.41; acc: 0.81
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.83
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.8
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.8
Batch: 600; loss: 0.35; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.78
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.99; acc: 0.67
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.48643607750629925; val_accuracy: 0.8455414012738853 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.8
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.73
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 1.06; acc: 0.73
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4568533917805951; val_accuracy: 0.856687898089172 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4611577564364026; val_accuracy: 0.8573845541401274 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.77
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.27; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.75
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3821550018753216; val_accuracy: 0.8782842356687898 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.89
Batch: 620; loss: 0.66; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3954883874601619; val_accuracy: 0.8742038216560509 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.8
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.4376789012057766; val_accuracy: 0.8626592356687898 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.79; acc: 0.7
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.78
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.83
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.39090646698976017; val_accuracy: 0.8772890127388535 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3806367089888852; val_accuracy: 0.8796775477707006 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.3856950307347972; val_accuracy: 0.8793789808917197 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.8
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.78
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.3770191120873591; val_accuracy: 0.882265127388535 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.83
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.8
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3835474299682174; val_accuracy: 0.8785828025477707 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.8
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.36; acc: 0.84
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.35; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.64; acc: 0.77
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.38729031939225594; val_accuracy: 0.8765923566878981 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.75
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.78
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.37539599560628273; val_accuracy: 0.8821656050955414 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.73; acc: 0.75
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.81
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3794429707489196; val_accuracy: 0.8773885350318471 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.81; acc: 0.78
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.81
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.75
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.8
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.37634178241536875; val_accuracy: 0.8818670382165605 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.78
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.36; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.8
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.81
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.78
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.37407006460959746; val_accuracy: 0.8818670382165605 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.83
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3748156666565853; val_accuracy: 0.8808718152866242 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.8
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.78
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3723439169442578; val_accuracy: 0.8820660828025477 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.75; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3765558062750063; val_accuracy: 0.8816679936305732 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.61; acc: 0.75
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.3; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.8
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.37294372441662343; val_accuracy: 0.8827627388535032 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.37223572103650704; val_accuracy: 0.8827627388535032 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.78
Batch: 360; loss: 0.57; acc: 0.78
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.78
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.37221459057300715; val_accuracy: 0.8825636942675159 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.8; acc: 0.81
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.8
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.77
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.37504254893702305; val_accuracy: 0.8820660828025477 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.37236428037760366; val_accuracy: 0.8824641719745223 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.77
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3755274106078087; val_accuracy: 0.8797770700636943 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.73; acc: 0.73
Batch: 240; loss: 0.65; acc: 0.75
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.28; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.37739726958001496; val_accuracy: 0.8808718152866242 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.78
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.36905061781026754; val_accuracy: 0.884156050955414 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.36900825167347673; val_accuracy: 0.8843550955414012 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.63; acc: 0.78
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3695805903737712; val_accuracy: 0.884156050955414 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.38; acc: 0.81
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3692786315349257; val_accuracy: 0.8846536624203821 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.12; acc: 1.0
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3696776420162742; val_accuracy: 0.8840565286624203 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.65; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.81
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.36920762493921694; val_accuracy: 0.8842555732484076 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.78
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.83; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.36886627540277067; val_accuracy: 0.8845541401273885 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.8
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.69; acc: 0.78
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.36798472009646666; val_accuracy: 0.8847531847133758 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.8
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.62; acc: 0.77
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.72
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.37041547130437413; val_accuracy: 0.8820660828025477 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.369250077825443; val_accuracy: 0.8844546178343949 

plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 62849
elements in E: 13185600
fraction nonzero: 0.004766487683533552
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.14
Batch: 80; loss: 2.24; acc: 0.19
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.19; acc: 0.27
Batch: 140; loss: 2.17; acc: 0.25
Batch: 160; loss: 2.17; acc: 0.23
Batch: 180; loss: 2.19; acc: 0.19
Batch: 200; loss: 2.15; acc: 0.27
Batch: 220; loss: 2.12; acc: 0.22
Batch: 240; loss: 2.01; acc: 0.3
Batch: 260; loss: 1.95; acc: 0.44
Batch: 280; loss: 1.91; acc: 0.41
Batch: 300; loss: 1.89; acc: 0.41
Batch: 320; loss: 1.69; acc: 0.42
Batch: 340; loss: 1.57; acc: 0.53
Batch: 360; loss: 1.69; acc: 0.5
Batch: 380; loss: 1.79; acc: 0.3
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.52; acc: 0.48
Batch: 440; loss: 1.59; acc: 0.39
Batch: 460; loss: 1.41; acc: 0.48
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.59
Batch: 520; loss: 0.91; acc: 0.7
Batch: 540; loss: 1.46; acc: 0.52
Batch: 560; loss: 1.27; acc: 0.62
Batch: 580; loss: 1.07; acc: 0.66
Batch: 600; loss: 1.1; acc: 0.61
Batch: 620; loss: 1.2; acc: 0.47
Batch: 640; loss: 0.83; acc: 0.7
Batch: 660; loss: 0.9; acc: 0.72
Batch: 680; loss: 0.99; acc: 0.61
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.88; acc: 0.66
Batch: 740; loss: 0.91; acc: 0.75
Batch: 760; loss: 0.99; acc: 0.7
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 1.57; train_accuracy: 0.48 

Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 1.01; acc: 0.7
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.7258336236522456; val_accuracy: 0.7751791401273885 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.73
Batch: 180; loss: 0.8; acc: 0.77
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.75; acc: 0.73
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.87; acc: 0.69
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 1.05; acc: 0.64
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.78
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.5679218567860355; val_accuracy: 0.8217555732484076 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.77
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 1.15; acc: 0.69
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.44234285043303373; val_accuracy: 0.8647492038216561 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.88; acc: 0.73
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.5079980329343468; val_accuracy: 0.8400676751592356 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.77
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.489857446615863; val_accuracy: 0.8545979299363057 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.81
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.6181982520279611; val_accuracy: 0.8127985668789809 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.28; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30754022236177875; val_accuracy: 0.9091361464968153 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.84
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.8
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.4744351159330386; val_accuracy: 0.8543988853503185 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.86; acc: 0.73
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.72
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.4060451646993874; val_accuracy: 0.8775875796178344 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.77
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3264738131243332; val_accuracy: 0.8973925159235668 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.29450705049523884; val_accuracy: 0.9071457006369427 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26638372622098133; val_accuracy: 0.9179936305732485 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.28179337651391695; val_accuracy: 0.9121218152866242 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2604636788415681; val_accuracy: 0.9194864649681529 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.260717726104958; val_accuracy: 0.9228702229299363 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.08; acc: 1.0
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.36507892902869327; val_accuracy: 0.8859474522292994 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.83
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3351697937413386; val_accuracy: 0.8964968152866242 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.86
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.25591711728436173; val_accuracy: 0.9252587579617835 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2565831501202978; val_accuracy: 0.9221735668789809 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26947328820824623; val_accuracy: 0.9179936305732485 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.24642604359301032; val_accuracy: 0.9270501592356688 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.2430100143548979; val_accuracy: 0.9276472929936306 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.86
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2531553010224916; val_accuracy: 0.9232683121019108 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.267215967012249; val_accuracy: 0.9173964968152867 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.25001215087665113; val_accuracy: 0.9253582802547771 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.84
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.2417499495990527; val_accuracy: 0.9271496815286624 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23919704755780044; val_accuracy: 0.9277468152866242 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.86
Batch: 660; loss: 0.24; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24312356702841012; val_accuracy: 0.9272492038216561 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.8
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.24579771285413937; val_accuracy: 0.9257563694267515 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23416276367464264; val_accuracy: 0.9303343949044586 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23633400919093828; val_accuracy: 0.9290406050955414 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.15; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2353335128515769; val_accuracy: 0.930234872611465 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.24; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23522815155755183; val_accuracy: 0.9296377388535032 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23749696478769658; val_accuracy: 0.9289410828025477 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.86
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23436006448071472; val_accuracy: 0.9299363057324841 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23368517075706818; val_accuracy: 0.9314291401273885 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.83
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23475806530161647; val_accuracy: 0.9292396496815286 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23447175452094168; val_accuracy: 0.9301353503184714 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.08; acc: 1.0
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23415187091394596; val_accuracy: 0.9293391719745223 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2321157304534487; val_accuracy: 0.9305334394904459 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.231888780144939; val_accuracy: 0.931031050955414 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.88
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2328759857993217; val_accuracy: 0.9308320063694268 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2327540330944737; val_accuracy: 0.9299363057324841 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23327075512071324; val_accuracy: 0.9306329617834395 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.37; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2325312569262875; val_accuracy: 0.9313296178343949 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2326794150077803; val_accuracy: 0.9306329617834395 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23190824878491034; val_accuracy: 0.9314291401273885 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23229814593674272; val_accuracy: 0.9308320063694268 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.231781354767217; val_accuracy: 0.9317277070063694 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23114088354454299; val_accuracy: 0.9313296178343949 

plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 83321
elements in E: 17580800
fraction nonzero: 0.004739317892246087
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.11
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.26; acc: 0.12
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.17; acc: 0.31
Batch: 120; loss: 2.11; acc: 0.42
Batch: 140; loss: 2.08; acc: 0.31
Batch: 160; loss: 2.05; acc: 0.31
Batch: 180; loss: 2.04; acc: 0.27
Batch: 200; loss: 1.92; acc: 0.34
Batch: 220; loss: 1.89; acc: 0.41
Batch: 240; loss: 1.78; acc: 0.33
Batch: 260; loss: 1.65; acc: 0.5
Batch: 280; loss: 1.7; acc: 0.45
Batch: 300; loss: 1.48; acc: 0.56
Batch: 320; loss: 1.26; acc: 0.58
Batch: 340; loss: 1.28; acc: 0.55
Batch: 360; loss: 1.4; acc: 0.58
Batch: 380; loss: 1.52; acc: 0.5
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 1.26; acc: 0.53
Batch: 440; loss: 1.24; acc: 0.56
Batch: 460; loss: 1.31; acc: 0.62
Batch: 480; loss: 1.18; acc: 0.67
Batch: 500; loss: 1.11; acc: 0.62
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.69
Batch: 560; loss: 1.04; acc: 0.72
Batch: 580; loss: 0.96; acc: 0.7
Batch: 600; loss: 0.78; acc: 0.69
Batch: 620; loss: 0.97; acc: 0.61
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 1.12; acc: 0.59
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.93; acc: 0.67
Batch: 760; loss: 0.89; acc: 0.69
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 1.39; train_accuracy: 0.54 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 0.92; acc: 0.69
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.67
Batch: 140; loss: 0.54; acc: 0.81
Val Epoch over. val_loss: 0.77093821441292; val_accuracy: 0.7383558917197452 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.73
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 1.06; acc: 0.69
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.72; acc: 0.72
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.78
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.75; acc: 0.78
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.72
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.61; acc: 0.73
Batch: 600; loss: 0.62; acc: 0.78
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.78
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.82; acc: 0.75
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.39361779410747966; val_accuracy: 0.8788813694267515 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.75
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.624602329958776; val_accuracy: 0.7953821656050956 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.78
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.32465292000846496; val_accuracy: 0.9029657643312102 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.8
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.7627515454960477; val_accuracy: 0.7758757961783439 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.78
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.84
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.17; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.632563162950953; val_accuracy: 0.8036425159235668 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.33267331393850835; val_accuracy: 0.8945063694267515 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.83
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.39025748720404446; val_accuracy: 0.8710191082802548 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.84
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24519850047910288; val_accuracy: 0.9232683121019108 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.276883316600019; val_accuracy: 0.9117237261146497 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2109469801282427; val_accuracy: 0.9365047770700637 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.11; acc: 1.0
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.20844122596607087; val_accuracy: 0.9378980891719745 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.18; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22206392675448375; val_accuracy: 0.9351114649681529 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.21626455526632868; val_accuracy: 0.9342157643312102 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.270462767618477; val_accuracy: 0.915406050955414 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2659568720657355; val_accuracy: 0.9204816878980892 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23009976279583705; val_accuracy: 0.931031050955414 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21282894733794935; val_accuracy: 0.9324243630573248 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20646237420618155; val_accuracy: 0.9378980891719745 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.45; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23576827940477688; val_accuracy: 0.9247611464968153 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20523212921277734; val_accuracy: 0.9389928343949044 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20403672076144796; val_accuracy: 0.9395899681528662 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.21159601344424447; val_accuracy: 0.9342157643312102 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19902696061855668; val_accuracy: 0.9394904458598726 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20225234809005335; val_accuracy: 0.9392914012738853 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.20601676077030268; val_accuracy: 0.9369028662420382 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2024701430349593; val_accuracy: 0.9411823248407644 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20270917422262727; val_accuracy: 0.9391918789808917 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20341878284694284; val_accuracy: 0.9397890127388535 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19762000574427804; val_accuracy: 0.9402866242038217 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.19968984247582733; val_accuracy: 0.939390923566879 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.19886794743264558; val_accuracy: 0.9398885350318471 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19812291205688648; val_accuracy: 0.941281847133758 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.19853735995140803; val_accuracy: 0.9395899681528662 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19833681798854452; val_accuracy: 0.9409832802547771 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.91
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19855447469433402; val_accuracy: 0.9397890127388535 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19692589973757982; val_accuracy: 0.9404856687898089 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.19980669828357212; val_accuracy: 0.9406847133757962 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1982483029080804; val_accuracy: 0.9399880573248408 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1984782349911465; val_accuracy: 0.9417794585987261 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1969279986183355; val_accuracy: 0.9410828025477707 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19663186319124926; val_accuracy: 0.9410828025477707 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19643717150019993; val_accuracy: 0.9415804140127388 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19786474445633068; val_accuracy: 0.940187101910828 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.89
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19700417928634936; val_accuracy: 0.9399880573248408 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19648188362076024; val_accuracy: 0.9414808917197452 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.91
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.88
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19647325703482718; val_accuracy: 0.9411823248407644 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1964376172537257; val_accuracy: 0.9413813694267515 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.86
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19683000008771373; val_accuracy: 0.9409832802547771 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19625996850478422; val_accuracy: 0.9409832802547771 

plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 104545
elements in E: 21976000
fraction nonzero: 0.0047572351656352384
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.11
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.19; acc: 0.17
Batch: 100; loss: 2.11; acc: 0.3
Batch: 120; loss: 2.05; acc: 0.41
Batch: 140; loss: 1.99; acc: 0.36
Batch: 160; loss: 2.0; acc: 0.36
Batch: 180; loss: 1.95; acc: 0.28
Batch: 200; loss: 1.78; acc: 0.36
Batch: 220; loss: 1.76; acc: 0.39
Batch: 240; loss: 1.53; acc: 0.44
Batch: 260; loss: 1.36; acc: 0.53
Batch: 280; loss: 1.41; acc: 0.45
Batch: 300; loss: 1.27; acc: 0.56
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 0.91; acc: 0.7
Batch: 360; loss: 0.9; acc: 0.77
Batch: 380; loss: 1.11; acc: 0.58
Batch: 400; loss: 0.97; acc: 0.75
Batch: 420; loss: 0.93; acc: 0.67
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 1.07; acc: 0.62
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.78
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.65; acc: 0.77
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 1.09; acc: 0.72
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.72
Batch: 760; loss: 0.99; acc: 0.66
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.88
Val Epoch over. val_loss: 0.5873733736147546; val_accuracy: 0.805234872611465 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.81
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.8
Batch: 420; loss: 0.57; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.92
Val Epoch over. val_loss: 0.4181166540855055; val_accuracy: 0.8655453821656051 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.8
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.8
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.24201220812596333; val_accuracy: 0.9268511146496815 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3007595554516194; val_accuracy: 0.9067476114649682 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23619188997111504; val_accuracy: 0.9294386942675159 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.34669993929327675; val_accuracy: 0.8938097133757962 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.23225718841051599; val_accuracy: 0.9315286624203821 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3293024251700207; val_accuracy: 0.8941082802547771 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.22035670446552288; val_accuracy: 0.9327229299363057 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2105368001588211; val_accuracy: 0.9388933121019108 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18584641372891747; val_accuracy: 0.9445660828025477 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.09; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18492182791470343; val_accuracy: 0.945859872611465 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.183197570976558; val_accuracy: 0.9452627388535032 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.1; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17814699365834522; val_accuracy: 0.9465565286624203 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.88
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.267890244959646; val_accuracy: 0.9173964968152867 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.28770681560798816; val_accuracy: 0.9091361464968153 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.89
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18371642062998123; val_accuracy: 0.9437699044585988 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.21; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18298488167250992; val_accuracy: 0.9425756369426752 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16853150480966658; val_accuracy: 0.949343152866242 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17280332814119045; val_accuracy: 0.9470541401273885 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1784823929333383; val_accuracy: 0.9467555732484076 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16395852472751762; val_accuracy: 0.9510350318471338 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.08; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16859116808624025; val_accuracy: 0.9484474522292994 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1592331853024899; val_accuracy: 0.9511345541401274 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15939589051209438; val_accuracy: 0.9516321656050956 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1581527806913397; val_accuracy: 0.9509355095541401 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16718960949664663; val_accuracy: 0.9471536624203821 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.89
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1622360604726205; val_accuracy: 0.9501393312101911 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16115328581754568; val_accuracy: 0.9507364649681529 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.157757128760883; val_accuracy: 0.9491441082802548 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1582292366749162; val_accuracy: 0.9527269108280255 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1569639616378933; val_accuracy: 0.9519307324840764 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15579960464387183; val_accuracy: 0.9514331210191083 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15633122100951566; val_accuracy: 0.9525278662420382 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1543928973234383; val_accuracy: 0.9506369426751592 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15560795364391272; val_accuracy: 0.9529259554140127 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1548610507824998; val_accuracy: 0.95203025477707 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1559947429663816; val_accuracy: 0.9522292993630573 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.86
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15728201726629476; val_accuracy: 0.950437898089172 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15670930378281386; val_accuracy: 0.9519307324840764 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.88
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.15375486628454962; val_accuracy: 0.9513335987261147 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1542300608744667; val_accuracy: 0.9523288216560509 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1536937932347416; val_accuracy: 0.9521297770700637 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15458547580204193; val_accuracy: 0.9517316878980892 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.15392716796060277; val_accuracy: 0.9518312101910829 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15346520579164954; val_accuracy: 0.9538216560509554 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15395545738802593; val_accuracy: 0.953125 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.14; acc: 0.92
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15405188548337123; val_accuracy: 0.9529259554140127 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.15400193308948712; val_accuracy: 0.9518312101910829 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1532358158450977; val_accuracy: 0.9516321656050956 

plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/reg_lenet/2020-01-19 18:09:26/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4385832/slurm_script: line 25: --print_freq=20: command not found
