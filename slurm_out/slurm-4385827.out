Namespace(batch_size=64, chunked=False, ddim_vs_acc=True, dense=False, device=device(type='cuda'), lr=1.0, model='MLP', n_epochs=50, non_wrapped=False, optimizer='SGD', parameter_correction=False, print_freq=20, print_prec=2, schedule=True, schedule_freq=10, schedule_gamma=0.4, seed=1, subspace_training=True, timestamp='2020-01-19 16:53:20')
nonzero elements in E: 44714
elements in E: 19921000
fraction nonzero: 0.0022445660358415744
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.19
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.16
Batch: 180; loss: 2.29; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.2
Batch: 220; loss: 2.27; acc: 0.16
Batch: 240; loss: 2.26; acc: 0.27
Batch: 260; loss: 2.27; acc: 0.25
Batch: 280; loss: 2.27; acc: 0.2
Batch: 300; loss: 2.26; acc: 0.25
Batch: 320; loss: 2.27; acc: 0.19
Batch: 340; loss: 2.25; acc: 0.3
Batch: 360; loss: 2.24; acc: 0.36
Batch: 380; loss: 2.21; acc: 0.44
Batch: 400; loss: 2.22; acc: 0.31
Batch: 420; loss: 2.23; acc: 0.33
Batch: 440; loss: 2.22; acc: 0.34
Batch: 460; loss: 2.24; acc: 0.3
Batch: 480; loss: 2.23; acc: 0.31
Batch: 500; loss: 2.2; acc: 0.39
Batch: 520; loss: 2.22; acc: 0.34
Batch: 540; loss: 2.22; acc: 0.3
Batch: 560; loss: 2.21; acc: 0.39
Batch: 580; loss: 2.2; acc: 0.34
Batch: 600; loss: 2.22; acc: 0.31
Batch: 620; loss: 2.19; acc: 0.38
Batch: 640; loss: 2.22; acc: 0.27
Batch: 660; loss: 2.18; acc: 0.34
Batch: 680; loss: 2.17; acc: 0.41
Batch: 700; loss: 2.15; acc: 0.41
Batch: 720; loss: 2.17; acc: 0.31
Batch: 740; loss: 2.14; acc: 0.38
Batch: 760; loss: 2.13; acc: 0.38
Batch: 780; loss: 2.17; acc: 0.38
Train Epoch over. train_loss: 2.23; train_accuracy: 0.28 

Batch: 0; loss: 2.15; acc: 0.39
Batch: 20; loss: 2.13; acc: 0.34
Batch: 40; loss: 2.07; acc: 0.48
Batch: 60; loss: 2.11; acc: 0.45
Batch: 80; loss: 2.07; acc: 0.45
Batch: 100; loss: 2.15; acc: 0.36
Batch: 120; loss: 2.13; acc: 0.38
Batch: 140; loss: 2.09; acc: 0.36
Val Epoch over. val_loss: 2.1142323001934464; val_accuracy: 0.3919187898089172 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.09; acc: 0.45
Batch: 20; loss: 2.12; acc: 0.36
Batch: 40; loss: 2.12; acc: 0.39
Batch: 60; loss: 2.11; acc: 0.31
Batch: 80; loss: 2.1; acc: 0.47
Batch: 100; loss: 2.05; acc: 0.52
Batch: 120; loss: 2.07; acc: 0.44
Batch: 140; loss: 2.03; acc: 0.42
Batch: 160; loss: 2.02; acc: 0.41
Batch: 180; loss: 2.01; acc: 0.48
Batch: 200; loss: 1.99; acc: 0.38
Batch: 220; loss: 2.05; acc: 0.39
Batch: 240; loss: 2.01; acc: 0.39
Batch: 260; loss: 2.04; acc: 0.36
Batch: 280; loss: 2.0; acc: 0.39
Batch: 300; loss: 1.94; acc: 0.38
Batch: 320; loss: 1.93; acc: 0.41
Batch: 340; loss: 1.91; acc: 0.53
Batch: 360; loss: 1.95; acc: 0.41
Batch: 380; loss: 1.88; acc: 0.5
Batch: 400; loss: 1.92; acc: 0.44
Batch: 420; loss: 1.84; acc: 0.44
Batch: 440; loss: 1.85; acc: 0.41
Batch: 460; loss: 1.85; acc: 0.42
Batch: 480; loss: 1.65; acc: 0.59
Batch: 500; loss: 1.69; acc: 0.52
Batch: 520; loss: 1.64; acc: 0.56
Batch: 540; loss: 1.71; acc: 0.47
Batch: 560; loss: 1.69; acc: 0.42
Batch: 580; loss: 1.66; acc: 0.56
Batch: 600; loss: 1.78; acc: 0.42
Batch: 620; loss: 1.78; acc: 0.41
Batch: 640; loss: 1.7; acc: 0.5
Batch: 660; loss: 1.73; acc: 0.42
Batch: 680; loss: 1.6; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.48
Batch: 720; loss: 1.62; acc: 0.55
Batch: 740; loss: 1.67; acc: 0.41
Batch: 760; loss: 1.5; acc: 0.48
Batch: 780; loss: 1.48; acc: 0.56
Train Epoch over. train_loss: 1.84; train_accuracy: 0.46 

Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.48; acc: 0.56
Batch: 40; loss: 1.2; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.53
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.49; acc: 0.52
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.33; acc: 0.59
Val Epoch over. val_loss: 1.4781586637922153; val_accuracy: 0.5360270700636943 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.4; acc: 0.62
Batch: 40; loss: 1.58; acc: 0.45
Batch: 60; loss: 1.47; acc: 0.52
Batch: 80; loss: 1.39; acc: 0.61
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 1.43; acc: 0.47
Batch: 160; loss: 1.42; acc: 0.5
Batch: 180; loss: 1.34; acc: 0.5
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.59
Batch: 240; loss: 1.45; acc: 0.53
Batch: 260; loss: 1.33; acc: 0.53
Batch: 280; loss: 1.46; acc: 0.5
Batch: 300; loss: 1.19; acc: 0.61
Batch: 320; loss: 1.31; acc: 0.61
Batch: 340; loss: 1.47; acc: 0.52
Batch: 360; loss: 1.18; acc: 0.69
Batch: 380; loss: 1.33; acc: 0.55
Batch: 400; loss: 1.18; acc: 0.55
Batch: 420; loss: 1.37; acc: 0.55
Batch: 440; loss: 1.36; acc: 0.52
Batch: 460; loss: 1.35; acc: 0.53
Batch: 480; loss: 1.41; acc: 0.55
Batch: 500; loss: 1.22; acc: 0.61
Batch: 520; loss: 1.28; acc: 0.56
Batch: 540; loss: 1.37; acc: 0.56
Batch: 560; loss: 1.41; acc: 0.45
Batch: 580; loss: 1.32; acc: 0.55
Batch: 600; loss: 1.2; acc: 0.58
Batch: 620; loss: 1.32; acc: 0.56
Batch: 640; loss: 1.29; acc: 0.58
Batch: 660; loss: 1.32; acc: 0.5
Batch: 680; loss: 1.28; acc: 0.56
Batch: 700; loss: 1.17; acc: 0.62
Batch: 720; loss: 1.32; acc: 0.56
Batch: 740; loss: 1.49; acc: 0.48
Batch: 760; loss: 1.43; acc: 0.5
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.47; acc: 0.44
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 0.92; acc: 0.7
Batch: 60; loss: 1.35; acc: 0.55
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 1.2; acc: 0.61
Batch: 120; loss: 1.52; acc: 0.47
Batch: 140; loss: 1.03; acc: 0.67
Val Epoch over. val_loss: 1.2485905255481695; val_accuracy: 0.5838972929936306 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.23; acc: 0.55
Batch: 60; loss: 1.35; acc: 0.56
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.41; acc: 0.53
Batch: 120; loss: 1.5; acc: 0.52
Batch: 140; loss: 1.31; acc: 0.52
Batch: 160; loss: 1.36; acc: 0.5
Batch: 180; loss: 1.13; acc: 0.61
Batch: 200; loss: 1.19; acc: 0.58
Batch: 220; loss: 1.3; acc: 0.61
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 1.42; acc: 0.56
Batch: 280; loss: 1.17; acc: 0.52
Batch: 300; loss: 1.46; acc: 0.47
Batch: 320; loss: 1.22; acc: 0.55
Batch: 340; loss: 1.39; acc: 0.48
Batch: 360; loss: 1.21; acc: 0.62
Batch: 380; loss: 1.25; acc: 0.61
Batch: 400; loss: 1.39; acc: 0.5
Batch: 420; loss: 1.13; acc: 0.61
Batch: 440; loss: 1.23; acc: 0.61
Batch: 460; loss: 1.39; acc: 0.58
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 1.21; acc: 0.61
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.44; acc: 0.56
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 1.33; acc: 0.56
Batch: 600; loss: 1.42; acc: 0.53
Batch: 620; loss: 1.1; acc: 0.62
Batch: 640; loss: 1.14; acc: 0.59
Batch: 660; loss: 1.3; acc: 0.59
Batch: 680; loss: 1.44; acc: 0.48
Batch: 700; loss: 1.25; acc: 0.59
Batch: 720; loss: 0.98; acc: 0.69
Batch: 740; loss: 1.14; acc: 0.67
Batch: 760; loss: 1.31; acc: 0.58
Batch: 780; loss: 1.3; acc: 0.53
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.4; acc: 0.56
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.91; acc: 0.69
Batch: 60; loss: 1.27; acc: 0.61
Batch: 80; loss: 1.0; acc: 0.69
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.46; acc: 0.52
Batch: 140; loss: 0.96; acc: 0.7
Val Epoch over. val_loss: 1.1938438476270932; val_accuracy: 0.6067874203821656 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.53
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 1.23; acc: 0.59
Batch: 60; loss: 1.28; acc: 0.5
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.64
Batch: 160; loss: 1.03; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.56
Batch: 200; loss: 1.32; acc: 0.52
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 1.34; acc: 0.55
Batch: 260; loss: 1.07; acc: 0.7
Batch: 280; loss: 1.27; acc: 0.61
Batch: 300; loss: 1.08; acc: 0.61
Batch: 320; loss: 1.42; acc: 0.56
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.38; acc: 0.56
Batch: 380; loss: 1.38; acc: 0.53
Batch: 400; loss: 1.19; acc: 0.62
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.02; acc: 0.64
Batch: 460; loss: 1.36; acc: 0.58
Batch: 480; loss: 1.49; acc: 0.52
Batch: 500; loss: 1.01; acc: 0.66
Batch: 520; loss: 1.25; acc: 0.53
Batch: 540; loss: 1.49; acc: 0.55
Batch: 560; loss: 0.96; acc: 0.67
Batch: 580; loss: 1.12; acc: 0.66
Batch: 600; loss: 1.16; acc: 0.7
Batch: 620; loss: 1.26; acc: 0.53
Batch: 640; loss: 1.34; acc: 0.53
Batch: 660; loss: 1.31; acc: 0.55
Batch: 680; loss: 1.25; acc: 0.56
Batch: 700; loss: 1.32; acc: 0.64
Batch: 720; loss: 1.26; acc: 0.56
Batch: 740; loss: 1.3; acc: 0.58
Batch: 760; loss: 1.51; acc: 0.52
Batch: 780; loss: 1.35; acc: 0.53
Train Epoch over. train_loss: 1.22; train_accuracy: 0.6 

Batch: 0; loss: 1.31; acc: 0.62
Batch: 20; loss: 1.07; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.64
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.69
Val Epoch over. val_loss: 1.155230043040719; val_accuracy: 0.6201234076433121 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.24; acc: 0.53
Batch: 120; loss: 1.14; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.69
Batch: 160; loss: 1.19; acc: 0.61
Batch: 180; loss: 1.29; acc: 0.53
Batch: 200; loss: 1.28; acc: 0.53
Batch: 220; loss: 1.14; acc: 0.61
Batch: 240; loss: 1.26; acc: 0.64
Batch: 260; loss: 1.08; acc: 0.62
Batch: 280; loss: 1.08; acc: 0.64
Batch: 300; loss: 1.01; acc: 0.61
Batch: 320; loss: 1.12; acc: 0.56
Batch: 340; loss: 0.98; acc: 0.7
Batch: 360; loss: 1.11; acc: 0.66
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.58
Batch: 420; loss: 1.14; acc: 0.59
Batch: 440; loss: 1.28; acc: 0.59
Batch: 460; loss: 1.38; acc: 0.48
Batch: 480; loss: 1.18; acc: 0.58
Batch: 500; loss: 1.04; acc: 0.67
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 1.29; acc: 0.55
Batch: 560; loss: 1.11; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.64
Batch: 600; loss: 1.09; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.58
Batch: 640; loss: 1.04; acc: 0.72
Batch: 660; loss: 1.11; acc: 0.59
Batch: 680; loss: 1.26; acc: 0.59
Batch: 700; loss: 1.27; acc: 0.58
Batch: 720; loss: 0.98; acc: 0.67
Batch: 740; loss: 0.99; acc: 0.64
Batch: 760; loss: 1.11; acc: 0.72
Batch: 780; loss: 1.18; acc: 0.61
Train Epoch over. train_loss: 1.19; train_accuracy: 0.61 

Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.07; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 1.39; acc: 0.56
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.12740353490137; val_accuracy: 0.6358479299363057 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.64
Batch: 120; loss: 1.27; acc: 0.58
Batch: 140; loss: 1.2; acc: 0.67
Batch: 160; loss: 1.37; acc: 0.5
Batch: 180; loss: 1.38; acc: 0.55
Batch: 200; loss: 1.24; acc: 0.61
Batch: 220; loss: 1.21; acc: 0.69
Batch: 240; loss: 1.27; acc: 0.53
Batch: 260; loss: 1.03; acc: 0.66
Batch: 280; loss: 1.26; acc: 0.59
Batch: 300; loss: 1.05; acc: 0.61
Batch: 320; loss: 1.22; acc: 0.62
Batch: 340; loss: 1.32; acc: 0.53
Batch: 360; loss: 1.34; acc: 0.56
Batch: 380; loss: 0.94; acc: 0.7
Batch: 400; loss: 1.39; acc: 0.58
Batch: 420; loss: 1.27; acc: 0.52
Batch: 440; loss: 1.15; acc: 0.61
Batch: 460; loss: 1.26; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.56
Batch: 500; loss: 1.06; acc: 0.61
Batch: 520; loss: 1.04; acc: 0.67
Batch: 540; loss: 1.21; acc: 0.64
Batch: 560; loss: 1.13; acc: 0.56
Batch: 580; loss: 1.09; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.67
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 1.32; acc: 0.61
Batch: 660; loss: 1.26; acc: 0.59
Batch: 680; loss: 1.25; acc: 0.64
Batch: 700; loss: 0.92; acc: 0.75
Batch: 720; loss: 1.17; acc: 0.61
Batch: 740; loss: 1.11; acc: 0.66
Batch: 760; loss: 1.19; acc: 0.61
Batch: 780; loss: 1.64; acc: 0.52
Train Epoch over. train_loss: 1.16; train_accuracy: 0.62 

Batch: 0; loss: 1.18; acc: 0.62
Batch: 20; loss: 1.06; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 0.97; acc: 0.69
Batch: 100; loss: 0.91; acc: 0.69
Batch: 120; loss: 1.39; acc: 0.56
Batch: 140; loss: 0.94; acc: 0.66
Val Epoch over. val_loss: 1.1081606965915414; val_accuracy: 0.638734076433121 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.64
Batch: 40; loss: 1.44; acc: 0.59
Batch: 60; loss: 1.3; acc: 0.58
Batch: 80; loss: 1.29; acc: 0.61
Batch: 100; loss: 0.94; acc: 0.69
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 1.18; acc: 0.61
Batch: 160; loss: 1.36; acc: 0.58
Batch: 180; loss: 1.03; acc: 0.58
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 1.22; acc: 0.58
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.33; acc: 0.59
Batch: 280; loss: 1.05; acc: 0.66
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.03; acc: 0.66
Batch: 340; loss: 1.15; acc: 0.58
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 1.23; acc: 0.62
Batch: 400; loss: 1.32; acc: 0.59
Batch: 420; loss: 1.02; acc: 0.67
Batch: 440; loss: 1.02; acc: 0.67
Batch: 460; loss: 0.95; acc: 0.73
Batch: 480; loss: 1.25; acc: 0.59
Batch: 500; loss: 1.17; acc: 0.61
Batch: 520; loss: 1.1; acc: 0.62
Batch: 540; loss: 1.11; acc: 0.62
Batch: 560; loss: 1.21; acc: 0.66
Batch: 580; loss: 1.23; acc: 0.64
Batch: 600; loss: 1.25; acc: 0.58
Batch: 620; loss: 1.27; acc: 0.55
Batch: 640; loss: 1.31; acc: 0.61
Batch: 660; loss: 1.18; acc: 0.56
Batch: 680; loss: 0.94; acc: 0.77
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 1.18; acc: 0.55
Batch: 740; loss: 0.98; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.5
Train Epoch over. train_loss: 1.15; train_accuracy: 0.63 

Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.03; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.7
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.91; acc: 0.69
Val Epoch over. val_loss: 1.0954377328514293; val_accuracy: 0.6488853503184714 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.59
Batch: 20; loss: 1.06; acc: 0.62
Batch: 40; loss: 1.13; acc: 0.62
Batch: 60; loss: 1.12; acc: 0.55
Batch: 80; loss: 1.23; acc: 0.58
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 0.99; acc: 0.67
Batch: 160; loss: 1.33; acc: 0.55
Batch: 180; loss: 0.95; acc: 0.64
Batch: 200; loss: 1.1; acc: 0.66
Batch: 220; loss: 1.21; acc: 0.58
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.96; acc: 0.7
Batch: 300; loss: 1.34; acc: 0.5
Batch: 320; loss: 1.02; acc: 0.69
Batch: 340; loss: 1.45; acc: 0.58
Batch: 360; loss: 1.35; acc: 0.56
Batch: 380; loss: 0.98; acc: 0.72
Batch: 400; loss: 0.93; acc: 0.66
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.58
Batch: 460; loss: 1.29; acc: 0.55
Batch: 480; loss: 1.14; acc: 0.59
Batch: 500; loss: 1.07; acc: 0.59
Batch: 520; loss: 1.26; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.64
Batch: 560; loss: 1.22; acc: 0.59
Batch: 580; loss: 1.08; acc: 0.66
Batch: 600; loss: 0.89; acc: 0.7
Batch: 620; loss: 1.02; acc: 0.64
Batch: 640; loss: 1.04; acc: 0.66
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.2; acc: 0.59
Batch: 720; loss: 1.33; acc: 0.53
Batch: 740; loss: 1.21; acc: 0.56
Batch: 760; loss: 1.2; acc: 0.69
Batch: 780; loss: 1.12; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.05; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.89; acc: 0.67
Val Epoch over. val_loss: 1.0878309387310294; val_accuracy: 0.6457006369426752 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.64
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.59
Batch: 80; loss: 1.22; acc: 0.58
Batch: 100; loss: 0.97; acc: 0.64
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.26; acc: 0.56
Batch: 160; loss: 1.05; acc: 0.67
Batch: 180; loss: 0.96; acc: 0.66
Batch: 200; loss: 0.9; acc: 0.67
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 0.95; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.58
Batch: 280; loss: 1.35; acc: 0.55
Batch: 300; loss: 1.4; acc: 0.55
Batch: 320; loss: 1.01; acc: 0.64
Batch: 340; loss: 1.02; acc: 0.7
Batch: 360; loss: 1.24; acc: 0.59
Batch: 380; loss: 1.11; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 1.08; acc: 0.66
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.34; acc: 0.61
Batch: 480; loss: 1.33; acc: 0.59
Batch: 500; loss: 1.38; acc: 0.55
Batch: 520; loss: 1.27; acc: 0.52
Batch: 540; loss: 1.1; acc: 0.61
Batch: 560; loss: 1.09; acc: 0.66
Batch: 580; loss: 1.11; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.58
Batch: 620; loss: 1.04; acc: 0.62
Batch: 640; loss: 1.2; acc: 0.56
Batch: 660; loss: 0.97; acc: 0.7
Batch: 680; loss: 0.83; acc: 0.73
Batch: 700; loss: 1.38; acc: 0.58
Batch: 720; loss: 1.19; acc: 0.56
Batch: 740; loss: 1.15; acc: 0.61
Batch: 760; loss: 1.17; acc: 0.59
Batch: 780; loss: 1.05; acc: 0.62
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.05; acc: 0.61
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.62
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.94; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.64
Val Epoch over. val_loss: 1.0817935630014748; val_accuracy: 0.6468949044585988 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 1.33; acc: 0.52
Batch: 40; loss: 1.03; acc: 0.72
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.19; acc: 0.69
Batch: 100; loss: 1.4; acc: 0.53
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 1.25; acc: 0.55
Batch: 160; loss: 1.32; acc: 0.52
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 1.3; acc: 0.55
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 0.88; acc: 0.7
Batch: 260; loss: 1.16; acc: 0.61
Batch: 280; loss: 1.12; acc: 0.58
Batch: 300; loss: 1.32; acc: 0.58
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.08; acc: 0.66
Batch: 360; loss: 1.21; acc: 0.56
Batch: 380; loss: 1.04; acc: 0.62
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 1.03; acc: 0.66
Batch: 440; loss: 0.91; acc: 0.73
Batch: 460; loss: 1.32; acc: 0.59
Batch: 480; loss: 0.93; acc: 0.67
Batch: 500; loss: 1.32; acc: 0.61
Batch: 520; loss: 1.11; acc: 0.59
Batch: 540; loss: 1.33; acc: 0.62
Batch: 560; loss: 1.31; acc: 0.52
Batch: 580; loss: 0.98; acc: 0.64
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 1.04; acc: 0.66
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.73
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 0.94; acc: 0.7
Batch: 760; loss: 1.01; acc: 0.67
Batch: 780; loss: 1.4; acc: 0.56
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.06; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.94; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 0.86; acc: 0.66
Val Epoch over. val_loss: 1.0789325351168395; val_accuracy: 0.6474920382165605 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.93; acc: 0.78
Batch: 60; loss: 0.98; acc: 0.66
Batch: 80; loss: 1.21; acc: 0.58
Batch: 100; loss: 1.07; acc: 0.67
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.69
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.01; acc: 0.72
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.22; acc: 0.58
Batch: 240; loss: 1.21; acc: 0.59
Batch: 260; loss: 1.2; acc: 0.53
Batch: 280; loss: 1.7; acc: 0.5
Batch: 300; loss: 1.2; acc: 0.55
Batch: 320; loss: 1.29; acc: 0.58
Batch: 340; loss: 1.05; acc: 0.64
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 1.42; acc: 0.47
Batch: 400; loss: 1.18; acc: 0.62
Batch: 420; loss: 0.91; acc: 0.66
Batch: 440; loss: 1.29; acc: 0.61
Batch: 460; loss: 0.86; acc: 0.67
Batch: 480; loss: 0.92; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.66
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 1.32; acc: 0.59
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 1.09; acc: 0.64
Batch: 600; loss: 0.96; acc: 0.66
Batch: 620; loss: 1.04; acc: 0.67
Batch: 640; loss: 1.13; acc: 0.62
Batch: 660; loss: 1.23; acc: 0.55
Batch: 680; loss: 1.02; acc: 0.66
Batch: 700; loss: 0.88; acc: 0.69
Batch: 720; loss: 0.78; acc: 0.7
Batch: 740; loss: 0.95; acc: 0.67
Batch: 760; loss: 1.0; acc: 0.72
Batch: 780; loss: 1.25; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.07; acc: 0.62
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 1.03; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.95; acc: 0.66
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 0.86; acc: 0.64
Val Epoch over. val_loss: 1.0772296121925304; val_accuracy: 0.6487858280254777 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.7
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 0.82; acc: 0.69
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.58
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 1.15; acc: 0.61
Batch: 160; loss: 1.15; acc: 0.66
Batch: 180; loss: 0.96; acc: 0.66
Batch: 200; loss: 1.21; acc: 0.62
Batch: 220; loss: 0.93; acc: 0.7
Batch: 240; loss: 1.07; acc: 0.73
Batch: 260; loss: 1.22; acc: 0.56
Batch: 280; loss: 1.02; acc: 0.75
Batch: 300; loss: 1.05; acc: 0.66
Batch: 320; loss: 0.86; acc: 0.77
Batch: 340; loss: 1.2; acc: 0.59
Batch: 360; loss: 1.09; acc: 0.66
Batch: 380; loss: 1.0; acc: 0.64
Batch: 400; loss: 1.15; acc: 0.67
Batch: 420; loss: 1.45; acc: 0.47
Batch: 440; loss: 1.11; acc: 0.66
Batch: 460; loss: 1.23; acc: 0.61
Batch: 480; loss: 1.15; acc: 0.61
Batch: 500; loss: 0.98; acc: 0.7
Batch: 520; loss: 0.96; acc: 0.67
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 1.01; acc: 0.67
Batch: 580; loss: 0.96; acc: 0.72
Batch: 600; loss: 1.11; acc: 0.61
Batch: 620; loss: 1.23; acc: 0.62
Batch: 640; loss: 1.37; acc: 0.58
Batch: 660; loss: 1.04; acc: 0.7
Batch: 680; loss: 1.43; acc: 0.53
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 0.9; acc: 0.69
Batch: 740; loss: 1.37; acc: 0.62
Batch: 760; loss: 1.34; acc: 0.59
Batch: 780; loss: 1.23; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.95; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.64
Val Epoch over. val_loss: 1.0759730612396434; val_accuracy: 0.6494824840764332 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.67
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 1.35; acc: 0.62
Batch: 100; loss: 1.23; acc: 0.56
Batch: 120; loss: 1.17; acc: 0.56
Batch: 140; loss: 1.19; acc: 0.67
Batch: 160; loss: 0.9; acc: 0.66
Batch: 180; loss: 1.18; acc: 0.55
Batch: 200; loss: 1.2; acc: 0.58
Batch: 220; loss: 0.96; acc: 0.62
Batch: 240; loss: 1.0; acc: 0.67
Batch: 260; loss: 1.31; acc: 0.58
Batch: 280; loss: 1.28; acc: 0.64
Batch: 300; loss: 1.01; acc: 0.66
Batch: 320; loss: 1.31; acc: 0.53
Batch: 340; loss: 0.97; acc: 0.62
Batch: 360; loss: 1.36; acc: 0.58
Batch: 380; loss: 1.0; acc: 0.7
Batch: 400; loss: 1.24; acc: 0.55
Batch: 420; loss: 1.45; acc: 0.52
Batch: 440; loss: 1.02; acc: 0.7
Batch: 460; loss: 1.28; acc: 0.61
Batch: 480; loss: 1.31; acc: 0.62
Batch: 500; loss: 1.16; acc: 0.55
Batch: 520; loss: 1.11; acc: 0.67
Batch: 540; loss: 0.87; acc: 0.72
Batch: 560; loss: 1.41; acc: 0.59
Batch: 580; loss: 1.08; acc: 0.64
Batch: 600; loss: 0.98; acc: 0.64
Batch: 620; loss: 0.91; acc: 0.66
Batch: 640; loss: 0.97; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.5
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.34; acc: 0.55
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 1.09; acc: 0.67
Batch: 780; loss: 1.2; acc: 0.64
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.07; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.66
Batch: 80; loss: 0.9; acc: 0.7
Batch: 100; loss: 0.95; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 0.86; acc: 0.64
Val Epoch over. val_loss: 1.072233271446957; val_accuracy: 0.648984872611465 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.98; acc: 0.67
Batch: 20; loss: 1.08; acc: 0.58
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 1.02; acc: 0.66
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 1.3; acc: 0.5
Batch: 140; loss: 0.96; acc: 0.62
Batch: 160; loss: 0.97; acc: 0.77
Batch: 180; loss: 1.31; acc: 0.55
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 1.18; acc: 0.58
Batch: 240; loss: 1.03; acc: 0.66
Batch: 260; loss: 1.27; acc: 0.58
Batch: 280; loss: 1.1; acc: 0.69
Batch: 300; loss: 1.07; acc: 0.66
Batch: 320; loss: 1.25; acc: 0.58
Batch: 340; loss: 1.4; acc: 0.55
Batch: 360; loss: 0.91; acc: 0.67
Batch: 380; loss: 0.9; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.66
Batch: 420; loss: 1.27; acc: 0.59
Batch: 440; loss: 0.9; acc: 0.73
Batch: 460; loss: 1.39; acc: 0.55
Batch: 480; loss: 1.18; acc: 0.58
Batch: 500; loss: 1.23; acc: 0.64
Batch: 520; loss: 1.16; acc: 0.64
Batch: 540; loss: 1.48; acc: 0.48
Batch: 560; loss: 0.86; acc: 0.75
Batch: 580; loss: 0.98; acc: 0.62
Batch: 600; loss: 1.24; acc: 0.59
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 1.02; acc: 0.67
Batch: 660; loss: 1.16; acc: 0.62
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.28; acc: 0.64
Batch: 720; loss: 0.96; acc: 0.7
Batch: 740; loss: 1.14; acc: 0.61
Batch: 760; loss: 0.99; acc: 0.69
Batch: 780; loss: 1.02; acc: 0.66
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 0.85; acc: 0.64
Val Epoch over. val_loss: 1.07069950802311; val_accuracy: 0.6499800955414012 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.08; acc: 0.59
Batch: 40; loss: 1.18; acc: 0.58
Batch: 60; loss: 0.94; acc: 0.72
Batch: 80; loss: 1.3; acc: 0.55
Batch: 100; loss: 1.32; acc: 0.59
Batch: 120; loss: 1.26; acc: 0.56
Batch: 140; loss: 1.17; acc: 0.61
Batch: 160; loss: 1.1; acc: 0.66
Batch: 180; loss: 1.25; acc: 0.59
Batch: 200; loss: 1.22; acc: 0.58
Batch: 220; loss: 1.0; acc: 0.64
Batch: 240; loss: 1.23; acc: 0.64
Batch: 260; loss: 1.04; acc: 0.75
Batch: 280; loss: 1.5; acc: 0.56
Batch: 300; loss: 1.06; acc: 0.61
Batch: 320; loss: 1.04; acc: 0.62
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 0.94; acc: 0.64
Batch: 380; loss: 0.97; acc: 0.69
Batch: 400; loss: 1.01; acc: 0.62
Batch: 420; loss: 1.11; acc: 0.59
Batch: 440; loss: 0.8; acc: 0.72
Batch: 460; loss: 0.87; acc: 0.69
Batch: 480; loss: 1.16; acc: 0.62
Batch: 500; loss: 0.86; acc: 0.67
Batch: 520; loss: 1.27; acc: 0.61
Batch: 540; loss: 1.04; acc: 0.62
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.46; acc: 0.53
Batch: 600; loss: 0.94; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.61
Batch: 700; loss: 1.36; acc: 0.59
Batch: 720; loss: 1.37; acc: 0.48
Batch: 740; loss: 0.93; acc: 0.7
Batch: 760; loss: 1.4; acc: 0.55
Batch: 780; loss: 1.18; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.1; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.62
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.85; acc: 0.67
Val Epoch over. val_loss: 1.0690449373737263; val_accuracy: 0.6498805732484076 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.58
Batch: 20; loss: 1.03; acc: 0.62
Batch: 40; loss: 0.88; acc: 0.75
Batch: 60; loss: 1.11; acc: 0.59
Batch: 80; loss: 0.87; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.69
Batch: 160; loss: 1.15; acc: 0.58
Batch: 180; loss: 0.89; acc: 0.73
Batch: 200; loss: 0.87; acc: 0.66
Batch: 220; loss: 1.09; acc: 0.61
Batch: 240; loss: 0.99; acc: 0.61
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 0.97; acc: 0.62
Batch: 300; loss: 1.36; acc: 0.62
Batch: 320; loss: 1.16; acc: 0.64
Batch: 340; loss: 1.1; acc: 0.64
Batch: 360; loss: 0.98; acc: 0.66
Batch: 380; loss: 0.99; acc: 0.72
Batch: 400; loss: 1.24; acc: 0.64
Batch: 420; loss: 1.23; acc: 0.53
Batch: 440; loss: 0.96; acc: 0.64
Batch: 460; loss: 1.16; acc: 0.62
Batch: 480; loss: 1.18; acc: 0.62
Batch: 500; loss: 0.87; acc: 0.72
Batch: 520; loss: 1.33; acc: 0.55
Batch: 540; loss: 0.91; acc: 0.67
Batch: 560; loss: 1.02; acc: 0.61
Batch: 580; loss: 1.02; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.62
Batch: 620; loss: 1.47; acc: 0.5
Batch: 640; loss: 0.95; acc: 0.67
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 1.39; acc: 0.61
Batch: 720; loss: 0.88; acc: 0.7
Batch: 740; loss: 1.02; acc: 0.67
Batch: 760; loss: 1.18; acc: 0.61
Batch: 780; loss: 1.27; acc: 0.55
Train Epoch over. train_loss: 1.11; train_accuracy: 0.63 

Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.1; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.01; acc: 0.64
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.62
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.66
Val Epoch over. val_loss: 1.0675500111215432; val_accuracy: 0.6501791401273885 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.67
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 1.18; acc: 0.62
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.1; acc: 0.56
Batch: 140; loss: 1.14; acc: 0.59
Batch: 160; loss: 0.96; acc: 0.64
Batch: 180; loss: 1.07; acc: 0.62
Batch: 200; loss: 1.13; acc: 0.59
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.24; acc: 0.56
Batch: 260; loss: 1.25; acc: 0.53
Batch: 280; loss: 1.15; acc: 0.59
Batch: 300; loss: 0.97; acc: 0.66
Batch: 320; loss: 0.92; acc: 0.69
Batch: 340; loss: 1.19; acc: 0.59
Batch: 360; loss: 1.13; acc: 0.61
Batch: 380; loss: 1.02; acc: 0.69
Batch: 400; loss: 1.16; acc: 0.58
Batch: 420; loss: 1.09; acc: 0.61
Batch: 440; loss: 1.16; acc: 0.59
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 1.11; acc: 0.69
Batch: 500; loss: 1.31; acc: 0.58
Batch: 520; loss: 1.28; acc: 0.53
Batch: 540; loss: 1.26; acc: 0.59
Batch: 560; loss: 0.97; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.64
Batch: 620; loss: 1.15; acc: 0.66
Batch: 640; loss: 1.12; acc: 0.62
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 1.19; acc: 0.62
Batch: 740; loss: 0.97; acc: 0.72
Batch: 760; loss: 1.28; acc: 0.56
Batch: 780; loss: 1.09; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.11; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.97; acc: 0.62
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.67
Val Epoch over. val_loss: 1.0669452942860354; val_accuracy: 0.6477906050955414 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.51; acc: 0.42
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 0.93; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 1.41; acc: 0.48
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 1.12; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.58
Batch: 180; loss: 0.99; acc: 0.62
Batch: 200; loss: 0.86; acc: 0.75
Batch: 220; loss: 1.13; acc: 0.59
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 0.99; acc: 0.67
Batch: 280; loss: 1.26; acc: 0.62
Batch: 300; loss: 1.27; acc: 0.66
Batch: 320; loss: 1.0; acc: 0.69
Batch: 340; loss: 1.53; acc: 0.48
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 1.16; acc: 0.59
Batch: 400; loss: 1.09; acc: 0.61
Batch: 420; loss: 1.07; acc: 0.53
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 1.17; acc: 0.59
Batch: 480; loss: 0.96; acc: 0.64
Batch: 500; loss: 1.27; acc: 0.59
Batch: 520; loss: 1.24; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.59
Batch: 560; loss: 1.11; acc: 0.64
Batch: 580; loss: 1.04; acc: 0.59
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 1.03; acc: 0.64
Batch: 660; loss: 1.23; acc: 0.59
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 1.35; acc: 0.56
Batch: 720; loss: 1.11; acc: 0.62
Batch: 740; loss: 1.13; acc: 0.58
Batch: 760; loss: 1.18; acc: 0.61
Batch: 780; loss: 1.2; acc: 0.58
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 1.01; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.97; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.69
Val Epoch over. val_loss: 1.065569292587839; val_accuracy: 0.648984872611465 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.61
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 1.23; acc: 0.56
Batch: 60; loss: 0.98; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.56
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 1.01; acc: 0.59
Batch: 180; loss: 1.12; acc: 0.56
Batch: 200; loss: 1.02; acc: 0.64
Batch: 220; loss: 1.03; acc: 0.66
Batch: 240; loss: 0.97; acc: 0.66
Batch: 260; loss: 0.92; acc: 0.67
Batch: 280; loss: 1.52; acc: 0.45
Batch: 300; loss: 1.19; acc: 0.55
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 1.24; acc: 0.5
Batch: 360; loss: 1.04; acc: 0.66
Batch: 380; loss: 0.8; acc: 0.72
Batch: 400; loss: 1.13; acc: 0.58
Batch: 420; loss: 1.28; acc: 0.64
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 0.94; acc: 0.66
Batch: 480; loss: 1.18; acc: 0.67
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.36; acc: 0.61
Batch: 540; loss: 1.15; acc: 0.61
Batch: 560; loss: 0.86; acc: 0.7
Batch: 580; loss: 0.85; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.62
Batch: 620; loss: 1.13; acc: 0.59
Batch: 640; loss: 1.4; acc: 0.58
Batch: 660; loss: 1.33; acc: 0.62
Batch: 680; loss: 1.17; acc: 0.62
Batch: 700; loss: 0.99; acc: 0.67
Batch: 720; loss: 1.05; acc: 0.62
Batch: 740; loss: 1.07; acc: 0.56
Batch: 760; loss: 1.19; acc: 0.59
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.83; acc: 0.67
Val Epoch over. val_loss: 1.0647391859133533; val_accuracy: 0.6496815286624203 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 0.86; acc: 0.67
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.26; acc: 0.64
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 1.11; acc: 0.61
Batch: 160; loss: 0.91; acc: 0.69
Batch: 180; loss: 1.08; acc: 0.69
Batch: 200; loss: 0.9; acc: 0.7
Batch: 220; loss: 1.27; acc: 0.56
Batch: 240; loss: 1.22; acc: 0.58
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 0.97; acc: 0.62
Batch: 320; loss: 0.98; acc: 0.59
Batch: 340; loss: 1.25; acc: 0.59
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 1.34; acc: 0.52
Batch: 400; loss: 1.07; acc: 0.66
Batch: 420; loss: 1.51; acc: 0.5
Batch: 440; loss: 0.97; acc: 0.7
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.86; acc: 0.72
Batch: 500; loss: 1.05; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.59
Batch: 540; loss: 1.04; acc: 0.64
Batch: 560; loss: 1.14; acc: 0.59
Batch: 580; loss: 1.1; acc: 0.69
Batch: 600; loss: 1.26; acc: 0.53
Batch: 620; loss: 1.04; acc: 0.64
Batch: 640; loss: 1.34; acc: 0.55
Batch: 660; loss: 1.08; acc: 0.62
Batch: 680; loss: 1.32; acc: 0.58
Batch: 700; loss: 0.84; acc: 0.73
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.29; acc: 0.58
Batch: 760; loss: 0.98; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.97; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.83; acc: 0.67
Val Epoch over. val_loss: 1.0644138888188988; val_accuracy: 0.6496815286624203 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.05; acc: 0.64
Batch: 40; loss: 1.41; acc: 0.55
Batch: 60; loss: 0.96; acc: 0.7
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.17; acc: 0.66
Batch: 180; loss: 0.94; acc: 0.69
Batch: 200; loss: 1.26; acc: 0.58
Batch: 220; loss: 1.08; acc: 0.61
Batch: 240; loss: 0.94; acc: 0.66
Batch: 260; loss: 1.03; acc: 0.61
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.11; acc: 0.64
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 1.18; acc: 0.66
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 1.04; acc: 0.64
Batch: 400; loss: 1.05; acc: 0.64
Batch: 420; loss: 1.1; acc: 0.7
Batch: 440; loss: 1.1; acc: 0.61
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.09; acc: 0.66
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 1.04; acc: 0.61
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 1.21; acc: 0.62
Batch: 600; loss: 0.94; acc: 0.67
Batch: 620; loss: 1.1; acc: 0.66
Batch: 640; loss: 1.35; acc: 0.61
Batch: 660; loss: 1.12; acc: 0.64
Batch: 680; loss: 0.98; acc: 0.72
Batch: 700; loss: 0.94; acc: 0.67
Batch: 720; loss: 1.32; acc: 0.55
Batch: 740; loss: 1.13; acc: 0.61
Batch: 760; loss: 1.01; acc: 0.66
Batch: 780; loss: 1.06; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0640724119107434; val_accuracy: 0.6493829617834395 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 1.24; acc: 0.53
Batch: 60; loss: 1.15; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 1.16; acc: 0.58
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.59
Batch: 160; loss: 0.94; acc: 0.67
Batch: 180; loss: 1.01; acc: 0.73
Batch: 200; loss: 1.05; acc: 0.64
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 1.24; acc: 0.59
Batch: 260; loss: 1.17; acc: 0.62
Batch: 280; loss: 1.1; acc: 0.62
Batch: 300; loss: 0.92; acc: 0.73
Batch: 320; loss: 1.22; acc: 0.59
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 0.91; acc: 0.64
Batch: 420; loss: 1.09; acc: 0.66
Batch: 440; loss: 1.0; acc: 0.66
Batch: 460; loss: 1.09; acc: 0.62
Batch: 480; loss: 1.11; acc: 0.59
Batch: 500; loss: 0.96; acc: 0.7
Batch: 520; loss: 1.07; acc: 0.62
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 0.95; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.61
Batch: 600; loss: 1.26; acc: 0.56
Batch: 620; loss: 1.27; acc: 0.58
Batch: 640; loss: 1.02; acc: 0.66
Batch: 660; loss: 0.92; acc: 0.7
Batch: 680; loss: 1.12; acc: 0.66
Batch: 700; loss: 1.19; acc: 0.61
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.06; acc: 0.64
Batch: 760; loss: 1.07; acc: 0.61
Batch: 780; loss: 1.3; acc: 0.56
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.83; acc: 0.69
Val Epoch over. val_loss: 1.0642024574765734; val_accuracy: 0.649781050955414 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.1; acc: 0.58
Batch: 20; loss: 0.97; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.59
Batch: 80; loss: 0.93; acc: 0.72
Batch: 100; loss: 1.31; acc: 0.66
Batch: 120; loss: 1.14; acc: 0.55
Batch: 140; loss: 0.8; acc: 0.77
Batch: 160; loss: 1.18; acc: 0.59
Batch: 180; loss: 1.02; acc: 0.64
Batch: 200; loss: 1.16; acc: 0.64
Batch: 220; loss: 1.0; acc: 0.66
Batch: 240; loss: 0.98; acc: 0.66
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 1.34; acc: 0.59
Batch: 300; loss: 1.13; acc: 0.62
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 0.99; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.61
Batch: 380; loss: 1.14; acc: 0.64
Batch: 400; loss: 1.13; acc: 0.61
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.33; acc: 0.53
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.02; acc: 0.67
Batch: 500; loss: 1.17; acc: 0.7
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.11; acc: 0.64
Batch: 560; loss: 1.11; acc: 0.64
Batch: 580; loss: 0.98; acc: 0.64
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 0.98; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.55
Batch: 660; loss: 0.88; acc: 0.69
Batch: 680; loss: 1.24; acc: 0.56
Batch: 700; loss: 0.87; acc: 0.7
Batch: 720; loss: 0.97; acc: 0.69
Batch: 740; loss: 0.88; acc: 0.75
Batch: 760; loss: 1.47; acc: 0.48
Batch: 780; loss: 1.02; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0636860572608413; val_accuracy: 0.6496815286624203 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 0.95; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.55
Batch: 80; loss: 1.1; acc: 0.59
Batch: 100; loss: 1.22; acc: 0.47
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.12; acc: 0.61
Batch: 180; loss: 1.11; acc: 0.58
Batch: 200; loss: 1.3; acc: 0.56
Batch: 220; loss: 0.99; acc: 0.69
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.3; acc: 0.56
Batch: 280; loss: 0.93; acc: 0.7
Batch: 300; loss: 1.02; acc: 0.59
Batch: 320; loss: 1.16; acc: 0.64
Batch: 340; loss: 1.39; acc: 0.58
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 1.28; acc: 0.55
Batch: 400; loss: 1.02; acc: 0.67
Batch: 420; loss: 1.06; acc: 0.7
Batch: 440; loss: 1.11; acc: 0.64
Batch: 460; loss: 1.11; acc: 0.61
Batch: 480; loss: 1.04; acc: 0.66
Batch: 500; loss: 1.18; acc: 0.62
Batch: 520; loss: 1.25; acc: 0.66
Batch: 540; loss: 1.06; acc: 0.64
Batch: 560; loss: 1.33; acc: 0.55
Batch: 580; loss: 1.09; acc: 0.62
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.06; acc: 0.59
Batch: 640; loss: 1.24; acc: 0.64
Batch: 660; loss: 1.19; acc: 0.59
Batch: 680; loss: 1.19; acc: 0.58
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 1.02; acc: 0.59
Batch: 760; loss: 0.87; acc: 0.77
Batch: 780; loss: 1.17; acc: 0.62
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0635997645414559; val_accuracy: 0.6486863057324841 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 1.08; acc: 0.59
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 1.16; acc: 0.64
Batch: 100; loss: 1.03; acc: 0.62
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.56
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 1.17; acc: 0.62
Batch: 200; loss: 1.21; acc: 0.58
Batch: 220; loss: 1.16; acc: 0.59
Batch: 240; loss: 1.17; acc: 0.66
Batch: 260; loss: 1.14; acc: 0.61
Batch: 280; loss: 0.97; acc: 0.64
Batch: 300; loss: 1.15; acc: 0.62
Batch: 320; loss: 1.0; acc: 0.67
Batch: 340; loss: 1.15; acc: 0.7
Batch: 360; loss: 1.32; acc: 0.52
Batch: 380; loss: 0.98; acc: 0.64
Batch: 400; loss: 1.01; acc: 0.69
Batch: 420; loss: 1.12; acc: 0.64
Batch: 440; loss: 1.3; acc: 0.55
Batch: 460; loss: 0.95; acc: 0.7
Batch: 480; loss: 1.03; acc: 0.62
Batch: 500; loss: 0.9; acc: 0.66
Batch: 520; loss: 1.32; acc: 0.53
Batch: 540; loss: 1.24; acc: 0.61
Batch: 560; loss: 1.28; acc: 0.64
Batch: 580; loss: 1.35; acc: 0.52
Batch: 600; loss: 0.81; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.58
Batch: 640; loss: 1.03; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.67
Batch: 680; loss: 0.92; acc: 0.66
Batch: 700; loss: 1.07; acc: 0.61
Batch: 720; loss: 1.07; acc: 0.66
Batch: 740; loss: 0.84; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.13; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0634132642654857; val_accuracy: 0.6510748407643312 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.4; acc: 0.55
Batch: 40; loss: 1.13; acc: 0.61
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 1.13; acc: 0.58
Batch: 100; loss: 1.09; acc: 0.64
Batch: 120; loss: 1.08; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.69
Batch: 160; loss: 0.84; acc: 0.75
Batch: 180; loss: 1.01; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 1.14; acc: 0.58
Batch: 240; loss: 0.88; acc: 0.75
Batch: 260; loss: 1.01; acc: 0.69
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.13; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.59
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.09; acc: 0.67
Batch: 400; loss: 1.19; acc: 0.61
Batch: 420; loss: 1.11; acc: 0.66
Batch: 440; loss: 1.35; acc: 0.5
Batch: 460; loss: 1.15; acc: 0.59
Batch: 480; loss: 0.99; acc: 0.7
Batch: 500; loss: 1.16; acc: 0.59
Batch: 520; loss: 1.03; acc: 0.66
Batch: 540; loss: 0.98; acc: 0.62
Batch: 560; loss: 1.07; acc: 0.64
Batch: 580; loss: 1.05; acc: 0.7
Batch: 600; loss: 1.19; acc: 0.56
Batch: 620; loss: 1.15; acc: 0.64
Batch: 640; loss: 1.08; acc: 0.61
Batch: 660; loss: 1.21; acc: 0.72
Batch: 680; loss: 0.88; acc: 0.7
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 1.41; acc: 0.47
Batch: 740; loss: 1.48; acc: 0.56
Batch: 760; loss: 1.18; acc: 0.56
Batch: 780; loss: 1.06; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.55
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0631572224531964; val_accuracy: 0.6498805732484076 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 1.05; acc: 0.62
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 1.47; acc: 0.5
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.61
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 1.06; acc: 0.62
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 1.21; acc: 0.62
Batch: 200; loss: 1.12; acc: 0.62
Batch: 220; loss: 1.19; acc: 0.58
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.28; acc: 0.56
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 0.87; acc: 0.69
Batch: 320; loss: 0.96; acc: 0.66
Batch: 340; loss: 1.45; acc: 0.52
Batch: 360; loss: 0.9; acc: 0.67
Batch: 380; loss: 0.93; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.58
Batch: 420; loss: 1.08; acc: 0.55
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 1.28; acc: 0.61
Batch: 500; loss: 1.06; acc: 0.64
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 1.12; acc: 0.66
Batch: 580; loss: 1.16; acc: 0.56
Batch: 600; loss: 0.99; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.59
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 1.21; acc: 0.59
Batch: 680; loss: 1.35; acc: 0.53
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 1.18; acc: 0.58
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.59
Batch: 780; loss: 0.86; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.063320875547494; val_accuracy: 0.6515724522292994 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 0.98; acc: 0.66
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.59
Batch: 100; loss: 1.2; acc: 0.61
Batch: 120; loss: 1.28; acc: 0.59
Batch: 140; loss: 1.35; acc: 0.52
Batch: 160; loss: 0.85; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.59
Batch: 200; loss: 1.13; acc: 0.61
Batch: 220; loss: 1.18; acc: 0.64
Batch: 240; loss: 1.26; acc: 0.56
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.11; acc: 0.62
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.46; acc: 0.53
Batch: 360; loss: 1.08; acc: 0.62
Batch: 380; loss: 1.01; acc: 0.66
Batch: 400; loss: 1.02; acc: 0.67
Batch: 420; loss: 1.36; acc: 0.55
Batch: 440; loss: 1.15; acc: 0.58
Batch: 460; loss: 0.97; acc: 0.67
Batch: 480; loss: 1.01; acc: 0.67
Batch: 500; loss: 1.35; acc: 0.56
Batch: 520; loss: 0.97; acc: 0.66
Batch: 540; loss: 1.01; acc: 0.61
Batch: 560; loss: 1.33; acc: 0.64
Batch: 580; loss: 1.22; acc: 0.62
Batch: 600; loss: 0.97; acc: 0.62
Batch: 620; loss: 1.06; acc: 0.61
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.28; acc: 0.55
Batch: 680; loss: 1.08; acc: 0.61
Batch: 700; loss: 1.15; acc: 0.56
Batch: 720; loss: 1.08; acc: 0.66
Batch: 740; loss: 1.25; acc: 0.66
Batch: 760; loss: 1.16; acc: 0.64
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0629198232274146; val_accuracy: 0.6510748407643312 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.0; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 1.03; acc: 0.59
Batch: 60; loss: 0.95; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 1.2; acc: 0.56
Batch: 140; loss: 1.33; acc: 0.64
Batch: 160; loss: 1.14; acc: 0.58
Batch: 180; loss: 0.94; acc: 0.72
Batch: 200; loss: 1.25; acc: 0.55
Batch: 220; loss: 1.04; acc: 0.61
Batch: 240; loss: 1.12; acc: 0.61
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 1.2; acc: 0.59
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.59
Batch: 340; loss: 0.83; acc: 0.7
Batch: 360; loss: 1.32; acc: 0.56
Batch: 380; loss: 1.09; acc: 0.66
Batch: 400; loss: 1.12; acc: 0.7
Batch: 420; loss: 1.02; acc: 0.66
Batch: 440; loss: 1.31; acc: 0.55
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 1.31; acc: 0.61
Batch: 500; loss: 1.24; acc: 0.59
Batch: 520; loss: 1.09; acc: 0.58
Batch: 540; loss: 1.1; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.66
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.58
Batch: 620; loss: 1.11; acc: 0.67
Batch: 640; loss: 1.28; acc: 0.64
Batch: 660; loss: 0.91; acc: 0.69
Batch: 680; loss: 1.39; acc: 0.52
Batch: 700; loss: 1.09; acc: 0.69
Batch: 720; loss: 1.32; acc: 0.53
Batch: 740; loss: 1.07; acc: 0.61
Batch: 760; loss: 0.97; acc: 0.67
Batch: 780; loss: 1.05; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.062973305677912; val_accuracy: 0.6509753184713376 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 1.23; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 1.47; acc: 0.59
Batch: 160; loss: 1.03; acc: 0.7
Batch: 180; loss: 1.01; acc: 0.67
Batch: 200; loss: 0.92; acc: 0.67
Batch: 220; loss: 1.16; acc: 0.61
Batch: 240; loss: 1.17; acc: 0.64
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 1.03; acc: 0.61
Batch: 300; loss: 1.01; acc: 0.61
Batch: 320; loss: 0.98; acc: 0.7
Batch: 340; loss: 1.0; acc: 0.7
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 0.97; acc: 0.72
Batch: 400; loss: 1.17; acc: 0.66
Batch: 420; loss: 1.37; acc: 0.55
Batch: 440; loss: 1.16; acc: 0.59
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 0.99; acc: 0.66
Batch: 540; loss: 1.14; acc: 0.61
Batch: 560; loss: 1.1; acc: 0.61
Batch: 580; loss: 1.19; acc: 0.55
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 0.94; acc: 0.69
Batch: 640; loss: 1.04; acc: 0.61
Batch: 660; loss: 1.15; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.67
Batch: 700; loss: 1.34; acc: 0.52
Batch: 720; loss: 1.12; acc: 0.59
Batch: 740; loss: 0.86; acc: 0.75
Batch: 760; loss: 1.04; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0627804110004644; val_accuracy: 0.6507762738853503 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.97; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.53
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.64
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 1.09; acc: 0.59
Batch: 160; loss: 0.98; acc: 0.72
Batch: 180; loss: 0.93; acc: 0.69
Batch: 200; loss: 1.22; acc: 0.58
Batch: 220; loss: 0.95; acc: 0.66
Batch: 240; loss: 1.27; acc: 0.58
Batch: 260; loss: 1.45; acc: 0.5
Batch: 280; loss: 0.87; acc: 0.75
Batch: 300; loss: 1.09; acc: 0.56
Batch: 320; loss: 1.06; acc: 0.73
Batch: 340; loss: 0.94; acc: 0.67
Batch: 360; loss: 1.13; acc: 0.7
Batch: 380; loss: 1.14; acc: 0.59
Batch: 400; loss: 1.09; acc: 0.7
Batch: 420; loss: 0.95; acc: 0.64
Batch: 440; loss: 1.27; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.56
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 0.98; acc: 0.73
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 0.92; acc: 0.67
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.19; acc: 0.56
Batch: 620; loss: 1.06; acc: 0.66
Batch: 640; loss: 1.22; acc: 0.61
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.17; acc: 0.62
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 0.96; acc: 0.64
Batch: 740; loss: 1.01; acc: 0.66
Batch: 760; loss: 1.19; acc: 0.62
Batch: 780; loss: 0.83; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.82; acc: 0.7
Val Epoch over. val_loss: 1.0627311601000986; val_accuracy: 0.6509753184713376 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.07; acc: 0.59
Batch: 40; loss: 1.02; acc: 0.67
Batch: 60; loss: 0.88; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.59
Batch: 120; loss: 0.92; acc: 0.67
Batch: 140; loss: 1.03; acc: 0.7
Batch: 160; loss: 1.12; acc: 0.64
Batch: 180; loss: 1.1; acc: 0.69
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.46; acc: 0.5
Batch: 240; loss: 1.05; acc: 0.62
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 1.21; acc: 0.55
Batch: 300; loss: 1.24; acc: 0.52
Batch: 320; loss: 1.24; acc: 0.53
Batch: 340; loss: 1.17; acc: 0.58
Batch: 360; loss: 1.17; acc: 0.64
Batch: 380; loss: 1.06; acc: 0.66
Batch: 400; loss: 0.98; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.58
Batch: 440; loss: 1.23; acc: 0.53
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 0.9; acc: 0.72
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 0.93; acc: 0.7
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 0.91; acc: 0.7
Batch: 580; loss: 1.37; acc: 0.53
Batch: 600; loss: 1.11; acc: 0.61
Batch: 620; loss: 1.07; acc: 0.69
Batch: 640; loss: 1.07; acc: 0.61
Batch: 660; loss: 1.24; acc: 0.56
Batch: 680; loss: 1.17; acc: 0.56
Batch: 700; loss: 1.15; acc: 0.64
Batch: 720; loss: 1.06; acc: 0.67
Batch: 740; loss: 1.07; acc: 0.59
Batch: 760; loss: 0.96; acc: 0.62
Batch: 780; loss: 1.35; acc: 0.56
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0627208040778044; val_accuracy: 0.6508757961783439 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.35; acc: 0.58
Batch: 20; loss: 1.24; acc: 0.56
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 1.02; acc: 0.64
Batch: 80; loss: 1.3; acc: 0.59
Batch: 100; loss: 0.85; acc: 0.75
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.67
Batch: 160; loss: 1.04; acc: 0.69
Batch: 180; loss: 1.17; acc: 0.62
Batch: 200; loss: 1.25; acc: 0.58
Batch: 220; loss: 1.05; acc: 0.59
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 1.27; acc: 0.66
Batch: 280; loss: 1.13; acc: 0.61
Batch: 300; loss: 1.08; acc: 0.66
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.21; acc: 0.62
Batch: 380; loss: 1.03; acc: 0.62
Batch: 400; loss: 1.22; acc: 0.58
Batch: 420; loss: 1.04; acc: 0.64
Batch: 440; loss: 1.03; acc: 0.64
Batch: 460; loss: 1.25; acc: 0.55
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.82; acc: 0.81
Batch: 520; loss: 1.29; acc: 0.64
Batch: 540; loss: 0.97; acc: 0.7
Batch: 560; loss: 1.32; acc: 0.61
Batch: 580; loss: 1.15; acc: 0.67
Batch: 600; loss: 0.97; acc: 0.69
Batch: 620; loss: 0.99; acc: 0.69
Batch: 640; loss: 1.1; acc: 0.64
Batch: 660; loss: 1.28; acc: 0.52
Batch: 680; loss: 1.15; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.55
Batch: 720; loss: 1.11; acc: 0.58
Batch: 740; loss: 1.08; acc: 0.59
Batch: 760; loss: 1.11; acc: 0.64
Batch: 780; loss: 1.11; acc: 0.64
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.55
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0626089003435366; val_accuracy: 0.6507762738853503 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 1.22; acc: 0.56
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.13; acc: 0.58
Batch: 100; loss: 1.26; acc: 0.66
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 1.25; acc: 0.61
Batch: 160; loss: 1.16; acc: 0.62
Batch: 180; loss: 0.98; acc: 0.66
Batch: 200; loss: 1.13; acc: 0.62
Batch: 220; loss: 1.02; acc: 0.7
Batch: 240; loss: 1.05; acc: 0.67
Batch: 260; loss: 0.99; acc: 0.75
Batch: 280; loss: 1.26; acc: 0.64
Batch: 300; loss: 1.08; acc: 0.67
Batch: 320; loss: 0.9; acc: 0.69
Batch: 340; loss: 1.33; acc: 0.55
Batch: 360; loss: 0.98; acc: 0.69
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 0.84; acc: 0.72
Batch: 420; loss: 1.43; acc: 0.55
Batch: 440; loss: 1.33; acc: 0.5
Batch: 460; loss: 1.28; acc: 0.61
Batch: 480; loss: 0.95; acc: 0.67
Batch: 500; loss: 0.94; acc: 0.72
Batch: 520; loss: 1.1; acc: 0.66
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 1.06; acc: 0.62
Batch: 580; loss: 1.29; acc: 0.55
Batch: 600; loss: 1.27; acc: 0.64
Batch: 620; loss: 1.12; acc: 0.55
Batch: 640; loss: 0.82; acc: 0.7
Batch: 660; loss: 1.05; acc: 0.64
Batch: 680; loss: 1.06; acc: 0.61
Batch: 700; loss: 1.14; acc: 0.66
Batch: 720; loss: 0.91; acc: 0.72
Batch: 740; loss: 1.15; acc: 0.59
Batch: 760; loss: 1.07; acc: 0.67
Batch: 780; loss: 1.12; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0626823082091703; val_accuracy: 0.6519705414012739 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 1.12; acc: 0.64
Batch: 60; loss: 1.0; acc: 0.73
Batch: 80; loss: 1.34; acc: 0.5
Batch: 100; loss: 1.28; acc: 0.59
Batch: 120; loss: 1.45; acc: 0.59
Batch: 140; loss: 1.13; acc: 0.62
Batch: 160; loss: 1.28; acc: 0.56
Batch: 180; loss: 0.99; acc: 0.67
Batch: 200; loss: 1.11; acc: 0.62
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 0.97; acc: 0.69
Batch: 280; loss: 1.19; acc: 0.62
Batch: 300; loss: 1.0; acc: 0.66
Batch: 320; loss: 1.02; acc: 0.59
Batch: 340; loss: 1.33; acc: 0.52
Batch: 360; loss: 1.03; acc: 0.7
Batch: 380; loss: 0.96; acc: 0.75
Batch: 400; loss: 0.91; acc: 0.7
Batch: 420; loss: 1.09; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.72
Batch: 460; loss: 0.85; acc: 0.69
Batch: 480; loss: 1.32; acc: 0.53
Batch: 500; loss: 0.98; acc: 0.67
Batch: 520; loss: 1.09; acc: 0.64
Batch: 540; loss: 0.94; acc: 0.7
Batch: 560; loss: 1.02; acc: 0.62
Batch: 580; loss: 1.32; acc: 0.61
Batch: 600; loss: 1.2; acc: 0.59
Batch: 620; loss: 0.98; acc: 0.67
Batch: 640; loss: 1.43; acc: 0.58
Batch: 660; loss: 1.18; acc: 0.62
Batch: 680; loss: 1.18; acc: 0.56
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 1.09; acc: 0.66
Batch: 740; loss: 1.22; acc: 0.59
Batch: 760; loss: 1.21; acc: 0.59
Batch: 780; loss: 1.22; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0625691839084503; val_accuracy: 0.6512738853503185 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 1.04; acc: 0.66
Batch: 60; loss: 1.11; acc: 0.59
Batch: 80; loss: 0.99; acc: 0.62
Batch: 100; loss: 0.96; acc: 0.66
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.66
Batch: 160; loss: 1.08; acc: 0.64
Batch: 180; loss: 1.13; acc: 0.61
Batch: 200; loss: 1.39; acc: 0.55
Batch: 220; loss: 1.12; acc: 0.59
Batch: 240; loss: 0.94; acc: 0.7
Batch: 260; loss: 1.52; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.67
Batch: 300; loss: 1.02; acc: 0.67
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 1.46; acc: 0.5
Batch: 360; loss: 1.36; acc: 0.52
Batch: 380; loss: 1.0; acc: 0.73
Batch: 400; loss: 0.79; acc: 0.72
Batch: 420; loss: 1.15; acc: 0.66
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 0.89; acc: 0.72
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.89; acc: 0.7
Batch: 520; loss: 1.13; acc: 0.56
Batch: 540; loss: 1.01; acc: 0.64
Batch: 560; loss: 1.1; acc: 0.61
Batch: 580; loss: 1.55; acc: 0.55
Batch: 600; loss: 1.17; acc: 0.56
Batch: 620; loss: 1.37; acc: 0.5
Batch: 640; loss: 1.09; acc: 0.62
Batch: 660; loss: 1.15; acc: 0.61
Batch: 680; loss: 1.02; acc: 0.64
Batch: 700; loss: 1.27; acc: 0.48
Batch: 720; loss: 0.94; acc: 0.69
Batch: 740; loss: 1.13; acc: 0.69
Batch: 760; loss: 0.99; acc: 0.67
Batch: 780; loss: 1.2; acc: 0.62
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.062561665750613; val_accuracy: 0.6511743630573248 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 1.16; acc: 0.59
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 1.39; acc: 0.5
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 1.31; acc: 0.61
Batch: 160; loss: 1.05; acc: 0.64
Batch: 180; loss: 1.02; acc: 0.67
Batch: 200; loss: 1.14; acc: 0.56
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.09; acc: 0.69
Batch: 260; loss: 0.99; acc: 0.67
Batch: 280; loss: 1.08; acc: 0.59
Batch: 300; loss: 1.14; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.66
Batch: 340; loss: 1.33; acc: 0.55
Batch: 360; loss: 1.07; acc: 0.64
Batch: 380; loss: 1.29; acc: 0.56
Batch: 400; loss: 1.2; acc: 0.56
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 1.04; acc: 0.66
Batch: 480; loss: 0.93; acc: 0.72
Batch: 500; loss: 1.42; acc: 0.56
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.62
Batch: 560; loss: 1.04; acc: 0.66
Batch: 580; loss: 1.18; acc: 0.67
Batch: 600; loss: 1.18; acc: 0.55
Batch: 620; loss: 1.28; acc: 0.59
Batch: 640; loss: 0.96; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.27; acc: 0.61
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 1.04; acc: 0.66
Batch: 740; loss: 1.38; acc: 0.5
Batch: 760; loss: 1.11; acc: 0.64
Batch: 780; loss: 0.78; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0624567205738868; val_accuracy: 0.6511743630573248 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.67
Batch: 20; loss: 0.98; acc: 0.69
Batch: 40; loss: 1.05; acc: 0.64
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.55
Batch: 100; loss: 0.95; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.47
Batch: 140; loss: 0.93; acc: 0.72
Batch: 160; loss: 1.28; acc: 0.66
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.32; acc: 0.59
Batch: 220; loss: 0.95; acc: 0.7
Batch: 240; loss: 1.14; acc: 0.66
Batch: 260; loss: 1.0; acc: 0.67
Batch: 280; loss: 1.08; acc: 0.67
Batch: 300; loss: 0.94; acc: 0.67
Batch: 320; loss: 1.09; acc: 0.61
Batch: 340; loss: 0.93; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.62
Batch: 380; loss: 1.24; acc: 0.62
Batch: 400; loss: 0.91; acc: 0.67
Batch: 420; loss: 1.06; acc: 0.61
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 1.5; acc: 0.53
Batch: 500; loss: 1.07; acc: 0.61
Batch: 520; loss: 1.11; acc: 0.56
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.34; acc: 0.62
Batch: 580; loss: 1.29; acc: 0.5
Batch: 600; loss: 1.09; acc: 0.66
Batch: 620; loss: 0.99; acc: 0.66
Batch: 640; loss: 1.25; acc: 0.58
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.13; acc: 0.58
Batch: 720; loss: 1.32; acc: 0.61
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.17; acc: 0.62
Batch: 780; loss: 0.94; acc: 0.67
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0624116628792635; val_accuracy: 0.6510748407643312 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 1.26; acc: 0.58
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 1.26; acc: 0.61
Batch: 100; loss: 0.95; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.58
Batch: 160; loss: 1.02; acc: 0.67
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.58
Batch: 220; loss: 1.18; acc: 0.61
Batch: 240; loss: 1.07; acc: 0.75
Batch: 260; loss: 0.95; acc: 0.66
Batch: 280; loss: 1.13; acc: 0.64
Batch: 300; loss: 1.01; acc: 0.62
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.31; acc: 0.58
Batch: 380; loss: 1.28; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.59
Batch: 420; loss: 1.17; acc: 0.58
Batch: 440; loss: 0.97; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.59
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 0.99; acc: 0.73
Batch: 540; loss: 0.89; acc: 0.67
Batch: 560; loss: 1.09; acc: 0.67
Batch: 580; loss: 1.22; acc: 0.59
Batch: 600; loss: 1.31; acc: 0.58
Batch: 620; loss: 1.03; acc: 0.59
Batch: 640; loss: 1.06; acc: 0.72
Batch: 660; loss: 1.35; acc: 0.53
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 1.14; acc: 0.61
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 1.14; acc: 0.64
Batch: 760; loss: 1.43; acc: 0.5
Batch: 780; loss: 0.89; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0624489723497135; val_accuracy: 0.6517714968152867 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.37; acc: 0.55
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.98; acc: 0.64
Batch: 160; loss: 1.16; acc: 0.59
Batch: 180; loss: 0.91; acc: 0.7
Batch: 200; loss: 0.76; acc: 0.77
Batch: 220; loss: 0.95; acc: 0.69
Batch: 240; loss: 1.05; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 1.21; acc: 0.61
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 1.23; acc: 0.55
Batch: 340; loss: 0.89; acc: 0.72
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.18; acc: 0.62
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 0.95; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.62
Batch: 460; loss: 0.96; acc: 0.7
Batch: 480; loss: 1.1; acc: 0.62
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 1.06; acc: 0.67
Batch: 540; loss: 1.18; acc: 0.61
Batch: 560; loss: 0.91; acc: 0.75
Batch: 580; loss: 0.92; acc: 0.69
Batch: 600; loss: 1.39; acc: 0.52
Batch: 620; loss: 1.1; acc: 0.67
Batch: 640; loss: 1.32; acc: 0.53
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.21; acc: 0.62
Batch: 700; loss: 1.23; acc: 0.59
Batch: 720; loss: 1.29; acc: 0.5
Batch: 740; loss: 1.38; acc: 0.53
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0624157695253944; val_accuracy: 0.6512738853503185 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.28; acc: 0.56
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.62
Batch: 60; loss: 0.92; acc: 0.66
Batch: 80; loss: 1.24; acc: 0.56
Batch: 100; loss: 0.92; acc: 0.69
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 1.29; acc: 0.55
Batch: 160; loss: 1.18; acc: 0.67
Batch: 180; loss: 0.81; acc: 0.77
Batch: 200; loss: 0.94; acc: 0.67
Batch: 220; loss: 1.09; acc: 0.64
Batch: 240; loss: 1.11; acc: 0.61
Batch: 260; loss: 1.14; acc: 0.61
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.1; acc: 0.62
Batch: 320; loss: 1.08; acc: 0.67
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 1.25; acc: 0.61
Batch: 380; loss: 1.25; acc: 0.59
Batch: 400; loss: 0.94; acc: 0.7
Batch: 420; loss: 1.02; acc: 0.69
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 0.95; acc: 0.7
Batch: 480; loss: 0.97; acc: 0.62
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.36; acc: 0.53
Batch: 540; loss: 1.23; acc: 0.55
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.53
Batch: 620; loss: 1.18; acc: 0.55
Batch: 640; loss: 1.35; acc: 0.56
Batch: 660; loss: 0.93; acc: 0.7
Batch: 680; loss: 1.19; acc: 0.62
Batch: 700; loss: 1.0; acc: 0.7
Batch: 720; loss: 1.45; acc: 0.48
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 1.24; acc: 0.62
Batch: 780; loss: 1.22; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0623720948863182; val_accuracy: 0.6509753184713376 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 1.44; acc: 0.59
Batch: 40; loss: 1.24; acc: 0.62
Batch: 60; loss: 1.1; acc: 0.66
Batch: 80; loss: 1.3; acc: 0.66
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.06; acc: 0.59
Batch: 140; loss: 1.32; acc: 0.61
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 1.18; acc: 0.58
Batch: 200; loss: 0.99; acc: 0.7
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 1.09; acc: 0.66
Batch: 260; loss: 1.48; acc: 0.48
Batch: 280; loss: 1.38; acc: 0.56
Batch: 300; loss: 1.34; acc: 0.52
Batch: 320; loss: 1.11; acc: 0.62
Batch: 340; loss: 1.17; acc: 0.62
Batch: 360; loss: 0.97; acc: 0.72
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 1.04; acc: 0.62
Batch: 420; loss: 1.25; acc: 0.61
Batch: 440; loss: 1.36; acc: 0.5
Batch: 460; loss: 1.13; acc: 0.64
Batch: 480; loss: 1.2; acc: 0.55
Batch: 500; loss: 1.03; acc: 0.69
Batch: 520; loss: 0.97; acc: 0.7
Batch: 540; loss: 1.22; acc: 0.58
Batch: 560; loss: 1.08; acc: 0.62
Batch: 580; loss: 1.09; acc: 0.67
Batch: 600; loss: 1.12; acc: 0.64
Batch: 620; loss: 1.11; acc: 0.56
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 1.25; acc: 0.53
Batch: 680; loss: 1.05; acc: 0.61
Batch: 700; loss: 1.2; acc: 0.56
Batch: 720; loss: 1.21; acc: 0.56
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.94; acc: 0.66
Batch: 780; loss: 0.85; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0623522640033891; val_accuracy: 0.650577229299363 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.31; acc: 0.64
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 1.04; acc: 0.67
Batch: 80; loss: 1.35; acc: 0.56
Batch: 100; loss: 1.2; acc: 0.62
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 1.07; acc: 0.64
Batch: 160; loss: 1.11; acc: 0.62
Batch: 180; loss: 0.9; acc: 0.67
Batch: 200; loss: 0.84; acc: 0.7
Batch: 220; loss: 1.06; acc: 0.59
Batch: 240; loss: 1.02; acc: 0.66
Batch: 260; loss: 1.06; acc: 0.64
Batch: 280; loss: 1.12; acc: 0.66
Batch: 300; loss: 1.33; acc: 0.58
Batch: 320; loss: 1.06; acc: 0.62
Batch: 340; loss: 1.0; acc: 0.73
Batch: 360; loss: 0.85; acc: 0.72
Batch: 380; loss: 0.98; acc: 0.72
Batch: 400; loss: 1.22; acc: 0.62
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.56
Batch: 460; loss: 1.15; acc: 0.67
Batch: 480; loss: 0.92; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.55
Batch: 520; loss: 0.95; acc: 0.67
Batch: 540; loss: 1.16; acc: 0.61
Batch: 560; loss: 0.96; acc: 0.72
Batch: 580; loss: 1.06; acc: 0.66
Batch: 600; loss: 1.18; acc: 0.66
Batch: 620; loss: 1.06; acc: 0.64
Batch: 640; loss: 0.98; acc: 0.73
Batch: 660; loss: 1.23; acc: 0.55
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 1.27; acc: 0.56
Batch: 720; loss: 1.06; acc: 0.61
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0623609746337697; val_accuracy: 0.6506767515923567 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.96; acc: 0.61
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.59
Batch: 100; loss: 1.28; acc: 0.52
Batch: 120; loss: 1.16; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.53
Batch: 160; loss: 0.96; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.61
Batch: 200; loss: 1.0; acc: 0.78
Batch: 220; loss: 0.95; acc: 0.62
Batch: 240; loss: 1.16; acc: 0.62
Batch: 260; loss: 1.02; acc: 0.69
Batch: 280; loss: 0.93; acc: 0.7
Batch: 300; loss: 0.97; acc: 0.73
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.14; acc: 0.59
Batch: 360; loss: 1.16; acc: 0.62
Batch: 380; loss: 0.85; acc: 0.66
Batch: 400; loss: 0.99; acc: 0.64
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.29; acc: 0.59
Batch: 460; loss: 1.06; acc: 0.73
Batch: 480; loss: 0.99; acc: 0.64
Batch: 500; loss: 1.18; acc: 0.67
Batch: 520; loss: 1.2; acc: 0.61
Batch: 540; loss: 1.02; acc: 0.66
Batch: 560; loss: 1.17; acc: 0.61
Batch: 580; loss: 1.35; acc: 0.55
Batch: 600; loss: 1.05; acc: 0.67
Batch: 620; loss: 1.09; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.52
Batch: 660; loss: 1.0; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.1; acc: 0.59
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 1.03; acc: 0.64
Batch: 760; loss: 0.98; acc: 0.56
Batch: 780; loss: 1.13; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.062315681178099; val_accuracy: 0.6509753184713376 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.61
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.13; acc: 0.59
Batch: 140; loss: 0.83; acc: 0.69
Batch: 160; loss: 1.13; acc: 0.62
Batch: 180; loss: 1.01; acc: 0.69
Batch: 200; loss: 0.83; acc: 0.77
Batch: 220; loss: 0.95; acc: 0.64
Batch: 240; loss: 1.27; acc: 0.58
Batch: 260; loss: 1.1; acc: 0.61
Batch: 280; loss: 1.0; acc: 0.69
Batch: 300; loss: 1.16; acc: 0.59
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.1; acc: 0.61
Batch: 360; loss: 0.96; acc: 0.7
Batch: 380; loss: 0.94; acc: 0.69
Batch: 400; loss: 1.07; acc: 0.64
Batch: 420; loss: 1.26; acc: 0.58
Batch: 440; loss: 1.19; acc: 0.64
Batch: 460; loss: 0.97; acc: 0.67
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 1.01; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.53
Batch: 540; loss: 0.98; acc: 0.59
Batch: 560; loss: 1.44; acc: 0.56
Batch: 580; loss: 0.98; acc: 0.66
Batch: 600; loss: 0.93; acc: 0.69
Batch: 620; loss: 0.98; acc: 0.64
Batch: 640; loss: 1.09; acc: 0.56
Batch: 660; loss: 1.18; acc: 0.62
Batch: 680; loss: 1.01; acc: 0.7
Batch: 700; loss: 1.37; acc: 0.58
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.55
Batch: 760; loss: 1.03; acc: 0.67
Batch: 780; loss: 1.04; acc: 0.67
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0623054694218241; val_accuracy: 0.6507762738853503 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.95; acc: 0.72
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.07; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.56
Batch: 120; loss: 1.5; acc: 0.52
Batch: 140; loss: 1.2; acc: 0.64
Batch: 160; loss: 1.01; acc: 0.67
Batch: 180; loss: 1.19; acc: 0.59
Batch: 200; loss: 1.26; acc: 0.58
Batch: 220; loss: 1.16; acc: 0.58
Batch: 240; loss: 0.92; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.61
Batch: 280; loss: 1.33; acc: 0.67
Batch: 300; loss: 1.31; acc: 0.56
Batch: 320; loss: 1.04; acc: 0.67
Batch: 340; loss: 0.73; acc: 0.77
Batch: 360; loss: 1.04; acc: 0.61
Batch: 380; loss: 1.15; acc: 0.62
Batch: 400; loss: 0.97; acc: 0.66
Batch: 420; loss: 1.35; acc: 0.56
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 0.82; acc: 0.67
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.09; acc: 0.64
Batch: 520; loss: 1.05; acc: 0.62
Batch: 540; loss: 0.88; acc: 0.69
Batch: 560; loss: 1.25; acc: 0.55
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.07; acc: 0.64
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 1.03; acc: 0.64
Batch: 660; loss: 1.09; acc: 0.62
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 0.96; acc: 0.66
Batch: 720; loss: 1.2; acc: 0.56
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.32; acc: 0.55
Batch: 780; loss: 1.11; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0623049204516564; val_accuracy: 0.6509753184713376 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 1.15; acc: 0.61
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 1.22; acc: 0.59
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.18; acc: 0.56
Batch: 140; loss: 1.22; acc: 0.55
Batch: 160; loss: 1.07; acc: 0.64
Batch: 180; loss: 1.25; acc: 0.53
Batch: 200; loss: 0.96; acc: 0.73
Batch: 220; loss: 1.03; acc: 0.72
Batch: 240; loss: 1.05; acc: 0.67
Batch: 260; loss: 1.01; acc: 0.69
Batch: 280; loss: 0.97; acc: 0.69
Batch: 300; loss: 0.92; acc: 0.64
Batch: 320; loss: 0.9; acc: 0.78
Batch: 340; loss: 0.97; acc: 0.66
Batch: 360; loss: 1.15; acc: 0.69
Batch: 380; loss: 1.3; acc: 0.62
Batch: 400; loss: 1.18; acc: 0.58
Batch: 420; loss: 1.03; acc: 0.69
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.34; acc: 0.55
Batch: 480; loss: 1.28; acc: 0.58
Batch: 500; loss: 0.93; acc: 0.75
Batch: 520; loss: 1.07; acc: 0.66
Batch: 540; loss: 1.18; acc: 0.61
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.2; acc: 0.62
Batch: 600; loss: 1.01; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.69
Batch: 640; loss: 1.07; acc: 0.69
Batch: 660; loss: 0.81; acc: 0.73
Batch: 680; loss: 0.93; acc: 0.66
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 1.27; acc: 0.53
Batch: 740; loss: 1.05; acc: 0.62
Batch: 760; loss: 1.23; acc: 0.52
Batch: 780; loss: 1.0; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0622641968119675; val_accuracy: 0.6504777070063694 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.69
Batch: 40; loss: 1.11; acc: 0.61
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.3; acc: 0.61
Batch: 160; loss: 1.06; acc: 0.67
Batch: 180; loss: 1.08; acc: 0.64
Batch: 200; loss: 1.15; acc: 0.67
Batch: 220; loss: 1.18; acc: 0.61
Batch: 240; loss: 1.4; acc: 0.58
Batch: 260; loss: 1.06; acc: 0.64
Batch: 280; loss: 1.37; acc: 0.62
Batch: 300; loss: 1.03; acc: 0.66
Batch: 320; loss: 1.12; acc: 0.64
Batch: 340; loss: 1.18; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.58
Batch: 380; loss: 1.24; acc: 0.62
Batch: 400; loss: 0.82; acc: 0.72
Batch: 420; loss: 1.26; acc: 0.61
Batch: 440; loss: 0.79; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 0.96; acc: 0.67
Batch: 520; loss: 1.01; acc: 0.61
Batch: 540; loss: 1.12; acc: 0.64
Batch: 560; loss: 1.04; acc: 0.69
Batch: 580; loss: 0.94; acc: 0.7
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.13; acc: 0.61
Batch: 640; loss: 0.97; acc: 0.62
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.01; acc: 0.66
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.67
Batch: 740; loss: 0.96; acc: 0.67
Batch: 760; loss: 1.25; acc: 0.61
Batch: 780; loss: 1.23; acc: 0.64
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0622525598592818; val_accuracy: 0.6504777070063694 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.18; acc: 0.61
Batch: 20; loss: 1.2; acc: 0.58
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 1.11; acc: 0.67
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 0.9; acc: 0.67
Batch: 140; loss: 1.27; acc: 0.56
Batch: 160; loss: 1.0; acc: 0.67
Batch: 180; loss: 1.0; acc: 0.61
Batch: 200; loss: 1.26; acc: 0.61
Batch: 220; loss: 1.21; acc: 0.58
Batch: 240; loss: 0.92; acc: 0.7
Batch: 260; loss: 1.11; acc: 0.64
Batch: 280; loss: 0.83; acc: 0.77
Batch: 300; loss: 1.15; acc: 0.61
Batch: 320; loss: 1.37; acc: 0.59
Batch: 340; loss: 0.99; acc: 0.72
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.24; acc: 0.59
Batch: 420; loss: 1.13; acc: 0.66
Batch: 440; loss: 1.12; acc: 0.64
Batch: 460; loss: 1.26; acc: 0.56
Batch: 480; loss: 1.09; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.53
Batch: 520; loss: 1.04; acc: 0.66
Batch: 540; loss: 1.07; acc: 0.67
Batch: 560; loss: 1.24; acc: 0.59
Batch: 580; loss: 1.12; acc: 0.64
Batch: 600; loss: 1.0; acc: 0.64
Batch: 620; loss: 1.13; acc: 0.67
Batch: 640; loss: 0.96; acc: 0.66
Batch: 660; loss: 1.08; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.69
Batch: 700; loss: 1.15; acc: 0.58
Batch: 720; loss: 1.11; acc: 0.61
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.25; acc: 0.58
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.81; acc: 0.7
Val Epoch over. val_loss: 1.0622340395192431; val_accuracy: 0.6503781847133758 

plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 88730
elements in E: 39842000
fraction nonzero: 0.002227046834998243
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.28; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.17
Batch: 180; loss: 2.27; acc: 0.23
Batch: 200; loss: 2.27; acc: 0.25
Batch: 220; loss: 2.24; acc: 0.31
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.31
Batch: 280; loss: 2.24; acc: 0.31
Batch: 300; loss: 2.23; acc: 0.28
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.21; acc: 0.39
Batch: 360; loss: 2.21; acc: 0.34
Batch: 380; loss: 2.18; acc: 0.42
Batch: 400; loss: 2.2; acc: 0.3
Batch: 420; loss: 2.15; acc: 0.5
Batch: 440; loss: 2.18; acc: 0.42
Batch: 460; loss: 2.19; acc: 0.36
Batch: 480; loss: 2.17; acc: 0.41
Batch: 500; loss: 2.14; acc: 0.45
Batch: 520; loss: 2.17; acc: 0.45
Batch: 540; loss: 2.18; acc: 0.33
Batch: 560; loss: 2.11; acc: 0.44
Batch: 580; loss: 2.11; acc: 0.44
Batch: 600; loss: 2.08; acc: 0.44
Batch: 620; loss: 2.08; acc: 0.45
Batch: 640; loss: 2.09; acc: 0.47
Batch: 660; loss: 2.04; acc: 0.53
Batch: 680; loss: 2.0; acc: 0.53
Batch: 700; loss: 2.05; acc: 0.5
Batch: 720; loss: 2.01; acc: 0.52
Batch: 740; loss: 2.0; acc: 0.48
Batch: 760; loss: 1.92; acc: 0.52
Batch: 780; loss: 1.96; acc: 0.48
Train Epoch over. train_loss: 2.17; train_accuracy: 0.36 

Batch: 0; loss: 1.99; acc: 0.44
Batch: 20; loss: 1.93; acc: 0.41
Batch: 40; loss: 1.77; acc: 0.69
Batch: 60; loss: 1.85; acc: 0.55
Batch: 80; loss: 1.83; acc: 0.66
Batch: 100; loss: 1.97; acc: 0.52
Batch: 120; loss: 2.02; acc: 0.33
Batch: 140; loss: 1.81; acc: 0.61
Val Epoch over. val_loss: 1.9191436638498003; val_accuracy: 0.5031847133757962 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.94; acc: 0.47
Batch: 20; loss: 1.9; acc: 0.59
Batch: 40; loss: 1.89; acc: 0.56
Batch: 60; loss: 1.92; acc: 0.42
Batch: 80; loss: 1.77; acc: 0.55
Batch: 100; loss: 1.69; acc: 0.55
Batch: 120; loss: 1.73; acc: 0.53
Batch: 140; loss: 1.8; acc: 0.52
Batch: 160; loss: 1.67; acc: 0.53
Batch: 180; loss: 1.77; acc: 0.47
Batch: 200; loss: 1.66; acc: 0.53
Batch: 220; loss: 1.62; acc: 0.47
Batch: 240; loss: 1.51; acc: 0.59
Batch: 260; loss: 1.63; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.47
Batch: 300; loss: 1.48; acc: 0.53
Batch: 320; loss: 1.54; acc: 0.45
Batch: 340; loss: 1.44; acc: 0.55
Batch: 360; loss: 1.48; acc: 0.52
Batch: 380; loss: 1.38; acc: 0.62
Batch: 400; loss: 1.49; acc: 0.55
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.25; acc: 0.67
Batch: 460; loss: 1.4; acc: 0.48
Batch: 480; loss: 1.26; acc: 0.55
Batch: 500; loss: 1.36; acc: 0.55
Batch: 520; loss: 1.27; acc: 0.56
Batch: 540; loss: 1.18; acc: 0.56
Batch: 560; loss: 1.26; acc: 0.58
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 1.38; acc: 0.56
Batch: 620; loss: 1.34; acc: 0.56
Batch: 640; loss: 1.16; acc: 0.59
Batch: 660; loss: 1.33; acc: 0.59
Batch: 680; loss: 1.26; acc: 0.59
Batch: 700; loss: 1.2; acc: 0.61
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 1.22; acc: 0.56
Batch: 760; loss: 0.93; acc: 0.62
Batch: 780; loss: 1.18; acc: 0.61
Train Epoch over. train_loss: 1.45; train_accuracy: 0.57 

Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.68; acc: 0.88
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.33; acc: 0.52
Batch: 140; loss: 0.8; acc: 0.81
Val Epoch over. val_loss: 1.0707785369484288; val_accuracy: 0.6605294585987261 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 1.28; acc: 0.62
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.16; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 1.35; acc: 0.59
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 0.96; acc: 0.69
Batch: 220; loss: 1.01; acc: 0.69
Batch: 240; loss: 1.07; acc: 0.59
Batch: 260; loss: 0.97; acc: 0.69
Batch: 280; loss: 1.24; acc: 0.61
Batch: 300; loss: 0.82; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.66
Batch: 340; loss: 1.19; acc: 0.61
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.92; acc: 0.7
Batch: 400; loss: 0.91; acc: 0.69
Batch: 420; loss: 1.04; acc: 0.67
Batch: 440; loss: 0.91; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.66
Batch: 480; loss: 1.21; acc: 0.62
Batch: 500; loss: 0.9; acc: 0.72
Batch: 520; loss: 1.3; acc: 0.56
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 0.97; acc: 0.66
Batch: 580; loss: 1.01; acc: 0.61
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 1.1; acc: 0.66
Batch: 640; loss: 0.94; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.59
Batch: 700; loss: 0.85; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.61
Batch: 740; loss: 1.25; acc: 0.62
Batch: 760; loss: 1.39; acc: 0.55
Batch: 780; loss: 0.84; acc: 0.69
Train Epoch over. train_loss: 1.04; train_accuracy: 0.66 

Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 0.67; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.34; acc: 0.55
Batch: 140; loss: 0.65; acc: 0.81
Val Epoch over. val_loss: 0.9045785891402299; val_accuracy: 0.7118829617834395 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.64
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.89; acc: 0.7
Batch: 60; loss: 1.08; acc: 0.61
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.07; acc: 0.69
Batch: 160; loss: 1.23; acc: 0.61
Batch: 180; loss: 0.78; acc: 0.73
Batch: 200; loss: 0.77; acc: 0.73
Batch: 220; loss: 0.99; acc: 0.66
Batch: 240; loss: 1.0; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.58
Batch: 280; loss: 0.83; acc: 0.7
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.1; acc: 0.7
Batch: 340; loss: 1.2; acc: 0.67
Batch: 360; loss: 0.72; acc: 0.77
Batch: 380; loss: 1.02; acc: 0.66
Batch: 400; loss: 0.92; acc: 0.72
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 0.95; acc: 0.66
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 1.08; acc: 0.67
Batch: 540; loss: 0.75; acc: 0.75
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 1.01; acc: 0.61
Batch: 620; loss: 0.91; acc: 0.62
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.0; acc: 0.67
Batch: 700; loss: 0.78; acc: 0.73
Batch: 720; loss: 0.75; acc: 0.77
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.87; acc: 0.67
Batch: 780; loss: 1.04; acc: 0.67
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.94; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.83
Batch: 120; loss: 1.24; acc: 0.55
Batch: 140; loss: 0.54; acc: 0.81
Val Epoch over. val_loss: 0.8360022173565664; val_accuracy: 0.7331807324840764 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.58
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.8; acc: 0.72
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 0.97; acc: 0.66
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.93; acc: 0.7
Batch: 160; loss: 0.84; acc: 0.72
Batch: 180; loss: 0.86; acc: 0.75
Batch: 200; loss: 0.87; acc: 0.72
Batch: 220; loss: 1.05; acc: 0.73
Batch: 240; loss: 0.65; acc: 0.77
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 0.76; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.73
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 0.78; acc: 0.75
Batch: 360; loss: 0.68; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.91; acc: 0.66
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 1.01; acc: 0.64
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.78; acc: 0.72
Batch: 520; loss: 0.91; acc: 0.69
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.7
Batch: 620; loss: 0.73; acc: 0.75
Batch: 640; loss: 0.88; acc: 0.7
Batch: 660; loss: 0.78; acc: 0.73
Batch: 680; loss: 0.75; acc: 0.72
Batch: 700; loss: 0.92; acc: 0.69
Batch: 720; loss: 0.94; acc: 0.67
Batch: 740; loss: 0.86; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.64
Batch: 780; loss: 0.94; acc: 0.67
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.87; acc: 0.73
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 0.47; acc: 0.86
Val Epoch over. val_loss: 0.7884142924645904; val_accuracy: 0.751890923566879 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.98; acc: 0.7
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.66
Batch: 260; loss: 0.78; acc: 0.73
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.72
Batch: 380; loss: 0.64; acc: 0.73
Batch: 400; loss: 0.87; acc: 0.73
Batch: 420; loss: 0.8; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 1.05; acc: 0.67
Batch: 480; loss: 0.77; acc: 0.73
Batch: 500; loss: 0.95; acc: 0.73
Batch: 520; loss: 0.7; acc: 0.77
Batch: 540; loss: 0.87; acc: 0.81
Batch: 560; loss: 0.84; acc: 0.78
Batch: 580; loss: 0.81; acc: 0.67
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 0.86; acc: 0.7
Batch: 640; loss: 0.75; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.73
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.72
Batch: 760; loss: 0.86; acc: 0.66
Batch: 780; loss: 0.99; acc: 0.75
Train Epoch over. train_loss: 0.83; train_accuracy: 0.73 

Batch: 0; loss: 0.91; acc: 0.69
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.43; acc: 0.88
Val Epoch over. val_loss: 0.7648079247231696; val_accuracy: 0.7578622611464968 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.92; acc: 0.69
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.76; acc: 0.7
Batch: 100; loss: 0.7; acc: 0.73
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.72
Batch: 160; loss: 0.75; acc: 0.75
Batch: 180; loss: 0.89; acc: 0.69
Batch: 200; loss: 0.91; acc: 0.7
Batch: 220; loss: 0.79; acc: 0.7
Batch: 240; loss: 0.83; acc: 0.75
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 1.06; acc: 0.73
Batch: 300; loss: 0.92; acc: 0.66
Batch: 320; loss: 0.88; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 0.81; acc: 0.67
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 1.17; acc: 0.64
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 0.81; acc: 0.72
Batch: 480; loss: 0.78; acc: 0.66
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.75
Batch: 540; loss: 0.77; acc: 0.73
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.73; acc: 0.73
Batch: 600; loss: 0.64; acc: 0.75
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.84; acc: 0.69
Batch: 660; loss: 1.06; acc: 0.69
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.66
Batch: 740; loss: 1.1; acc: 0.64
Batch: 760; loss: 0.84; acc: 0.64
Batch: 780; loss: 0.89; acc: 0.67
Train Epoch over. train_loss: 0.81; train_accuracy: 0.74 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.88
Val Epoch over. val_loss: 0.7540851263863266; val_accuracy: 0.7628383757961783 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 0.77; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.8; acc: 0.78
Batch: 160; loss: 0.82; acc: 0.75
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.87; acc: 0.66
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 0.95; acc: 0.67
Batch: 260; loss: 0.9; acc: 0.69
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.7; acc: 0.78
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.77; acc: 0.75
Batch: 360; loss: 0.81; acc: 0.72
Batch: 380; loss: 0.82; acc: 0.66
Batch: 400; loss: 0.77; acc: 0.72
Batch: 420; loss: 0.92; acc: 0.7
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.97; acc: 0.69
Batch: 500; loss: 0.79; acc: 0.69
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 1.0; acc: 0.73
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.89; acc: 0.72
Batch: 640; loss: 0.98; acc: 0.69
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.88; acc: 0.73
Batch: 720; loss: 0.79; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.75
Batch: 780; loss: 0.87; acc: 0.66
Train Epoch over. train_loss: 0.79; train_accuracy: 0.74 

Batch: 0; loss: 0.84; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.37; acc: 0.88
Val Epoch over. val_loss: 0.7380585336381462; val_accuracy: 0.7655254777070064 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.66
Batch: 140; loss: 0.76; acc: 0.73
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.68; acc: 0.78
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.75
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 0.93; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.97; acc: 0.67
Batch: 500; loss: 0.66; acc: 0.72
Batch: 520; loss: 1.05; acc: 0.72
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.66; acc: 0.75
Batch: 600; loss: 0.85; acc: 0.7
Batch: 620; loss: 0.74; acc: 0.73
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.82; acc: 0.7
Batch: 700; loss: 0.93; acc: 0.73
Batch: 720; loss: 0.89; acc: 0.69
Batch: 740; loss: 0.77; acc: 0.69
Batch: 760; loss: 0.76; acc: 0.77
Batch: 780; loss: 0.78; acc: 0.8
Train Epoch over. train_loss: 0.77; train_accuracy: 0.75 

Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.73
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.67
Batch: 140; loss: 0.33; acc: 0.89
Val Epoch over. val_loss: 0.7268984141243491; val_accuracy: 0.7647292993630573 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.71; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.66
Batch: 100; loss: 0.75; acc: 0.72
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 0.83; acc: 0.72
Batch: 260; loss: 0.85; acc: 0.67
Batch: 280; loss: 0.94; acc: 0.69
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.69
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.78; acc: 0.72
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.87; acc: 0.62
Batch: 480; loss: 1.13; acc: 0.67
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 0.77; acc: 0.73
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.8; acc: 0.72
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.78
Batch: 640; loss: 0.78; acc: 0.67
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.77
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.75
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.75 

Batch: 0; loss: 0.8; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 1.32; acc: 0.67
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.7153503463906088; val_accuracy: 0.7705015923566879 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 1.34; acc: 0.56
Batch: 120; loss: 0.82; acc: 0.72
Batch: 140; loss: 0.93; acc: 0.64
Batch: 160; loss: 0.8; acc: 0.75
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.77; acc: 0.72
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 0.99; acc: 0.7
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 1.02; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.73
Batch: 400; loss: 0.88; acc: 0.67
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.8; acc: 0.77
Batch: 460; loss: 0.85; acc: 0.66
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.99; acc: 0.61
Batch: 520; loss: 0.93; acc: 0.66
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 0.9; acc: 0.75
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.74; acc: 0.73
Batch: 640; loss: 0.68; acc: 0.73
Batch: 660; loss: 0.94; acc: 0.73
Batch: 680; loss: 0.91; acc: 0.73
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.75
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.85; acc: 0.66
Batch: 780; loss: 0.79; acc: 0.72
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.69
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.31; acc: 0.66
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.7099429966917463; val_accuracy: 0.7727906050955414 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.85; acc: 0.7
Batch: 100; loss: 0.58; acc: 0.77
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.69
Batch: 180; loss: 0.75; acc: 0.73
Batch: 200; loss: 0.68; acc: 0.78
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 0.97; acc: 0.69
Batch: 260; loss: 0.71; acc: 0.73
Batch: 280; loss: 0.99; acc: 0.67
Batch: 300; loss: 0.76; acc: 0.75
Batch: 320; loss: 0.93; acc: 0.75
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 0.84; acc: 0.75
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.73
Batch: 460; loss: 0.72; acc: 0.7
Batch: 480; loss: 0.64; acc: 0.77
Batch: 500; loss: 0.65; acc: 0.75
Batch: 520; loss: 0.67; acc: 0.75
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.97; acc: 0.67
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.75
Batch: 660; loss: 1.01; acc: 0.7
Batch: 680; loss: 0.86; acc: 0.8
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.81; acc: 0.75
Batch: 780; loss: 0.64; acc: 0.73
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.7
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.7081986931478901; val_accuracy: 0.7740843949044586 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.7
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.75
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.77; acc: 0.66
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.73
Batch: 260; loss: 0.72; acc: 0.69
Batch: 280; loss: 0.67; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.75
Batch: 320; loss: 0.55; acc: 0.77
Batch: 340; loss: 0.97; acc: 0.66
Batch: 360; loss: 0.94; acc: 0.69
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.94; acc: 0.67
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.79; acc: 0.75
Batch: 500; loss: 0.64; acc: 0.77
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.83; acc: 0.7
Batch: 620; loss: 1.0; acc: 0.72
Batch: 640; loss: 0.85; acc: 0.67
Batch: 660; loss: 0.6; acc: 0.77
Batch: 680; loss: 0.98; acc: 0.64
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.9; acc: 0.73
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.78; acc: 0.69
Batch: 20; loss: 1.08; acc: 0.7
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.7051615303109406; val_accuracy: 0.7752786624203821 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.78; acc: 0.69
Batch: 60; loss: 0.57; acc: 0.73
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.75
Batch: 160; loss: 0.69; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.66; acc: 0.73
Batch: 280; loss: 0.76; acc: 0.73
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.8; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.82; acc: 0.77
Batch: 380; loss: 0.71; acc: 0.73
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 1.21; acc: 0.61
Batch: 440; loss: 0.78; acc: 0.77
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.75
Batch: 520; loss: 0.85; acc: 0.66
Batch: 540; loss: 0.7; acc: 0.8
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.72; acc: 0.7
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.77
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.8; acc: 0.7
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.84; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 1.29; acc: 0.67
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.7017046287181271; val_accuracy: 0.7773686305732485 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.77; acc: 0.69
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.66
Batch: 180; loss: 0.82; acc: 0.7
Batch: 200; loss: 0.65; acc: 0.8
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 1.05; acc: 0.72
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.9; acc: 0.69
Batch: 320; loss: 0.93; acc: 0.72
Batch: 340; loss: 0.85; acc: 0.72
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 0.72; acc: 0.78
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.82; acc: 0.73
Batch: 500; loss: 0.7; acc: 0.77
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.72
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.76; acc: 0.77
Batch: 640; loss: 0.83; acc: 0.69
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.92; acc: 0.69
Batch: 700; loss: 0.89; acc: 0.75
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.76; acc: 0.67
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.6973748827815816; val_accuracy: 0.7799562101910829 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.73
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 1.03; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 1.13; acc: 0.69
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.73
Batch: 240; loss: 0.87; acc: 0.72
Batch: 260; loss: 0.85; acc: 0.75
Batch: 280; loss: 1.06; acc: 0.75
Batch: 300; loss: 0.79; acc: 0.73
Batch: 320; loss: 0.57; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.73
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.75
Batch: 420; loss: 0.52; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.73
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.73
Batch: 520; loss: 0.9; acc: 0.78
Batch: 540; loss: 0.67; acc: 0.75
Batch: 560; loss: 0.99; acc: 0.73
Batch: 580; loss: 1.16; acc: 0.7
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.73
Batch: 640; loss: 0.65; acc: 0.73
Batch: 660; loss: 0.79; acc: 0.72
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.89; acc: 0.7
Batch: 720; loss: 1.06; acc: 0.62
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.82; acc: 0.64
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.6948043089004079; val_accuracy: 0.7790605095541401 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.78
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.66; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.8; acc: 0.7
Batch: 160; loss: 0.71; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.73
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.84; acc: 0.75
Batch: 300; loss: 0.82; acc: 0.7
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.72
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.88; acc: 0.69
Batch: 420; loss: 0.85; acc: 0.77
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.99; acc: 0.72
Batch: 480; loss: 0.78; acc: 0.73
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 1.09; acc: 0.67
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.75
Batch: 580; loss: 0.88; acc: 0.69
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 1.05; acc: 0.72
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.95; acc: 0.69
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.72
Batch: 760; loss: 0.89; acc: 0.77
Batch: 780; loss: 0.87; acc: 0.72
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.6926410822731675; val_accuracy: 0.7796576433121019 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.64
Batch: 100; loss: 0.79; acc: 0.67
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.93; acc: 0.67
Batch: 160; loss: 0.67; acc: 0.78
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.91; acc: 0.75
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.73; acc: 0.72
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.81; acc: 0.75
Batch: 380; loss: 0.81; acc: 0.72
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.85; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.69
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.75
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.75; acc: 0.73
Batch: 560; loss: 0.83; acc: 0.72
Batch: 580; loss: 0.76; acc: 0.7
Batch: 600; loss: 0.9; acc: 0.69
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.64; acc: 0.75
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.89; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.78
Batch: 720; loss: 0.9; acc: 0.7
Batch: 740; loss: 0.71; acc: 0.73
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.76 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.6915990222411551; val_accuracy: 0.7816480891719745 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.96; acc: 0.72
Batch: 120; loss: 0.82; acc: 0.72
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 0.78; acc: 0.73
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.71; acc: 0.77
Batch: 280; loss: 0.86; acc: 0.69
Batch: 300; loss: 0.74; acc: 0.75
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 1.15; acc: 0.72
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.75
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.63; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.69
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 1.01; acc: 0.72
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.9; acc: 0.7
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.72
Batch: 760; loss: 0.67; acc: 0.75
Batch: 780; loss: 0.78; acc: 0.7
Train Epoch over. train_loss: 0.73; train_accuracy: 0.76 

Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.6914695913244964; val_accuracy: 0.7814490445859873 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.74; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.64
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.77
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.89; acc: 0.75
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.45; acc: 0.8
Batch: 260; loss: 0.68; acc: 0.72
Batch: 280; loss: 1.16; acc: 0.59
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.73; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.73
Batch: 420; loss: 0.86; acc: 0.72
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.78; acc: 0.72
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.7
Batch: 520; loss: 0.85; acc: 0.7
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.81; acc: 0.73
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.75; acc: 0.72
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.8; acc: 0.7
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.78
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6881465325309972; val_accuracy: 0.7825437898089171 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.9; acc: 0.67
Batch: 100; loss: 0.85; acc: 0.67
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.75
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.95; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.75
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.88; acc: 0.75
Batch: 400; loss: 0.76; acc: 0.75
Batch: 420; loss: 0.88; acc: 0.7
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.86; acc: 0.72
Batch: 520; loss: 0.92; acc: 0.72
Batch: 540; loss: 0.78; acc: 0.77
Batch: 560; loss: 0.71; acc: 0.72
Batch: 580; loss: 0.78; acc: 0.75
Batch: 600; loss: 0.75; acc: 0.75
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.84; acc: 0.67
Batch: 700; loss: 0.65; acc: 0.77
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.81; acc: 0.7
Batch: 760; loss: 0.53; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6876323509747815; val_accuracy: 0.7822452229299363 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.9; acc: 0.69
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.69
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.73
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.73; acc: 0.73
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.77
Batch: 320; loss: 0.86; acc: 0.67
Batch: 340; loss: 0.94; acc: 0.75
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.64; acc: 0.78
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.77
Batch: 580; loss: 0.77; acc: 0.72
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 0.88; acc: 0.75
Batch: 660; loss: 0.82; acc: 0.8
Batch: 680; loss: 0.73; acc: 0.77
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.98; acc: 0.73
Batch: 740; loss: 0.7; acc: 0.77
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6874079068375242; val_accuracy: 0.78234474522293 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.72
Batch: 40; loss: 0.84; acc: 0.69
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.53; acc: 0.8
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.78; acc: 0.73
Batch: 220; loss: 0.62; acc: 0.78
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 0.74; acc: 0.73
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.66
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 0.9; acc: 0.78
Batch: 400; loss: 0.79; acc: 0.72
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.88; acc: 0.73
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 0.62; acc: 0.73
Batch: 520; loss: 0.73; acc: 0.73
Batch: 540; loss: 0.73; acc: 0.75
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.69
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.87; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.75
Batch: 720; loss: 0.77; acc: 0.83
Batch: 740; loss: 0.8; acc: 0.7
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6869481431830461; val_accuracy: 0.7820461783439491 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.7
Batch: 100; loss: 0.94; acc: 0.7
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.72
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.68; acc: 0.73
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.81
Batch: 240; loss: 0.87; acc: 0.75
Batch: 260; loss: 0.87; acc: 0.7
Batch: 280; loss: 0.77; acc: 0.73
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.75
Batch: 380; loss: 0.73; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.69; acc: 0.75
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.78
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.72
Batch: 640; loss: 0.69; acc: 0.77
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6864865856945135; val_accuracy: 0.7826433121019108 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.84; acc: 0.69
Batch: 120; loss: 0.63; acc: 0.75
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 1.26; acc: 0.62
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.75
Batch: 280; loss: 0.55; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.75
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.93; acc: 0.78
Batch: 380; loss: 1.06; acc: 0.7
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.97; acc: 0.72
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.67; acc: 0.75
Batch: 480; loss: 1.03; acc: 0.67
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.7
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.8; acc: 0.7
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.75
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.88; acc: 0.7
Batch: 720; loss: 0.97; acc: 0.69
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.6863477932419747; val_accuracy: 0.7827428343949044 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.72
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.69; acc: 0.72
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.59; acc: 0.8
Batch: 340; loss: 0.71; acc: 0.72
Batch: 360; loss: 0.8; acc: 0.73
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.72
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.75; acc: 0.72
Batch: 460; loss: 0.63; acc: 0.77
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.73
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.83; acc: 0.73
Batch: 580; loss: 1.04; acc: 0.69
Batch: 600; loss: 0.6; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.77
Batch: 640; loss: 0.84; acc: 0.64
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.8
Batch: 700; loss: 0.73; acc: 0.75
Batch: 720; loss: 0.54; acc: 0.77
Batch: 740; loss: 0.67; acc: 0.73
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.93; acc: 0.7
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.98
Val Epoch over. val_loss: 0.6857401937436146; val_accuracy: 0.78125 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.75; acc: 0.73
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.69
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.77
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.77
Batch: 260; loss: 0.88; acc: 0.72
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.93; acc: 0.73
Batch: 320; loss: 0.72; acc: 0.75
Batch: 340; loss: 0.75; acc: 0.72
Batch: 360; loss: 0.75; acc: 0.77
Batch: 380; loss: 0.63; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.75; acc: 0.73
Batch: 440; loss: 0.87; acc: 0.66
Batch: 460; loss: 0.95; acc: 0.66
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.66; acc: 0.77
Batch: 540; loss: 0.78; acc: 0.75
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.87; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.64
Batch: 640; loss: 0.84; acc: 0.78
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.87; acc: 0.72
Batch: 740; loss: 1.23; acc: 0.56
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6859803476910682; val_accuracy: 0.7814490445859873 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 1.23; acc: 0.59
Batch: 120; loss: 0.63; acc: 0.75
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.83; acc: 0.7
Batch: 180; loss: 0.89; acc: 0.75
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.69
Batch: 240; loss: 0.73; acc: 0.73
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 1.25; acc: 0.59
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.88; acc: 0.72
Batch: 520; loss: 0.49; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 1.08; acc: 0.69
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.92; acc: 0.77
Batch: 640; loss: 0.96; acc: 0.64
Batch: 660; loss: 0.96; acc: 0.75
Batch: 680; loss: 1.07; acc: 0.67
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.62; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.6850781049698021; val_accuracy: 0.7813495222929936 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.64
Batch: 160; loss: 0.7; acc: 0.8
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.9; acc: 0.7
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.84; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.7
Batch: 380; loss: 0.71; acc: 0.77
Batch: 400; loss: 0.81; acc: 0.75
Batch: 420; loss: 0.77; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.78
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 0.73; acc: 0.75
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.72; acc: 0.73
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.72
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 1.02; acc: 0.64
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.81; acc: 0.72
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6846237943810263; val_accuracy: 0.7826433121019108 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.78
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.75
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.69; acc: 0.72
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.75; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.77
Batch: 240; loss: 0.85; acc: 0.72
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.72
Batch: 300; loss: 1.11; acc: 0.73
Batch: 320; loss: 0.7; acc: 0.73
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.99; acc: 0.66
Batch: 380; loss: 0.93; acc: 0.73
Batch: 400; loss: 0.92; acc: 0.75
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.78; acc: 0.77
Batch: 460; loss: 0.71; acc: 0.77
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.86; acc: 0.72
Batch: 520; loss: 0.88; acc: 0.69
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.84; acc: 0.72
Batch: 580; loss: 0.65; acc: 0.75
Batch: 600; loss: 0.73; acc: 0.72
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.73
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 0.53; acc: 0.8
Batch: 720; loss: 0.75; acc: 0.72
Batch: 740; loss: 0.64; acc: 0.73
Batch: 760; loss: 0.58; acc: 0.75
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6843914342154364; val_accuracy: 0.7826433121019108 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.73
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.85; acc: 0.7
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.97; acc: 0.7
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.72
Batch: 240; loss: 0.78; acc: 0.72
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.83; acc: 0.75
Batch: 300; loss: 0.58; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 0.6; acc: 0.8
Batch: 400; loss: 0.93; acc: 0.75
Batch: 420; loss: 1.01; acc: 0.73
Batch: 440; loss: 0.84; acc: 0.8
Batch: 460; loss: 0.55; acc: 0.77
Batch: 480; loss: 0.84; acc: 0.72
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.78
Batch: 540; loss: 0.69; acc: 0.73
Batch: 560; loss: 0.75; acc: 0.78
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.73
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.97; acc: 0.67
Batch: 720; loss: 0.96; acc: 0.77
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.77
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6839821870159951; val_accuracy: 0.7817476114649682 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.69
Batch: 100; loss: 0.94; acc: 0.66
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.67; acc: 0.75
Batch: 180; loss: 0.63; acc: 0.78
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.53; acc: 0.8
Batch: 240; loss: 0.92; acc: 0.72
Batch: 260; loss: 0.85; acc: 0.75
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.95; acc: 0.66
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 1.17; acc: 0.73
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 1.2; acc: 0.72
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 0.83; acc: 0.7
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.74; acc: 0.72
Batch: 600; loss: 0.69; acc: 0.67
Batch: 620; loss: 0.76; acc: 0.73
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.8; acc: 0.67
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.78
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.8
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.73
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.6839608225473173; val_accuracy: 0.7821457006369427 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.91; acc: 0.69
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.77
Batch: 320; loss: 0.69; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.75
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.68; acc: 0.75
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 0.93; acc: 0.67
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.73; acc: 0.69
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.7
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.84; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.59; acc: 0.77
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.9; acc: 0.67
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.91; acc: 0.73
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6839144072335237; val_accuracy: 0.7827428343949044 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.72; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 0.57; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.86; acc: 0.7
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.83; acc: 0.72
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.77
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.67
Batch: 480; loss: 0.66; acc: 0.72
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 0.87; acc: 0.77
Batch: 600; loss: 0.57; acc: 0.73
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.88; acc: 0.69
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6837833121324041; val_accuracy: 0.7830414012738853 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.75
Batch: 160; loss: 0.72; acc: 0.72
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.93; acc: 0.7
Batch: 220; loss: 0.77; acc: 0.72
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.86; acc: 0.7
Batch: 300; loss: 0.84; acc: 0.75
Batch: 320; loss: 0.95; acc: 0.69
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.86; acc: 0.69
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.73
Batch: 580; loss: 0.88; acc: 0.69
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.72
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.78
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.9; acc: 0.67
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6837462184915117; val_accuracy: 0.7822452229299363 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 0.77; acc: 0.69
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.85; acc: 0.7
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.73
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 0.87; acc: 0.67
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.59; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.6; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.77
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.91; acc: 0.64
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.84; acc: 0.83
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 1.17; acc: 0.56
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 0.65; acc: 0.77
Batch: 680; loss: 0.71; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.73
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.89; acc: 0.73
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6834059169717656; val_accuracy: 0.7818471337579618 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.67
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.73
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.79; acc: 0.7
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.79; acc: 0.7
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.78
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 0.79; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.94; acc: 0.69
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.8; acc: 0.7
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.95; acc: 0.72
Batch: 440; loss: 0.87; acc: 0.67
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.79; acc: 0.66
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 1.06; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.75
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.91; acc: 0.73
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.75; acc: 0.73
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.85; acc: 0.72
Batch: 760; loss: 0.84; acc: 0.73
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6832136221372398; val_accuracy: 0.78234474522293 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.89; acc: 0.73
Batch: 80; loss: 0.72; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 1.05; acc: 0.64
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.71; acc: 0.75
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.67; acc: 0.77
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.8; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.78
Batch: 380; loss: 0.95; acc: 0.62
Batch: 400; loss: 0.86; acc: 0.8
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.76; acc: 0.75
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.75; acc: 0.75
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.9; acc: 0.75
Batch: 640; loss: 0.71; acc: 0.77
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.94; acc: 0.72
Batch: 700; loss: 0.82; acc: 0.73
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.9; acc: 0.7
Batch: 760; loss: 0.79; acc: 0.73
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6833074747756788; val_accuracy: 0.7817476114649682 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.79; acc: 0.72
Batch: 160; loss: 0.79; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.77
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.76; acc: 0.72
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.83; acc: 0.81
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 1.17; acc: 0.61
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.73; acc: 0.7
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 1.01; acc: 0.67
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.88; acc: 0.72
Batch: 720; loss: 1.09; acc: 0.61
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.69
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.6832549549212121; val_accuracy: 0.7824442675159236 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.78; acc: 0.7
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 0.86; acc: 0.67
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.79; acc: 0.8
Batch: 160; loss: 0.52; acc: 0.8
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.89; acc: 0.72
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.85; acc: 0.7
Batch: 280; loss: 0.88; acc: 0.7
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.67
Batch: 340; loss: 0.65; acc: 0.73
Batch: 360; loss: 0.89; acc: 0.67
Batch: 380; loss: 0.7; acc: 0.75
Batch: 400; loss: 0.91; acc: 0.77
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.83; acc: 0.75
Batch: 480; loss: 0.69; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.79; acc: 0.78
Batch: 540; loss: 0.66; acc: 0.77
Batch: 560; loss: 0.92; acc: 0.67
Batch: 580; loss: 0.72; acc: 0.77
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.77
Batch: 700; loss: 1.0; acc: 0.67
Batch: 720; loss: 0.68; acc: 0.7
Batch: 740; loss: 0.78; acc: 0.77
Batch: 760; loss: 0.88; acc: 0.7
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6830517211157805; val_accuracy: 0.7822452229299363 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.7
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.76; acc: 0.73
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.88; acc: 0.75
Batch: 260; loss: 0.94; acc: 0.67
Batch: 280; loss: 0.86; acc: 0.75
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 0.83; acc: 0.77
Batch: 420; loss: 0.75; acc: 0.77
Batch: 440; loss: 0.66; acc: 0.72
Batch: 460; loss: 0.77; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.87; acc: 0.75
Batch: 520; loss: 0.72; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 0.7; acc: 0.77
Batch: 580; loss: 0.63; acc: 0.77
Batch: 600; loss: 0.85; acc: 0.77
Batch: 620; loss: 0.56; acc: 0.77
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.78
Batch: 720; loss: 0.61; acc: 0.77
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6830229022700316; val_accuracy: 0.7821457006369427 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.78; acc: 0.72
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.7
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.64; acc: 0.77
Batch: 260; loss: 0.71; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.83; acc: 0.7
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.58; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.96; acc: 0.75
Batch: 540; loss: 0.83; acc: 0.69
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.79; acc: 0.77
Batch: 600; loss: 0.66; acc: 0.75
Batch: 620; loss: 0.71; acc: 0.77
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 0.89; acc: 0.73
Batch: 680; loss: 0.82; acc: 0.8
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6829260494678643; val_accuracy: 0.7825437898089171 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.93; acc: 0.72
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.85; acc: 0.72
Batch: 200; loss: 0.85; acc: 0.75
Batch: 220; loss: 0.84; acc: 0.75
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.92; acc: 0.7
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 1.16; acc: 0.62
Batch: 320; loss: 0.82; acc: 0.7
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.75
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.72
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.69
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.75
Batch: 560; loss: 0.78; acc: 0.78
Batch: 580; loss: 0.63; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.77
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.75
Batch: 660; loss: 0.93; acc: 0.7
Batch: 680; loss: 0.55; acc: 0.81
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 0.95; acc: 0.66
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6829729101080804; val_accuracy: 0.7821457006369427 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.72
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 0.93; acc: 0.75
Batch: 160; loss: 0.77; acc: 0.67
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.78; acc: 0.77
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.81; acc: 0.73
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.86; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.83; acc: 0.73
Batch: 560; loss: 0.64; acc: 0.78
Batch: 580; loss: 0.79; acc: 0.73
Batch: 600; loss: 0.86; acc: 0.73
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.93; acc: 0.67
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.75
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.77
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6829113696411158; val_accuracy: 0.7814490445859873 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.88; acc: 0.61
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.58; acc: 0.77
Batch: 240; loss: 0.63; acc: 0.75
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.7
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.73; acc: 0.75
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.8; acc: 0.77
Batch: 440; loss: 0.58; acc: 0.75
Batch: 460; loss: 0.64; acc: 0.72
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.7
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 0.93; acc: 0.72
Batch: 580; loss: 0.9; acc: 0.66
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.75
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.79; acc: 0.75
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.74; acc: 0.73
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6829066145571934; val_accuracy: 0.7822452229299363 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.75
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 1.0; acc: 0.66
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 0.7; acc: 0.78
Batch: 380; loss: 0.95; acc: 0.73
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.68; acc: 0.69
Batch: 440; loss: 0.71; acc: 0.78
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.69; acc: 0.75
Batch: 540; loss: 0.83; acc: 0.7
Batch: 560; loss: 0.69; acc: 0.75
Batch: 580; loss: 0.78; acc: 0.77
Batch: 600; loss: 0.61; acc: 0.78
Batch: 620; loss: 0.62; acc: 0.77
Batch: 640; loss: 0.76; acc: 0.73
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 1.03; acc: 0.66
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.71; acc: 0.75
Batch: 740; loss: 0.69; acc: 0.77
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6828505723339737; val_accuracy: 0.7817476114649682 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.77
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.68; acc: 0.77
Batch: 280; loss: 0.68; acc: 0.73
Batch: 300; loss: 0.95; acc: 0.67
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 0.82; acc: 0.75
Batch: 360; loss: 0.53; acc: 0.81
Batch: 380; loss: 0.67; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.7
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.74; acc: 0.75
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 0.76; acc: 0.72
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.66
Batch: 760; loss: 0.65; acc: 0.77
Batch: 780; loss: 0.78; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6827757848296195; val_accuracy: 0.7819466560509554 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.7; acc: 0.77
Batch: 60; loss: 0.95; acc: 0.64
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.63; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.73
Batch: 280; loss: 0.66; acc: 0.75
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.85; acc: 0.7
Batch: 340; loss: 0.55; acc: 0.75
Batch: 360; loss: 0.82; acc: 0.73
Batch: 380; loss: 0.73; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.77
Batch: 420; loss: 0.86; acc: 0.75
Batch: 440; loss: 0.96; acc: 0.77
Batch: 460; loss: 0.83; acc: 0.75
Batch: 480; loss: 0.81; acc: 0.77
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.84; acc: 0.67
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 0.95; acc: 0.66
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.52; acc: 0.78
Batch: 680; loss: 0.74; acc: 0.73
Batch: 700; loss: 0.9; acc: 0.69
Batch: 720; loss: 0.72; acc: 0.75
Batch: 740; loss: 0.86; acc: 0.66
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6827329243444333; val_accuracy: 0.78234474522293 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.63; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.75
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.85; acc: 0.73
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.78
Batch: 380; loss: 0.69; acc: 0.75
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.78
Batch: 520; loss: 0.54; acc: 0.78
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.7; acc: 0.73
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 0.93; acc: 0.7
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.73
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.95; acc: 0.7
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.77
Batch: 780; loss: 0.86; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.6826936272299213; val_accuracy: 0.7819466560509554 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.85; acc: 0.69
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.82; acc: 0.72
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 1.08; acc: 0.7
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 1.07; acc: 0.7
Batch: 220; loss: 0.8; acc: 0.75
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.74; acc: 0.72
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.77
Batch: 420; loss: 0.67; acc: 0.67
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 0.81; acc: 0.73
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.66
Batch: 520; loss: 0.6; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.9; acc: 0.75
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.61; acc: 0.78
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 1.03; acc: 0.69
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.682715248530078; val_accuracy: 0.7814490445859873 

plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 179335
elements in E: 79684000
fraction nonzero: 0.002250577280257015
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.31
Batch: 80; loss: 2.26; acc: 0.25
Batch: 100; loss: 2.25; acc: 0.27
Batch: 120; loss: 2.25; acc: 0.27
Batch: 140; loss: 2.21; acc: 0.48
Batch: 160; loss: 2.22; acc: 0.36
Batch: 180; loss: 2.2; acc: 0.41
Batch: 200; loss: 2.18; acc: 0.38
Batch: 220; loss: 2.16; acc: 0.42
Batch: 240; loss: 2.11; acc: 0.45
Batch: 260; loss: 2.12; acc: 0.47
Batch: 280; loss: 2.11; acc: 0.55
Batch: 300; loss: 2.03; acc: 0.53
Batch: 320; loss: 2.06; acc: 0.5
Batch: 340; loss: 1.97; acc: 0.55
Batch: 360; loss: 1.98; acc: 0.52
Batch: 380; loss: 1.87; acc: 0.59
Batch: 400; loss: 1.87; acc: 0.56
Batch: 420; loss: 1.8; acc: 0.67
Batch: 440; loss: 1.71; acc: 0.66
Batch: 460; loss: 1.75; acc: 0.55
Batch: 480; loss: 1.62; acc: 0.62
Batch: 500; loss: 1.57; acc: 0.67
Batch: 520; loss: 1.66; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.64
Batch: 560; loss: 1.44; acc: 0.56
Batch: 580; loss: 1.41; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.32; acc: 0.62
Batch: 660; loss: 1.16; acc: 0.72
Batch: 680; loss: 1.01; acc: 0.7
Batch: 700; loss: 1.09; acc: 0.69
Batch: 720; loss: 1.08; acc: 0.77
Batch: 740; loss: 1.02; acc: 0.69
Batch: 760; loss: 0.89; acc: 0.77
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 1.76; train_accuracy: 0.52 

Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.59
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.64; acc: 0.86
Val Epoch over. val_loss: 0.8948768922098124; val_accuracy: 0.7412420382165605 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.96; acc: 0.7
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.93; acc: 0.66
Batch: 160; loss: 0.88; acc: 0.75
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.77; acc: 0.73
Batch: 260; loss: 0.95; acc: 0.64
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.96; acc: 0.62
Batch: 320; loss: 0.8; acc: 0.7
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.96; acc: 0.72
Batch: 420; loss: 0.73; acc: 0.69
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.71; acc: 0.75
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.93; acc: 0.77
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.76; acc: 0.73
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.86; acc: 0.73
Batch: 620; loss: 0.73; acc: 0.77
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.77
Batch: 720; loss: 0.84; acc: 0.77
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.73
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.64; acc: 0.75
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.6062609680518982; val_accuracy: 0.8125995222929936 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.75
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.73
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.77
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.72
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.7; acc: 0.75
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.92; acc: 0.67
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 0.53; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.77
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.77
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.8; acc: 0.7
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.54102420128265; val_accuracy: 0.8304140127388535 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.75
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.65; acc: 0.75
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.69; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.59; acc: 0.75
Batch: 340; loss: 0.74; acc: 0.72
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.51; acc: 0.78
Batch: 500; loss: 0.75; acc: 0.73
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.78
Batch: 660; loss: 0.73; acc: 0.72
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.84; acc: 0.75
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.5164879681483195; val_accuracy: 0.8408638535031847 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.83; acc: 0.78
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.77
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.66; acc: 0.77
Batch: 680; loss: 0.77; acc: 0.72
Batch: 700; loss: 0.64; acc: 0.77
Batch: 720; loss: 0.6; acc: 0.77
Batch: 740; loss: 0.58; acc: 0.78
Batch: 760; loss: 0.65; acc: 0.73
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4909098245156039; val_accuracy: 0.8479299363057324 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.8
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.83
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.51; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.77
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.48419448552997246; val_accuracy: 0.8463375796178344 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.78
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.8
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.47879732656440915; val_accuracy: 0.8471337579617835 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.81
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.78
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.73; acc: 0.77
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4609738637212735; val_accuracy: 0.8553941082802548 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.75
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.77
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.87; acc: 0.72
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.64; acc: 0.77
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.77
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.458508146084418; val_accuracy: 0.8542993630573248 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.75
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.77; acc: 0.75
Batch: 380; loss: 0.58; acc: 0.78
Batch: 400; loss: 0.36; acc: 0.83
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.74; acc: 0.77
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.81
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.66; acc: 0.8
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.45774978650793147; val_accuracy: 0.8557921974522293 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.8; acc: 0.75
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.81
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.65; acc: 0.75
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.75
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.73
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.45067231304896105; val_accuracy: 0.8589769108280255 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.77; acc: 0.73
Batch: 300; loss: 0.41; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.78
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.44903884960967266; val_accuracy: 0.8589769108280255 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.62; acc: 0.75
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.34; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4463745643663558; val_accuracy: 0.8614649681528662 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.81; acc: 0.77
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.81
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.44705493031603516; val_accuracy: 0.8619625796178344 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.75
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.48; acc: 0.81
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.8
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.77
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.81
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.77; acc: 0.77
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4439827777018213; val_accuracy: 0.8641520700636943 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.81; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.61; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4451304714486098; val_accuracy: 0.8615644904458599 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.75
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.93; acc: 0.78
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.81
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.67; acc: 0.73
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.48; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.8
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.4415831915132559; val_accuracy: 0.8625597133757962 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.78
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.83
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.78
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.78
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4397552164781625; val_accuracy: 0.8628582802547771 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.42; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.43980662330130865; val_accuracy: 0.8655453821656051 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.8
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.77
Batch: 500; loss: 0.6; acc: 0.77
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.81
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.61; acc: 0.8
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4378170506304996; val_accuracy: 0.8642515923566879 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.8
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.85; acc: 0.77
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4363409682254123; val_accuracy: 0.8640525477707006 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.75
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.77
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.69; acc: 0.77
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4364308101261497; val_accuracy: 0.8645501592356688 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.75
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.83
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.68; acc: 0.73
Batch: 720; loss: 0.49; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4349561501175735; val_accuracy: 0.8662420382165605 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.73
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.95; acc: 0.77
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4352883414667883; val_accuracy: 0.8648487261146497 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.8
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.73
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.78
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.64; acc: 0.77
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4343880076127447; val_accuracy: 0.8637539808917197 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.84
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.78
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.434282407591677; val_accuracy: 0.8661425159235668 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.79; acc: 0.8
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.8
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43485053652411054; val_accuracy: 0.8662420382165605 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.89; acc: 0.72
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.62; acc: 0.75
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.8
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.4338755931728964; val_accuracy: 0.8665406050955414 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.49; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.75
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.75
Batch: 760; loss: 0.36; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43298192849014977; val_accuracy: 0.8668391719745223 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.77
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4331976099378744; val_accuracy: 0.8664410828025477 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.81
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4321809961063087; val_accuracy: 0.8672372611464968 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.8
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4318290675047097; val_accuracy: 0.8673367834394905 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.77
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.78
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.68; acc: 0.72
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.8; acc: 0.78
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.73
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43185292198589653; val_accuracy: 0.8671377388535032 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.78
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.8
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.78
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43168403255711696; val_accuracy: 0.8673367834394905 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.7; acc: 0.73
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.78
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.73; acc: 0.73
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.75
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.77
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4317410748190941; val_accuracy: 0.8675358280254777 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.78
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43179719301925346; val_accuracy: 0.8679339171974523 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.9; acc: 0.8
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.82; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.54; acc: 0.78
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.8
Batch: 760; loss: 0.5; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4310026855985071; val_accuracy: 0.8681329617834395 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.77
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.78
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.77; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.430884087398933; val_accuracy: 0.8686305732484076 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.78
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.52; acc: 0.8
Batch: 520; loss: 0.5; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.78
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.77
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.430826617748874; val_accuracy: 0.868531050955414 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.78
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.78
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43062960394438665; val_accuracy: 0.8691281847133758 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.8
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.8
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.78
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43042988040644653; val_accuracy: 0.8692277070063694 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.73; acc: 0.75
Batch: 780; loss: 0.62; acc: 0.75
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4302864576316184; val_accuracy: 0.8691281847133758 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.44; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.77
Batch: 320; loss: 0.53; acc: 0.78
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.77
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.84
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4303529157665125; val_accuracy: 0.8698248407643312 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.8
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.77; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4302684157896953; val_accuracy: 0.8696257961783439 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.81
Batch: 300; loss: 0.35; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.78
Batch: 580; loss: 0.59; acc: 0.77
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4302809735771957; val_accuracy: 0.8692277070063694 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.78
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.42; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.81
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.59; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4301120295266437; val_accuracy: 0.8694267515923567 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.77
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.81
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4300925824197994; val_accuracy: 0.8694267515923567 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.8
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.81
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.64; acc: 0.77
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.43008045966078523; val_accuracy: 0.8692277070063694 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.81
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.81; acc: 0.72
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.78
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.78
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.42993803436209443; val_accuracy: 0.8692277070063694 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.42984610587168653; val_accuracy: 0.8692277070063694 

plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 267890
elements in E: 119526000
fraction nonzero: 0.002241269681910212
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.2
Batch: 60; loss: 2.25; acc: 0.34
Batch: 80; loss: 2.24; acc: 0.39
Batch: 100; loss: 2.23; acc: 0.33
Batch: 120; loss: 2.21; acc: 0.34
Batch: 140; loss: 2.16; acc: 0.58
Batch: 160; loss: 2.14; acc: 0.53
Batch: 180; loss: 2.11; acc: 0.52
Batch: 200; loss: 2.09; acc: 0.52
Batch: 220; loss: 2.03; acc: 0.64
Batch: 240; loss: 1.98; acc: 0.58
Batch: 260; loss: 1.94; acc: 0.58
Batch: 280; loss: 1.92; acc: 0.61
Batch: 300; loss: 1.74; acc: 0.72
Batch: 320; loss: 1.76; acc: 0.66
Batch: 340; loss: 1.62; acc: 0.62
Batch: 360; loss: 1.53; acc: 0.73
Batch: 380; loss: 1.41; acc: 0.69
Batch: 400; loss: 1.35; acc: 0.69
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.72
Batch: 460; loss: 1.13; acc: 0.75
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 0.83; acc: 0.84
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.98; acc: 0.72
Batch: 560; loss: 0.94; acc: 0.72
Batch: 580; loss: 0.91; acc: 0.77
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.77
Batch: 640; loss: 0.92; acc: 0.66
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.68; acc: 0.77
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.81; acc: 0.78
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 1.44; train_accuracy: 0.62 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.91
Val Epoch over. val_loss: 0.602395414167149; val_accuracy: 0.8221536624203821 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.77
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 0.81; acc: 0.7
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.78
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.74; acc: 0.78
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.7
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.62; acc: 0.77
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.45497309378567774; val_accuracy: 0.8642515923566879 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.78
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.71; acc: 0.77
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.77
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.41357398251439353; val_accuracy: 0.8766918789808917 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.3; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.8
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.40113318265433523; val_accuracy: 0.8784832802547771 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.78
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.81
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.3830524312842424; val_accuracy: 0.883359872611465 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.54; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.83
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3783797859481186; val_accuracy: 0.8859474522292994 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.81
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.77
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.81
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.37235036767592095; val_accuracy: 0.8864450636942676 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3556749163919194; val_accuracy: 0.8939092356687898 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.83
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3511409368959202; val_accuracy: 0.894406847133758 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.8
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.34726395605096394; val_accuracy: 0.8960987261146497 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.8
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.34197012956734674; val_accuracy: 0.8982882165605095 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.81
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3433357434952335; val_accuracy: 0.897093949044586 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.33929942795045814; val_accuracy: 0.8974920382165605 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3393387209837604; val_accuracy: 0.8993829617834395 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.81
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.52; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.8
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.33767647766003944; val_accuracy: 0.898984872611465 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.8
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33978927908999146; val_accuracy: 0.8984872611464968 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3341963011652801; val_accuracy: 0.9003781847133758 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.83
Batch: 400; loss: 0.21; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.33646349347890564; val_accuracy: 0.8988853503184714 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3335016928375906; val_accuracy: 0.8999800955414012 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.8
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3312625342113957; val_accuracy: 0.9009753184713376 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.81
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.58; acc: 0.77
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3314663436097704; val_accuracy: 0.9006767515923567 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.09; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3316474867284678; val_accuracy: 0.900577229299363 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3306723799390398; val_accuracy: 0.9002786624203821 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.78
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3305669333429853; val_accuracy: 0.9014729299363057 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.83
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33071062282012526; val_accuracy: 0.8999800955414012 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.78
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33073769984351603; val_accuracy: 0.900577229299363 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.85; acc: 0.83
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.33196161307726696; val_accuracy: 0.9010748407643312 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.75
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.32958555269013545; val_accuracy: 0.9008757961783439 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3299729275950201; val_accuracy: 0.9002786624203821 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3300068769959887; val_accuracy: 0.9009753184713376 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.81
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.32930423570856165; val_accuracy: 0.9010748407643312 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.97; acc: 0.75
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3292624942341428; val_accuracy: 0.9003781847133758 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3292969272584672; val_accuracy: 0.9007762738853503 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.65; acc: 0.78
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32939212195053225; val_accuracy: 0.9000796178343949 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3295063410120405; val_accuracy: 0.9007762738853503 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3292954083365999; val_accuracy: 0.9006767515923567 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.328818311025003; val_accuracy: 0.9003781847133758 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.83
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.32902044707992273; val_accuracy: 0.9003781847133758 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3287659816586288; val_accuracy: 0.9000796178343949 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3288268556070935; val_accuracy: 0.9002786624203821 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.77
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3287866005472317; val_accuracy: 0.9002786624203821 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3286797857018793; val_accuracy: 0.9003781847133758 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3289017516906094; val_accuracy: 0.8999800955414012 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.81
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.84
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.84
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32882296450578485; val_accuracy: 0.9003781847133758 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.81
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.32869986544369134; val_accuracy: 0.9001791401273885 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

slurmstepd: error: _is_a_lwp: open() /proc/12532/status failed: No such file or directory
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3286843650090467; val_accuracy: 0.9008757961783439 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3286266635842384; val_accuracy: 0.9002786624203821 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32865670152530546; val_accuracy: 0.9001791401273885 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.36; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3285891866418207; val_accuracy: 0.9002786624203821 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32867019338782427; val_accuracy: 0.900577229299363 

plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_600_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 357007
elements in E: 159368000
fraction nonzero: 0.002240142312132925
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.25; acc: 0.28
Batch: 60; loss: 2.23; acc: 0.42
Batch: 80; loss: 2.21; acc: 0.39
Batch: 100; loss: 2.18; acc: 0.39
Batch: 120; loss: 2.17; acc: 0.33
Batch: 140; loss: 2.06; acc: 0.56
Batch: 160; loss: 2.08; acc: 0.48
Batch: 180; loss: 2.02; acc: 0.61
Batch: 200; loss: 1.95; acc: 0.62
Batch: 220; loss: 1.88; acc: 0.61
Batch: 240; loss: 1.74; acc: 0.64
Batch: 260; loss: 1.66; acc: 0.75
Batch: 280; loss: 1.65; acc: 0.64
Batch: 300; loss: 1.43; acc: 0.73
Batch: 320; loss: 1.37; acc: 0.75
Batch: 340; loss: 1.15; acc: 0.81
Batch: 360; loss: 1.09; acc: 0.77
Batch: 380; loss: 0.95; acc: 0.8
Batch: 400; loss: 0.99; acc: 0.78
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.8
Batch: 460; loss: 0.81; acc: 0.75
Batch: 480; loss: 0.79; acc: 0.78
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.77; acc: 0.75
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.67; acc: 0.77
Train Epoch over. train_loss: 1.25; train_accuracy: 0.68 

Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.502208830539588; val_accuracy: 0.8482285031847133 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.75
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.72; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.407530570058686; val_accuracy: 0.8735071656050956 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.78
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.78
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.78
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3693399586874968; val_accuracy: 0.8888335987261147 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3710509429976439; val_accuracy: 0.8894307324840764 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.35839391096382384; val_accuracy: 0.8884355095541401 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3387754793474629; val_accuracy: 0.8949044585987261 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.3396650067750056; val_accuracy: 0.8965963375796179 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.84
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.3268503702132945; val_accuracy: 0.898984872611465 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.75; acc: 0.81
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3188373024106785; val_accuracy: 0.9017714968152867 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.8
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3179110263942913; val_accuracy: 0.9047571656050956 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.8
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3140491420865818; val_accuracy: 0.9054538216560509 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3137540580692944; val_accuracy: 0.9042595541401274 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.3074606541473015; val_accuracy: 0.9082404458598726 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.3072325051969783; val_accuracy: 0.9063495222929936 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.30910363997433593; val_accuracy: 0.9042595541401274 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.77; acc: 0.81
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.39; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.81
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.3052106127854745; val_accuracy: 0.9057523885350318 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3011579530396659; val_accuracy: 0.9094347133757962 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2996838654919415; val_accuracy: 0.9098328025477707 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.1; acc: 1.0
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.3015675242444512; val_accuracy: 0.9090366242038217 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.84
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3008232056667471; val_accuracy: 0.9098328025477707 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.29744048468816053; val_accuracy: 0.9091361464968153 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.8
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.09; acc: 1.0
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.29690704803178264; val_accuracy: 0.9097332802547771 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2962276562099244; val_accuracy: 0.910031847133758 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.88
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29582131426235675; val_accuracy: 0.9109275477707006 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.29505845607750736; val_accuracy: 0.910828025477707 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.83
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2961192591602256; val_accuracy: 0.9102308917197452 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.83
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2956939922396544; val_accuracy: 0.9102308917197452 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.1; acc: 1.0
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2943631492934789; val_accuracy: 0.910828025477707 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.8
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.295179023176052; val_accuracy: 0.9095342356687898 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29491734257928887; val_accuracy: 0.9102308917197452 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29390059034251104; val_accuracy: 0.910828025477707 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29340739318045084; val_accuracy: 0.910828025477707 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.11; acc: 1.0
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29358831396812846; val_accuracy: 0.9102308917197452 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.84
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.88
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2930871111809448; val_accuracy: 0.9101313694267515 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29368145450664934; val_accuracy: 0.9103304140127388 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.84
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2929313230998577; val_accuracy: 0.9101313694267515 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29302797127206615; val_accuracy: 0.9106289808917197 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.81
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2928987673608361; val_accuracy: 0.9104299363057324 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.86
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.81
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2925435773506286; val_accuracy: 0.9107285031847133 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.83
Batch: 380; loss: 0.28; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29255923457965727; val_accuracy: 0.9110270700636943 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2924533782965818; val_accuracy: 0.9107285031847133 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2922637494411438; val_accuracy: 0.9104299363057324 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2922922443527325; val_accuracy: 0.9105294585987261 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29218654434202584; val_accuracy: 0.9110270700636943 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29220137090250187; val_accuracy: 0.9104299363057324 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2924036931744806; val_accuracy: 0.9106289808917197 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2920683447010578; val_accuracy: 0.9104299363057324 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.83
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.8
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29192614749928186; val_accuracy: 0.9110270700636943 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.84
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2920182198048777; val_accuracy: 0.9107285031847133 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.64; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.86
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29191406882682425; val_accuracy: 0.9112261146496815 

plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_800_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 446202
elements in E: 199210000
fraction nonzero: 0.002239857436875659
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.25; acc: 0.22
Batch: 60; loss: 2.22; acc: 0.41
Batch: 80; loss: 2.21; acc: 0.44
Batch: 100; loss: 2.17; acc: 0.41
Batch: 120; loss: 2.15; acc: 0.39
Batch: 140; loss: 2.06; acc: 0.52
Batch: 160; loss: 2.06; acc: 0.44
Batch: 180; loss: 2.01; acc: 0.56
Batch: 200; loss: 1.95; acc: 0.53
Batch: 220; loss: 1.82; acc: 0.66
Batch: 240; loss: 1.72; acc: 0.58
Batch: 260; loss: 1.64; acc: 0.7
Batch: 280; loss: 1.62; acc: 0.61
Batch: 300; loss: 1.34; acc: 0.78
Batch: 320; loss: 1.26; acc: 0.83
Batch: 340; loss: 1.08; acc: 0.8
Batch: 360; loss: 1.06; acc: 0.8
Batch: 380; loss: 0.92; acc: 0.75
Batch: 400; loss: 0.97; acc: 0.81
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.79; acc: 0.83
Batch: 460; loss: 0.85; acc: 0.75
Batch: 480; loss: 0.79; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.68; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.65; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.78
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 1.22; train_accuracy: 0.69 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.46623586369737696; val_accuracy: 0.8642515923566879 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.7; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.8
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.81
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.78
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.36276064082315773; val_accuracy: 0.8929140127388535 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.81
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.68; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32950445729645955; val_accuracy: 0.902468152866242 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31706083513748873; val_accuracy: 0.9057523885350318 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.84
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.83
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.84
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.3048085125673349; val_accuracy: 0.9092356687898089 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.3054627885769127; val_accuracy: 0.9091361464968153 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2976435882982555; val_accuracy: 0.9120222929936306 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2882102276821425; val_accuracy: 0.9158041401273885 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2856152951480097; val_accuracy: 0.9147093949044586 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.83
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2862549957813351; val_accuracy: 0.914609872611465 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2753971964595424; val_accuracy: 0.9159036624203821 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.83
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.27836847597151804; val_accuracy: 0.9153065286624203 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2740329989012639; val_accuracy: 0.9176950636942676 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2754936502048164; val_accuracy: 0.9160031847133758 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2727080782650003; val_accuracy: 0.9160031847133758 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.59; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.28374904244663607; val_accuracy: 0.9131170382165605 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.84
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26941108210071635; val_accuracy: 0.9169984076433121 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2684286218492468; val_accuracy: 0.9174960191082803 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.09; acc: 1.0
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2705023868638239; val_accuracy: 0.9163017515923567 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26925932205501635; val_accuracy: 0.9177945859872612 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26650174904116397; val_accuracy: 0.9164012738853503 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26835059296268565; val_accuracy: 0.917296974522293 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.51; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26587005854127516; val_accuracy: 0.9166998407643312 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26566024874426; val_accuracy: 0.917296974522293 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.84
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26631780602275185; val_accuracy: 0.9176950636942676 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26617985942466243; val_accuracy: 0.917296974522293 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2666239246820948; val_accuracy: 0.9184912420382165 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26489466037245313; val_accuracy: 0.9190883757961783 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.08; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26510001365451297; val_accuracy: 0.9189888535031847 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26418963072311347; val_accuracy: 0.9184912420382165 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.1; acc: 1.0
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2641852259588469; val_accuracy: 0.9192874203821656 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2633927353201019; val_accuracy: 0.919187898089172 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.91
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26362725012716215; val_accuracy: 0.9189888535031847 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26356331890176055; val_accuracy: 0.9182921974522293 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26342700943825353; val_accuracy: 0.9186902866242038 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2632108950501035; val_accuracy: 0.9194864649681529 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2626543571804739; val_accuracy: 0.9196855095541401 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.61; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2635069658659446; val_accuracy: 0.9188893312101911 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.86
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26317824219252656; val_accuracy: 0.9187898089171974 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26245207757137384; val_accuracy: 0.9197850318471338 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.86
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26276597784013506; val_accuracy: 0.9189888535031847 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26269276634712885; val_accuracy: 0.9188893312101911 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2631074835303103; val_accuracy: 0.9181926751592356 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26270132763370585; val_accuracy: 0.919187898089172 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26297416632911963; val_accuracy: 0.9184912420382165 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26264818724553296; val_accuracy: 0.919187898089172 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2626765680493443; val_accuracy: 0.9193869426751592 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.83
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26239940777990467; val_accuracy: 0.9194864649681529 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2624181040865221; val_accuracy: 0.919187898089172 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.86
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26241055354001414; val_accuracy: 0.9192874203821656 

plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_1000_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/MLP/2020-01-19 16:53:20/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4385827/slurm_script: line 25: --print_freq=20: command not found
