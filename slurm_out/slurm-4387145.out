model : lenet
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-20 16:25:19
nonzero elements in E: 444260
elements in E: 444260
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.31; acc: 0.16
Batch: 80; loss: 2.33; acc: 0.05
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.31; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.2
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.08
Batch: 220; loss: 2.33; acc: 0.05
Batch: 240; loss: 2.32; acc: 0.05
Batch: 260; loss: 2.31; acc: 0.12
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.31; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.12
Batch: 360; loss: 2.31; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.31; acc: 0.08
Batch: 420; loss: 2.32; acc: 0.05
Batch: 440; loss: 2.31; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.31; acc: 0.06
Batch: 500; loss: 2.31; acc: 0.09
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.29; acc: 0.12
Batch: 560; loss: 2.31; acc: 0.06
Batch: 580; loss: 2.31; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.06
Batch: 640; loss: 2.31; acc: 0.06
Batch: 660; loss: 2.3; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.06
Batch: 700; loss: 2.31; acc: 0.06
Batch: 720; loss: 2.31; acc: 0.05
Batch: 740; loss: 2.3; acc: 0.12
Batch: 760; loss: 2.31; acc: 0.11
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.31; train_accuracy: 0.09 

Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.31; acc: 0.05
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.02
Batch: 140; loss: 2.3; acc: 0.05
Val Epoch over. val_loss: 2.303206533383412; val_accuracy: 0.07951831210191083 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.31; acc: 0.05
Batch: 40; loss: 2.31; acc: 0.05
Batch: 60; loss: 2.31; acc: 0.02
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.31; acc: 0.0
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.31; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.31; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.31; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.31; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.31; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.31; acc: 0.09
Batch: 440; loss: 2.31; acc: 0.06
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.3; acc: 0.12
Batch: 700; loss: 2.29; acc: 0.06
Batch: 720; loss: 2.31; acc: 0.06
Batch: 740; loss: 2.3; acc: 0.06
Batch: 760; loss: 2.29; acc: 0.08
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.09 

Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.16
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.05
Val Epoch over. val_loss: 2.29977723747302; val_accuracy: 0.0973328025477707 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.31; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.09
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.14
Batch: 220; loss: 2.29; acc: 0.09
Batch: 240; loss: 2.29; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.03
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.2
Batch: 320; loss: 2.29; acc: 0.14
Batch: 340; loss: 2.29; acc: 0.14
Batch: 360; loss: 2.28; acc: 0.17
Batch: 380; loss: 2.31; acc: 0.09
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.08
Batch: 480; loss: 2.29; acc: 0.09
Batch: 500; loss: 2.28; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.28; acc: 0.19
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.31; acc: 0.08
Batch: 620; loss: 2.32; acc: 0.06
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.16
Batch: 700; loss: 2.3; acc: 0.06
Batch: 720; loss: 2.29; acc: 0.08
Batch: 740; loss: 2.31; acc: 0.08
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.27; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.16
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.27; acc: 0.08
Val Epoch over. val_loss: 2.2885574550385686; val_accuracy: 0.11176353503184713 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.31; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.27; acc: 0.17
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.25; acc: 0.19
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.26; acc: 0.17
Batch: 220; loss: 2.23; acc: 0.2
Batch: 240; loss: 2.27; acc: 0.12
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.26; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.05
Batch: 360; loss: 2.27; acc: 0.19
Batch: 380; loss: 2.27; acc: 0.19
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.32; acc: 0.11
Batch: 440; loss: 2.27; acc: 0.09
Batch: 460; loss: 2.29; acc: 0.19
Batch: 480; loss: 2.27; acc: 0.08
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.23; acc: 0.16
Batch: 540; loss: 2.28; acc: 0.17
Batch: 560; loss: 2.25; acc: 0.16
Batch: 580; loss: 2.29; acc: 0.06
Batch: 600; loss: 2.29; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.17
Batch: 640; loss: 2.32; acc: 0.08
Batch: 660; loss: 2.3; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.17
Batch: 700; loss: 2.26; acc: 0.17
Batch: 720; loss: 2.28; acc: 0.12
Batch: 740; loss: 2.26; acc: 0.16
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.29; acc: 0.06
Train Epoch over. train_loss: 2.28; train_accuracy: 0.12 

Batch: 0; loss: 2.27; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.26; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.19
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.12
Val Epoch over. val_loss: 2.279066791959629; val_accuracy: 0.1259952229299363 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.24; acc: 0.22
Batch: 20; loss: 2.26; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.26; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.28; acc: 0.09
Batch: 120; loss: 2.32; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.29; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.26; acc: 0.17
Batch: 280; loss: 2.25; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.19
Batch: 320; loss: 2.22; acc: 0.22
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.26; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.31; acc: 0.09
Batch: 440; loss: 2.26; acc: 0.12
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.24; acc: 0.2
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.27; acc: 0.09
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.26; acc: 0.16
Batch: 580; loss: 2.27; acc: 0.12
Batch: 600; loss: 2.27; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.35; acc: 0.0
Batch: 660; loss: 2.28; acc: 0.09
Batch: 680; loss: 2.3; acc: 0.14
Batch: 700; loss: 2.26; acc: 0.12
Batch: 720; loss: 2.27; acc: 0.08
Batch: 740; loss: 2.26; acc: 0.12
Batch: 760; loss: 2.26; acc: 0.09
Batch: 780; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.28; train_accuracy: 0.12 

Batch: 0; loss: 2.27; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.26; acc: 0.19
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.11
Val Epoch over. val_loss: 2.276129938234949; val_accuracy: 0.1201234076433121 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.24; acc: 0.17
Batch: 20; loss: 2.25; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.26; acc: 0.09
Batch: 100; loss: 2.22; acc: 0.16
Batch: 120; loss: 2.26; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.06
Batch: 160; loss: 2.27; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.27; acc: 0.14
Batch: 220; loss: 2.24; acc: 0.12
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.24; acc: 0.19
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.26; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.03
Batch: 360; loss: 2.24; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.26; acc: 0.14
Batch: 420; loss: 2.26; acc: 0.2
Batch: 440; loss: 2.28; acc: 0.05
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.09
Batch: 500; loss: 2.23; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.05
Batch: 540; loss: 2.29; acc: 0.06
Batch: 560; loss: 2.28; acc: 0.08
Batch: 580; loss: 2.26; acc: 0.08
Batch: 600; loss: 2.23; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.09
Batch: 640; loss: 2.29; acc: 0.06
Batch: 660; loss: 2.3; acc: 0.08
Batch: 680; loss: 2.3; acc: 0.08
Batch: 700; loss: 2.25; acc: 0.12
Batch: 720; loss: 2.25; acc: 0.12
Batch: 740; loss: 2.27; acc: 0.11
Batch: 760; loss: 2.22; acc: 0.22
Batch: 780; loss: 2.26; acc: 0.17
Train Epoch over. train_loss: 2.27; train_accuracy: 0.1 

Batch: 0; loss: 2.26; acc: 0.14
Batch: 20; loss: 2.24; acc: 0.11
Batch: 40; loss: 2.27; acc: 0.08
Batch: 60; loss: 2.23; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.16
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.09
Val Epoch over. val_loss: 2.2626172190259215; val_accuracy: 0.10380175159235669 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.02
Batch: 40; loss: 2.25; acc: 0.06
Batch: 60; loss: 2.28; acc: 0.09
Batch: 80; loss: 2.26; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.08
Batch: 120; loss: 2.26; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.08
Batch: 160; loss: 2.24; acc: 0.08
Batch: 180; loss: 2.27; acc: 0.09
Batch: 200; loss: 2.23; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.06
Batch: 240; loss: 2.27; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.22; acc: 0.11
Batch: 300; loss: 2.26; acc: 0.11
Batch: 320; loss: 2.31; acc: 0.06
Batch: 340; loss: 2.23; acc: 0.12
Batch: 360; loss: 2.23; acc: 0.09
Batch: 380; loss: 2.32; acc: 0.05
Batch: 400; loss: 2.23; acc: 0.05
Batch: 420; loss: 2.21; acc: 0.14
Batch: 440; loss: 2.26; acc: 0.11
Batch: 460; loss: 2.2; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.08
Batch: 500; loss: 2.26; acc: 0.11
Batch: 520; loss: 2.32; acc: 0.05
Batch: 540; loss: 2.26; acc: 0.11
Batch: 560; loss: 2.23; acc: 0.12
Batch: 580; loss: 2.2; acc: 0.09
Batch: 600; loss: 2.22; acc: 0.14
Batch: 620; loss: 2.19; acc: 0.12
Batch: 640; loss: 2.23; acc: 0.19
Batch: 660; loss: 2.2; acc: 0.14
Batch: 680; loss: 2.26; acc: 0.17
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.14
Batch: 740; loss: 2.2; acc: 0.19
Batch: 760; loss: 2.18; acc: 0.17
Batch: 780; loss: 2.26; acc: 0.16
Train Epoch over. train_loss: 2.25; train_accuracy: 0.1 

Batch: 0; loss: 2.23; acc: 0.08
Batch: 20; loss: 2.21; acc: 0.14
Batch: 40; loss: 2.14; acc: 0.16
Batch: 60; loss: 2.13; acc: 0.19
Batch: 80; loss: 2.16; acc: 0.2
Batch: 100; loss: 2.24; acc: 0.11
Batch: 120; loss: 2.18; acc: 0.17
Batch: 140; loss: 2.18; acc: 0.2
Val Epoch over. val_loss: 2.1982561964897593; val_accuracy: 0.1685907643312102 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.12
Batch: 20; loss: 2.28; acc: 0.11
Batch: 40; loss: 2.16; acc: 0.2
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.16; acc: 0.22
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.2; acc: 0.12
Batch: 140; loss: 2.16; acc: 0.16
Batch: 160; loss: 2.18; acc: 0.2
Batch: 180; loss: 2.13; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.16
Batch: 220; loss: 2.13; acc: 0.23
Batch: 240; loss: 2.14; acc: 0.22
Batch: 260; loss: 2.25; acc: 0.17
Batch: 280; loss: 2.19; acc: 0.19
Batch: 300; loss: 2.24; acc: 0.17
Batch: 320; loss: 2.25; acc: 0.19
Batch: 340; loss: 2.22; acc: 0.2
Batch: 360; loss: 2.3; acc: 0.06
Batch: 380; loss: 2.17; acc: 0.19
Batch: 400; loss: 2.19; acc: 0.22
Batch: 420; loss: 2.22; acc: 0.22
Batch: 440; loss: 2.18; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.23
Batch: 480; loss: 2.25; acc: 0.16
Batch: 500; loss: 2.2; acc: 0.25
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.15; acc: 0.2
Batch: 560; loss: 2.13; acc: 0.19
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.14; acc: 0.3
Batch: 620; loss: 2.23; acc: 0.11
Batch: 640; loss: 2.22; acc: 0.22
Batch: 660; loss: 2.23; acc: 0.19
Batch: 680; loss: 2.17; acc: 0.22
Batch: 700; loss: 2.23; acc: 0.17
Batch: 720; loss: 2.2; acc: 0.17
Batch: 740; loss: 2.24; acc: 0.17
Batch: 760; loss: 2.16; acc: 0.12
Batch: 780; loss: 2.17; acc: 0.2
Train Epoch over. train_loss: 2.19; train_accuracy: 0.2 

Batch: 0; loss: 2.21; acc: 0.2
Batch: 20; loss: 2.16; acc: 0.17
Batch: 40; loss: 2.08; acc: 0.25
Batch: 60; loss: 2.08; acc: 0.2
Batch: 80; loss: 2.13; acc: 0.3
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.18; acc: 0.27
Val Epoch over. val_loss: 2.1711535977709824; val_accuracy: 0.2213375796178344 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.23; acc: 0.23
Batch: 40; loss: 2.24; acc: 0.17
Batch: 60; loss: 2.2; acc: 0.25
Batch: 80; loss: 2.3; acc: 0.17
Batch: 100; loss: 2.14; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.25
Batch: 140; loss: 2.12; acc: 0.28
Batch: 160; loss: 2.25; acc: 0.16
Batch: 180; loss: 2.21; acc: 0.2
Batch: 200; loss: 2.11; acc: 0.22
Batch: 220; loss: 2.19; acc: 0.17
Batch: 240; loss: 2.05; acc: 0.19
Batch: 260; loss: 2.21; acc: 0.17
Batch: 280; loss: 2.18; acc: 0.14
Batch: 300; loss: 2.16; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.14
Batch: 340; loss: 2.18; acc: 0.22
Batch: 360; loss: 2.12; acc: 0.28
Batch: 380; loss: 2.12; acc: 0.27
Batch: 400; loss: 2.28; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.2
Batch: 440; loss: 2.08; acc: 0.38
Batch: 460; loss: 2.14; acc: 0.17
Batch: 480; loss: 2.11; acc: 0.28
Batch: 500; loss: 2.23; acc: 0.2
Batch: 520; loss: 2.19; acc: 0.2
Batch: 540; loss: 2.14; acc: 0.19
Batch: 560; loss: 2.12; acc: 0.23
Batch: 580; loss: 2.21; acc: 0.12
Batch: 600; loss: 2.22; acc: 0.17
Batch: 620; loss: 2.12; acc: 0.22
Batch: 640; loss: 2.19; acc: 0.25
Batch: 660; loss: 2.27; acc: 0.2
Batch: 680; loss: 2.17; acc: 0.16
Batch: 700; loss: 2.2; acc: 0.2
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.18; acc: 0.19
Batch: 760; loss: 2.15; acc: 0.23
Batch: 780; loss: 2.13; acc: 0.19
Train Epoch over. train_loss: 2.18; train_accuracy: 0.21 

Batch: 0; loss: 2.19; acc: 0.19
Batch: 20; loss: 2.14; acc: 0.22
Batch: 40; loss: 2.09; acc: 0.27
Batch: 60; loss: 2.09; acc: 0.17
Batch: 80; loss: 2.16; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.1685110770972673; val_accuracy: 0.21496815286624205 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.15; acc: 0.25
Batch: 20; loss: 2.07; acc: 0.27
Batch: 40; loss: 2.23; acc: 0.09
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.08; acc: 0.28
Batch: 100; loss: 2.27; acc: 0.16
Batch: 120; loss: 2.13; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.18; acc: 0.11
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.06; acc: 0.25
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.13; acc: 0.2
Batch: 280; loss: 2.15; acc: 0.23
Batch: 300; loss: 2.18; acc: 0.2
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.22; acc: 0.2
Batch: 360; loss: 2.32; acc: 0.17
Batch: 380; loss: 2.11; acc: 0.3
Batch: 400; loss: 2.18; acc: 0.22
Batch: 420; loss: 2.19; acc: 0.31
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.1; acc: 0.22
Batch: 480; loss: 2.13; acc: 0.25
Batch: 500; loss: 2.18; acc: 0.3
Batch: 520; loss: 2.13; acc: 0.23
Batch: 540; loss: 2.11; acc: 0.33
Batch: 560; loss: 2.29; acc: 0.14
Batch: 580; loss: 2.07; acc: 0.22
Batch: 600; loss: 2.14; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.17
Batch: 640; loss: 2.19; acc: 0.22
Batch: 660; loss: 2.18; acc: 0.2
Batch: 680; loss: 2.32; acc: 0.11
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.26; acc: 0.2
Batch: 740; loss: 2.2; acc: 0.22
Batch: 760; loss: 2.11; acc: 0.22
Batch: 780; loss: 2.19; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.25
Batch: 120; loss: 2.13; acc: 0.23
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1681541986526196; val_accuracy: 0.21248009554140126 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.12; acc: 0.23
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.24; acc: 0.22
Batch: 100; loss: 2.22; acc: 0.22
Batch: 120; loss: 2.25; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.15; acc: 0.17
Batch: 180; loss: 2.08; acc: 0.31
Batch: 200; loss: 2.22; acc: 0.19
Batch: 220; loss: 2.14; acc: 0.19
Batch: 240; loss: 2.26; acc: 0.11
Batch: 260; loss: 2.21; acc: 0.22
Batch: 280; loss: 2.07; acc: 0.31
Batch: 300; loss: 2.27; acc: 0.12
Batch: 320; loss: 2.14; acc: 0.2
Batch: 340; loss: 2.19; acc: 0.2
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.15; acc: 0.19
Batch: 400; loss: 2.14; acc: 0.22
Batch: 420; loss: 2.25; acc: 0.12
Batch: 440; loss: 2.04; acc: 0.3
Batch: 460; loss: 2.15; acc: 0.14
Batch: 480; loss: 2.18; acc: 0.22
Batch: 500; loss: 2.2; acc: 0.14
Batch: 520; loss: 2.16; acc: 0.2
Batch: 540; loss: 2.11; acc: 0.2
Batch: 560; loss: 2.22; acc: 0.23
Batch: 580; loss: 2.27; acc: 0.16
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.15; acc: 0.25
Batch: 640; loss: 2.13; acc: 0.23
Batch: 660; loss: 2.19; acc: 0.22
Batch: 680; loss: 2.09; acc: 0.27
Batch: 700; loss: 2.15; acc: 0.16
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.14; acc: 0.17
Batch: 760; loss: 2.27; acc: 0.17
Batch: 780; loss: 2.22; acc: 0.11
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.28
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1678676810234214; val_accuracy: 0.2125796178343949 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.25
Batch: 40; loss: 2.22; acc: 0.17
Batch: 60; loss: 2.07; acc: 0.23
Batch: 80; loss: 2.15; acc: 0.23
Batch: 100; loss: 2.22; acc: 0.12
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.17; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.25
Batch: 180; loss: 2.16; acc: 0.25
Batch: 200; loss: 2.17; acc: 0.22
Batch: 220; loss: 2.1; acc: 0.25
Batch: 240; loss: 2.24; acc: 0.27
Batch: 260; loss: 2.18; acc: 0.12
Batch: 280; loss: 2.22; acc: 0.17
Batch: 300; loss: 2.14; acc: 0.22
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.18; acc: 0.12
Batch: 360; loss: 2.24; acc: 0.08
Batch: 380; loss: 2.23; acc: 0.2
Batch: 400; loss: 2.16; acc: 0.2
Batch: 420; loss: 2.15; acc: 0.2
Batch: 440; loss: 2.26; acc: 0.06
Batch: 460; loss: 2.11; acc: 0.25
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.15; acc: 0.19
Batch: 520; loss: 2.15; acc: 0.2
Batch: 540; loss: 2.21; acc: 0.12
Batch: 560; loss: 2.15; acc: 0.23
Batch: 580; loss: 2.21; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.19
Batch: 620; loss: 2.21; acc: 0.14
Batch: 640; loss: 2.15; acc: 0.14
Batch: 660; loss: 2.13; acc: 0.14
Batch: 680; loss: 2.21; acc: 0.22
Batch: 700; loss: 2.15; acc: 0.25
Batch: 720; loss: 2.17; acc: 0.11
Batch: 740; loss: 2.07; acc: 0.31
Batch: 760; loss: 2.14; acc: 0.23
Batch: 780; loss: 2.11; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.17
Batch: 40; loss: 2.1; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.1678778897425173; val_accuracy: 0.2085987261146497 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.33; acc: 0.06
Batch: 20; loss: 2.15; acc: 0.16
Batch: 40; loss: 2.23; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.17
Batch: 80; loss: 2.24; acc: 0.14
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.06; acc: 0.22
Batch: 140; loss: 2.13; acc: 0.19
Batch: 160; loss: 2.16; acc: 0.25
Batch: 180; loss: 2.33; acc: 0.14
Batch: 200; loss: 2.15; acc: 0.19
Batch: 220; loss: 2.17; acc: 0.19
Batch: 240; loss: 2.21; acc: 0.23
Batch: 260; loss: 2.16; acc: 0.19
Batch: 280; loss: 2.16; acc: 0.16
Batch: 300; loss: 2.07; acc: 0.23
Batch: 320; loss: 2.14; acc: 0.14
Batch: 340; loss: 2.17; acc: 0.14
Batch: 360; loss: 2.19; acc: 0.25
Batch: 380; loss: 2.11; acc: 0.27
Batch: 400; loss: 2.21; acc: 0.14
Batch: 420; loss: 2.21; acc: 0.16
Batch: 440; loss: 2.2; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.19
Batch: 480; loss: 2.25; acc: 0.2
Batch: 500; loss: 2.17; acc: 0.19
Batch: 520; loss: 2.24; acc: 0.14
Batch: 540; loss: 2.07; acc: 0.27
Batch: 560; loss: 2.21; acc: 0.14
Batch: 580; loss: 2.14; acc: 0.23
Batch: 600; loss: 2.07; acc: 0.3
Batch: 620; loss: 2.16; acc: 0.23
Batch: 640; loss: 2.2; acc: 0.23
Batch: 660; loss: 2.12; acc: 0.2
Batch: 680; loss: 2.16; acc: 0.25
Batch: 700; loss: 2.14; acc: 0.31
Batch: 720; loss: 2.12; acc: 0.22
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.16; acc: 0.16
Batch: 780; loss: 2.16; acc: 0.3
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.13; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.168060724902305; val_accuracy: 0.21068869426751594 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.13; acc: 0.28
Batch: 20; loss: 2.03; acc: 0.23
Batch: 40; loss: 2.16; acc: 0.23
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.09; acc: 0.27
Batch: 100; loss: 2.11; acc: 0.2
Batch: 120; loss: 2.25; acc: 0.12
Batch: 140; loss: 2.12; acc: 0.2
Batch: 160; loss: 2.14; acc: 0.2
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.21; acc: 0.17
Batch: 220; loss: 2.26; acc: 0.17
Batch: 240; loss: 2.26; acc: 0.22
Batch: 260; loss: 2.24; acc: 0.23
Batch: 280; loss: 2.13; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.12
Batch: 320; loss: 2.18; acc: 0.17
Batch: 340; loss: 2.15; acc: 0.27
Batch: 360; loss: 2.14; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.11
Batch: 400; loss: 2.13; acc: 0.2
Batch: 420; loss: 2.2; acc: 0.19
Batch: 440; loss: 2.15; acc: 0.09
Batch: 460; loss: 2.24; acc: 0.22
Batch: 480; loss: 2.15; acc: 0.2
Batch: 500; loss: 2.15; acc: 0.27
Batch: 520; loss: 2.24; acc: 0.23
Batch: 540; loss: 2.13; acc: 0.19
Batch: 560; loss: 2.12; acc: 0.17
Batch: 580; loss: 2.22; acc: 0.17
Batch: 600; loss: 2.07; acc: 0.28
Batch: 620; loss: 2.21; acc: 0.17
Batch: 640; loss: 2.19; acc: 0.12
Batch: 660; loss: 2.3; acc: 0.19
Batch: 680; loss: 2.2; acc: 0.22
Batch: 700; loss: 2.16; acc: 0.23
Batch: 720; loss: 2.24; acc: 0.11
Batch: 740; loss: 2.15; acc: 0.22
Batch: 760; loss: 2.11; acc: 0.28
Batch: 780; loss: 2.21; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.1681609685253944; val_accuracy: 0.21297770700636942 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.22
Batch: 20; loss: 2.22; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.22
Batch: 80; loss: 2.15; acc: 0.28
Batch: 100; loss: 2.1; acc: 0.28
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.17
Batch: 160; loss: 2.23; acc: 0.22
Batch: 180; loss: 2.15; acc: 0.23
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.12; acc: 0.17
Batch: 240; loss: 2.09; acc: 0.2
Batch: 260; loss: 2.15; acc: 0.28
Batch: 280; loss: 2.16; acc: 0.17
Batch: 300; loss: 2.13; acc: 0.27
Batch: 320; loss: 2.14; acc: 0.17
Batch: 340; loss: 2.19; acc: 0.17
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.2; acc: 0.14
Batch: 420; loss: 2.19; acc: 0.25
Batch: 440; loss: 2.15; acc: 0.23
Batch: 460; loss: 2.26; acc: 0.17
Batch: 480; loss: 2.19; acc: 0.19
Batch: 500; loss: 2.18; acc: 0.2
Batch: 520; loss: 2.13; acc: 0.14
Batch: 540; loss: 2.09; acc: 0.23
Batch: 560; loss: 2.2; acc: 0.19
Batch: 580; loss: 2.23; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.17
Batch: 620; loss: 2.26; acc: 0.19
Batch: 640; loss: 2.18; acc: 0.14
Batch: 660; loss: 2.18; acc: 0.19
Batch: 680; loss: 2.12; acc: 0.22
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.14; acc: 0.28
Batch: 740; loss: 2.16; acc: 0.16
Batch: 760; loss: 2.19; acc: 0.3
Batch: 780; loss: 2.29; acc: 0.16
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.1679183874919916; val_accuracy: 0.2135748407643312 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.1; acc: 0.16
Batch: 60; loss: 2.12; acc: 0.23
Batch: 80; loss: 2.25; acc: 0.2
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.23; acc: 0.2
Batch: 140; loss: 2.19; acc: 0.19
Batch: 160; loss: 2.21; acc: 0.12
Batch: 180; loss: 2.21; acc: 0.23
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.12; acc: 0.2
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.19
Batch: 280; loss: 2.12; acc: 0.27
Batch: 300; loss: 2.15; acc: 0.16
Batch: 320; loss: 2.12; acc: 0.19
Batch: 340; loss: 2.12; acc: 0.17
Batch: 360; loss: 2.18; acc: 0.22
Batch: 380; loss: 2.16; acc: 0.17
Batch: 400; loss: 2.26; acc: 0.14
Batch: 420; loss: 2.11; acc: 0.33
Batch: 440; loss: 2.12; acc: 0.33
Batch: 460; loss: 2.15; acc: 0.23
Batch: 480; loss: 2.1; acc: 0.28
Batch: 500; loss: 2.15; acc: 0.22
Batch: 520; loss: 2.12; acc: 0.23
Batch: 540; loss: 2.26; acc: 0.16
Batch: 560; loss: 2.15; acc: 0.2
Batch: 580; loss: 2.16; acc: 0.2
Batch: 600; loss: 2.11; acc: 0.23
Batch: 620; loss: 2.02; acc: 0.27
Batch: 640; loss: 2.14; acc: 0.19
Batch: 660; loss: 2.22; acc: 0.16
Batch: 680; loss: 2.21; acc: 0.2
Batch: 700; loss: 2.18; acc: 0.28
Batch: 720; loss: 2.02; acc: 0.28
Batch: 740; loss: 2.17; acc: 0.23
Batch: 760; loss: 2.14; acc: 0.17
Batch: 780; loss: 2.21; acc: 0.14
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.19
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1677895343987044; val_accuracy: 0.20879777070063693 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.19; acc: 0.19
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.13; acc: 0.25
Batch: 100; loss: 2.18; acc: 0.16
Batch: 120; loss: 2.18; acc: 0.2
Batch: 140; loss: 2.16; acc: 0.25
Batch: 160; loss: 2.18; acc: 0.23
Batch: 180; loss: 2.13; acc: 0.31
Batch: 200; loss: 2.17; acc: 0.23
Batch: 220; loss: 2.1; acc: 0.17
Batch: 240; loss: 2.19; acc: 0.27
Batch: 260; loss: 2.11; acc: 0.23
Batch: 280; loss: 2.25; acc: 0.17
Batch: 300; loss: 2.12; acc: 0.25
Batch: 320; loss: 2.14; acc: 0.17
Batch: 340; loss: 2.07; acc: 0.25
Batch: 360; loss: 2.17; acc: 0.16
Batch: 380; loss: 2.14; acc: 0.27
Batch: 400; loss: 2.13; acc: 0.2
Batch: 420; loss: 2.15; acc: 0.17
Batch: 440; loss: 2.12; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.19
Batch: 480; loss: 2.15; acc: 0.27
Batch: 500; loss: 2.14; acc: 0.2
Batch: 520; loss: 2.22; acc: 0.12
Batch: 540; loss: 2.13; acc: 0.27
Batch: 560; loss: 2.22; acc: 0.17
Batch: 580; loss: 2.28; acc: 0.17
Batch: 600; loss: 2.34; acc: 0.11
Batch: 620; loss: 2.25; acc: 0.17
Batch: 640; loss: 2.2; acc: 0.19
Batch: 660; loss: 2.13; acc: 0.19
Batch: 680; loss: 2.08; acc: 0.25
Batch: 700; loss: 2.12; acc: 0.2
Batch: 720; loss: 2.2; acc: 0.12
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.12; acc: 0.23
Batch: 780; loss: 2.18; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679105363833675; val_accuracy: 0.20939490445859874 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.23; acc: 0.16
Batch: 60; loss: 2.19; acc: 0.28
Batch: 80; loss: 2.1; acc: 0.2
Batch: 100; loss: 2.11; acc: 0.22
Batch: 120; loss: 2.22; acc: 0.19
Batch: 140; loss: 2.2; acc: 0.19
Batch: 160; loss: 2.18; acc: 0.2
Batch: 180; loss: 2.15; acc: 0.27
Batch: 200; loss: 2.27; acc: 0.2
Batch: 220; loss: 2.22; acc: 0.14
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.22; acc: 0.16
Batch: 280; loss: 2.06; acc: 0.33
Batch: 300; loss: 2.24; acc: 0.23
Batch: 320; loss: 2.17; acc: 0.27
Batch: 340; loss: 2.11; acc: 0.23
Batch: 360; loss: 2.12; acc: 0.2
Batch: 380; loss: 2.09; acc: 0.25
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.24; acc: 0.19
Batch: 440; loss: 2.13; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.14
Batch: 480; loss: 2.14; acc: 0.2
Batch: 500; loss: 2.17; acc: 0.19
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.17; acc: 0.2
Batch: 560; loss: 2.04; acc: 0.38
Batch: 580; loss: 2.34; acc: 0.11
Batch: 600; loss: 2.07; acc: 0.28
Batch: 620; loss: 2.21; acc: 0.17
Batch: 640; loss: 2.22; acc: 0.17
Batch: 660; loss: 2.15; acc: 0.25
Batch: 680; loss: 2.18; acc: 0.2
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.18; acc: 0.17
Batch: 740; loss: 2.16; acc: 0.14
Batch: 760; loss: 2.15; acc: 0.2
Batch: 780; loss: 2.12; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1678381285090356; val_accuracy: 0.20979299363057324 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.13; acc: 0.22
Batch: 20; loss: 2.12; acc: 0.3
Batch: 40; loss: 2.21; acc: 0.25
Batch: 60; loss: 2.15; acc: 0.2
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.2; acc: 0.2
Batch: 120; loss: 2.13; acc: 0.28
Batch: 140; loss: 2.17; acc: 0.25
Batch: 160; loss: 2.18; acc: 0.19
Batch: 180; loss: 2.16; acc: 0.22
Batch: 200; loss: 2.29; acc: 0.14
Batch: 220; loss: 2.25; acc: 0.23
Batch: 240; loss: 2.23; acc: 0.16
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.08; acc: 0.28
Batch: 300; loss: 2.14; acc: 0.22
Batch: 320; loss: 2.1; acc: 0.25
Batch: 340; loss: 2.25; acc: 0.23
Batch: 360; loss: 2.12; acc: 0.22
Batch: 380; loss: 2.06; acc: 0.2
Batch: 400; loss: 2.2; acc: 0.12
Batch: 420; loss: 2.19; acc: 0.19
Batch: 440; loss: 2.1; acc: 0.33
Batch: 460; loss: 2.09; acc: 0.22
Batch: 480; loss: 2.18; acc: 0.2
Batch: 500; loss: 2.22; acc: 0.2
Batch: 520; loss: 2.23; acc: 0.16
Batch: 540; loss: 2.24; acc: 0.14
Batch: 560; loss: 2.36; acc: 0.16
Batch: 580; loss: 2.18; acc: 0.28
Batch: 600; loss: 2.21; acc: 0.16
Batch: 620; loss: 2.21; acc: 0.22
Batch: 640; loss: 2.2; acc: 0.2
Batch: 660; loss: 2.25; acc: 0.14
Batch: 680; loss: 2.34; acc: 0.11
Batch: 700; loss: 2.12; acc: 0.23
Batch: 720; loss: 2.18; acc: 0.17
Batch: 740; loss: 2.19; acc: 0.17
Batch: 760; loss: 2.18; acc: 0.22
Batch: 780; loss: 2.16; acc: 0.22
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.168005932668212; val_accuracy: 0.21377388535031847 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.12; acc: 0.27
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.21; acc: 0.17
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.35; acc: 0.09
Batch: 120; loss: 2.24; acc: 0.2
Batch: 140; loss: 2.22; acc: 0.14
Batch: 160; loss: 2.14; acc: 0.25
Batch: 180; loss: 2.15; acc: 0.27
Batch: 200; loss: 2.18; acc: 0.17
Batch: 220; loss: 2.04; acc: 0.34
Batch: 240; loss: 2.15; acc: 0.2
Batch: 260; loss: 2.25; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.22
Batch: 320; loss: 2.17; acc: 0.23
Batch: 340; loss: 2.23; acc: 0.11
Batch: 360; loss: 2.17; acc: 0.2
Batch: 380; loss: 2.18; acc: 0.17
Batch: 400; loss: 2.21; acc: 0.16
Batch: 420; loss: 2.09; acc: 0.3
Batch: 440; loss: 2.18; acc: 0.16
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.15; acc: 0.27
Batch: 500; loss: 2.13; acc: 0.23
Batch: 520; loss: 2.21; acc: 0.12
Batch: 540; loss: 2.15; acc: 0.25
Batch: 560; loss: 2.16; acc: 0.25
Batch: 580; loss: 2.27; acc: 0.09
Batch: 600; loss: 2.17; acc: 0.17
Batch: 620; loss: 2.13; acc: 0.23
Batch: 640; loss: 2.23; acc: 0.23
Batch: 660; loss: 2.13; acc: 0.2
Batch: 680; loss: 2.21; acc: 0.17
Batch: 700; loss: 2.16; acc: 0.22
Batch: 720; loss: 2.26; acc: 0.12
Batch: 740; loss: 2.22; acc: 0.17
Batch: 760; loss: 2.17; acc: 0.22
Batch: 780; loss: 2.15; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.22
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.22
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167780809341722; val_accuracy: 0.2073049363057325 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.06; acc: 0.31
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.12; acc: 0.17
Batch: 80; loss: 2.22; acc: 0.16
Batch: 100; loss: 2.18; acc: 0.23
Batch: 120; loss: 2.18; acc: 0.28
Batch: 140; loss: 2.19; acc: 0.23
Batch: 160; loss: 2.14; acc: 0.27
Batch: 180; loss: 2.26; acc: 0.16
Batch: 200; loss: 2.08; acc: 0.27
Batch: 220; loss: 2.25; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.1; acc: 0.19
Batch: 280; loss: 2.22; acc: 0.2
Batch: 300; loss: 2.14; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.23
Batch: 340; loss: 2.2; acc: 0.19
Batch: 360; loss: 2.19; acc: 0.17
Batch: 380; loss: 2.16; acc: 0.14
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.33; acc: 0.09
Batch: 440; loss: 2.16; acc: 0.2
Batch: 460; loss: 2.11; acc: 0.22
Batch: 480; loss: 2.18; acc: 0.22
Batch: 500; loss: 2.13; acc: 0.2
Batch: 520; loss: 2.18; acc: 0.19
Batch: 540; loss: 2.14; acc: 0.22
Batch: 560; loss: 2.15; acc: 0.2
Batch: 580; loss: 2.21; acc: 0.2
Batch: 600; loss: 2.18; acc: 0.22
Batch: 620; loss: 2.13; acc: 0.22
Batch: 640; loss: 2.23; acc: 0.22
Batch: 660; loss: 2.14; acc: 0.22
Batch: 680; loss: 2.22; acc: 0.16
Batch: 700; loss: 2.23; acc: 0.23
Batch: 720; loss: 2.2; acc: 0.17
Batch: 740; loss: 2.23; acc: 0.12
Batch: 760; loss: 2.13; acc: 0.17
Batch: 780; loss: 2.26; acc: 0.12
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167996701161573; val_accuracy: 0.21238057324840764 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.16; acc: 0.25
Batch: 20; loss: 2.19; acc: 0.19
Batch: 40; loss: 2.03; acc: 0.28
Batch: 60; loss: 2.16; acc: 0.25
Batch: 80; loss: 2.19; acc: 0.2
Batch: 100; loss: 2.1; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.21; acc: 0.14
Batch: 160; loss: 2.12; acc: 0.2
Batch: 180; loss: 2.27; acc: 0.16
Batch: 200; loss: 2.06; acc: 0.25
Batch: 220; loss: 2.22; acc: 0.2
Batch: 240; loss: 2.18; acc: 0.23
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.18; acc: 0.2
Batch: 300; loss: 2.18; acc: 0.23
Batch: 320; loss: 2.19; acc: 0.19
Batch: 340; loss: 2.13; acc: 0.25
Batch: 360; loss: 2.19; acc: 0.27
Batch: 380; loss: 2.22; acc: 0.23
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.23
Batch: 440; loss: 2.19; acc: 0.17
Batch: 460; loss: 2.17; acc: 0.2
Batch: 480; loss: 2.23; acc: 0.25
Batch: 500; loss: 2.18; acc: 0.22
Batch: 520; loss: 2.23; acc: 0.17
Batch: 540; loss: 2.21; acc: 0.16
Batch: 560; loss: 2.26; acc: 0.19
Batch: 580; loss: 2.16; acc: 0.28
Batch: 600; loss: 2.22; acc: 0.2
Batch: 620; loss: 2.25; acc: 0.19
Batch: 640; loss: 2.04; acc: 0.28
Batch: 660; loss: 2.28; acc: 0.16
Batch: 680; loss: 2.16; acc: 0.12
Batch: 700; loss: 2.23; acc: 0.16
Batch: 720; loss: 2.11; acc: 0.22
Batch: 740; loss: 2.1; acc: 0.2
Batch: 760; loss: 2.29; acc: 0.12
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.19
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167833708653784; val_accuracy: 0.21009156050955413 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.12; acc: 0.23
Batch: 40; loss: 2.14; acc: 0.17
Batch: 60; loss: 2.2; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.08
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 2.18; acc: 0.17
Batch: 140; loss: 2.21; acc: 0.19
Batch: 160; loss: 2.05; acc: 0.27
Batch: 180; loss: 2.22; acc: 0.11
Batch: 200; loss: 2.17; acc: 0.16
Batch: 220; loss: 2.2; acc: 0.22
Batch: 240; loss: 2.09; acc: 0.23
Batch: 260; loss: 2.12; acc: 0.23
Batch: 280; loss: 2.23; acc: 0.22
Batch: 300; loss: 2.12; acc: 0.22
Batch: 320; loss: 2.21; acc: 0.19
Batch: 340; loss: 2.24; acc: 0.16
Batch: 360; loss: 2.1; acc: 0.25
Batch: 380; loss: 2.06; acc: 0.28
Batch: 400; loss: 2.18; acc: 0.3
Batch: 420; loss: 2.18; acc: 0.12
Batch: 440; loss: 2.12; acc: 0.25
Batch: 460; loss: 2.15; acc: 0.16
Batch: 480; loss: 2.27; acc: 0.14
Batch: 500; loss: 2.1; acc: 0.23
Batch: 520; loss: 2.1; acc: 0.22
Batch: 540; loss: 2.24; acc: 0.11
Batch: 560; loss: 2.23; acc: 0.14
Batch: 580; loss: 2.15; acc: 0.22
Batch: 600; loss: 2.11; acc: 0.12
Batch: 620; loss: 2.15; acc: 0.22
Batch: 640; loss: 2.24; acc: 0.09
Batch: 660; loss: 2.16; acc: 0.19
Batch: 680; loss: 2.12; acc: 0.27
Batch: 700; loss: 2.03; acc: 0.33
Batch: 720; loss: 2.12; acc: 0.25
Batch: 740; loss: 2.08; acc: 0.31
Batch: 760; loss: 2.2; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167927394247359; val_accuracy: 0.21347531847133758 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.22; acc: 0.16
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.14; acc: 0.22
Batch: 80; loss: 2.2; acc: 0.16
Batch: 100; loss: 2.16; acc: 0.23
Batch: 120; loss: 2.19; acc: 0.12
Batch: 140; loss: 2.19; acc: 0.28
Batch: 160; loss: 2.1; acc: 0.23
Batch: 180; loss: 2.14; acc: 0.2
Batch: 200; loss: 2.13; acc: 0.23
Batch: 220; loss: 2.02; acc: 0.25
Batch: 240; loss: 2.12; acc: 0.16
Batch: 260; loss: 2.2; acc: 0.2
Batch: 280; loss: 2.13; acc: 0.25
Batch: 300; loss: 2.21; acc: 0.16
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.25
Batch: 360; loss: 2.26; acc: 0.17
Batch: 380; loss: 2.14; acc: 0.19
Batch: 400; loss: 2.14; acc: 0.2
Batch: 420; loss: 2.21; acc: 0.17
Batch: 440; loss: 2.21; acc: 0.17
Batch: 460; loss: 2.24; acc: 0.17
Batch: 480; loss: 2.16; acc: 0.19
Batch: 500; loss: 2.09; acc: 0.23
Batch: 520; loss: 2.25; acc: 0.22
Batch: 540; loss: 2.1; acc: 0.3
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.2
Batch: 600; loss: 2.15; acc: 0.25
Batch: 620; loss: 2.11; acc: 0.25
Batch: 640; loss: 2.22; acc: 0.22
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.18; acc: 0.22
Batch: 700; loss: 2.11; acc: 0.23
Batch: 720; loss: 2.19; acc: 0.25
Batch: 740; loss: 2.14; acc: 0.22
Batch: 760; loss: 2.16; acc: 0.22
Batch: 780; loss: 2.22; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.1679824917179764; val_accuracy: 0.21367436305732485 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.07; acc: 0.23
Batch: 60; loss: 2.27; acc: 0.16
Batch: 80; loss: 2.16; acc: 0.23
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.16; acc: 0.2
Batch: 140; loss: 2.18; acc: 0.2
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.15; acc: 0.19
Batch: 200; loss: 2.05; acc: 0.27
Batch: 220; loss: 2.33; acc: 0.16
Batch: 240; loss: 2.07; acc: 0.33
Batch: 260; loss: 2.21; acc: 0.25
Batch: 280; loss: 2.07; acc: 0.27
Batch: 300; loss: 2.14; acc: 0.3
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.12; acc: 0.2
Batch: 360; loss: 2.14; acc: 0.27
Batch: 380; loss: 2.05; acc: 0.25
Batch: 400; loss: 2.12; acc: 0.27
Batch: 420; loss: 2.18; acc: 0.17
Batch: 440; loss: 2.16; acc: 0.23
Batch: 460; loss: 2.22; acc: 0.16
Batch: 480; loss: 2.13; acc: 0.19
Batch: 500; loss: 2.16; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.3
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.09; acc: 0.22
Batch: 580; loss: 2.21; acc: 0.19
Batch: 600; loss: 2.18; acc: 0.23
Batch: 620; loss: 2.26; acc: 0.2
Batch: 640; loss: 2.14; acc: 0.22
Batch: 660; loss: 2.06; acc: 0.22
Batch: 680; loss: 2.24; acc: 0.28
Batch: 700; loss: 2.14; acc: 0.16
Batch: 720; loss: 2.19; acc: 0.23
Batch: 740; loss: 2.15; acc: 0.25
Batch: 760; loss: 2.16; acc: 0.23
Batch: 780; loss: 2.12; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679380615805366; val_accuracy: 0.2105891719745223 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.1; acc: 0.25
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.08; acc: 0.28
Batch: 60; loss: 2.19; acc: 0.2
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.17
Batch: 140; loss: 2.24; acc: 0.2
Batch: 160; loss: 2.09; acc: 0.25
Batch: 180; loss: 2.14; acc: 0.22
Batch: 200; loss: 2.14; acc: 0.22
Batch: 220; loss: 2.21; acc: 0.12
Batch: 240; loss: 2.2; acc: 0.17
Batch: 260; loss: 2.23; acc: 0.17
Batch: 280; loss: 2.24; acc: 0.22
Batch: 300; loss: 2.13; acc: 0.27
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.09; acc: 0.3
Batch: 360; loss: 2.16; acc: 0.19
Batch: 380; loss: 2.22; acc: 0.27
Batch: 400; loss: 2.13; acc: 0.2
Batch: 420; loss: 2.08; acc: 0.19
Batch: 440; loss: 2.14; acc: 0.31
Batch: 460; loss: 2.18; acc: 0.27
Batch: 480; loss: 2.21; acc: 0.19
Batch: 500; loss: 2.12; acc: 0.25
Batch: 520; loss: 2.22; acc: 0.22
Batch: 540; loss: 2.16; acc: 0.23
Batch: 560; loss: 2.21; acc: 0.19
Batch: 580; loss: 2.19; acc: 0.17
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.09; acc: 0.28
Batch: 640; loss: 2.25; acc: 0.19
Batch: 660; loss: 2.25; acc: 0.14
Batch: 680; loss: 2.22; acc: 0.17
Batch: 700; loss: 2.09; acc: 0.3
Batch: 720; loss: 2.14; acc: 0.27
Batch: 740; loss: 2.15; acc: 0.22
Batch: 760; loss: 2.22; acc: 0.25
Batch: 780; loss: 2.18; acc: 0.16
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.13; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679748365074207; val_accuracy: 0.21068869426751594 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.08; acc: 0.28
Batch: 20; loss: 2.11; acc: 0.28
Batch: 40; loss: 2.1; acc: 0.2
Batch: 60; loss: 2.19; acc: 0.17
Batch: 80; loss: 2.25; acc: 0.12
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.24; acc: 0.17
Batch: 140; loss: 2.13; acc: 0.23
Batch: 160; loss: 2.09; acc: 0.3
Batch: 180; loss: 2.13; acc: 0.22
Batch: 200; loss: 2.18; acc: 0.2
Batch: 220; loss: 2.21; acc: 0.19
Batch: 240; loss: 2.17; acc: 0.2
Batch: 260; loss: 2.17; acc: 0.19
Batch: 280; loss: 2.23; acc: 0.23
Batch: 300; loss: 2.27; acc: 0.16
Batch: 320; loss: 2.1; acc: 0.31
Batch: 340; loss: 2.16; acc: 0.23
Batch: 360; loss: 2.27; acc: 0.19
Batch: 380; loss: 2.16; acc: 0.31
Batch: 400; loss: 2.24; acc: 0.17
Batch: 420; loss: 2.11; acc: 0.23
Batch: 440; loss: 2.2; acc: 0.2
Batch: 460; loss: 2.21; acc: 0.27
Batch: 480; loss: 2.18; acc: 0.23
Batch: 500; loss: 2.12; acc: 0.2
Batch: 520; loss: 2.15; acc: 0.14
Batch: 540; loss: 2.22; acc: 0.14
Batch: 560; loss: 2.28; acc: 0.19
Batch: 580; loss: 2.19; acc: 0.22
Batch: 600; loss: 2.07; acc: 0.23
Batch: 620; loss: 2.11; acc: 0.23
Batch: 640; loss: 2.22; acc: 0.17
Batch: 660; loss: 2.22; acc: 0.2
Batch: 680; loss: 2.07; acc: 0.31
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.15; acc: 0.27
Batch: 740; loss: 2.2; acc: 0.25
Batch: 760; loss: 2.11; acc: 0.22
Batch: 780; loss: 2.21; acc: 0.25
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679679975388155; val_accuracy: 0.21208200636942676 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.26; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.14
Batch: 60; loss: 2.05; acc: 0.33
Batch: 80; loss: 2.09; acc: 0.23
Batch: 100; loss: 2.21; acc: 0.28
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.17; acc: 0.14
Batch: 160; loss: 2.16; acc: 0.22
Batch: 180; loss: 2.1; acc: 0.27
Batch: 200; loss: 2.17; acc: 0.2
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 2.17; acc: 0.14
Batch: 260; loss: 2.05; acc: 0.3
Batch: 280; loss: 2.22; acc: 0.2
Batch: 300; loss: 2.16; acc: 0.2
Batch: 320; loss: 2.11; acc: 0.22
Batch: 340; loss: 2.05; acc: 0.3
Batch: 360; loss: 2.14; acc: 0.19
Batch: 380; loss: 2.2; acc: 0.22
Batch: 400; loss: 2.23; acc: 0.14
Batch: 420; loss: 2.17; acc: 0.16
Batch: 440; loss: 2.14; acc: 0.22
Batch: 460; loss: 2.25; acc: 0.11
Batch: 480; loss: 2.12; acc: 0.2
Batch: 500; loss: 2.21; acc: 0.16
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.13; acc: 0.17
Batch: 560; loss: 2.23; acc: 0.17
Batch: 580; loss: 2.13; acc: 0.31
Batch: 600; loss: 2.13; acc: 0.2
Batch: 620; loss: 2.19; acc: 0.2
Batch: 640; loss: 2.17; acc: 0.22
Batch: 660; loss: 2.12; acc: 0.22
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.25; acc: 0.2
Batch: 720; loss: 2.04; acc: 0.31
Batch: 740; loss: 2.1; acc: 0.22
Batch: 760; loss: 2.25; acc: 0.19
Batch: 780; loss: 2.22; acc: 0.16
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679170291135264; val_accuracy: 0.21108678343949044 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.1; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.2
Batch: 80; loss: 2.21; acc: 0.14
Batch: 100; loss: 2.14; acc: 0.27
Batch: 120; loss: 2.15; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.17
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.18; acc: 0.23
Batch: 200; loss: 2.16; acc: 0.19
Batch: 220; loss: 2.1; acc: 0.31
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.17
Batch: 280; loss: 2.16; acc: 0.22
Batch: 300; loss: 2.2; acc: 0.17
Batch: 320; loss: 2.26; acc: 0.11
Batch: 340; loss: 2.12; acc: 0.19
Batch: 360; loss: 2.21; acc: 0.16
Batch: 380; loss: 2.1; acc: 0.28
Batch: 400; loss: 2.21; acc: 0.19
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.11; acc: 0.2
Batch: 460; loss: 2.15; acc: 0.19
Batch: 480; loss: 2.15; acc: 0.17
Batch: 500; loss: 2.13; acc: 0.19
Batch: 520; loss: 2.13; acc: 0.17
Batch: 540; loss: 2.13; acc: 0.19
Batch: 560; loss: 2.16; acc: 0.27
Batch: 580; loss: 2.08; acc: 0.28
Batch: 600; loss: 2.09; acc: 0.28
Batch: 620; loss: 2.12; acc: 0.28
Batch: 640; loss: 2.16; acc: 0.19
Batch: 660; loss: 2.17; acc: 0.19
Batch: 680; loss: 2.17; acc: 0.27
Batch: 700; loss: 2.16; acc: 0.2
Batch: 720; loss: 2.08; acc: 0.22
Batch: 740; loss: 2.21; acc: 0.12
Batch: 760; loss: 2.26; acc: 0.12
Batch: 780; loss: 2.15; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.167945957487556; val_accuracy: 0.2118829617834395 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.16; acc: 0.28
Batch: 20; loss: 2.21; acc: 0.25
Batch: 40; loss: 2.1; acc: 0.33
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.25; acc: 0.08
Batch: 120; loss: 2.17; acc: 0.16
Batch: 140; loss: 2.26; acc: 0.16
Batch: 160; loss: 2.22; acc: 0.12
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.18; acc: 0.23
Batch: 220; loss: 2.13; acc: 0.25
Batch: 240; loss: 2.17; acc: 0.16
Batch: 260; loss: 2.21; acc: 0.17
Batch: 280; loss: 2.2; acc: 0.19
Batch: 300; loss: 2.16; acc: 0.22
Batch: 320; loss: 2.21; acc: 0.2
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.15; acc: 0.19
Batch: 380; loss: 2.07; acc: 0.27
Batch: 400; loss: 2.2; acc: 0.2
Batch: 420; loss: 2.22; acc: 0.22
Batch: 440; loss: 2.11; acc: 0.27
Batch: 460; loss: 2.25; acc: 0.14
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.2; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.25
Batch: 560; loss: 2.15; acc: 0.17
Batch: 580; loss: 2.15; acc: 0.2
Batch: 600; loss: 2.13; acc: 0.28
Batch: 620; loss: 2.13; acc: 0.25
Batch: 640; loss: 2.16; acc: 0.2
Batch: 660; loss: 2.24; acc: 0.12
Batch: 680; loss: 2.01; acc: 0.31
Batch: 700; loss: 2.13; acc: 0.16
Batch: 720; loss: 2.23; acc: 0.25
Batch: 740; loss: 2.19; acc: 0.19
Batch: 760; loss: 2.25; acc: 0.2
Batch: 780; loss: 2.12; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167902289682133; val_accuracy: 0.21088773885350318 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.24; acc: 0.14
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.11
Batch: 60; loss: 2.14; acc: 0.2
Batch: 80; loss: 2.21; acc: 0.16
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.14
Batch: 140; loss: 2.11; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.16
Batch: 180; loss: 2.2; acc: 0.17
Batch: 200; loss: 2.15; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.27
Batch: 240; loss: 2.19; acc: 0.2
Batch: 260; loss: 2.19; acc: 0.16
Batch: 280; loss: 2.1; acc: 0.19
Batch: 300; loss: 2.21; acc: 0.22
Batch: 320; loss: 2.18; acc: 0.2
Batch: 340; loss: 2.23; acc: 0.22
Batch: 360; loss: 2.15; acc: 0.2
Batch: 380; loss: 2.22; acc: 0.2
Batch: 400; loss: 2.14; acc: 0.17
Batch: 420; loss: 2.1; acc: 0.27
Batch: 440; loss: 2.16; acc: 0.22
Batch: 460; loss: 2.14; acc: 0.25
Batch: 480; loss: 2.21; acc: 0.2
Batch: 500; loss: 2.21; acc: 0.14
Batch: 520; loss: 2.18; acc: 0.2
Batch: 540; loss: 2.16; acc: 0.25
Batch: 560; loss: 2.14; acc: 0.19
Batch: 580; loss: 2.15; acc: 0.14
Batch: 600; loss: 2.21; acc: 0.23
Batch: 620; loss: 2.15; acc: 0.19
Batch: 640; loss: 2.14; acc: 0.25
Batch: 660; loss: 2.17; acc: 0.14
Batch: 680; loss: 2.2; acc: 0.17
Batch: 700; loss: 2.08; acc: 0.27
Batch: 720; loss: 2.29; acc: 0.14
Batch: 740; loss: 2.16; acc: 0.23
Batch: 760; loss: 2.22; acc: 0.16
Batch: 780; loss: 2.15; acc: 0.14
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679227944392307; val_accuracy: 0.21148487261146498 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.14; acc: 0.23
Batch: 40; loss: 2.09; acc: 0.22
Batch: 60; loss: 2.12; acc: 0.31
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.2
Batch: 120; loss: 2.22; acc: 0.17
Batch: 140; loss: 2.19; acc: 0.25
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.17; acc: 0.2
Batch: 200; loss: 2.15; acc: 0.22
Batch: 220; loss: 2.04; acc: 0.31
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.11; acc: 0.3
Batch: 280; loss: 2.23; acc: 0.19
Batch: 300; loss: 2.15; acc: 0.2
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.2; acc: 0.19
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.2; acc: 0.19
Batch: 400; loss: 2.23; acc: 0.19
Batch: 420; loss: 2.11; acc: 0.25
Batch: 440; loss: 2.14; acc: 0.22
Batch: 460; loss: 2.1; acc: 0.2
Batch: 480; loss: 2.21; acc: 0.2
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.15; acc: 0.25
Batch: 540; loss: 2.22; acc: 0.2
Batch: 560; loss: 2.17; acc: 0.3
Batch: 580; loss: 2.15; acc: 0.23
Batch: 600; loss: 2.09; acc: 0.23
Batch: 620; loss: 2.28; acc: 0.16
Batch: 640; loss: 2.12; acc: 0.22
Batch: 660; loss: 2.11; acc: 0.31
Batch: 680; loss: 2.21; acc: 0.23
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.27
Batch: 740; loss: 2.16; acc: 0.2
Batch: 760; loss: 2.16; acc: 0.17
Batch: 780; loss: 2.13; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679383083513586; val_accuracy: 0.21228105095541402 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.26; acc: 0.06
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.23; acc: 0.2
Batch: 60; loss: 2.17; acc: 0.16
Batch: 80; loss: 2.12; acc: 0.22
Batch: 100; loss: 2.1; acc: 0.25
Batch: 120; loss: 2.05; acc: 0.33
Batch: 140; loss: 2.18; acc: 0.23
Batch: 160; loss: 2.09; acc: 0.25
Batch: 180; loss: 2.2; acc: 0.17
Batch: 200; loss: 2.14; acc: 0.23
Batch: 220; loss: 2.12; acc: 0.27
Batch: 240; loss: 2.24; acc: 0.12
Batch: 260; loss: 2.19; acc: 0.22
Batch: 280; loss: 2.12; acc: 0.25
Batch: 300; loss: 2.22; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.22
Batch: 340; loss: 2.08; acc: 0.28
Batch: 360; loss: 2.2; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.22
Batch: 400; loss: 2.12; acc: 0.22
Batch: 420; loss: 2.2; acc: 0.12
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.02; acc: 0.36
Batch: 480; loss: 2.2; acc: 0.19
Batch: 500; loss: 2.13; acc: 0.25
Batch: 520; loss: 2.16; acc: 0.12
Batch: 540; loss: 2.24; acc: 0.16
Batch: 560; loss: 2.16; acc: 0.23
Batch: 580; loss: 2.12; acc: 0.17
Batch: 600; loss: 2.24; acc: 0.11
Batch: 620; loss: 2.13; acc: 0.27
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.28; acc: 0.14
Batch: 680; loss: 2.11; acc: 0.28
Batch: 700; loss: 2.2; acc: 0.16
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.23; acc: 0.12
Batch: 760; loss: 2.24; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167900266920685; val_accuracy: 0.21088773885350318 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.08; acc: 0.25
Batch: 20; loss: 2.22; acc: 0.22
Batch: 40; loss: 2.09; acc: 0.2
Batch: 60; loss: 2.15; acc: 0.19
Batch: 80; loss: 2.1; acc: 0.23
Batch: 100; loss: 2.13; acc: 0.2
Batch: 120; loss: 2.25; acc: 0.2
Batch: 140; loss: 2.14; acc: 0.2
Batch: 160; loss: 2.13; acc: 0.16
Batch: 180; loss: 2.24; acc: 0.09
Batch: 200; loss: 2.08; acc: 0.2
Batch: 220; loss: 2.2; acc: 0.12
Batch: 240; loss: 2.21; acc: 0.11
Batch: 260; loss: 2.19; acc: 0.19
Batch: 280; loss: 2.26; acc: 0.09
Batch: 300; loss: 2.2; acc: 0.23
Batch: 320; loss: 2.2; acc: 0.11
Batch: 340; loss: 2.2; acc: 0.22
Batch: 360; loss: 2.19; acc: 0.16
Batch: 380; loss: 2.24; acc: 0.19
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.19; acc: 0.09
Batch: 440; loss: 2.23; acc: 0.23
Batch: 460; loss: 2.34; acc: 0.08
Batch: 480; loss: 2.17; acc: 0.19
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.21; acc: 0.19
Batch: 540; loss: 2.11; acc: 0.3
Batch: 560; loss: 2.16; acc: 0.28
Batch: 580; loss: 2.26; acc: 0.16
Batch: 600; loss: 2.12; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.14
Batch: 640; loss: 2.09; acc: 0.23
Batch: 660; loss: 2.21; acc: 0.16
Batch: 680; loss: 2.13; acc: 0.23
Batch: 700; loss: 2.19; acc: 0.25
Batch: 720; loss: 2.21; acc: 0.16
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.15; acc: 0.17
Batch: 780; loss: 2.23; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679184011592985; val_accuracy: 0.2115843949044586 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.16
Batch: 20; loss: 2.15; acc: 0.2
Batch: 40; loss: 2.13; acc: 0.17
Batch: 60; loss: 2.13; acc: 0.22
Batch: 80; loss: 2.22; acc: 0.2
Batch: 100; loss: 2.13; acc: 0.22
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.18; acc: 0.28
Batch: 160; loss: 2.21; acc: 0.17
Batch: 180; loss: 2.17; acc: 0.28
Batch: 200; loss: 2.2; acc: 0.22
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.21; acc: 0.23
Batch: 260; loss: 2.14; acc: 0.17
Batch: 280; loss: 2.18; acc: 0.19
Batch: 300; loss: 2.14; acc: 0.25
Batch: 320; loss: 2.15; acc: 0.28
Batch: 340; loss: 2.19; acc: 0.19
Batch: 360; loss: 2.21; acc: 0.19
Batch: 380; loss: 2.22; acc: 0.19
Batch: 400; loss: 2.19; acc: 0.23
Batch: 420; loss: 2.18; acc: 0.17
Batch: 440; loss: 2.15; acc: 0.25
Batch: 460; loss: 2.25; acc: 0.19
Batch: 480; loss: 2.2; acc: 0.17
Batch: 500; loss: 2.2; acc: 0.22
Batch: 520; loss: 2.12; acc: 0.23
Batch: 540; loss: 2.08; acc: 0.27
Batch: 560; loss: 2.05; acc: 0.27
Batch: 580; loss: 2.15; acc: 0.25
Batch: 600; loss: 2.21; acc: 0.2
Batch: 620; loss: 2.15; acc: 0.17
Batch: 640; loss: 2.18; acc: 0.28
Batch: 660; loss: 2.09; acc: 0.27
Batch: 680; loss: 2.26; acc: 0.2
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.12; acc: 0.19
Batch: 760; loss: 2.16; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679234831196488; val_accuracy: 0.21178343949044587 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.05; acc: 0.33
Batch: 20; loss: 2.25; acc: 0.16
Batch: 40; loss: 2.17; acc: 0.17
Batch: 60; loss: 2.16; acc: 0.2
Batch: 80; loss: 2.22; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.21; acc: 0.22
Batch: 140; loss: 2.09; acc: 0.19
Batch: 160; loss: 2.17; acc: 0.28
Batch: 180; loss: 2.21; acc: 0.19
Batch: 200; loss: 2.26; acc: 0.11
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 2.13; acc: 0.23
Batch: 260; loss: 2.1; acc: 0.3
Batch: 280; loss: 2.15; acc: 0.16
Batch: 300; loss: 2.17; acc: 0.22
Batch: 320; loss: 2.27; acc: 0.12
Batch: 340; loss: 2.14; acc: 0.22
Batch: 360; loss: 2.03; acc: 0.28
Batch: 380; loss: 2.13; acc: 0.14
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.21; acc: 0.25
Batch: 440; loss: 2.08; acc: 0.23
Batch: 460; loss: 2.11; acc: 0.25
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.14; acc: 0.23
Batch: 520; loss: 2.29; acc: 0.16
Batch: 540; loss: 2.25; acc: 0.16
Batch: 560; loss: 2.2; acc: 0.2
Batch: 580; loss: 2.21; acc: 0.16
Batch: 600; loss: 2.18; acc: 0.17
Batch: 620; loss: 2.24; acc: 0.16
Batch: 640; loss: 2.23; acc: 0.14
Batch: 660; loss: 2.13; acc: 0.25
Batch: 680; loss: 2.15; acc: 0.2
Batch: 700; loss: 2.12; acc: 0.23
Batch: 720; loss: 2.28; acc: 0.17
Batch: 740; loss: 2.18; acc: 0.19
Batch: 760; loss: 2.1; acc: 0.25
Batch: 780; loss: 2.18; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167939562706431; val_accuracy: 0.21208200636942676 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.15; acc: 0.17
Batch: 20; loss: 2.12; acc: 0.19
Batch: 40; loss: 2.27; acc: 0.17
Batch: 60; loss: 2.18; acc: 0.14
Batch: 80; loss: 2.21; acc: 0.14
Batch: 100; loss: 2.11; acc: 0.31
Batch: 120; loss: 2.24; acc: 0.12
Batch: 140; loss: 2.24; acc: 0.09
Batch: 160; loss: 2.15; acc: 0.17
Batch: 180; loss: 2.19; acc: 0.27
Batch: 200; loss: 2.27; acc: 0.2
Batch: 220; loss: 2.23; acc: 0.14
Batch: 240; loss: 2.16; acc: 0.22
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.2
Batch: 300; loss: 2.36; acc: 0.14
Batch: 320; loss: 2.07; acc: 0.34
Batch: 340; loss: 2.16; acc: 0.23
Batch: 360; loss: 2.22; acc: 0.19
Batch: 380; loss: 2.26; acc: 0.14
Batch: 400; loss: 2.13; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.09
Batch: 440; loss: 2.27; acc: 0.23
Batch: 460; loss: 2.15; acc: 0.23
Batch: 480; loss: 2.16; acc: 0.28
Batch: 500; loss: 2.21; acc: 0.16
Batch: 520; loss: 2.11; acc: 0.23
Batch: 540; loss: 2.21; acc: 0.19
Batch: 560; loss: 2.15; acc: 0.2
Batch: 580; loss: 2.24; acc: 0.14
Batch: 600; loss: 2.2; acc: 0.27
Batch: 620; loss: 2.19; acc: 0.23
Batch: 640; loss: 2.25; acc: 0.23
Batch: 660; loss: 2.18; acc: 0.17
Batch: 680; loss: 2.23; acc: 0.14
Batch: 700; loss: 2.16; acc: 0.2
Batch: 720; loss: 2.13; acc: 0.25
Batch: 740; loss: 2.15; acc: 0.25
Batch: 760; loss: 2.17; acc: 0.25
Batch: 780; loss: 2.16; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.16793680342899; val_accuracy: 0.21218152866242038 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.22; acc: 0.17
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.21; acc: 0.19
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.16; acc: 0.2
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.16; acc: 0.2
Batch: 140; loss: 2.19; acc: 0.2
Batch: 160; loss: 2.09; acc: 0.2
Batch: 180; loss: 2.28; acc: 0.2
Batch: 200; loss: 2.28; acc: 0.14
Batch: 220; loss: 2.18; acc: 0.27
Batch: 240; loss: 2.22; acc: 0.19
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.22; acc: 0.12
Batch: 300; loss: 2.17; acc: 0.28
Batch: 320; loss: 2.18; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.14
Batch: 360; loss: 2.12; acc: 0.25
Batch: 380; loss: 2.13; acc: 0.2
Batch: 400; loss: 2.16; acc: 0.23
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.15; acc: 0.17
Batch: 460; loss: 2.28; acc: 0.2
Batch: 480; loss: 2.17; acc: 0.19
Batch: 500; loss: 2.13; acc: 0.23
Batch: 520; loss: 2.04; acc: 0.27
Batch: 540; loss: 2.12; acc: 0.3
Batch: 560; loss: 2.23; acc: 0.19
Batch: 580; loss: 2.21; acc: 0.23
Batch: 600; loss: 2.11; acc: 0.16
Batch: 620; loss: 2.19; acc: 0.23
Batch: 640; loss: 2.23; acc: 0.16
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.07; acc: 0.34
Batch: 700; loss: 2.19; acc: 0.17
Batch: 720; loss: 2.18; acc: 0.25
Batch: 740; loss: 2.2; acc: 0.25
Batch: 760; loss: 2.15; acc: 0.19
Batch: 780; loss: 2.21; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679346379201125; val_accuracy: 0.21078821656050956 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.14; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.22
Batch: 60; loss: 2.09; acc: 0.28
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.2; acc: 0.17
Batch: 120; loss: 2.11; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.17
Batch: 160; loss: 2.21; acc: 0.2
Batch: 180; loss: 2.17; acc: 0.16
Batch: 200; loss: 2.22; acc: 0.16
Batch: 220; loss: 2.17; acc: 0.19
Batch: 240; loss: 2.22; acc: 0.17
Batch: 260; loss: 2.15; acc: 0.33
Batch: 280; loss: 2.12; acc: 0.27
Batch: 300; loss: 2.12; acc: 0.25
Batch: 320; loss: 2.11; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.2; acc: 0.19
Batch: 400; loss: 2.17; acc: 0.2
Batch: 420; loss: 2.23; acc: 0.2
Batch: 440; loss: 2.28; acc: 0.17
Batch: 460; loss: 2.17; acc: 0.16
Batch: 480; loss: 2.28; acc: 0.14
Batch: 500; loss: 2.13; acc: 0.27
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.17; acc: 0.23
Batch: 560; loss: 2.13; acc: 0.23
Batch: 580; loss: 2.13; acc: 0.27
Batch: 600; loss: 2.26; acc: 0.2
Batch: 620; loss: 2.14; acc: 0.16
Batch: 640; loss: 2.1; acc: 0.27
Batch: 660; loss: 2.17; acc: 0.25
Batch: 680; loss: 2.07; acc: 0.25
Batch: 700; loss: 2.17; acc: 0.22
Batch: 720; loss: 2.14; acc: 0.25
Batch: 740; loss: 2.16; acc: 0.2
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.09; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167928765533836; val_accuracy: 0.2115843949044586 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.28
Batch: 20; loss: 2.19; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.2
Batch: 80; loss: 2.28; acc: 0.17
Batch: 100; loss: 2.08; acc: 0.25
Batch: 120; loss: 2.12; acc: 0.22
Batch: 140; loss: 2.14; acc: 0.2
Batch: 160; loss: 2.16; acc: 0.22
Batch: 180; loss: 2.2; acc: 0.19
Batch: 200; loss: 2.14; acc: 0.22
Batch: 220; loss: 2.25; acc: 0.17
Batch: 240; loss: 2.2; acc: 0.22
Batch: 260; loss: 2.21; acc: 0.19
Batch: 280; loss: 2.19; acc: 0.17
Batch: 300; loss: 2.12; acc: 0.17
Batch: 320; loss: 2.18; acc: 0.25
Batch: 340; loss: 2.23; acc: 0.19
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.0; acc: 0.28
Batch: 400; loss: 2.14; acc: 0.17
Batch: 420; loss: 2.15; acc: 0.23
Batch: 440; loss: 2.09; acc: 0.3
Batch: 460; loss: 2.16; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.12
Batch: 500; loss: 2.23; acc: 0.19
Batch: 520; loss: 2.18; acc: 0.22
Batch: 540; loss: 2.13; acc: 0.25
Batch: 560; loss: 2.1; acc: 0.23
Batch: 580; loss: 2.13; acc: 0.25
Batch: 600; loss: 2.18; acc: 0.22
Batch: 620; loss: 2.14; acc: 0.22
Batch: 640; loss: 2.17; acc: 0.17
Batch: 660; loss: 2.07; acc: 0.23
Batch: 680; loss: 2.1; acc: 0.22
Batch: 700; loss: 2.19; acc: 0.23
Batch: 720; loss: 2.22; acc: 0.2
Batch: 740; loss: 2.07; acc: 0.3
Batch: 760; loss: 2.09; acc: 0.25
Batch: 780; loss: 2.17; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167940249868259; val_accuracy: 0.21218152866242038 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.09; acc: 0.28
Batch: 20; loss: 2.25; acc: 0.17
Batch: 40; loss: 2.22; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.1; acc: 0.31
Batch: 100; loss: 2.12; acc: 0.19
Batch: 120; loss: 2.31; acc: 0.09
Batch: 140; loss: 2.18; acc: 0.17
Batch: 160; loss: 2.13; acc: 0.3
Batch: 180; loss: 2.16; acc: 0.12
Batch: 200; loss: 2.16; acc: 0.2
Batch: 220; loss: 2.12; acc: 0.2
Batch: 240; loss: 2.2; acc: 0.16
Batch: 260; loss: 2.2; acc: 0.17
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.12
Batch: 340; loss: 2.23; acc: 0.22
Batch: 360; loss: 2.1; acc: 0.28
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.09; acc: 0.3
Batch: 420; loss: 2.27; acc: 0.12
Batch: 440; loss: 2.11; acc: 0.3
Batch: 460; loss: 2.15; acc: 0.22
Batch: 480; loss: 2.09; acc: 0.17
Batch: 500; loss: 2.15; acc: 0.28
Batch: 520; loss: 2.29; acc: 0.2
Batch: 540; loss: 2.1; acc: 0.28
Batch: 560; loss: 2.13; acc: 0.22
Batch: 580; loss: 2.15; acc: 0.2
Batch: 600; loss: 2.15; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.11
Batch: 640; loss: 2.13; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.16
Batch: 680; loss: 2.23; acc: 0.17
Batch: 700; loss: 2.16; acc: 0.19
Batch: 720; loss: 2.13; acc: 0.23
Batch: 740; loss: 2.19; acc: 0.12
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.14; acc: 0.25
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679290874748474; val_accuracy: 0.21218152866242038 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.13; acc: 0.33
Batch: 20; loss: 2.05; acc: 0.27
Batch: 40; loss: 2.2; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.22
Batch: 80; loss: 2.24; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.19; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.15; acc: 0.23
Batch: 180; loss: 2.23; acc: 0.2
Batch: 200; loss: 2.21; acc: 0.22
Batch: 220; loss: 2.32; acc: 0.16
Batch: 240; loss: 2.18; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.17
Batch: 280; loss: 2.22; acc: 0.17
Batch: 300; loss: 2.12; acc: 0.28
Batch: 320; loss: 2.13; acc: 0.16
Batch: 340; loss: 2.23; acc: 0.17
Batch: 360; loss: 2.17; acc: 0.17
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.11; acc: 0.22
Batch: 420; loss: 2.14; acc: 0.23
Batch: 440; loss: 2.21; acc: 0.25
Batch: 460; loss: 2.15; acc: 0.17
Batch: 480; loss: 2.25; acc: 0.16
Batch: 500; loss: 2.04; acc: 0.23
Batch: 520; loss: 2.1; acc: 0.3
Batch: 540; loss: 2.27; acc: 0.2
Batch: 560; loss: 2.31; acc: 0.11
Batch: 580; loss: 2.17; acc: 0.22
Batch: 600; loss: 2.21; acc: 0.23
Batch: 620; loss: 2.21; acc: 0.2
Batch: 640; loss: 2.13; acc: 0.23
Batch: 660; loss: 2.11; acc: 0.28
Batch: 680; loss: 2.21; acc: 0.2
Batch: 700; loss: 2.04; acc: 0.38
Batch: 720; loss: 2.2; acc: 0.2
Batch: 740; loss: 2.13; acc: 0.27
Batch: 760; loss: 2.16; acc: 0.22
Batch: 780; loss: 2.06; acc: 0.28
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167930911301048; val_accuracy: 0.2115843949044586 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.06; acc: 0.19
Batch: 20; loss: 2.19; acc: 0.14
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.1; acc: 0.25
Batch: 80; loss: 2.16; acc: 0.27
Batch: 100; loss: 2.16; acc: 0.25
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.07; acc: 0.27
Batch: 160; loss: 2.16; acc: 0.19
Batch: 180; loss: 2.27; acc: 0.19
Batch: 200; loss: 2.16; acc: 0.22
Batch: 220; loss: 2.1; acc: 0.27
Batch: 240; loss: 2.16; acc: 0.19
Batch: 260; loss: 2.17; acc: 0.23
Batch: 280; loss: 2.16; acc: 0.2
Batch: 300; loss: 2.26; acc: 0.14
Batch: 320; loss: 2.27; acc: 0.12
Batch: 340; loss: 2.12; acc: 0.23
Batch: 360; loss: 2.17; acc: 0.14
Batch: 380; loss: 2.18; acc: 0.27
Batch: 400; loss: 2.19; acc: 0.25
Batch: 420; loss: 2.22; acc: 0.25
Batch: 440; loss: 2.03; acc: 0.39
Batch: 460; loss: 2.22; acc: 0.14
Batch: 480; loss: 2.11; acc: 0.28
Batch: 500; loss: 2.13; acc: 0.22
Batch: 520; loss: 2.19; acc: 0.12
Batch: 540; loss: 2.2; acc: 0.22
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.19; acc: 0.17
Batch: 600; loss: 2.21; acc: 0.16
Batch: 620; loss: 2.18; acc: 0.25
Batch: 640; loss: 2.27; acc: 0.17
Batch: 660; loss: 2.09; acc: 0.2
Batch: 680; loss: 2.23; acc: 0.14
Batch: 700; loss: 2.14; acc: 0.22
Batch: 720; loss: 2.24; acc: 0.17
Batch: 740; loss: 2.12; acc: 0.22
Batch: 760; loss: 2.16; acc: 0.23
Batch: 780; loss: 2.14; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679343790005725; val_accuracy: 0.21178343949044587 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.09; acc: 0.22
Batch: 40; loss: 2.23; acc: 0.16
Batch: 60; loss: 2.16; acc: 0.16
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.2
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Batch: 160; loss: 2.19; acc: 0.14
Batch: 180; loss: 2.2; acc: 0.16
Batch: 200; loss: 2.2; acc: 0.23
Batch: 220; loss: 2.23; acc: 0.16
Batch: 240; loss: 2.19; acc: 0.22
Batch: 260; loss: 2.32; acc: 0.12
Batch: 280; loss: 2.15; acc: 0.25
Batch: 300; loss: 2.16; acc: 0.28
Batch: 320; loss: 2.2; acc: 0.22
Batch: 340; loss: 2.15; acc: 0.2
Batch: 360; loss: 2.05; acc: 0.25
Batch: 380; loss: 2.16; acc: 0.23
Batch: 400; loss: 2.29; acc: 0.16
Batch: 420; loss: 2.16; acc: 0.22
Batch: 440; loss: 2.15; acc: 0.28
Batch: 460; loss: 2.11; acc: 0.28
Batch: 480; loss: 2.17; acc: 0.25
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.21; acc: 0.16
Batch: 540; loss: 2.11; acc: 0.25
Batch: 560; loss: 2.06; acc: 0.3
Batch: 580; loss: 2.24; acc: 0.17
Batch: 600; loss: 2.2; acc: 0.22
Batch: 620; loss: 2.16; acc: 0.23
Batch: 640; loss: 2.2; acc: 0.16
Batch: 660; loss: 2.17; acc: 0.2
Batch: 680; loss: 2.08; acc: 0.27
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.12; acc: 0.27
Batch: 740; loss: 2.15; acc: 0.2
Batch: 760; loss: 2.27; acc: 0.16
Batch: 780; loss: 2.21; acc: 0.14
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679457813311536; val_accuracy: 0.2119824840764331 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.17; acc: 0.23
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.2
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.19; acc: 0.22
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.17; acc: 0.12
Batch: 180; loss: 2.1; acc: 0.23
Batch: 200; loss: 2.14; acc: 0.2
Batch: 220; loss: 2.02; acc: 0.31
Batch: 240; loss: 2.24; acc: 0.2
Batch: 260; loss: 2.14; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.2
Batch: 320; loss: 2.02; acc: 0.33
Batch: 340; loss: 2.21; acc: 0.17
Batch: 360; loss: 2.15; acc: 0.25
Batch: 380; loss: 2.09; acc: 0.31
Batch: 400; loss: 2.11; acc: 0.28
Batch: 420; loss: 2.26; acc: 0.17
Batch: 440; loss: 2.19; acc: 0.2
Batch: 460; loss: 2.12; acc: 0.17
Batch: 480; loss: 2.14; acc: 0.19
Batch: 500; loss: 2.18; acc: 0.14
Batch: 520; loss: 2.08; acc: 0.36
Batch: 540; loss: 2.2; acc: 0.17
Batch: 560; loss: 2.14; acc: 0.28
Batch: 580; loss: 2.17; acc: 0.28
Batch: 600; loss: 2.17; acc: 0.23
Batch: 620; loss: 2.19; acc: 0.22
Batch: 640; loss: 2.13; acc: 0.3
Batch: 660; loss: 2.2; acc: 0.12
Batch: 680; loss: 2.11; acc: 0.27
Batch: 700; loss: 2.12; acc: 0.23
Batch: 720; loss: 2.2; acc: 0.14
Batch: 740; loss: 2.13; acc: 0.23
Batch: 760; loss: 2.19; acc: 0.22
Batch: 780; loss: 2.2; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679343698890348; val_accuracy: 0.21168391719745222 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.12; acc: 0.19
Batch: 20; loss: 2.1; acc: 0.22
Batch: 40; loss: 2.23; acc: 0.22
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.26; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.19
Batch: 120; loss: 2.17; acc: 0.19
Batch: 140; loss: 2.24; acc: 0.12
Batch: 160; loss: 2.17; acc: 0.22
Batch: 180; loss: 2.13; acc: 0.25
Batch: 200; loss: 2.05; acc: 0.27
Batch: 220; loss: 2.15; acc: 0.22
Batch: 240; loss: 2.15; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.14
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.17; acc: 0.2
Batch: 320; loss: 2.17; acc: 0.22
Batch: 340; loss: 2.08; acc: 0.28
Batch: 360; loss: 2.25; acc: 0.14
Batch: 380; loss: 2.1; acc: 0.27
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.1; acc: 0.23
Batch: 440; loss: 2.24; acc: 0.09
Batch: 460; loss: 2.15; acc: 0.19
Batch: 480; loss: 2.18; acc: 0.14
Batch: 500; loss: 2.13; acc: 0.22
Batch: 520; loss: 2.1; acc: 0.3
Batch: 540; loss: 2.24; acc: 0.19
Batch: 560; loss: 2.14; acc: 0.17
Batch: 580; loss: 2.13; acc: 0.23
Batch: 600; loss: 2.2; acc: 0.17
Batch: 620; loss: 2.23; acc: 0.16
Batch: 640; loss: 2.14; acc: 0.17
Batch: 660; loss: 2.15; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.17
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.17; acc: 0.16
Batch: 740; loss: 2.24; acc: 0.17
Batch: 760; loss: 2.17; acc: 0.16
Batch: 780; loss: 2.16; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167934325849934; val_accuracy: 0.2115843949044586 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.14; acc: 0.25
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.25; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Batch: 160; loss: 2.17; acc: 0.27
Batch: 180; loss: 2.12; acc: 0.23
Batch: 200; loss: 2.2; acc: 0.12
Batch: 220; loss: 2.27; acc: 0.12
Batch: 240; loss: 2.27; acc: 0.25
Batch: 260; loss: 2.23; acc: 0.2
Batch: 280; loss: 2.24; acc: 0.2
Batch: 300; loss: 2.08; acc: 0.33
Batch: 320; loss: 2.19; acc: 0.12
Batch: 340; loss: 2.13; acc: 0.22
Batch: 360; loss: 2.27; acc: 0.16
Batch: 380; loss: 2.17; acc: 0.22
Batch: 400; loss: 2.26; acc: 0.17
Batch: 420; loss: 2.16; acc: 0.19
Batch: 440; loss: 2.2; acc: 0.2
Batch: 460; loss: 2.1; acc: 0.25
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.18; acc: 0.14
Batch: 520; loss: 2.24; acc: 0.11
Batch: 540; loss: 2.22; acc: 0.25
Batch: 560; loss: 2.17; acc: 0.22
Batch: 580; loss: 2.15; acc: 0.25
Batch: 600; loss: 2.17; acc: 0.19
Batch: 620; loss: 2.16; acc: 0.22
Batch: 640; loss: 2.17; acc: 0.12
Batch: 660; loss: 2.08; acc: 0.22
Batch: 680; loss: 2.13; acc: 0.19
Batch: 700; loss: 2.11; acc: 0.25
Batch: 720; loss: 2.12; acc: 0.16
Batch: 740; loss: 2.1; acc: 0.17
Batch: 760; loss: 2.14; acc: 0.22
Batch: 780; loss: 2.21; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679384085782774; val_accuracy: 0.21168391719745222 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.29; acc: 0.17
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.15; acc: 0.27
Batch: 60; loss: 2.21; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.16
Batch: 100; loss: 2.18; acc: 0.16
Batch: 120; loss: 2.12; acc: 0.22
Batch: 140; loss: 2.11; acc: 0.27
Batch: 160; loss: 2.23; acc: 0.14
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.26; acc: 0.2
Batch: 220; loss: 2.16; acc: 0.23
Batch: 240; loss: 2.16; acc: 0.16
Batch: 260; loss: 2.17; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.19
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.25; acc: 0.11
Batch: 360; loss: 2.15; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.19
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.11; acc: 0.3
Batch: 440; loss: 2.22; acc: 0.19
Batch: 460; loss: 2.16; acc: 0.22
Batch: 480; loss: 2.14; acc: 0.23
Batch: 500; loss: 2.11; acc: 0.3
Batch: 520; loss: 2.23; acc: 0.22
Batch: 540; loss: 2.22; acc: 0.12
Batch: 560; loss: 2.14; acc: 0.14
Batch: 580; loss: 2.19; acc: 0.23
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.17; acc: 0.17
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 2.16; acc: 0.19
Batch: 680; loss: 2.2; acc: 0.14
Batch: 700; loss: 2.1; acc: 0.23
Batch: 720; loss: 2.11; acc: 0.22
Batch: 740; loss: 2.09; acc: 0.22
Batch: 760; loss: 2.16; acc: 0.25
Batch: 780; loss: 2.15; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167942031933244; val_accuracy: 0.2118829617834395 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.14; acc: 0.2
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.22; acc: 0.22
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.11; acc: 0.19
Batch: 100; loss: 2.18; acc: 0.22
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.14; acc: 0.22
Batch: 160; loss: 2.14; acc: 0.16
Batch: 180; loss: 2.06; acc: 0.25
Batch: 200; loss: 2.13; acc: 0.2
Batch: 220; loss: 2.06; acc: 0.31
Batch: 240; loss: 2.26; acc: 0.09
Batch: 260; loss: 2.17; acc: 0.23
Batch: 280; loss: 2.29; acc: 0.14
Batch: 300; loss: 2.16; acc: 0.27
Batch: 320; loss: 2.19; acc: 0.25
Batch: 340; loss: 2.14; acc: 0.27
Batch: 360; loss: 2.15; acc: 0.22
Batch: 380; loss: 2.2; acc: 0.19
Batch: 400; loss: 2.12; acc: 0.17
Batch: 420; loss: 2.18; acc: 0.2
Batch: 440; loss: 2.2; acc: 0.22
Batch: 460; loss: 2.18; acc: 0.16
Batch: 480; loss: 2.26; acc: 0.22
Batch: 500; loss: 2.23; acc: 0.12
Batch: 520; loss: 2.08; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.28
Batch: 560; loss: 2.21; acc: 0.23
Batch: 580; loss: 2.26; acc: 0.11
Batch: 600; loss: 2.2; acc: 0.08
Batch: 620; loss: 2.18; acc: 0.25
Batch: 640; loss: 2.31; acc: 0.17
Batch: 660; loss: 2.22; acc: 0.14
Batch: 680; loss: 2.19; acc: 0.12
Batch: 700; loss: 2.19; acc: 0.27
Batch: 720; loss: 2.09; acc: 0.22
Batch: 740; loss: 2.32; acc: 0.2
Batch: 760; loss: 2.23; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.25
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679371625754484; val_accuracy: 0.21148487261146498 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.1; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.22
Batch: 40; loss: 2.24; acc: 0.14
Batch: 60; loss: 2.18; acc: 0.22
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.18; acc: 0.17
Batch: 120; loss: 2.25; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.2
Batch: 160; loss: 2.16; acc: 0.23
Batch: 180; loss: 2.2; acc: 0.23
Batch: 200; loss: 2.19; acc: 0.19
Batch: 220; loss: 2.17; acc: 0.22
Batch: 240; loss: 2.19; acc: 0.19
Batch: 260; loss: 2.03; acc: 0.31
Batch: 280; loss: 2.24; acc: 0.22
Batch: 300; loss: 2.06; acc: 0.22
Batch: 320; loss: 2.19; acc: 0.27
Batch: 340; loss: 2.11; acc: 0.27
Batch: 360; loss: 2.12; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.27
Batch: 400; loss: 2.27; acc: 0.12
Batch: 420; loss: 2.16; acc: 0.25
Batch: 440; loss: 2.28; acc: 0.14
Batch: 460; loss: 2.12; acc: 0.2
Batch: 480; loss: 2.12; acc: 0.22
Batch: 500; loss: 2.08; acc: 0.2
Batch: 520; loss: 2.31; acc: 0.12
Batch: 540; loss: 2.05; acc: 0.28
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.13; acc: 0.23
Batch: 600; loss: 2.1; acc: 0.31
Batch: 620; loss: 2.25; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.2
Batch: 660; loss: 2.16; acc: 0.23
Batch: 680; loss: 2.07; acc: 0.23
Batch: 700; loss: 2.18; acc: 0.23
Batch: 720; loss: 2.22; acc: 0.11
Batch: 740; loss: 2.15; acc: 0.12
Batch: 760; loss: 2.11; acc: 0.2
Batch: 780; loss: 2.08; acc: 0.3
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679393546596453; val_accuracy: 0.2112858280254777 

plots/subspace_training/lenet/2020-01-20 16:25:19/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 1110650
elements in E: 1110650
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.05
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.31; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.31; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.31; acc: 0.06
Batch: 260; loss: 2.31; acc: 0.06
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.14
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.31; acc: 0.05
Batch: 360; loss: 2.32; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.31; acc: 0.06
Batch: 420; loss: 2.29; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.16
Batch: 520; loss: 2.31; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.19
Batch: 560; loss: 2.32; acc: 0.03
Batch: 580; loss: 2.31; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.19
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.16
Batch: 700; loss: 2.29; acc: 0.17
Batch: 720; loss: 2.3; acc: 0.09
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.14
Val Epoch over. val_loss: 2.2958397834923616; val_accuracy: 0.10947452229299363 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.28; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.17
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.29; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.14
Batch: 340; loss: 2.28; acc: 0.2
Batch: 360; loss: 2.31; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.29; acc: 0.16
Batch: 420; loss: 2.29; acc: 0.19
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.3; acc: 0.19
Batch: 480; loss: 2.29; acc: 0.22
Batch: 500; loss: 2.28; acc: 0.27
Batch: 520; loss: 2.28; acc: 0.23
Batch: 540; loss: 2.28; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.27
Batch: 580; loss: 2.28; acc: 0.38
Batch: 600; loss: 2.28; acc: 0.3
Batch: 620; loss: 2.29; acc: 0.31
Batch: 640; loss: 2.3; acc: 0.17
Batch: 660; loss: 2.28; acc: 0.19
Batch: 680; loss: 2.28; acc: 0.3
Batch: 700; loss: 2.28; acc: 0.23
Batch: 720; loss: 2.29; acc: 0.33
Batch: 740; loss: 2.29; acc: 0.31
Batch: 760; loss: 2.29; acc: 0.25
Batch: 780; loss: 2.29; acc: 0.28
Train Epoch over. train_loss: 2.29; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.22
Batch: 20; loss: 2.29; acc: 0.33
Batch: 40; loss: 2.28; acc: 0.31
Batch: 60; loss: 2.29; acc: 0.3
Batch: 80; loss: 2.28; acc: 0.31
Batch: 100; loss: 2.3; acc: 0.27
Batch: 120; loss: 2.29; acc: 0.31
Batch: 140; loss: 2.28; acc: 0.25
Val Epoch over. val_loss: 2.2866195933834; val_accuracy: 0.25308519108280253 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.25
Batch: 20; loss: 2.28; acc: 0.28
Batch: 40; loss: 2.28; acc: 0.31
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.22
Batch: 100; loss: 2.28; acc: 0.28
Batch: 120; loss: 2.28; acc: 0.23
Batch: 140; loss: 2.28; acc: 0.31
Batch: 160; loss: 2.29; acc: 0.22
Batch: 180; loss: 2.28; acc: 0.28
Batch: 200; loss: 2.29; acc: 0.19
Batch: 220; loss: 2.28; acc: 0.3
Batch: 240; loss: 2.28; acc: 0.34
Batch: 260; loss: 2.28; acc: 0.33
Batch: 280; loss: 2.3; acc: 0.17
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.28; acc: 0.22
Batch: 340; loss: 2.28; acc: 0.3
Batch: 360; loss: 2.28; acc: 0.27
Batch: 380; loss: 2.28; acc: 0.25
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.27; acc: 0.33
Batch: 440; loss: 2.28; acc: 0.33
Batch: 460; loss: 2.28; acc: 0.22
Batch: 480; loss: 2.28; acc: 0.19
Batch: 500; loss: 2.27; acc: 0.3
Batch: 520; loss: 2.27; acc: 0.31
Batch: 540; loss: 2.29; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.19
Batch: 580; loss: 2.27; acc: 0.31
Batch: 600; loss: 2.27; acc: 0.25
Batch: 620; loss: 2.28; acc: 0.14
Batch: 640; loss: 2.27; acc: 0.23
Batch: 660; loss: 2.27; acc: 0.25
Batch: 680; loss: 2.26; acc: 0.31
Batch: 700; loss: 2.26; acc: 0.25
Batch: 720; loss: 2.27; acc: 0.25
Batch: 740; loss: 2.25; acc: 0.42
Batch: 760; loss: 2.27; acc: 0.28
Batch: 780; loss: 2.26; acc: 0.3
Train Epoch over. train_loss: 2.28; train_accuracy: 0.26 

Batch: 0; loss: 2.26; acc: 0.28
Batch: 20; loss: 2.27; acc: 0.28
Batch: 40; loss: 2.25; acc: 0.44
Batch: 60; loss: 2.26; acc: 0.36
Batch: 80; loss: 2.26; acc: 0.31
Batch: 100; loss: 2.27; acc: 0.28
Batch: 120; loss: 2.26; acc: 0.38
Batch: 140; loss: 2.25; acc: 0.3
Val Epoch over. val_loss: 2.2617470929577093; val_accuracy: 0.28662420382165604 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.23
Batch: 20; loss: 2.26; acc: 0.22
Batch: 40; loss: 2.27; acc: 0.23
Batch: 60; loss: 2.28; acc: 0.17
Batch: 80; loss: 2.26; acc: 0.31
Batch: 100; loss: 2.26; acc: 0.3
Batch: 120; loss: 2.26; acc: 0.31
Batch: 140; loss: 2.27; acc: 0.23
Batch: 160; loss: 2.24; acc: 0.33
Batch: 180; loss: 2.23; acc: 0.31
Batch: 200; loss: 2.27; acc: 0.27
Batch: 220; loss: 2.23; acc: 0.31
Batch: 240; loss: 2.24; acc: 0.2
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.23; acc: 0.36
Batch: 300; loss: 2.25; acc: 0.28
Batch: 320; loss: 2.22; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.3
Batch: 360; loss: 2.2; acc: 0.28
Batch: 380; loss: 2.26; acc: 0.09
Batch: 400; loss: 2.21; acc: 0.25
Batch: 420; loss: 2.2; acc: 0.3
Batch: 440; loss: 2.2; acc: 0.3
Batch: 460; loss: 2.17; acc: 0.33
Batch: 480; loss: 2.19; acc: 0.19
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.16; acc: 0.23
Batch: 560; loss: 2.17; acc: 0.3
Batch: 580; loss: 2.16; acc: 0.28
Batch: 600; loss: 2.14; acc: 0.27
Batch: 620; loss: 2.17; acc: 0.19
Batch: 640; loss: 2.1; acc: 0.33
Batch: 660; loss: 2.15; acc: 0.23
Batch: 680; loss: 2.11; acc: 0.38
Batch: 700; loss: 2.07; acc: 0.33
Batch: 720; loss: 2.05; acc: 0.34
Batch: 740; loss: 2.07; acc: 0.31
Batch: 760; loss: 2.03; acc: 0.33
Batch: 780; loss: 2.1; acc: 0.31
Train Epoch over. train_loss: 2.2; train_accuracy: 0.26 

Batch: 0; loss: 2.14; acc: 0.23
Batch: 20; loss: 2.1; acc: 0.28
Batch: 40; loss: 1.97; acc: 0.34
Batch: 60; loss: 2.01; acc: 0.3
Batch: 80; loss: 2.01; acc: 0.27
Batch: 100; loss: 2.07; acc: 0.27
Batch: 120; loss: 2.06; acc: 0.27
Batch: 140; loss: 2.01; acc: 0.33
Val Epoch over. val_loss: 2.0655057377116695; val_accuracy: 0.2891122611464968 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.07; acc: 0.42
Batch: 20; loss: 2.03; acc: 0.33
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 1.94; acc: 0.39
Batch: 80; loss: 2.13; acc: 0.19
Batch: 100; loss: 1.96; acc: 0.31
Batch: 120; loss: 1.92; acc: 0.38
Batch: 140; loss: 2.01; acc: 0.31
Batch: 160; loss: 2.0; acc: 0.38
Batch: 180; loss: 1.94; acc: 0.33
Batch: 200; loss: 1.92; acc: 0.3
Batch: 220; loss: 2.0; acc: 0.27
Batch: 240; loss: 1.94; acc: 0.27
Batch: 260; loss: 1.89; acc: 0.36
Batch: 280; loss: 1.81; acc: 0.36
Batch: 300; loss: 1.95; acc: 0.33
Batch: 320; loss: 1.93; acc: 0.38
Batch: 340; loss: 1.96; acc: 0.3
Batch: 360; loss: 1.91; acc: 0.41
Batch: 380; loss: 1.97; acc: 0.27
Batch: 400; loss: 1.81; acc: 0.47
Batch: 420; loss: 2.02; acc: 0.33
Batch: 440; loss: 1.99; acc: 0.25
Batch: 460; loss: 1.91; acc: 0.28
Batch: 480; loss: 1.82; acc: 0.44
Batch: 500; loss: 1.83; acc: 0.34
Batch: 520; loss: 1.76; acc: 0.41
Batch: 540; loss: 1.83; acc: 0.41
Batch: 560; loss: 1.76; acc: 0.44
Batch: 580; loss: 1.95; acc: 0.33
Batch: 600; loss: 1.93; acc: 0.3
Batch: 620; loss: 1.73; acc: 0.42
Batch: 640; loss: 1.9; acc: 0.3
Batch: 660; loss: 1.87; acc: 0.36
Batch: 680; loss: 1.94; acc: 0.27
Batch: 700; loss: 1.84; acc: 0.39
Batch: 720; loss: 1.87; acc: 0.39
Batch: 740; loss: 1.78; acc: 0.38
Batch: 760; loss: 1.93; acc: 0.33
Batch: 780; loss: 2.01; acc: 0.27
Train Epoch over. train_loss: 1.91; train_accuracy: 0.34 

Batch: 0; loss: 1.96; acc: 0.33
Batch: 20; loss: 2.0; acc: 0.23
Batch: 40; loss: 1.76; acc: 0.38
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.75; acc: 0.45
Batch: 100; loss: 2.02; acc: 0.23
Batch: 120; loss: 1.74; acc: 0.52
Batch: 140; loss: 1.8; acc: 0.38
Val Epoch over. val_loss: 1.8503221311386984; val_accuracy: 0.3656449044585987 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.31
Batch: 20; loss: 1.92; acc: 0.31
Batch: 40; loss: 1.75; acc: 0.42
Batch: 60; loss: 1.74; acc: 0.47
Batch: 80; loss: 1.8; acc: 0.34
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.75; acc: 0.41
Batch: 160; loss: 1.86; acc: 0.36
Batch: 180; loss: 1.83; acc: 0.33
Batch: 200; loss: 1.75; acc: 0.41
Batch: 220; loss: 1.92; acc: 0.33
Batch: 240; loss: 1.77; acc: 0.42
Batch: 260; loss: 1.78; acc: 0.36
Batch: 280; loss: 1.86; acc: 0.33
Batch: 300; loss: 1.98; acc: 0.33
Batch: 320; loss: 1.8; acc: 0.36
Batch: 340; loss: 1.8; acc: 0.39
Batch: 360; loss: 1.84; acc: 0.33
Batch: 380; loss: 2.0; acc: 0.22
Batch: 400; loss: 1.77; acc: 0.36
Batch: 420; loss: 1.86; acc: 0.36
Batch: 440; loss: 1.77; acc: 0.39
Batch: 460; loss: 1.93; acc: 0.36
Batch: 480; loss: 1.99; acc: 0.31
Batch: 500; loss: 1.85; acc: 0.33
Batch: 520; loss: 1.8; acc: 0.39
Batch: 540; loss: 1.85; acc: 0.38
Batch: 560; loss: 1.81; acc: 0.39
Batch: 580; loss: 2.0; acc: 0.28
Batch: 600; loss: 1.77; acc: 0.41
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.67; acc: 0.42
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.86; acc: 0.36
Batch: 700; loss: 1.75; acc: 0.42
Batch: 720; loss: 2.18; acc: 0.27
Batch: 740; loss: 1.74; acc: 0.33
Batch: 760; loss: 1.71; acc: 0.47
Batch: 780; loss: 1.77; acc: 0.41
Train Epoch over. train_loss: 1.85; train_accuracy: 0.36 

Batch: 0; loss: 1.95; acc: 0.3
Batch: 20; loss: 1.9; acc: 0.31
Batch: 40; loss: 1.73; acc: 0.41
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.3
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.75; acc: 0.36
Val Epoch over. val_loss: 1.8193537255001675; val_accuracy: 0.3708200636942675 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.45
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.87; acc: 0.41
Batch: 60; loss: 1.8; acc: 0.36
Batch: 80; loss: 1.89; acc: 0.39
Batch: 100; loss: 1.75; acc: 0.39
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.96; acc: 0.31
Batch: 160; loss: 1.8; acc: 0.39
Batch: 180; loss: 1.82; acc: 0.44
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.94; acc: 0.28
Batch: 240; loss: 1.76; acc: 0.38
Batch: 260; loss: 1.86; acc: 0.39
Batch: 280; loss: 1.85; acc: 0.33
Batch: 300; loss: 1.89; acc: 0.31
Batch: 320; loss: 1.72; acc: 0.36
Batch: 340; loss: 1.67; acc: 0.45
Batch: 360; loss: 1.9; acc: 0.38
Batch: 380; loss: 1.86; acc: 0.36
Batch: 400; loss: 2.21; acc: 0.27
Batch: 420; loss: 1.94; acc: 0.27
Batch: 440; loss: 1.78; acc: 0.44
Batch: 460; loss: 1.87; acc: 0.39
Batch: 480; loss: 1.77; acc: 0.38
Batch: 500; loss: 1.93; acc: 0.39
Batch: 520; loss: 1.94; acc: 0.31
Batch: 540; loss: 1.8; acc: 0.45
Batch: 560; loss: 1.91; acc: 0.28
Batch: 580; loss: 1.74; acc: 0.41
Batch: 600; loss: 1.96; acc: 0.36
Batch: 620; loss: 1.95; acc: 0.38
Batch: 640; loss: 1.84; acc: 0.39
Batch: 660; loss: 1.98; acc: 0.28
Batch: 680; loss: 1.75; acc: 0.42
Batch: 700; loss: 1.94; acc: 0.33
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.62; acc: 0.53
Batch: 760; loss: 1.88; acc: 0.44
Batch: 780; loss: 1.71; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.33
Batch: 20; loss: 1.9; acc: 0.33
Batch: 40; loss: 1.72; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.41
Batch: 100; loss: 1.86; acc: 0.3
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.76; acc: 0.36
Val Epoch over. val_loss: 1.816589800415525; val_accuracy: 0.3638535031847134 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.38
Batch: 20; loss: 1.83; acc: 0.38
Batch: 40; loss: 1.96; acc: 0.28
Batch: 60; loss: 1.74; acc: 0.41
Batch: 80; loss: 1.89; acc: 0.3
Batch: 100; loss: 1.66; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 2.11; acc: 0.2
Batch: 160; loss: 1.92; acc: 0.31
Batch: 180; loss: 1.79; acc: 0.39
Batch: 200; loss: 1.86; acc: 0.31
Batch: 220; loss: 1.73; acc: 0.44
Batch: 240; loss: 1.98; acc: 0.3
Batch: 260; loss: 1.77; acc: 0.38
Batch: 280; loss: 1.81; acc: 0.36
Batch: 300; loss: 1.68; acc: 0.39
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 2.04; acc: 0.22
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 1.82; acc: 0.36
Batch: 400; loss: 1.94; acc: 0.3
Batch: 420; loss: 1.88; acc: 0.34
Batch: 440; loss: 2.06; acc: 0.31
Batch: 460; loss: 1.91; acc: 0.33
Batch: 480; loss: 1.8; acc: 0.42
Batch: 500; loss: 1.55; acc: 0.48
Batch: 520; loss: 1.83; acc: 0.33
Batch: 540; loss: 2.08; acc: 0.2
Batch: 560; loss: 1.79; acc: 0.36
Batch: 580; loss: 1.72; acc: 0.44
Batch: 600; loss: 1.93; acc: 0.41
Batch: 620; loss: 1.81; acc: 0.45
Batch: 640; loss: 1.88; acc: 0.34
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.76; acc: 0.33
Batch: 700; loss: 1.84; acc: 0.3
Batch: 720; loss: 1.78; acc: 0.36
Batch: 740; loss: 1.77; acc: 0.38
Batch: 760; loss: 1.92; acc: 0.25
Batch: 780; loss: 1.7; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.33
Batch: 20; loss: 1.86; acc: 0.33
Batch: 40; loss: 1.73; acc: 0.36
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.86; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8104398022791384; val_accuracy: 0.3724124203821656 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.25
Batch: 40; loss: 1.89; acc: 0.38
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.85; acc: 0.38
Batch: 100; loss: 1.71; acc: 0.39
Batch: 120; loss: 1.83; acc: 0.38
Batch: 140; loss: 1.76; acc: 0.41
Batch: 160; loss: 1.71; acc: 0.42
Batch: 180; loss: 1.89; acc: 0.36
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.83; acc: 0.38
Batch: 240; loss: 1.81; acc: 0.36
Batch: 260; loss: 1.99; acc: 0.27
Batch: 280; loss: 1.93; acc: 0.33
Batch: 300; loss: 1.88; acc: 0.33
Batch: 320; loss: 1.9; acc: 0.31
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 1.74; acc: 0.44
Batch: 400; loss: 1.98; acc: 0.27
Batch: 420; loss: 1.75; acc: 0.45
Batch: 440; loss: 1.71; acc: 0.45
Batch: 460; loss: 1.87; acc: 0.36
Batch: 480; loss: 1.7; acc: 0.42
Batch: 500; loss: 1.89; acc: 0.3
Batch: 520; loss: 1.87; acc: 0.34
Batch: 540; loss: 1.88; acc: 0.34
Batch: 560; loss: 1.97; acc: 0.38
Batch: 580; loss: 1.88; acc: 0.31
Batch: 600; loss: 1.81; acc: 0.38
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.82; acc: 0.3
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.7; acc: 0.39
Batch: 700; loss: 1.98; acc: 0.27
Batch: 720; loss: 1.9; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.36
Batch: 760; loss: 1.87; acc: 0.36
Batch: 780; loss: 1.79; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.94; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.42
Batch: 100; loss: 1.84; acc: 0.31
Batch: 120; loss: 1.76; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8075379490093062; val_accuracy: 0.3724124203821656 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.9; acc: 0.39
Batch: 20; loss: 1.8; acc: 0.3
Batch: 40; loss: 2.0; acc: 0.25
Batch: 60; loss: 1.81; acc: 0.48
Batch: 80; loss: 1.73; acc: 0.41
Batch: 100; loss: 1.78; acc: 0.38
Batch: 120; loss: 1.78; acc: 0.34
Batch: 140; loss: 1.89; acc: 0.33
Batch: 160; loss: 1.67; acc: 0.42
Batch: 180; loss: 1.77; acc: 0.38
Batch: 200; loss: 1.66; acc: 0.47
Batch: 220; loss: 1.84; acc: 0.31
Batch: 240; loss: 1.91; acc: 0.33
Batch: 260; loss: 1.99; acc: 0.22
Batch: 280; loss: 1.94; acc: 0.3
Batch: 300; loss: 1.77; acc: 0.39
Batch: 320; loss: 1.87; acc: 0.36
Batch: 340; loss: 1.98; acc: 0.31
Batch: 360; loss: 1.93; acc: 0.31
Batch: 380; loss: 1.86; acc: 0.39
Batch: 400; loss: 1.79; acc: 0.38
Batch: 420; loss: 1.88; acc: 0.33
Batch: 440; loss: 1.68; acc: 0.42
Batch: 460; loss: 1.78; acc: 0.39
Batch: 480; loss: 1.94; acc: 0.3
Batch: 500; loss: 1.79; acc: 0.34
Batch: 520; loss: 1.79; acc: 0.34
Batch: 540; loss: 2.01; acc: 0.28
Batch: 560; loss: 1.89; acc: 0.36
Batch: 580; loss: 1.94; acc: 0.33
Batch: 600; loss: 1.67; acc: 0.39
Batch: 620; loss: 1.78; acc: 0.34
Batch: 640; loss: 1.72; acc: 0.42
Batch: 660; loss: 1.83; acc: 0.38
Batch: 680; loss: 1.94; acc: 0.39
Batch: 700; loss: 2.13; acc: 0.2
Batch: 720; loss: 1.9; acc: 0.33
Batch: 740; loss: 1.95; acc: 0.28
Batch: 760; loss: 1.82; acc: 0.36
Batch: 780; loss: 1.74; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.33
Batch: 20; loss: 1.83; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.82; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.38
Batch: 140; loss: 1.74; acc: 0.36
Val Epoch over. val_loss: 1.8115133813991668; val_accuracy: 0.36246019108280253 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.97; acc: 0.34
Batch: 60; loss: 1.91; acc: 0.36
Batch: 80; loss: 1.81; acc: 0.36
Batch: 100; loss: 1.91; acc: 0.28
Batch: 120; loss: 1.83; acc: 0.36
Batch: 140; loss: 1.64; acc: 0.41
Batch: 160; loss: 1.72; acc: 0.5
Batch: 180; loss: 1.84; acc: 0.34
Batch: 200; loss: 1.83; acc: 0.33
Batch: 220; loss: 1.77; acc: 0.36
Batch: 240; loss: 1.85; acc: 0.36
Batch: 260; loss: 1.78; acc: 0.45
Batch: 280; loss: 1.82; acc: 0.3
Batch: 300; loss: 1.85; acc: 0.34
Batch: 320; loss: 1.64; acc: 0.39
Batch: 340; loss: 1.96; acc: 0.36
Batch: 360; loss: 1.86; acc: 0.31
Batch: 380; loss: 1.87; acc: 0.31
Batch: 400; loss: 1.71; acc: 0.45
Batch: 420; loss: 1.82; acc: 0.38
Batch: 440; loss: 1.83; acc: 0.44
Batch: 460; loss: 1.73; acc: 0.44
Batch: 480; loss: 1.94; acc: 0.33
Batch: 500; loss: 1.87; acc: 0.36
Batch: 520; loss: 1.86; acc: 0.41
Batch: 540; loss: 1.79; acc: 0.41
Batch: 560; loss: 1.83; acc: 0.31
Batch: 580; loss: 1.86; acc: 0.33
Batch: 600; loss: 1.89; acc: 0.34
Batch: 620; loss: 1.84; acc: 0.33
Batch: 640; loss: 1.94; acc: 0.23
Batch: 660; loss: 1.86; acc: 0.31
Batch: 680; loss: 1.93; acc: 0.22
Batch: 700; loss: 1.72; acc: 0.45
Batch: 720; loss: 1.83; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.42
Batch: 760; loss: 1.88; acc: 0.31
Batch: 780; loss: 2.04; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.31
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8082734483062841; val_accuracy: 0.36833200636942676 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.98; acc: 0.34
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.84; acc: 0.36
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.9; acc: 0.34
Batch: 100; loss: 1.66; acc: 0.39
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.83; acc: 0.33
Batch: 160; loss: 1.68; acc: 0.47
Batch: 180; loss: 1.73; acc: 0.41
Batch: 200; loss: 1.65; acc: 0.45
Batch: 220; loss: 1.93; acc: 0.3
Batch: 240; loss: 1.76; acc: 0.36
Batch: 260; loss: 1.69; acc: 0.41
Batch: 280; loss: 1.89; acc: 0.31
Batch: 300; loss: 1.91; acc: 0.31
Batch: 320; loss: 1.78; acc: 0.36
Batch: 340; loss: 1.71; acc: 0.38
Batch: 360; loss: 1.71; acc: 0.44
Batch: 380; loss: 1.71; acc: 0.41
Batch: 400; loss: 1.8; acc: 0.45
Batch: 420; loss: 1.76; acc: 0.41
Batch: 440; loss: 1.68; acc: 0.41
Batch: 460; loss: 1.87; acc: 0.38
Batch: 480; loss: 1.9; acc: 0.33
Batch: 500; loss: 1.9; acc: 0.23
Batch: 520; loss: 1.74; acc: 0.38
Batch: 540; loss: 2.0; acc: 0.27
Batch: 560; loss: 1.86; acc: 0.34
Batch: 580; loss: 1.71; acc: 0.41
Batch: 600; loss: 1.68; acc: 0.44
Batch: 620; loss: 1.89; acc: 0.3
Batch: 640; loss: 2.02; acc: 0.27
Batch: 660; loss: 1.91; acc: 0.33
Batch: 680; loss: 1.75; acc: 0.36
Batch: 700; loss: 1.87; acc: 0.36
Batch: 720; loss: 1.98; acc: 0.28
Batch: 740; loss: 2.09; acc: 0.2
Batch: 760; loss: 1.91; acc: 0.28
Batch: 780; loss: 2.0; acc: 0.3
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.36
Batch: 20; loss: 1.87; acc: 0.36
Batch: 40; loss: 1.72; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.85; acc: 0.3
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8096840374029366; val_accuracy: 0.367734872611465 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.8; acc: 0.34
Batch: 20; loss: 1.83; acc: 0.31
Batch: 40; loss: 1.96; acc: 0.23
Batch: 60; loss: 1.95; acc: 0.27
Batch: 80; loss: 1.79; acc: 0.33
Batch: 100; loss: 1.95; acc: 0.23
Batch: 120; loss: 1.89; acc: 0.31
Batch: 140; loss: 1.83; acc: 0.34
Batch: 160; loss: 1.81; acc: 0.39
Batch: 180; loss: 1.75; acc: 0.44
Batch: 200; loss: 1.9; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.33
Batch: 240; loss: 1.84; acc: 0.39
Batch: 260; loss: 1.82; acc: 0.34
Batch: 280; loss: 1.82; acc: 0.33
Batch: 300; loss: 1.74; acc: 0.39
Batch: 320; loss: 1.98; acc: 0.28
Batch: 340; loss: 1.83; acc: 0.36
Batch: 360; loss: 1.83; acc: 0.36
Batch: 380; loss: 1.77; acc: 0.44
Batch: 400; loss: 1.91; acc: 0.36
Batch: 420; loss: 1.71; acc: 0.42
Batch: 440; loss: 1.9; acc: 0.36
Batch: 460; loss: 1.85; acc: 0.38
Batch: 480; loss: 1.82; acc: 0.41
Batch: 500; loss: 1.85; acc: 0.33
Batch: 520; loss: 1.8; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.39
Batch: 560; loss: 1.89; acc: 0.41
Batch: 580; loss: 1.88; acc: 0.34
Batch: 600; loss: 1.87; acc: 0.34
Batch: 620; loss: 1.85; acc: 0.39
Batch: 640; loss: 1.74; acc: 0.41
Batch: 660; loss: 1.78; acc: 0.39
Batch: 680; loss: 1.92; acc: 0.33
Batch: 700; loss: 1.98; acc: 0.31
Batch: 720; loss: 1.92; acc: 0.36
Batch: 740; loss: 1.6; acc: 0.45
Batch: 760; loss: 1.89; acc: 0.28
Batch: 780; loss: 1.73; acc: 0.38
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8071253686953501; val_accuracy: 0.3698248407643312 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.82; acc: 0.36
Batch: 20; loss: 1.86; acc: 0.28
Batch: 40; loss: 1.73; acc: 0.34
Batch: 60; loss: 1.93; acc: 0.34
Batch: 80; loss: 1.79; acc: 0.42
Batch: 100; loss: 1.72; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.25
Batch: 140; loss: 1.82; acc: 0.33
Batch: 160; loss: 1.74; acc: 0.39
Batch: 180; loss: 1.79; acc: 0.34
Batch: 200; loss: 1.76; acc: 0.42
Batch: 220; loss: 1.67; acc: 0.38
Batch: 240; loss: 1.93; acc: 0.28
Batch: 260; loss: 1.72; acc: 0.44
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.8; acc: 0.36
Batch: 320; loss: 1.92; acc: 0.28
Batch: 340; loss: 1.8; acc: 0.39
Batch: 360; loss: 1.79; acc: 0.33
Batch: 380; loss: 1.98; acc: 0.27
Batch: 400; loss: 1.9; acc: 0.33
Batch: 420; loss: 1.83; acc: 0.38
Batch: 440; loss: 2.0; acc: 0.17
Batch: 460; loss: 1.88; acc: 0.36
Batch: 480; loss: 1.79; acc: 0.34
Batch: 500; loss: 1.8; acc: 0.25
Batch: 520; loss: 1.83; acc: 0.42
Batch: 540; loss: 1.9; acc: 0.36
Batch: 560; loss: 1.86; acc: 0.31
Batch: 580; loss: 1.97; acc: 0.27
Batch: 600; loss: 1.89; acc: 0.33
Batch: 620; loss: 1.68; acc: 0.41
Batch: 640; loss: 2.03; acc: 0.25
Batch: 660; loss: 1.83; acc: 0.39
Batch: 680; loss: 1.82; acc: 0.36
Batch: 700; loss: 1.81; acc: 0.38
Batch: 720; loss: 1.78; acc: 0.36
Batch: 740; loss: 1.87; acc: 0.27
Batch: 760; loss: 1.73; acc: 0.48
Batch: 780; loss: 1.89; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8087517083830136; val_accuracy: 0.368531050955414 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.78; acc: 0.33
Batch: 60; loss: 1.74; acc: 0.38
Batch: 80; loss: 1.92; acc: 0.36
Batch: 100; loss: 1.86; acc: 0.38
Batch: 120; loss: 1.84; acc: 0.34
Batch: 140; loss: 1.83; acc: 0.38
Batch: 160; loss: 1.81; acc: 0.34
Batch: 180; loss: 1.86; acc: 0.39
Batch: 200; loss: 1.99; acc: 0.22
Batch: 220; loss: 1.67; acc: 0.47
Batch: 240; loss: 1.88; acc: 0.39
Batch: 260; loss: 1.69; acc: 0.47
Batch: 280; loss: 1.78; acc: 0.42
Batch: 300; loss: 1.94; acc: 0.36
Batch: 320; loss: 1.77; acc: 0.41
Batch: 340; loss: 1.93; acc: 0.34
Batch: 360; loss: 1.75; acc: 0.42
Batch: 380; loss: 1.81; acc: 0.33
Batch: 400; loss: 1.97; acc: 0.31
Batch: 420; loss: 1.73; acc: 0.45
Batch: 440; loss: 1.76; acc: 0.33
Batch: 460; loss: 1.8; acc: 0.33
Batch: 480; loss: 1.84; acc: 0.39
Batch: 500; loss: 2.04; acc: 0.27
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.94; acc: 0.34
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.94; acc: 0.25
Batch: 620; loss: 1.73; acc: 0.42
Batch: 640; loss: 1.86; acc: 0.34
Batch: 660; loss: 1.68; acc: 0.41
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.72; acc: 0.42
Batch: 720; loss: 1.7; acc: 0.44
Batch: 740; loss: 1.96; acc: 0.33
Batch: 760; loss: 1.89; acc: 0.36
Batch: 780; loss: 1.68; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.74; acc: 0.36
Val Epoch over. val_loss: 1.8091296048680687; val_accuracy: 0.36733678343949044 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.83; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.84; acc: 0.34
Batch: 60; loss: 1.81; acc: 0.42
Batch: 80; loss: 1.76; acc: 0.44
Batch: 100; loss: 1.94; acc: 0.3
Batch: 120; loss: 1.81; acc: 0.3
Batch: 140; loss: 1.87; acc: 0.31
Batch: 160; loss: 1.98; acc: 0.3
Batch: 180; loss: 1.87; acc: 0.33
Batch: 200; loss: 1.89; acc: 0.31
Batch: 220; loss: 1.85; acc: 0.31
Batch: 240; loss: 1.97; acc: 0.38
Batch: 260; loss: 1.87; acc: 0.27
Batch: 280; loss: 1.85; acc: 0.41
Batch: 300; loss: 1.72; acc: 0.38
Batch: 320; loss: 1.79; acc: 0.33
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.63; acc: 0.5
Batch: 380; loss: 1.72; acc: 0.41
Batch: 400; loss: 1.87; acc: 0.36
Batch: 420; loss: 1.9; acc: 0.33
Batch: 440; loss: 2.08; acc: 0.28
Batch: 460; loss: 1.9; acc: 0.3
Batch: 480; loss: 1.98; acc: 0.36
Batch: 500; loss: 1.88; acc: 0.28
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.78; acc: 0.45
Batch: 560; loss: 1.91; acc: 0.34
Batch: 580; loss: 1.93; acc: 0.33
Batch: 600; loss: 1.84; acc: 0.34
Batch: 620; loss: 1.68; acc: 0.42
Batch: 640; loss: 1.81; acc: 0.42
Batch: 660; loss: 1.68; acc: 0.36
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 2.0; acc: 0.3
Batch: 720; loss: 1.86; acc: 0.33
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.88; acc: 0.33
Batch: 780; loss: 1.85; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.3
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.808962026219459; val_accuracy: 0.36833200636942676 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.85; acc: 0.33
Batch: 20; loss: 1.85; acc: 0.41
Batch: 40; loss: 1.76; acc: 0.38
Batch: 60; loss: 1.75; acc: 0.38
Batch: 80; loss: 1.75; acc: 0.38
Batch: 100; loss: 1.76; acc: 0.39
Batch: 120; loss: 1.9; acc: 0.28
Batch: 140; loss: 1.92; acc: 0.25
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.67; acc: 0.47
Batch: 200; loss: 1.9; acc: 0.34
Batch: 220; loss: 1.68; acc: 0.44
Batch: 240; loss: 1.82; acc: 0.36
Batch: 260; loss: 1.61; acc: 0.5
Batch: 280; loss: 1.79; acc: 0.31
Batch: 300; loss: 1.76; acc: 0.42
Batch: 320; loss: 1.82; acc: 0.31
Batch: 340; loss: 1.78; acc: 0.38
Batch: 360; loss: 2.03; acc: 0.36
Batch: 380; loss: 1.94; acc: 0.33
Batch: 400; loss: 2.08; acc: 0.27
Batch: 420; loss: 1.85; acc: 0.33
Batch: 440; loss: 1.76; acc: 0.44
Batch: 460; loss: 1.78; acc: 0.34
Batch: 480; loss: 1.81; acc: 0.34
Batch: 500; loss: 1.75; acc: 0.36
Batch: 520; loss: 1.93; acc: 0.27
Batch: 540; loss: 1.91; acc: 0.36
Batch: 560; loss: 2.0; acc: 0.27
Batch: 580; loss: 1.82; acc: 0.33
Batch: 600; loss: 2.0; acc: 0.25
Batch: 620; loss: 1.88; acc: 0.34
Batch: 640; loss: 1.98; acc: 0.23
Batch: 660; loss: 1.79; acc: 0.48
Batch: 680; loss: 1.91; acc: 0.3
Batch: 700; loss: 1.63; acc: 0.5
Batch: 720; loss: 1.85; acc: 0.36
Batch: 740; loss: 1.71; acc: 0.39
Batch: 760; loss: 1.87; acc: 0.38
Batch: 780; loss: 1.74; acc: 0.48
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8068295011095181; val_accuracy: 0.3692277070063694 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 1.8; acc: 0.38
Batch: 40; loss: 1.82; acc: 0.41
Batch: 60; loss: 1.77; acc: 0.39
Batch: 80; loss: 1.9; acc: 0.3
Batch: 100; loss: 1.8; acc: 0.42
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.97; acc: 0.31
Batch: 160; loss: 1.77; acc: 0.42
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.82; acc: 0.31
Batch: 220; loss: 1.82; acc: 0.39
Batch: 240; loss: 1.86; acc: 0.38
Batch: 260; loss: 1.94; acc: 0.28
Batch: 280; loss: 1.82; acc: 0.36
Batch: 300; loss: 1.77; acc: 0.36
Batch: 320; loss: 1.86; acc: 0.34
Batch: 340; loss: 1.74; acc: 0.39
Batch: 360; loss: 1.9; acc: 0.34
Batch: 380; loss: 1.81; acc: 0.33
Batch: 400; loss: 1.8; acc: 0.34
Batch: 420; loss: 1.97; acc: 0.31
Batch: 440; loss: 1.8; acc: 0.38
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.92; acc: 0.33
Batch: 500; loss: 1.82; acc: 0.33
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.71; acc: 0.44
Batch: 560; loss: 1.73; acc: 0.33
Batch: 580; loss: 1.71; acc: 0.44
Batch: 600; loss: 1.7; acc: 0.38
Batch: 620; loss: 1.8; acc: 0.31
Batch: 640; loss: 1.74; acc: 0.36
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.81; acc: 0.41
Batch: 700; loss: 2.07; acc: 0.25
Batch: 720; loss: 1.86; acc: 0.42
Batch: 740; loss: 2.03; acc: 0.17
Batch: 760; loss: 1.92; acc: 0.2
Batch: 780; loss: 1.78; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.3
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8084324036434198; val_accuracy: 0.3682324840764331 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.91; acc: 0.3
Batch: 20; loss: 1.77; acc: 0.39
Batch: 40; loss: 1.71; acc: 0.42
Batch: 60; loss: 1.71; acc: 0.39
Batch: 80; loss: 1.77; acc: 0.39
Batch: 100; loss: 1.81; acc: 0.41
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.82; acc: 0.28
Batch: 160; loss: 1.84; acc: 0.39
Batch: 180; loss: 2.04; acc: 0.25
Batch: 200; loss: 2.19; acc: 0.23
Batch: 220; loss: 1.77; acc: 0.34
Batch: 240; loss: 1.98; acc: 0.2
Batch: 260; loss: 1.77; acc: 0.38
Batch: 280; loss: 1.75; acc: 0.38
Batch: 300; loss: 1.79; acc: 0.42
Batch: 320; loss: 1.86; acc: 0.31
Batch: 340; loss: 1.83; acc: 0.39
Batch: 360; loss: 1.83; acc: 0.31
Batch: 380; loss: 1.84; acc: 0.33
Batch: 400; loss: 1.88; acc: 0.36
Batch: 420; loss: 1.72; acc: 0.41
Batch: 440; loss: 1.85; acc: 0.36
Batch: 460; loss: 1.75; acc: 0.38
Batch: 480; loss: 1.82; acc: 0.41
Batch: 500; loss: 1.71; acc: 0.47
Batch: 520; loss: 1.89; acc: 0.31
Batch: 540; loss: 2.02; acc: 0.23
Batch: 560; loss: 1.77; acc: 0.44
Batch: 580; loss: 1.8; acc: 0.38
Batch: 600; loss: 1.84; acc: 0.41
Batch: 620; loss: 1.89; acc: 0.34
Batch: 640; loss: 2.06; acc: 0.3
Batch: 660; loss: 1.81; acc: 0.38
Batch: 680; loss: 1.79; acc: 0.38
Batch: 700; loss: 1.86; acc: 0.34
Batch: 720; loss: 1.8; acc: 0.36
Batch: 740; loss: 1.81; acc: 0.38
Batch: 760; loss: 1.94; acc: 0.33
Batch: 780; loss: 1.76; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8087596247909934; val_accuracy: 0.367734872611465 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.79; acc: 0.36
Batch: 60; loss: 1.78; acc: 0.31
Batch: 80; loss: 1.88; acc: 0.33
Batch: 100; loss: 1.75; acc: 0.42
Batch: 120; loss: 1.9; acc: 0.31
Batch: 140; loss: 1.85; acc: 0.27
Batch: 160; loss: 1.75; acc: 0.34
Batch: 180; loss: 1.69; acc: 0.44
Batch: 200; loss: 1.89; acc: 0.33
Batch: 220; loss: 1.91; acc: 0.36
Batch: 240; loss: 1.59; acc: 0.38
Batch: 260; loss: 1.87; acc: 0.36
Batch: 280; loss: 1.99; acc: 0.34
Batch: 300; loss: 1.79; acc: 0.34
Batch: 320; loss: 1.84; acc: 0.31
Batch: 340; loss: 1.82; acc: 0.36
Batch: 360; loss: 1.74; acc: 0.36
Batch: 380; loss: 1.75; acc: 0.42
Batch: 400; loss: 1.83; acc: 0.34
Batch: 420; loss: 1.9; acc: 0.3
Batch: 440; loss: 1.93; acc: 0.25
Batch: 460; loss: 1.77; acc: 0.39
Batch: 480; loss: 1.91; acc: 0.31
Batch: 500; loss: 2.03; acc: 0.27
Batch: 520; loss: 1.99; acc: 0.28
Batch: 540; loss: 1.74; acc: 0.38
Batch: 560; loss: 1.74; acc: 0.34
Batch: 580; loss: 1.95; acc: 0.31
Batch: 600; loss: 1.92; acc: 0.36
Batch: 620; loss: 1.91; acc: 0.25
Batch: 640; loss: 1.73; acc: 0.39
Batch: 660; loss: 1.9; acc: 0.38
Batch: 680; loss: 1.82; acc: 0.44
Batch: 700; loss: 1.75; acc: 0.39
Batch: 720; loss: 1.81; acc: 0.33
Batch: 740; loss: 1.78; acc: 0.44
Batch: 760; loss: 1.81; acc: 0.39
Batch: 780; loss: 1.82; acc: 0.45
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8078350527271343; val_accuracy: 0.3684315286624204 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.87; acc: 0.34
Batch: 60; loss: 1.86; acc: 0.34
Batch: 80; loss: 1.8; acc: 0.33
Batch: 100; loss: 1.85; acc: 0.3
Batch: 120; loss: 1.91; acc: 0.28
Batch: 140; loss: 1.74; acc: 0.39
Batch: 160; loss: 1.84; acc: 0.38
Batch: 180; loss: 1.82; acc: 0.41
Batch: 200; loss: 1.83; acc: 0.38
Batch: 220; loss: 1.87; acc: 0.38
Batch: 240; loss: 2.04; acc: 0.3
Batch: 260; loss: 1.74; acc: 0.44
Batch: 280; loss: 1.89; acc: 0.3
Batch: 300; loss: 1.63; acc: 0.44
Batch: 320; loss: 1.81; acc: 0.31
Batch: 340; loss: 1.76; acc: 0.41
Batch: 360; loss: 1.98; acc: 0.31
Batch: 380; loss: 1.85; acc: 0.39
Batch: 400; loss: 1.84; acc: 0.34
Batch: 420; loss: 1.85; acc: 0.38
Batch: 440; loss: 1.86; acc: 0.34
Batch: 460; loss: 1.92; acc: 0.41
Batch: 480; loss: 1.78; acc: 0.42
Batch: 500; loss: 2.0; acc: 0.34
Batch: 520; loss: 1.82; acc: 0.33
Batch: 540; loss: 1.76; acc: 0.38
Batch: 560; loss: 1.94; acc: 0.3
Batch: 580; loss: 1.99; acc: 0.36
Batch: 600; loss: 1.86; acc: 0.3
Batch: 620; loss: 1.86; acc: 0.27
Batch: 640; loss: 1.69; acc: 0.38
Batch: 660; loss: 1.87; acc: 0.39
Batch: 680; loss: 1.86; acc: 0.34
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.87; acc: 0.36
Batch: 740; loss: 1.85; acc: 0.31
Batch: 760; loss: 1.81; acc: 0.31
Batch: 780; loss: 1.86; acc: 0.44
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075189172841941; val_accuracy: 0.3700238853503185 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.83; acc: 0.33
Batch: 60; loss: 1.9; acc: 0.34
Batch: 80; loss: 1.99; acc: 0.33
Batch: 100; loss: 1.79; acc: 0.3
Batch: 120; loss: 1.82; acc: 0.31
Batch: 140; loss: 1.88; acc: 0.38
Batch: 160; loss: 1.9; acc: 0.3
Batch: 180; loss: 1.75; acc: 0.45
Batch: 200; loss: 1.68; acc: 0.41
Batch: 220; loss: 1.71; acc: 0.41
Batch: 240; loss: 1.76; acc: 0.39
Batch: 260; loss: 1.76; acc: 0.36
Batch: 280; loss: 1.9; acc: 0.28
Batch: 300; loss: 1.88; acc: 0.36
Batch: 320; loss: 1.78; acc: 0.39
Batch: 340; loss: 1.96; acc: 0.34
Batch: 360; loss: 1.81; acc: 0.45
Batch: 380; loss: 1.6; acc: 0.44
Batch: 400; loss: 1.77; acc: 0.39
Batch: 420; loss: 1.75; acc: 0.41
Batch: 440; loss: 1.77; acc: 0.38
Batch: 460; loss: 1.87; acc: 0.38
Batch: 480; loss: 1.95; acc: 0.34
Batch: 500; loss: 1.82; acc: 0.36
Batch: 520; loss: 2.03; acc: 0.31
Batch: 540; loss: 1.76; acc: 0.47
Batch: 560; loss: 1.66; acc: 0.44
Batch: 580; loss: 1.71; acc: 0.36
Batch: 600; loss: 1.73; acc: 0.38
Batch: 620; loss: 1.76; acc: 0.38
Batch: 640; loss: 1.74; acc: 0.41
Batch: 660; loss: 1.82; acc: 0.39
Batch: 680; loss: 1.71; acc: 0.39
Batch: 700; loss: 1.99; acc: 0.33
Batch: 720; loss: 1.89; acc: 0.31
Batch: 740; loss: 1.98; acc: 0.28
Batch: 760; loss: 1.84; acc: 0.39
Batch: 780; loss: 1.75; acc: 0.36
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8074699253033681; val_accuracy: 0.3692277070063694 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.08; acc: 0.31
Batch: 20; loss: 1.99; acc: 0.33
Batch: 40; loss: 1.97; acc: 0.31
Batch: 60; loss: 1.84; acc: 0.38
Batch: 80; loss: 1.86; acc: 0.31
Batch: 100; loss: 1.77; acc: 0.33
Batch: 120; loss: 1.9; acc: 0.36
Batch: 140; loss: 1.79; acc: 0.36
Batch: 160; loss: 1.83; acc: 0.38
Batch: 180; loss: 1.88; acc: 0.38
Batch: 200; loss: 1.95; acc: 0.28
Batch: 220; loss: 2.0; acc: 0.3
Batch: 240; loss: 1.82; acc: 0.45
Batch: 260; loss: 1.95; acc: 0.38
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.76; acc: 0.39
Batch: 320; loss: 1.95; acc: 0.27
Batch: 340; loss: 1.91; acc: 0.3
Batch: 360; loss: 1.68; acc: 0.47
Batch: 380; loss: 1.8; acc: 0.38
Batch: 400; loss: 1.82; acc: 0.34
Batch: 420; loss: 1.9; acc: 0.39
Batch: 440; loss: 2.0; acc: 0.23
Batch: 460; loss: 1.83; acc: 0.33
Batch: 480; loss: 1.76; acc: 0.34
Batch: 500; loss: 1.82; acc: 0.33
Batch: 520; loss: 1.88; acc: 0.36
Batch: 540; loss: 1.8; acc: 0.36
Batch: 560; loss: 1.74; acc: 0.36
Batch: 580; loss: 1.93; acc: 0.27
Batch: 600; loss: 1.7; acc: 0.47
Batch: 620; loss: 1.89; acc: 0.34
Batch: 640; loss: 1.85; acc: 0.33
Batch: 660; loss: 1.97; acc: 0.3
Batch: 680; loss: 1.91; acc: 0.25
Batch: 700; loss: 2.0; acc: 0.3
Batch: 720; loss: 1.9; acc: 0.39
Batch: 740; loss: 1.69; acc: 0.42
Batch: 760; loss: 1.85; acc: 0.33
Batch: 780; loss: 1.89; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8078009429251312; val_accuracy: 0.3691281847133758 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.86; acc: 0.39
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.84; acc: 0.44
Batch: 60; loss: 1.88; acc: 0.31
Batch: 80; loss: 1.86; acc: 0.38
Batch: 100; loss: 1.94; acc: 0.28
Batch: 120; loss: 2.05; acc: 0.28
Batch: 140; loss: 1.83; acc: 0.31
Batch: 160; loss: 1.7; acc: 0.48
Batch: 180; loss: 1.76; acc: 0.34
Batch: 200; loss: 1.87; acc: 0.27
Batch: 220; loss: 1.82; acc: 0.34
Batch: 240; loss: 1.94; acc: 0.31
Batch: 260; loss: 1.79; acc: 0.38
Batch: 280; loss: 1.82; acc: 0.33
Batch: 300; loss: 1.81; acc: 0.33
Batch: 320; loss: 1.77; acc: 0.36
Batch: 340; loss: 1.93; acc: 0.3
Batch: 360; loss: 1.88; acc: 0.34
Batch: 380; loss: 1.82; acc: 0.33
Batch: 400; loss: 1.87; acc: 0.31
Batch: 420; loss: 1.78; acc: 0.42
Batch: 440; loss: 2.0; acc: 0.31
Batch: 460; loss: 1.88; acc: 0.27
Batch: 480; loss: 1.89; acc: 0.36
Batch: 500; loss: 1.79; acc: 0.36
Batch: 520; loss: 1.79; acc: 0.38
Batch: 540; loss: 1.71; acc: 0.44
Batch: 560; loss: 1.83; acc: 0.31
Batch: 580; loss: 1.73; acc: 0.42
Batch: 600; loss: 1.79; acc: 0.42
Batch: 620; loss: 1.95; acc: 0.28
Batch: 640; loss: 1.88; acc: 0.41
Batch: 660; loss: 1.77; acc: 0.39
Batch: 680; loss: 1.82; acc: 0.36
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.88; acc: 0.33
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.92; acc: 0.34
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807729256380895; val_accuracy: 0.3700238853503185 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.92; acc: 0.22
Batch: 20; loss: 1.98; acc: 0.31
Batch: 40; loss: 1.92; acc: 0.34
Batch: 60; loss: 1.72; acc: 0.38
Batch: 80; loss: 1.69; acc: 0.45
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.81; acc: 0.44
Batch: 140; loss: 1.84; acc: 0.31
Batch: 160; loss: 1.85; acc: 0.3
Batch: 180; loss: 1.96; acc: 0.3
Batch: 200; loss: 1.97; acc: 0.27
Batch: 220; loss: 1.72; acc: 0.44
Batch: 240; loss: 1.87; acc: 0.27
Batch: 260; loss: 1.81; acc: 0.3
Batch: 280; loss: 1.86; acc: 0.41
Batch: 300; loss: 1.96; acc: 0.36
Batch: 320; loss: 1.71; acc: 0.41
Batch: 340; loss: 1.69; acc: 0.42
Batch: 360; loss: 1.92; acc: 0.31
Batch: 380; loss: 1.74; acc: 0.39
Batch: 400; loss: 1.79; acc: 0.41
Batch: 420; loss: 2.01; acc: 0.3
Batch: 440; loss: 1.99; acc: 0.25
Batch: 460; loss: 1.87; acc: 0.42
Batch: 480; loss: 1.97; acc: 0.28
Batch: 500; loss: 1.61; acc: 0.44
Batch: 520; loss: 1.81; acc: 0.36
Batch: 540; loss: 2.0; acc: 0.34
Batch: 560; loss: 1.83; acc: 0.34
Batch: 580; loss: 1.83; acc: 0.36
Batch: 600; loss: 1.73; acc: 0.34
Batch: 620; loss: 1.77; acc: 0.41
Batch: 640; loss: 1.81; acc: 0.45
Batch: 660; loss: 1.74; acc: 0.38
Batch: 680; loss: 1.92; acc: 0.31
Batch: 700; loss: 1.76; acc: 0.42
Batch: 720; loss: 1.81; acc: 0.3
Batch: 740; loss: 1.84; acc: 0.31
Batch: 760; loss: 1.89; acc: 0.33
Batch: 780; loss: 2.05; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8075307045772577; val_accuracy: 0.37062101910828027 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.78; acc: 0.44
Batch: 20; loss: 1.93; acc: 0.27
Batch: 40; loss: 1.78; acc: 0.39
Batch: 60; loss: 1.81; acc: 0.36
Batch: 80; loss: 1.69; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.3
Batch: 120; loss: 2.01; acc: 0.3
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.91; acc: 0.38
Batch: 180; loss: 1.77; acc: 0.41
Batch: 200; loss: 1.83; acc: 0.42
Batch: 220; loss: 1.77; acc: 0.42
Batch: 240; loss: 1.9; acc: 0.31
Batch: 260; loss: 1.78; acc: 0.33
Batch: 280; loss: 1.94; acc: 0.34
Batch: 300; loss: 1.95; acc: 0.28
Batch: 320; loss: 1.91; acc: 0.31
Batch: 340; loss: 1.78; acc: 0.39
Batch: 360; loss: 1.95; acc: 0.34
Batch: 380; loss: 1.56; acc: 0.48
Batch: 400; loss: 1.75; acc: 0.41
Batch: 420; loss: 1.83; acc: 0.36
Batch: 440; loss: 1.96; acc: 0.33
Batch: 460; loss: 1.77; acc: 0.33
Batch: 480; loss: 1.87; acc: 0.34
Batch: 500; loss: 1.78; acc: 0.38
Batch: 520; loss: 2.0; acc: 0.27
Batch: 540; loss: 1.88; acc: 0.45
Batch: 560; loss: 2.01; acc: 0.25
Batch: 580; loss: 1.85; acc: 0.39
Batch: 600; loss: 2.0; acc: 0.25
Batch: 620; loss: 1.82; acc: 0.34
Batch: 640; loss: 1.9; acc: 0.27
Batch: 660; loss: 1.95; acc: 0.36
Batch: 680; loss: 1.86; acc: 0.36
Batch: 700; loss: 1.64; acc: 0.45
Batch: 720; loss: 1.72; acc: 0.48
Batch: 740; loss: 1.89; acc: 0.27
Batch: 760; loss: 1.74; acc: 0.41
Batch: 780; loss: 1.83; acc: 0.38
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.74; acc: 0.36
Val Epoch over. val_loss: 1.807612424443482; val_accuracy: 0.36634156050955413 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.0; acc: 0.3
Batch: 20; loss: 1.73; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.39
Batch: 60; loss: 1.77; acc: 0.41
Batch: 80; loss: 2.0; acc: 0.3
Batch: 100; loss: 1.85; acc: 0.28
Batch: 120; loss: 1.86; acc: 0.23
Batch: 140; loss: 1.8; acc: 0.38
Batch: 160; loss: 1.72; acc: 0.47
Batch: 180; loss: 1.82; acc: 0.38
Batch: 200; loss: 1.93; acc: 0.23
Batch: 220; loss: 1.75; acc: 0.42
Batch: 240; loss: 1.76; acc: 0.41
Batch: 260; loss: 1.77; acc: 0.34
Batch: 280; loss: 1.86; acc: 0.36
Batch: 300; loss: 1.9; acc: 0.25
Batch: 320; loss: 1.78; acc: 0.33
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.99; acc: 0.25
Batch: 380; loss: 1.91; acc: 0.31
Batch: 400; loss: 1.72; acc: 0.42
Batch: 420; loss: 1.85; acc: 0.3
Batch: 440; loss: 2.02; acc: 0.23
Batch: 460; loss: 1.8; acc: 0.33
Batch: 480; loss: 1.84; acc: 0.3
Batch: 500; loss: 1.9; acc: 0.33
Batch: 520; loss: 1.76; acc: 0.34
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.96; acc: 0.27
Batch: 580; loss: 1.7; acc: 0.38
Batch: 600; loss: 1.78; acc: 0.36
Batch: 620; loss: 2.0; acc: 0.25
Batch: 640; loss: 1.84; acc: 0.41
Batch: 660; loss: 1.73; acc: 0.42
Batch: 680; loss: 1.81; acc: 0.33
Batch: 700; loss: 1.92; acc: 0.28
Batch: 720; loss: 1.9; acc: 0.31
Batch: 740; loss: 1.77; acc: 0.34
Batch: 760; loss: 1.76; acc: 0.39
Batch: 780; loss: 1.7; acc: 0.48
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8070680032110518; val_accuracy: 0.37121815286624205 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.91; acc: 0.36
Batch: 20; loss: 1.81; acc: 0.38
Batch: 40; loss: 1.69; acc: 0.41
Batch: 60; loss: 1.9; acc: 0.36
Batch: 80; loss: 1.86; acc: 0.31
Batch: 100; loss: 1.94; acc: 0.34
Batch: 120; loss: 1.81; acc: 0.39
Batch: 140; loss: 1.77; acc: 0.44
Batch: 160; loss: 1.82; acc: 0.34
Batch: 180; loss: 1.83; acc: 0.36
Batch: 200; loss: 1.72; acc: 0.45
Batch: 220; loss: 1.87; acc: 0.27
Batch: 240; loss: 1.94; acc: 0.34
Batch: 260; loss: 1.93; acc: 0.28
Batch: 280; loss: 1.84; acc: 0.3
Batch: 300; loss: 1.85; acc: 0.3
Batch: 320; loss: 1.79; acc: 0.38
Batch: 340; loss: 1.8; acc: 0.3
Batch: 360; loss: 1.58; acc: 0.41
Batch: 380; loss: 1.83; acc: 0.41
Batch: 400; loss: 1.75; acc: 0.38
Batch: 420; loss: 1.89; acc: 0.3
Batch: 440; loss: 1.86; acc: 0.38
Batch: 460; loss: 1.8; acc: 0.39
Batch: 480; loss: 1.93; acc: 0.34
Batch: 500; loss: 1.89; acc: 0.41
Batch: 520; loss: 1.76; acc: 0.36
Batch: 540; loss: 1.95; acc: 0.3
Batch: 560; loss: 1.86; acc: 0.3
Batch: 580; loss: 1.88; acc: 0.36
Batch: 600; loss: 1.75; acc: 0.45
Batch: 620; loss: 1.92; acc: 0.27
Batch: 640; loss: 1.83; acc: 0.38
Batch: 660; loss: 1.87; acc: 0.39
Batch: 680; loss: 1.99; acc: 0.27
Batch: 700; loss: 1.78; acc: 0.41
Batch: 720; loss: 1.92; acc: 0.33
Batch: 740; loss: 1.77; acc: 0.42
Batch: 760; loss: 1.82; acc: 0.36
Batch: 780; loss: 1.73; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075039083031332; val_accuracy: 0.37121815286624205 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.85; acc: 0.41
Batch: 20; loss: 1.79; acc: 0.39
Batch: 40; loss: 1.73; acc: 0.3
Batch: 60; loss: 1.78; acc: 0.36
Batch: 80; loss: 1.88; acc: 0.28
Batch: 100; loss: 1.75; acc: 0.36
Batch: 120; loss: 1.87; acc: 0.33
Batch: 140; loss: 1.82; acc: 0.34
Batch: 160; loss: 1.89; acc: 0.3
Batch: 180; loss: 1.82; acc: 0.36
Batch: 200; loss: 1.92; acc: 0.27
Batch: 220; loss: 1.74; acc: 0.33
Batch: 240; loss: 1.85; acc: 0.47
Batch: 260; loss: 2.03; acc: 0.2
Batch: 280; loss: 1.82; acc: 0.34
Batch: 300; loss: 1.88; acc: 0.27
Batch: 320; loss: 1.82; acc: 0.38
Batch: 340; loss: 1.9; acc: 0.31
Batch: 360; loss: 1.82; acc: 0.31
Batch: 380; loss: 1.79; acc: 0.41
Batch: 400; loss: 1.82; acc: 0.36
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.74; acc: 0.41
Batch: 460; loss: 1.75; acc: 0.44
Batch: 480; loss: 1.88; acc: 0.28
Batch: 500; loss: 1.96; acc: 0.39
Batch: 520; loss: 1.86; acc: 0.38
Batch: 540; loss: 1.73; acc: 0.36
Batch: 560; loss: 1.83; acc: 0.44
Batch: 580; loss: 1.94; acc: 0.27
Batch: 600; loss: 1.86; acc: 0.36
Batch: 620; loss: 1.8; acc: 0.39
Batch: 640; loss: 1.69; acc: 0.45
Batch: 660; loss: 1.73; acc: 0.48
Batch: 680; loss: 1.86; acc: 0.28
Batch: 700; loss: 1.83; acc: 0.36
Batch: 720; loss: 1.92; acc: 0.31
Batch: 740; loss: 1.76; acc: 0.45
Batch: 760; loss: 2.04; acc: 0.28
Batch: 780; loss: 1.91; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075313097352435; val_accuracy: 0.37072054140127386 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.89; acc: 0.28
Batch: 60; loss: 1.89; acc: 0.31
Batch: 80; loss: 1.82; acc: 0.39
Batch: 100; loss: 1.79; acc: 0.39
Batch: 120; loss: 1.79; acc: 0.31
Batch: 140; loss: 1.89; acc: 0.31
Batch: 160; loss: 1.86; acc: 0.36
Batch: 180; loss: 1.85; acc: 0.36
Batch: 200; loss: 1.62; acc: 0.47
Batch: 220; loss: 1.55; acc: 0.45
Batch: 240; loss: 1.94; acc: 0.3
Batch: 260; loss: 1.79; acc: 0.34
Batch: 280; loss: 1.93; acc: 0.3
Batch: 300; loss: 1.91; acc: 0.31
Batch: 320; loss: 1.88; acc: 0.33
Batch: 340; loss: 1.84; acc: 0.38
Batch: 360; loss: 1.99; acc: 0.31
Batch: 380; loss: 1.83; acc: 0.34
Batch: 400; loss: 1.81; acc: 0.38
Batch: 420; loss: 1.99; acc: 0.31
Batch: 440; loss: 1.71; acc: 0.44
Batch: 460; loss: 1.85; acc: 0.34
Batch: 480; loss: 1.98; acc: 0.31
Batch: 500; loss: 2.07; acc: 0.33
Batch: 520; loss: 1.86; acc: 0.3
Batch: 540; loss: 1.81; acc: 0.28
Batch: 560; loss: 1.96; acc: 0.23
Batch: 580; loss: 1.89; acc: 0.33
Batch: 600; loss: 1.77; acc: 0.33
Batch: 620; loss: 1.86; acc: 0.34
Batch: 640; loss: 1.79; acc: 0.36
Batch: 660; loss: 1.86; acc: 0.38
Batch: 680; loss: 1.71; acc: 0.45
Batch: 700; loss: 1.89; acc: 0.31
Batch: 720; loss: 1.82; acc: 0.3
Batch: 740; loss: 1.76; acc: 0.41
Batch: 760; loss: 1.89; acc: 0.36
Batch: 780; loss: 1.9; acc: 0.27
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8076426929729; val_accuracy: 0.370421974522293 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.9; acc: 0.27
Batch: 60; loss: 1.85; acc: 0.38
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 1.87; acc: 0.36
Batch: 120; loss: 1.68; acc: 0.39
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.36
Batch: 180; loss: 1.87; acc: 0.34
Batch: 200; loss: 1.86; acc: 0.28
Batch: 220; loss: 1.88; acc: 0.33
Batch: 240; loss: 1.68; acc: 0.36
Batch: 260; loss: 1.87; acc: 0.34
Batch: 280; loss: 1.82; acc: 0.33
Batch: 300; loss: 1.79; acc: 0.38
Batch: 320; loss: 1.78; acc: 0.41
Batch: 340; loss: 1.94; acc: 0.27
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.45
Batch: 400; loss: 1.76; acc: 0.42
Batch: 420; loss: 1.85; acc: 0.34
Batch: 440; loss: 1.92; acc: 0.28
Batch: 460; loss: 1.86; acc: 0.36
Batch: 480; loss: 1.72; acc: 0.47
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.81; acc: 0.36
Batch: 540; loss: 1.8; acc: 0.33
Batch: 560; loss: 1.89; acc: 0.28
Batch: 580; loss: 1.86; acc: 0.33
Batch: 600; loss: 1.72; acc: 0.38
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.81; acc: 0.34
Batch: 660; loss: 1.78; acc: 0.38
Batch: 680; loss: 1.8; acc: 0.39
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.98; acc: 0.36
Batch: 740; loss: 1.94; acc: 0.34
Batch: 760; loss: 1.77; acc: 0.39
Batch: 780; loss: 1.81; acc: 0.36
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807179664350619; val_accuracy: 0.3705214968152866 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.83; acc: 0.44
Batch: 20; loss: 1.75; acc: 0.39
Batch: 40; loss: 1.89; acc: 0.39
Batch: 60; loss: 1.69; acc: 0.36
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.8; acc: 0.34
Batch: 120; loss: 1.91; acc: 0.3
Batch: 140; loss: 1.93; acc: 0.28
Batch: 160; loss: 1.88; acc: 0.34
Batch: 180; loss: 1.92; acc: 0.28
Batch: 200; loss: 1.69; acc: 0.39
Batch: 220; loss: 1.83; acc: 0.36
Batch: 240; loss: 1.93; acc: 0.28
Batch: 260; loss: 1.74; acc: 0.36
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.92; acc: 0.27
Batch: 320; loss: 1.91; acc: 0.25
Batch: 340; loss: 1.85; acc: 0.28
Batch: 360; loss: 1.9; acc: 0.38
Batch: 380; loss: 1.95; acc: 0.34
Batch: 400; loss: 1.8; acc: 0.34
Batch: 420; loss: 1.67; acc: 0.33
Batch: 440; loss: 1.73; acc: 0.47
Batch: 460; loss: 1.89; acc: 0.36
Batch: 480; loss: 1.97; acc: 0.27
Batch: 500; loss: 1.86; acc: 0.34
Batch: 520; loss: 2.01; acc: 0.31
Batch: 540; loss: 1.89; acc: 0.31
Batch: 560; loss: 1.98; acc: 0.34
Batch: 580; loss: 1.62; acc: 0.45
Batch: 600; loss: 1.84; acc: 0.42
Batch: 620; loss: 1.75; acc: 0.42
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.8; acc: 0.36
Batch: 680; loss: 1.86; acc: 0.39
Batch: 700; loss: 1.76; acc: 0.33
Batch: 720; loss: 1.72; acc: 0.38
Batch: 740; loss: 1.77; acc: 0.41
Batch: 760; loss: 1.98; acc: 0.31
Batch: 780; loss: 1.89; acc: 0.34
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8072817933027912; val_accuracy: 0.3705214968152866 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 1.8; acc: 0.41
Batch: 40; loss: 1.87; acc: 0.41
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.91; acc: 0.33
Batch: 100; loss: 1.85; acc: 0.27
Batch: 120; loss: 1.78; acc: 0.39
Batch: 140; loss: 1.92; acc: 0.33
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.82; acc: 0.39
Batch: 200; loss: 1.95; acc: 0.31
Batch: 220; loss: 1.69; acc: 0.53
Batch: 240; loss: 1.93; acc: 0.41
Batch: 260; loss: 1.71; acc: 0.42
Batch: 280; loss: 1.83; acc: 0.3
Batch: 300; loss: 1.9; acc: 0.28
Batch: 320; loss: 1.93; acc: 0.33
Batch: 340; loss: 1.97; acc: 0.28
Batch: 360; loss: 1.7; acc: 0.41
Batch: 380; loss: 1.86; acc: 0.31
Batch: 400; loss: 1.81; acc: 0.39
Batch: 420; loss: 1.73; acc: 0.42
Batch: 440; loss: 1.79; acc: 0.31
Batch: 460; loss: 1.82; acc: 0.34
Batch: 480; loss: 1.75; acc: 0.41
Batch: 500; loss: 1.51; acc: 0.55
Batch: 520; loss: 1.7; acc: 0.36
Batch: 540; loss: 1.74; acc: 0.44
Batch: 560; loss: 1.75; acc: 0.36
Batch: 580; loss: 1.91; acc: 0.28
Batch: 600; loss: 1.89; acc: 0.31
Batch: 620; loss: 1.85; acc: 0.23
Batch: 640; loss: 1.96; acc: 0.27
Batch: 660; loss: 1.81; acc: 0.39
Batch: 680; loss: 1.75; acc: 0.36
Batch: 700; loss: 1.8; acc: 0.36
Batch: 720; loss: 1.81; acc: 0.38
Batch: 740; loss: 1.84; acc: 0.34
Batch: 760; loss: 1.92; acc: 0.34
Batch: 780; loss: 1.83; acc: 0.38
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074242742198288; val_accuracy: 0.37022292993630573 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.88; acc: 0.31
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.84; acc: 0.3
Batch: 80; loss: 1.78; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.22
Batch: 120; loss: 1.8; acc: 0.28
Batch: 140; loss: 1.77; acc: 0.41
Batch: 160; loss: 1.88; acc: 0.34
Batch: 180; loss: 1.83; acc: 0.41
Batch: 200; loss: 1.98; acc: 0.34
Batch: 220; loss: 1.8; acc: 0.41
Batch: 240; loss: 1.96; acc: 0.36
Batch: 260; loss: 1.85; acc: 0.34
Batch: 280; loss: 1.88; acc: 0.39
Batch: 300; loss: 1.75; acc: 0.38
Batch: 320; loss: 1.92; acc: 0.23
Batch: 340; loss: 1.9; acc: 0.31
Batch: 360; loss: 1.91; acc: 0.33
Batch: 380; loss: 1.92; acc: 0.38
Batch: 400; loss: 1.95; acc: 0.33
Batch: 420; loss: 1.84; acc: 0.31
Batch: 440; loss: 2.0; acc: 0.3
Batch: 460; loss: 1.81; acc: 0.34
Batch: 480; loss: 1.82; acc: 0.31
Batch: 500; loss: 1.94; acc: 0.28
Batch: 520; loss: 1.72; acc: 0.5
Batch: 540; loss: 1.89; acc: 0.3
Batch: 560; loss: 1.68; acc: 0.48
Batch: 580; loss: 2.1; acc: 0.23
Batch: 600; loss: 1.71; acc: 0.47
Batch: 620; loss: 1.77; acc: 0.34
Batch: 640; loss: 1.79; acc: 0.44
Batch: 660; loss: 1.88; acc: 0.34
Batch: 680; loss: 1.82; acc: 0.39
Batch: 700; loss: 1.63; acc: 0.47
Batch: 720; loss: 1.76; acc: 0.34
Batch: 740; loss: 1.75; acc: 0.36
Batch: 760; loss: 1.78; acc: 0.33
Batch: 780; loss: 1.69; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075234100317499; val_accuracy: 0.3695262738853503 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.81; acc: 0.39
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.31
Batch: 60; loss: 1.79; acc: 0.41
Batch: 80; loss: 1.74; acc: 0.42
Batch: 100; loss: 1.92; acc: 0.3
Batch: 120; loss: 1.97; acc: 0.25
Batch: 140; loss: 1.8; acc: 0.41
Batch: 160; loss: 1.66; acc: 0.47
Batch: 180; loss: 1.76; acc: 0.34
Batch: 200; loss: 1.98; acc: 0.3
Batch: 220; loss: 1.84; acc: 0.34
Batch: 240; loss: 1.87; acc: 0.36
Batch: 260; loss: 1.85; acc: 0.45
Batch: 280; loss: 1.79; acc: 0.39
Batch: 300; loss: 1.79; acc: 0.39
Batch: 320; loss: 1.9; acc: 0.31
Batch: 340; loss: 1.97; acc: 0.33
Batch: 360; loss: 1.87; acc: 0.33
Batch: 380; loss: 1.9; acc: 0.3
Batch: 400; loss: 1.76; acc: 0.38
Batch: 420; loss: 1.95; acc: 0.34
Batch: 440; loss: 1.81; acc: 0.36
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.69; acc: 0.39
Batch: 500; loss: 1.7; acc: 0.41
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.86; acc: 0.39
Batch: 560; loss: 1.93; acc: 0.31
Batch: 580; loss: 1.88; acc: 0.33
Batch: 600; loss: 1.85; acc: 0.3
Batch: 620; loss: 1.67; acc: 0.48
Batch: 640; loss: 1.97; acc: 0.3
Batch: 660; loss: 1.76; acc: 0.42
Batch: 680; loss: 1.62; acc: 0.48
Batch: 700; loss: 1.87; acc: 0.36
Batch: 720; loss: 1.91; acc: 0.33
Batch: 740; loss: 1.92; acc: 0.27
Batch: 760; loss: 1.82; acc: 0.33
Batch: 780; loss: 1.89; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8074690551514838; val_accuracy: 0.37062101910828027 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.86; acc: 0.31
Batch: 20; loss: 1.74; acc: 0.34
Batch: 40; loss: 2.05; acc: 0.28
Batch: 60; loss: 1.84; acc: 0.34
Batch: 80; loss: 1.64; acc: 0.45
Batch: 100; loss: 2.0; acc: 0.33
Batch: 120; loss: 1.89; acc: 0.3
Batch: 140; loss: 2.04; acc: 0.19
Batch: 160; loss: 1.96; acc: 0.3
Batch: 180; loss: 1.81; acc: 0.38
Batch: 200; loss: 1.91; acc: 0.3
Batch: 220; loss: 1.65; acc: 0.42
Batch: 240; loss: 1.83; acc: 0.41
Batch: 260; loss: 1.83; acc: 0.36
Batch: 280; loss: 1.94; acc: 0.33
Batch: 300; loss: 1.63; acc: 0.42
Batch: 320; loss: 1.92; acc: 0.31
Batch: 340; loss: 1.83; acc: 0.41
Batch: 360; loss: 1.83; acc: 0.34
Batch: 380; loss: 1.88; acc: 0.34
Batch: 400; loss: 1.82; acc: 0.36
Batch: 420; loss: 1.82; acc: 0.38
Batch: 440; loss: 1.74; acc: 0.34
Batch: 460; loss: 1.81; acc: 0.3
Batch: 480; loss: 1.84; acc: 0.41
Batch: 500; loss: 1.83; acc: 0.39
Batch: 520; loss: 2.02; acc: 0.34
Batch: 540; loss: 1.8; acc: 0.36
Batch: 560; loss: 1.84; acc: 0.33
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.85; acc: 0.31
Batch: 620; loss: 1.83; acc: 0.39
Batch: 640; loss: 1.67; acc: 0.52
Batch: 660; loss: 1.8; acc: 0.38
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.6; acc: 0.55
Batch: 720; loss: 1.89; acc: 0.38
Batch: 740; loss: 1.91; acc: 0.41
Batch: 760; loss: 1.9; acc: 0.31
Batch: 780; loss: 1.67; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8073173655066521; val_accuracy: 0.370421974522293 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 2.0; acc: 0.3
Batch: 60; loss: 1.71; acc: 0.38
Batch: 80; loss: 1.91; acc: 0.31
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.81; acc: 0.36
Batch: 160; loss: 1.87; acc: 0.39
Batch: 180; loss: 1.87; acc: 0.3
Batch: 200; loss: 1.95; acc: 0.3
Batch: 220; loss: 1.75; acc: 0.3
Batch: 240; loss: 1.89; acc: 0.31
Batch: 260; loss: 1.89; acc: 0.27
Batch: 280; loss: 1.88; acc: 0.28
Batch: 300; loss: 1.89; acc: 0.33
Batch: 320; loss: 1.84; acc: 0.34
Batch: 340; loss: 1.77; acc: 0.33
Batch: 360; loss: 1.72; acc: 0.44
Batch: 380; loss: 1.99; acc: 0.22
Batch: 400; loss: 1.87; acc: 0.41
Batch: 420; loss: 1.78; acc: 0.39
Batch: 440; loss: 2.04; acc: 0.27
Batch: 460; loss: 1.88; acc: 0.33
Batch: 480; loss: 1.94; acc: 0.3
Batch: 500; loss: 1.85; acc: 0.34
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.83; acc: 0.39
Batch: 560; loss: 1.88; acc: 0.39
Batch: 580; loss: 1.87; acc: 0.33
Batch: 600; loss: 1.85; acc: 0.36
Batch: 620; loss: 1.82; acc: 0.33
Batch: 640; loss: 1.94; acc: 0.27
Batch: 660; loss: 1.83; acc: 0.3
Batch: 680; loss: 1.82; acc: 0.34
Batch: 700; loss: 1.94; acc: 0.22
Batch: 720; loss: 1.76; acc: 0.39
Batch: 740; loss: 1.87; acc: 0.34
Batch: 760; loss: 1.71; acc: 0.41
Batch: 780; loss: 1.8; acc: 0.48
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807604198243208; val_accuracy: 0.36992436305732485 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.99; acc: 0.31
Batch: 20; loss: 1.96; acc: 0.36
Batch: 40; loss: 1.77; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.44
Batch: 80; loss: 1.86; acc: 0.39
Batch: 100; loss: 1.74; acc: 0.52
Batch: 120; loss: 1.83; acc: 0.41
Batch: 140; loss: 1.85; acc: 0.31
Batch: 160; loss: 1.76; acc: 0.45
Batch: 180; loss: 1.7; acc: 0.44
Batch: 200; loss: 1.66; acc: 0.45
Batch: 220; loss: 1.91; acc: 0.33
Batch: 240; loss: 2.03; acc: 0.3
Batch: 260; loss: 1.69; acc: 0.38
Batch: 280; loss: 2.13; acc: 0.27
Batch: 300; loss: 1.98; acc: 0.36
Batch: 320; loss: 1.77; acc: 0.39
Batch: 340; loss: 1.76; acc: 0.41
Batch: 360; loss: 2.14; acc: 0.23
Batch: 380; loss: 1.91; acc: 0.23
Batch: 400; loss: 1.91; acc: 0.34
Batch: 420; loss: 1.93; acc: 0.33
Batch: 440; loss: 1.79; acc: 0.33
Batch: 460; loss: 1.89; acc: 0.38
Batch: 480; loss: 1.88; acc: 0.33
Batch: 500; loss: 1.87; acc: 0.36
Batch: 520; loss: 1.82; acc: 0.44
Batch: 540; loss: 1.78; acc: 0.39
Batch: 560; loss: 1.87; acc: 0.31
Batch: 580; loss: 1.96; acc: 0.28
Batch: 600; loss: 1.8; acc: 0.39
Batch: 620; loss: 2.03; acc: 0.31
Batch: 640; loss: 1.93; acc: 0.34
Batch: 660; loss: 1.92; acc: 0.34
Batch: 680; loss: 1.79; acc: 0.3
Batch: 700; loss: 1.9; acc: 0.31
Batch: 720; loss: 1.74; acc: 0.42
Batch: 740; loss: 1.93; acc: 0.28
Batch: 760; loss: 1.89; acc: 0.34
Batch: 780; loss: 1.93; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8073631342808911; val_accuracy: 0.3705214968152866 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.75; acc: 0.44
Batch: 60; loss: 1.68; acc: 0.45
Batch: 80; loss: 1.98; acc: 0.23
Batch: 100; loss: 1.81; acc: 0.38
Batch: 120; loss: 1.84; acc: 0.28
Batch: 140; loss: 1.89; acc: 0.3
Batch: 160; loss: 1.85; acc: 0.34
Batch: 180; loss: 1.88; acc: 0.38
Batch: 200; loss: 1.77; acc: 0.38
Batch: 220; loss: 1.7; acc: 0.42
Batch: 240; loss: 1.84; acc: 0.31
Batch: 260; loss: 1.93; acc: 0.3
Batch: 280; loss: 1.78; acc: 0.36
Batch: 300; loss: 1.83; acc: 0.3
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.92; acc: 0.34
Batch: 360; loss: 1.86; acc: 0.38
Batch: 380; loss: 1.93; acc: 0.27
Batch: 400; loss: 1.96; acc: 0.3
Batch: 420; loss: 1.87; acc: 0.36
Batch: 440; loss: 1.85; acc: 0.44
Batch: 460; loss: 1.86; acc: 0.38
Batch: 480; loss: 1.73; acc: 0.41
Batch: 500; loss: 1.78; acc: 0.27
Batch: 520; loss: 1.76; acc: 0.34
Batch: 540; loss: 1.74; acc: 0.34
Batch: 560; loss: 1.72; acc: 0.41
Batch: 580; loss: 1.82; acc: 0.34
Batch: 600; loss: 1.65; acc: 0.47
Batch: 620; loss: 1.88; acc: 0.33
Batch: 640; loss: 2.05; acc: 0.28
Batch: 660; loss: 1.87; acc: 0.31
Batch: 680; loss: 1.83; acc: 0.44
Batch: 700; loss: 1.93; acc: 0.39
Batch: 720; loss: 1.87; acc: 0.39
Batch: 740; loss: 1.92; acc: 0.39
Batch: 760; loss: 2.06; acc: 0.28
Batch: 780; loss: 1.88; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807669533286125; val_accuracy: 0.3700238853503185 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.92; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.34
Batch: 40; loss: 1.81; acc: 0.33
Batch: 60; loss: 1.93; acc: 0.34
Batch: 80; loss: 1.77; acc: 0.45
Batch: 100; loss: 1.89; acc: 0.34
Batch: 120; loss: 1.93; acc: 0.28
Batch: 140; loss: 2.01; acc: 0.31
Batch: 160; loss: 1.99; acc: 0.2
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.86; acc: 0.34
Batch: 220; loss: 1.82; acc: 0.41
Batch: 240; loss: 1.93; acc: 0.27
Batch: 260; loss: 1.65; acc: 0.41
Batch: 280; loss: 1.91; acc: 0.31
Batch: 300; loss: 1.75; acc: 0.42
Batch: 320; loss: 1.83; acc: 0.33
Batch: 340; loss: 1.77; acc: 0.48
Batch: 360; loss: 1.94; acc: 0.36
Batch: 380; loss: 1.8; acc: 0.38
Batch: 400; loss: 1.86; acc: 0.33
Batch: 420; loss: 1.8; acc: 0.38
Batch: 440; loss: 1.7; acc: 0.36
Batch: 460; loss: 1.79; acc: 0.41
Batch: 480; loss: 1.77; acc: 0.39
Batch: 500; loss: 1.65; acc: 0.39
Batch: 520; loss: 1.85; acc: 0.39
Batch: 540; loss: 1.9; acc: 0.3
Batch: 560; loss: 1.92; acc: 0.28
Batch: 580; loss: 1.84; acc: 0.39
Batch: 600; loss: 1.84; acc: 0.42
Batch: 620; loss: 1.94; acc: 0.28
Batch: 640; loss: 1.74; acc: 0.39
Batch: 660; loss: 1.8; acc: 0.42
Batch: 680; loss: 1.93; acc: 0.28
Batch: 700; loss: 1.75; acc: 0.42
Batch: 720; loss: 1.8; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.41
Batch: 760; loss: 1.92; acc: 0.3
Batch: 780; loss: 1.73; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074864984317949; val_accuracy: 0.3698248407643312 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.78; acc: 0.45
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.78; acc: 0.38
Batch: 80; loss: 1.88; acc: 0.36
Batch: 100; loss: 1.9; acc: 0.34
Batch: 120; loss: 1.64; acc: 0.41
Batch: 140; loss: 2.02; acc: 0.31
Batch: 160; loss: 1.86; acc: 0.36
Batch: 180; loss: 1.69; acc: 0.42
Batch: 200; loss: 1.85; acc: 0.42
Batch: 220; loss: 1.89; acc: 0.41
Batch: 240; loss: 1.81; acc: 0.33
Batch: 260; loss: 1.68; acc: 0.38
Batch: 280; loss: 1.97; acc: 0.33
Batch: 300; loss: 1.95; acc: 0.33
Batch: 320; loss: 1.85; acc: 0.34
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.91; acc: 0.3
Batch: 380; loss: 1.77; acc: 0.42
Batch: 400; loss: 1.79; acc: 0.36
Batch: 420; loss: 1.97; acc: 0.36
Batch: 440; loss: 1.82; acc: 0.34
Batch: 460; loss: 1.78; acc: 0.36
Batch: 480; loss: 2.01; acc: 0.27
Batch: 500; loss: 1.78; acc: 0.34
Batch: 520; loss: 1.77; acc: 0.41
Batch: 540; loss: 1.8; acc: 0.39
Batch: 560; loss: 1.81; acc: 0.34
Batch: 580; loss: 1.91; acc: 0.31
Batch: 600; loss: 1.73; acc: 0.44
Batch: 620; loss: 1.75; acc: 0.33
Batch: 640; loss: 1.88; acc: 0.39
Batch: 660; loss: 1.89; acc: 0.31
Batch: 680; loss: 2.05; acc: 0.23
Batch: 700; loss: 1.9; acc: 0.33
Batch: 720; loss: 1.84; acc: 0.36
Batch: 740; loss: 1.71; acc: 0.42
Batch: 760; loss: 1.81; acc: 0.41
Batch: 780; loss: 1.99; acc: 0.2
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074457615044466; val_accuracy: 0.37062101910828027 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.86; acc: 0.36
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.77; acc: 0.38
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.83; acc: 0.34
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.78; acc: 0.38
Batch: 160; loss: 1.79; acc: 0.41
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.74; acc: 0.45
Batch: 220; loss: 1.99; acc: 0.31
Batch: 240; loss: 1.73; acc: 0.34
Batch: 260; loss: 1.79; acc: 0.39
Batch: 280; loss: 1.82; acc: 0.36
Batch: 300; loss: 1.73; acc: 0.42
Batch: 320; loss: 1.75; acc: 0.42
Batch: 340; loss: 1.98; acc: 0.28
Batch: 360; loss: 1.7; acc: 0.41
Batch: 380; loss: 1.77; acc: 0.39
Batch: 400; loss: 1.94; acc: 0.36
Batch: 420; loss: 1.89; acc: 0.34
Batch: 440; loss: 1.89; acc: 0.36
Batch: 460; loss: 1.68; acc: 0.44
Batch: 480; loss: 1.96; acc: 0.28
Batch: 500; loss: 2.15; acc: 0.27
Batch: 520; loss: 1.93; acc: 0.34
Batch: 540; loss: 1.86; acc: 0.38
Batch: 560; loss: 1.86; acc: 0.31
Batch: 580; loss: 1.9; acc: 0.36
Batch: 600; loss: 1.89; acc: 0.39
Batch: 620; loss: 1.89; acc: 0.33
Batch: 640; loss: 1.93; acc: 0.28
Batch: 660; loss: 1.82; acc: 0.33
Batch: 680; loss: 1.86; acc: 0.41
Batch: 700; loss: 1.97; acc: 0.27
Batch: 720; loss: 1.82; acc: 0.42
Batch: 740; loss: 1.88; acc: 0.36
Batch: 760; loss: 1.82; acc: 0.41
Batch: 780; loss: 1.69; acc: 0.47
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807431608248668; val_accuracy: 0.3705214968152866 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.72; acc: 0.41
Batch: 20; loss: 1.89; acc: 0.42
Batch: 40; loss: 1.82; acc: 0.36
Batch: 60; loss: 1.73; acc: 0.44
Batch: 80; loss: 1.73; acc: 0.5
Batch: 100; loss: 1.7; acc: 0.47
Batch: 120; loss: 1.75; acc: 0.38
Batch: 140; loss: 1.79; acc: 0.34
Batch: 160; loss: 1.74; acc: 0.42
Batch: 180; loss: 2.09; acc: 0.25
Batch: 200; loss: 1.91; acc: 0.31
Batch: 220; loss: 1.84; acc: 0.33
Batch: 240; loss: 1.76; acc: 0.42
Batch: 260; loss: 1.88; acc: 0.39
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.77; acc: 0.42
Batch: 320; loss: 1.83; acc: 0.33
Batch: 340; loss: 1.74; acc: 0.38
Batch: 360; loss: 1.95; acc: 0.28
Batch: 380; loss: 1.75; acc: 0.39
Batch: 400; loss: 1.83; acc: 0.42
Batch: 420; loss: 1.93; acc: 0.27
Batch: 440; loss: 1.91; acc: 0.33
Batch: 460; loss: 1.85; acc: 0.33
Batch: 480; loss: 1.84; acc: 0.36
Batch: 500; loss: 1.91; acc: 0.28
Batch: 520; loss: 1.87; acc: 0.38
Batch: 540; loss: 1.84; acc: 0.36
Batch: 560; loss: 1.69; acc: 0.42
Batch: 580; loss: 1.89; acc: 0.34
Batch: 600; loss: 1.82; acc: 0.36
Batch: 620; loss: 1.76; acc: 0.42
Batch: 640; loss: 1.92; acc: 0.38
Batch: 660; loss: 1.99; acc: 0.39
Batch: 680; loss: 1.9; acc: 0.31
Batch: 700; loss: 1.99; acc: 0.28
Batch: 720; loss: 1.86; acc: 0.33
Batch: 740; loss: 1.81; acc: 0.31
Batch: 760; loss: 1.85; acc: 0.34
Batch: 780; loss: 1.74; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074647810808413; val_accuracy: 0.3703224522292994 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.88; acc: 0.34
Batch: 20; loss: 1.68; acc: 0.45
Batch: 40; loss: 1.84; acc: 0.34
Batch: 60; loss: 2.08; acc: 0.23
Batch: 80; loss: 1.8; acc: 0.38
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.85; acc: 0.27
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.74; acc: 0.38
Batch: 180; loss: 1.94; acc: 0.34
Batch: 200; loss: 1.84; acc: 0.38
Batch: 220; loss: 1.91; acc: 0.3
Batch: 240; loss: 1.96; acc: 0.28
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.96; acc: 0.31
Batch: 300; loss: 1.8; acc: 0.3
Batch: 320; loss: 1.9; acc: 0.33
Batch: 340; loss: 1.76; acc: 0.38
Batch: 360; loss: 1.75; acc: 0.42
Batch: 380; loss: 1.85; acc: 0.33
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.84; acc: 0.3
Batch: 440; loss: 1.8; acc: 0.41
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 2.03; acc: 0.23
Batch: 500; loss: 1.77; acc: 0.41
Batch: 520; loss: 1.65; acc: 0.44
Batch: 540; loss: 1.94; acc: 0.34
Batch: 560; loss: 1.8; acc: 0.39
Batch: 580; loss: 1.79; acc: 0.33
Batch: 600; loss: 1.73; acc: 0.39
Batch: 620; loss: 1.79; acc: 0.42
Batch: 640; loss: 1.68; acc: 0.44
Batch: 660; loss: 1.65; acc: 0.41
Batch: 680; loss: 1.76; acc: 0.41
Batch: 700; loss: 1.7; acc: 0.39
Batch: 720; loss: 1.83; acc: 0.25
Batch: 740; loss: 1.88; acc: 0.39
Batch: 760; loss: 1.66; acc: 0.47
Batch: 780; loss: 1.92; acc: 0.34
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807406155167112; val_accuracy: 0.37091958598726116 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.41
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.82; acc: 0.38
Batch: 60; loss: 1.81; acc: 0.31
Batch: 80; loss: 1.94; acc: 0.36
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.8; acc: 0.33
Batch: 140; loss: 1.93; acc: 0.28
Batch: 160; loss: 1.89; acc: 0.31
Batch: 180; loss: 1.73; acc: 0.39
Batch: 200; loss: 1.72; acc: 0.42
Batch: 220; loss: 1.81; acc: 0.31
Batch: 240; loss: 1.9; acc: 0.33
Batch: 260; loss: 1.7; acc: 0.44
Batch: 280; loss: 1.82; acc: 0.28
Batch: 300; loss: 1.77; acc: 0.41
Batch: 320; loss: 1.82; acc: 0.36
Batch: 340; loss: 1.85; acc: 0.36
Batch: 360; loss: 1.95; acc: 0.28
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.79; acc: 0.38
Batch: 420; loss: 1.98; acc: 0.28
Batch: 440; loss: 1.88; acc: 0.3
Batch: 460; loss: 1.9; acc: 0.36
Batch: 480; loss: 1.74; acc: 0.39
Batch: 500; loss: 2.0; acc: 0.3
Batch: 520; loss: 1.82; acc: 0.39
Batch: 540; loss: 1.77; acc: 0.28
Batch: 560; loss: 1.86; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.33
Batch: 600; loss: 2.01; acc: 0.25
Batch: 620; loss: 1.95; acc: 0.23
Batch: 640; loss: 1.86; acc: 0.31
Batch: 660; loss: 1.92; acc: 0.25
Batch: 680; loss: 1.86; acc: 0.33
Batch: 700; loss: 1.76; acc: 0.38
Batch: 720; loss: 1.7; acc: 0.52
Batch: 740; loss: 1.94; acc: 0.3
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.9; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074089425384618; val_accuracy: 0.37101910828025475 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.04; acc: 0.25
Batch: 20; loss: 1.79; acc: 0.39
Batch: 40; loss: 1.82; acc: 0.41
Batch: 60; loss: 1.79; acc: 0.34
Batch: 80; loss: 1.89; acc: 0.33
Batch: 100; loss: 1.92; acc: 0.38
Batch: 120; loss: 1.71; acc: 0.39
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.9; acc: 0.34
Batch: 180; loss: 1.74; acc: 0.36
Batch: 200; loss: 1.89; acc: 0.28
Batch: 220; loss: 1.9; acc: 0.28
Batch: 240; loss: 1.81; acc: 0.36
Batch: 260; loss: 1.73; acc: 0.38
Batch: 280; loss: 1.8; acc: 0.38
Batch: 300; loss: 1.88; acc: 0.31
Batch: 320; loss: 1.79; acc: 0.41
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.74; acc: 0.42
Batch: 380; loss: 1.87; acc: 0.33
Batch: 400; loss: 1.9; acc: 0.36
Batch: 420; loss: 1.99; acc: 0.27
Batch: 440; loss: 1.96; acc: 0.31
Batch: 460; loss: 2.01; acc: 0.28
Batch: 480; loss: 1.79; acc: 0.42
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.99; acc: 0.25
Batch: 540; loss: 1.81; acc: 0.36
Batch: 560; loss: 1.95; acc: 0.34
Batch: 580; loss: 1.92; acc: 0.39
Batch: 600; loss: 1.77; acc: 0.39
Batch: 620; loss: 1.65; acc: 0.53
Batch: 640; loss: 1.95; acc: 0.34
Batch: 660; loss: 1.94; acc: 0.34
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 1.82; acc: 0.39
Batch: 720; loss: 1.79; acc: 0.42
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.83; acc: 0.33
Batch: 780; loss: 1.82; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8073852031853548; val_accuracy: 0.3708200636942675 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.9; acc: 0.34
Batch: 20; loss: 1.82; acc: 0.31
Batch: 40; loss: 1.75; acc: 0.38
Batch: 60; loss: 1.89; acc: 0.27
Batch: 80; loss: 1.63; acc: 0.47
Batch: 100; loss: 1.93; acc: 0.38
Batch: 120; loss: 1.87; acc: 0.31
Batch: 140; loss: 2.03; acc: 0.28
Batch: 160; loss: 1.77; acc: 0.39
Batch: 180; loss: 1.85; acc: 0.33
Batch: 200; loss: 1.89; acc: 0.33
Batch: 220; loss: 1.82; acc: 0.34
Batch: 240; loss: 1.76; acc: 0.36
Batch: 260; loss: 1.91; acc: 0.42
Batch: 280; loss: 1.78; acc: 0.34
Batch: 300; loss: 1.92; acc: 0.33
Batch: 320; loss: 1.81; acc: 0.41
Batch: 340; loss: 1.82; acc: 0.34
Batch: 360; loss: 1.9; acc: 0.31
Batch: 380; loss: 1.79; acc: 0.38
Batch: 400; loss: 1.83; acc: 0.36
Batch: 420; loss: 1.77; acc: 0.41
Batch: 440; loss: 1.78; acc: 0.44
Batch: 460; loss: 1.8; acc: 0.36
Batch: 480; loss: 1.78; acc: 0.36
Batch: 500; loss: 1.97; acc: 0.27
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.81; acc: 0.33
Batch: 560; loss: 1.85; acc: 0.31
Batch: 580; loss: 1.88; acc: 0.33
Batch: 600; loss: 1.73; acc: 0.42
Batch: 620; loss: 1.93; acc: 0.31
Batch: 640; loss: 1.8; acc: 0.42
Batch: 660; loss: 1.83; acc: 0.36
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.9; acc: 0.38
Batch: 720; loss: 1.96; acc: 0.3
Batch: 740; loss: 1.92; acc: 0.31
Batch: 760; loss: 1.77; acc: 0.36
Batch: 780; loss: 1.84; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074476111466717; val_accuracy: 0.37072054140127386 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.74; acc: 0.42
Batch: 20; loss: 1.76; acc: 0.47
Batch: 40; loss: 1.79; acc: 0.41
Batch: 60; loss: 1.92; acc: 0.31
Batch: 80; loss: 1.77; acc: 0.41
Batch: 100; loss: 1.85; acc: 0.27
Batch: 120; loss: 1.76; acc: 0.36
Batch: 140; loss: 1.92; acc: 0.33
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.89; acc: 0.31
Batch: 200; loss: 1.97; acc: 0.22
Batch: 220; loss: 1.86; acc: 0.34
Batch: 240; loss: 1.8; acc: 0.33
Batch: 260; loss: 1.61; acc: 0.42
Batch: 280; loss: 1.77; acc: 0.42
Batch: 300; loss: 1.83; acc: 0.36
Batch: 320; loss: 1.67; acc: 0.39
Batch: 340; loss: 2.04; acc: 0.28
Batch: 360; loss: 1.9; acc: 0.3
Batch: 380; loss: 1.83; acc: 0.36
Batch: 400; loss: 1.88; acc: 0.39
Batch: 420; loss: 1.69; acc: 0.42
Batch: 440; loss: 1.84; acc: 0.38
Batch: 460; loss: 1.78; acc: 0.33
Batch: 480; loss: 1.82; acc: 0.34
Batch: 500; loss: 2.0; acc: 0.25
Batch: 520; loss: 1.72; acc: 0.3
Batch: 540; loss: 1.7; acc: 0.39
Batch: 560; loss: 1.96; acc: 0.33
Batch: 580; loss: 1.69; acc: 0.47
Batch: 600; loss: 1.92; acc: 0.28
Batch: 620; loss: 1.92; acc: 0.38
Batch: 640; loss: 1.92; acc: 0.28
Batch: 660; loss: 1.66; acc: 0.47
Batch: 680; loss: 1.71; acc: 0.44
Batch: 700; loss: 1.95; acc: 0.27
Batch: 720; loss: 1.86; acc: 0.34
Batch: 740; loss: 1.94; acc: 0.33
Batch: 760; loss: 1.87; acc: 0.34
Batch: 780; loss: 1.86; acc: 0.36
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074535488323042; val_accuracy: 0.370421974522293 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 1.74; acc: 0.39
Batch: 40; loss: 1.77; acc: 0.39
Batch: 60; loss: 1.76; acc: 0.39
Batch: 80; loss: 1.8; acc: 0.38
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.85; acc: 0.33
Batch: 140; loss: 1.91; acc: 0.34
Batch: 160; loss: 1.82; acc: 0.39
Batch: 180; loss: 1.74; acc: 0.39
Batch: 200; loss: 1.77; acc: 0.39
Batch: 220; loss: 1.75; acc: 0.39
Batch: 240; loss: 1.7; acc: 0.39
Batch: 260; loss: 1.97; acc: 0.31
Batch: 280; loss: 1.64; acc: 0.39
Batch: 300; loss: 1.85; acc: 0.41
Batch: 320; loss: 1.83; acc: 0.34
Batch: 340; loss: 1.79; acc: 0.39
Batch: 360; loss: 1.77; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.77; acc: 0.42
Batch: 420; loss: 1.94; acc: 0.33
Batch: 440; loss: 1.85; acc: 0.39
Batch: 460; loss: 1.94; acc: 0.31
Batch: 480; loss: 1.93; acc: 0.3
Batch: 500; loss: 1.94; acc: 0.3
Batch: 520; loss: 2.04; acc: 0.31
Batch: 540; loss: 1.97; acc: 0.27
Batch: 560; loss: 1.76; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.36
Batch: 600; loss: 1.83; acc: 0.36
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.71; acc: 0.47
Batch: 660; loss: 1.94; acc: 0.33
Batch: 680; loss: 1.95; acc: 0.33
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 2.09; acc: 0.25
Batch: 740; loss: 1.75; acc: 0.47
Batch: 760; loss: 1.75; acc: 0.45
Batch: 780; loss: 1.95; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074811051605613; val_accuracy: 0.3708200636942675 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.83; acc: 0.36
Batch: 20; loss: 1.81; acc: 0.33
Batch: 40; loss: 1.77; acc: 0.34
Batch: 60; loss: 1.94; acc: 0.34
Batch: 80; loss: 1.93; acc: 0.3
Batch: 100; loss: 1.85; acc: 0.38
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.82; acc: 0.36
Batch: 160; loss: 1.78; acc: 0.34
Batch: 180; loss: 1.65; acc: 0.47
Batch: 200; loss: 1.68; acc: 0.41
Batch: 220; loss: 1.85; acc: 0.39
Batch: 240; loss: 1.81; acc: 0.34
Batch: 260; loss: 1.89; acc: 0.28
Batch: 280; loss: 1.84; acc: 0.33
Batch: 300; loss: 1.86; acc: 0.38
Batch: 320; loss: 1.86; acc: 0.36
Batch: 340; loss: 1.88; acc: 0.33
Batch: 360; loss: 1.79; acc: 0.39
Batch: 380; loss: 1.72; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.42
Batch: 420; loss: 1.81; acc: 0.34
Batch: 440; loss: 1.82; acc: 0.41
Batch: 460; loss: 1.6; acc: 0.5
Batch: 480; loss: 1.95; acc: 0.28
Batch: 500; loss: 2.04; acc: 0.3
Batch: 520; loss: 1.77; acc: 0.41
Batch: 540; loss: 1.86; acc: 0.33
Batch: 560; loss: 1.72; acc: 0.48
Batch: 580; loss: 1.74; acc: 0.39
Batch: 600; loss: 1.83; acc: 0.34
Batch: 620; loss: 1.99; acc: 0.22
Batch: 640; loss: 1.77; acc: 0.39
Batch: 660; loss: 1.8; acc: 0.41
Batch: 680; loss: 1.85; acc: 0.39
Batch: 700; loss: 1.96; acc: 0.33
Batch: 720; loss: 1.85; acc: 0.34
Batch: 740; loss: 1.86; acc: 0.36
Batch: 760; loss: 1.66; acc: 0.38
Batch: 780; loss: 1.86; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075358146315168; val_accuracy: 0.3705214968152866 

plots/subspace_training/lenet/2020-01-20 16:25:19/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 2221300
elements in E: 2221300
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.31; acc: 0.09
Batch: 80; loss: 2.29; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.31; acc: 0.09
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.31; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.31; acc: 0.05
Batch: 420; loss: 2.3; acc: 0.05
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.29; acc: 0.11
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.09
Batch: 520; loss: 2.27; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.05
Batch: 560; loss: 2.28; acc: 0.12
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.28; acc: 0.16
Batch: 640; loss: 2.29; acc: 0.08
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.05
Batch: 700; loss: 2.27; acc: 0.2
Batch: 720; loss: 2.29; acc: 0.08
Batch: 740; loss: 2.28; acc: 0.2
Batch: 760; loss: 2.27; acc: 0.19
Batch: 780; loss: 2.27; acc: 0.22
Train Epoch over. train_loss: 2.29; train_accuracy: 0.11 

Batch: 0; loss: 2.28; acc: 0.22
Batch: 20; loss: 2.26; acc: 0.31
Batch: 40; loss: 2.27; acc: 0.3
Batch: 60; loss: 2.27; acc: 0.2
Batch: 80; loss: 2.27; acc: 0.2
Batch: 100; loss: 2.29; acc: 0.17
Batch: 120; loss: 2.28; acc: 0.22
Batch: 140; loss: 2.27; acc: 0.2
Val Epoch over. val_loss: 2.2753795165165216; val_accuracy: 0.1992436305732484 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.22
Batch: 40; loss: 2.28; acc: 0.22
Batch: 60; loss: 2.26; acc: 0.27
Batch: 80; loss: 2.27; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.27
Batch: 120; loss: 2.26; acc: 0.33
Batch: 140; loss: 2.25; acc: 0.31
Batch: 160; loss: 2.26; acc: 0.23
Batch: 180; loss: 2.25; acc: 0.3
Batch: 200; loss: 2.27; acc: 0.27
Batch: 220; loss: 2.26; acc: 0.28
Batch: 240; loss: 2.25; acc: 0.22
Batch: 260; loss: 2.22; acc: 0.38
Batch: 280; loss: 2.23; acc: 0.38
Batch: 300; loss: 2.24; acc: 0.25
Batch: 320; loss: 2.22; acc: 0.33
Batch: 340; loss: 2.23; acc: 0.3
Batch: 360; loss: 2.22; acc: 0.27
Batch: 380; loss: 2.2; acc: 0.34
Batch: 400; loss: 2.2; acc: 0.3
Batch: 420; loss: 2.27; acc: 0.11
Batch: 440; loss: 2.17; acc: 0.38
Batch: 460; loss: 2.25; acc: 0.19
Batch: 480; loss: 2.17; acc: 0.31
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.1; acc: 0.36
Batch: 540; loss: 2.12; acc: 0.34
Batch: 560; loss: 2.13; acc: 0.28
Batch: 580; loss: 2.15; acc: 0.33
Batch: 600; loss: 2.11; acc: 0.28
Batch: 620; loss: 2.12; acc: 0.33
Batch: 640; loss: 2.09; acc: 0.3
Batch: 660; loss: 1.99; acc: 0.33
Batch: 680; loss: 2.07; acc: 0.27
Batch: 700; loss: 1.95; acc: 0.34
Batch: 720; loss: 1.94; acc: 0.39
Batch: 740; loss: 1.87; acc: 0.38
Batch: 760; loss: 1.86; acc: 0.41
Batch: 780; loss: 1.85; acc: 0.39
Train Epoch over. train_loss: 2.17; train_accuracy: 0.28 

Batch: 0; loss: 1.82; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.61; acc: 0.52
Batch: 60; loss: 1.76; acc: 0.52
Batch: 80; loss: 1.82; acc: 0.42
Batch: 100; loss: 1.89; acc: 0.38
Batch: 120; loss: 1.87; acc: 0.42
Batch: 140; loss: 1.81; acc: 0.36
Val Epoch over. val_loss: 1.837604162799325; val_accuracy: 0.39072452229299365 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.66; acc: 0.42
Batch: 60; loss: 1.73; acc: 0.33
Batch: 80; loss: 1.85; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.34
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.28; acc: 0.62
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.39
Batch: 200; loss: 1.47; acc: 0.53
Batch: 220; loss: 1.47; acc: 0.52
Batch: 240; loss: 1.47; acc: 0.44
Batch: 260; loss: 1.54; acc: 0.52
Batch: 280; loss: 1.52; acc: 0.41
Batch: 300; loss: 1.48; acc: 0.44
Batch: 320; loss: 1.31; acc: 0.56
Batch: 340; loss: 1.25; acc: 0.59
Batch: 360; loss: 1.36; acc: 0.5
Batch: 380; loss: 1.33; acc: 0.55
Batch: 400; loss: 1.22; acc: 0.61
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.31; acc: 0.53
Batch: 460; loss: 1.62; acc: 0.47
Batch: 480; loss: 1.23; acc: 0.58
Batch: 500; loss: 1.27; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.45
Batch: 540; loss: 1.44; acc: 0.55
Batch: 560; loss: 1.54; acc: 0.45
Batch: 580; loss: 1.47; acc: 0.52
Batch: 600; loss: 1.26; acc: 0.58
Batch: 620; loss: 1.4; acc: 0.5
Batch: 640; loss: 1.29; acc: 0.66
Batch: 660; loss: 1.59; acc: 0.44
Batch: 680; loss: 1.52; acc: 0.5
Batch: 700; loss: 1.2; acc: 0.55
Batch: 720; loss: 1.25; acc: 0.59
Batch: 740; loss: 1.43; acc: 0.55
Batch: 760; loss: 1.65; acc: 0.42
Batch: 780; loss: 1.12; acc: 0.59
Train Epoch over. train_loss: 1.5; train_accuracy: 0.49 

Batch: 0; loss: 1.55; acc: 0.48
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 1.12; acc: 0.55
Batch: 60; loss: 1.34; acc: 0.52
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.48; acc: 0.47
Batch: 120; loss: 1.54; acc: 0.45
Batch: 140; loss: 1.34; acc: 0.48
Val Epoch over. val_loss: 1.406753076489564; val_accuracy: 0.5155254777070064 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.55
Batch: 20; loss: 1.58; acc: 0.47
Batch: 40; loss: 1.48; acc: 0.45
Batch: 60; loss: 1.39; acc: 0.45
Batch: 80; loss: 1.66; acc: 0.39
Batch: 100; loss: 1.29; acc: 0.52
Batch: 120; loss: 1.43; acc: 0.48
Batch: 140; loss: 1.62; acc: 0.42
Batch: 160; loss: 1.55; acc: 0.44
Batch: 180; loss: 1.53; acc: 0.52
Batch: 200; loss: 1.51; acc: 0.41
Batch: 220; loss: 1.33; acc: 0.56
Batch: 240; loss: 1.34; acc: 0.48
Batch: 260; loss: 1.58; acc: 0.38
Batch: 280; loss: 1.62; acc: 0.36
Batch: 300; loss: 1.23; acc: 0.55
Batch: 320; loss: 1.29; acc: 0.59
Batch: 340; loss: 1.38; acc: 0.59
Batch: 360; loss: 1.45; acc: 0.5
Batch: 380; loss: 1.63; acc: 0.45
Batch: 400; loss: 1.21; acc: 0.53
Batch: 420; loss: 1.3; acc: 0.56
Batch: 440; loss: 1.52; acc: 0.44
Batch: 460; loss: 1.4; acc: 0.5
Batch: 480; loss: 1.69; acc: 0.38
Batch: 500; loss: 1.43; acc: 0.5
Batch: 520; loss: 1.28; acc: 0.59
Batch: 540; loss: 1.45; acc: 0.48
Batch: 560; loss: 1.36; acc: 0.58
Batch: 580; loss: 1.79; acc: 0.39
Batch: 600; loss: 1.61; acc: 0.45
Batch: 620; loss: 1.57; acc: 0.45
Batch: 640; loss: 1.26; acc: 0.53
Batch: 660; loss: 1.43; acc: 0.48
Batch: 680; loss: 1.33; acc: 0.58
Batch: 700; loss: 1.31; acc: 0.56
Batch: 720; loss: 1.23; acc: 0.53
Batch: 740; loss: 1.59; acc: 0.47
Batch: 760; loss: 1.46; acc: 0.55
Batch: 780; loss: 1.35; acc: 0.59
Train Epoch over. train_loss: 1.43; train_accuracy: 0.52 

Batch: 0; loss: 1.65; acc: 0.45
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 1.04; acc: 0.62
Batch: 60; loss: 1.35; acc: 0.52
Batch: 80; loss: 1.18; acc: 0.55
Batch: 100; loss: 1.41; acc: 0.52
Batch: 120; loss: 1.51; acc: 0.48
Batch: 140; loss: 1.27; acc: 0.53
Val Epoch over. val_loss: 1.3746642185624238; val_accuracy: 0.5237858280254777 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.47
Batch: 20; loss: 1.53; acc: 0.45
Batch: 40; loss: 1.39; acc: 0.55
Batch: 60; loss: 1.61; acc: 0.47
Batch: 80; loss: 1.36; acc: 0.5
Batch: 100; loss: 1.33; acc: 0.64
Batch: 120; loss: 1.32; acc: 0.53
Batch: 140; loss: 1.15; acc: 0.62
Batch: 160; loss: 1.3; acc: 0.55
Batch: 180; loss: 1.26; acc: 0.58
Batch: 200; loss: 1.52; acc: 0.44
Batch: 220; loss: 1.42; acc: 0.53
Batch: 240; loss: 1.47; acc: 0.47
Batch: 260; loss: 1.29; acc: 0.58
Batch: 280; loss: 1.31; acc: 0.53
Batch: 300; loss: 1.42; acc: 0.52
Batch: 320; loss: 1.34; acc: 0.58
Batch: 340; loss: 1.37; acc: 0.52
Batch: 360; loss: 1.53; acc: 0.5
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.28; acc: 0.56
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.24; acc: 0.58
Batch: 460; loss: 1.23; acc: 0.55
Batch: 480; loss: 1.56; acc: 0.5
Batch: 500; loss: 1.5; acc: 0.52
Batch: 520; loss: 1.19; acc: 0.61
Batch: 540; loss: 1.27; acc: 0.55
Batch: 560; loss: 1.42; acc: 0.53
Batch: 580; loss: 1.23; acc: 0.58
Batch: 600; loss: 1.52; acc: 0.47
Batch: 620; loss: 1.01; acc: 0.69
Batch: 640; loss: 1.12; acc: 0.61
Batch: 660; loss: 1.38; acc: 0.5
Batch: 680; loss: 1.54; acc: 0.45
Batch: 700; loss: 1.3; acc: 0.66
Batch: 720; loss: 1.3; acc: 0.59
Batch: 740; loss: 0.99; acc: 0.64
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.21; acc: 0.56
Train Epoch over. train_loss: 1.35; train_accuracy: 0.55 

Batch: 0; loss: 1.43; acc: 0.55
Batch: 20; loss: 1.28; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.58
Batch: 80; loss: 0.98; acc: 0.66
Batch: 100; loss: 1.31; acc: 0.53
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 1.12; acc: 0.62
Val Epoch over. val_loss: 1.2540407700903098; val_accuracy: 0.5814092356687898 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 1.09; acc: 0.67
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 1.29; acc: 0.58
Batch: 120; loss: 1.57; acc: 0.5
Batch: 140; loss: 1.53; acc: 0.56
Batch: 160; loss: 1.31; acc: 0.55
Batch: 180; loss: 1.03; acc: 0.61
Batch: 200; loss: 1.45; acc: 0.55
Batch: 220; loss: 1.47; acc: 0.45
Batch: 240; loss: 1.37; acc: 0.53
Batch: 260; loss: 1.47; acc: 0.56
Batch: 280; loss: 1.7; acc: 0.47
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.27; acc: 0.56
Batch: 340; loss: 1.12; acc: 0.66
Batch: 360; loss: 1.21; acc: 0.61
Batch: 380; loss: 1.38; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.55
Batch: 420; loss: 1.5; acc: 0.47
Batch: 440; loss: 1.39; acc: 0.5
Batch: 460; loss: 1.44; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.55
Batch: 500; loss: 1.28; acc: 0.64
Batch: 520; loss: 1.38; acc: 0.61
Batch: 540; loss: 0.99; acc: 0.64
Batch: 560; loss: 1.4; acc: 0.53
Batch: 580; loss: 1.07; acc: 0.69
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.21; acc: 0.59
Batch: 640; loss: 1.34; acc: 0.48
Batch: 660; loss: 1.21; acc: 0.64
Batch: 680; loss: 1.38; acc: 0.53
Batch: 700; loss: 1.33; acc: 0.59
Batch: 720; loss: 1.19; acc: 0.61
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 1.19; acc: 0.64
Batch: 780; loss: 1.18; acc: 0.61
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.39; acc: 0.58
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.92; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 0.92; acc: 0.64
Batch: 100; loss: 1.28; acc: 0.52
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.62
Val Epoch over. val_loss: 1.22417442282294; val_accuracy: 0.5979299363057324 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 0.97; acc: 0.69
Batch: 40; loss: 1.31; acc: 0.55
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.67
Batch: 100; loss: 1.21; acc: 0.53
Batch: 120; loss: 1.4; acc: 0.56
Batch: 140; loss: 1.02; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.62
Batch: 180; loss: 1.26; acc: 0.59
Batch: 200; loss: 1.57; acc: 0.44
Batch: 220; loss: 1.22; acc: 0.59
Batch: 240; loss: 1.24; acc: 0.61
Batch: 260; loss: 1.49; acc: 0.5
Batch: 280; loss: 1.12; acc: 0.64
Batch: 300; loss: 1.38; acc: 0.61
Batch: 320; loss: 1.4; acc: 0.47
Batch: 340; loss: 1.27; acc: 0.56
Batch: 360; loss: 1.34; acc: 0.56
Batch: 380; loss: 1.62; acc: 0.5
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.28; acc: 0.61
Batch: 460; loss: 1.2; acc: 0.67
Batch: 480; loss: 1.28; acc: 0.61
Batch: 500; loss: 1.19; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.61
Batch: 540; loss: 1.33; acc: 0.58
Batch: 560; loss: 1.38; acc: 0.48
Batch: 580; loss: 1.4; acc: 0.53
Batch: 600; loss: 1.48; acc: 0.48
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.66
Batch: 660; loss: 1.43; acc: 0.53
Batch: 680; loss: 1.37; acc: 0.58
Batch: 700; loss: 1.21; acc: 0.56
Batch: 720; loss: 1.24; acc: 0.55
Batch: 740; loss: 1.22; acc: 0.56
Batch: 760; loss: 1.27; acc: 0.59
Batch: 780; loss: 1.16; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.35; acc: 0.53
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.26; acc: 0.58
Batch: 80; loss: 0.96; acc: 0.66
Batch: 100; loss: 1.3; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.59
Val Epoch over. val_loss: 1.2080870369437393; val_accuracy: 0.601015127388535 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 1.25; acc: 0.59
Batch: 60; loss: 1.25; acc: 0.53
Batch: 80; loss: 1.26; acc: 0.64
Batch: 100; loss: 1.24; acc: 0.59
Batch: 120; loss: 1.2; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.64
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 1.43; acc: 0.53
Batch: 200; loss: 1.35; acc: 0.62
Batch: 220; loss: 1.24; acc: 0.62
Batch: 240; loss: 1.26; acc: 0.59
Batch: 260; loss: 1.43; acc: 0.52
Batch: 280; loss: 1.4; acc: 0.55
Batch: 300; loss: 1.22; acc: 0.66
Batch: 320; loss: 1.31; acc: 0.61
Batch: 340; loss: 1.19; acc: 0.61
Batch: 360; loss: 1.49; acc: 0.45
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 0.92; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.55
Batch: 440; loss: 1.23; acc: 0.53
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 1.49; acc: 0.47
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.07; acc: 0.53
Batch: 540; loss: 1.08; acc: 0.69
Batch: 560; loss: 1.44; acc: 0.48
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.38; acc: 0.52
Batch: 620; loss: 1.42; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.66
Batch: 660; loss: 1.38; acc: 0.55
Batch: 680; loss: 1.08; acc: 0.66
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.23; acc: 0.61
Batch: 760; loss: 1.15; acc: 0.58
Batch: 780; loss: 1.29; acc: 0.52
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.34; acc: 0.53
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.64
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.01; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.55
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 1.23; acc: 0.56
Val Epoch over. val_loss: 1.211446897998737; val_accuracy: 0.5966361464968153 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.44
Batch: 20; loss: 1.46; acc: 0.48
Batch: 40; loss: 1.29; acc: 0.58
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.55
Batch: 160; loss: 1.18; acc: 0.61
Batch: 180; loss: 1.22; acc: 0.61
Batch: 200; loss: 1.35; acc: 0.61
Batch: 220; loss: 0.86; acc: 0.75
Batch: 240; loss: 1.2; acc: 0.7
Batch: 260; loss: 1.08; acc: 0.64
Batch: 280; loss: 1.29; acc: 0.59
Batch: 300; loss: 1.35; acc: 0.52
Batch: 320; loss: 1.13; acc: 0.59
Batch: 340; loss: 1.48; acc: 0.59
Batch: 360; loss: 1.24; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.52
Batch: 400; loss: 1.08; acc: 0.62
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.23; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.55
Batch: 480; loss: 1.43; acc: 0.52
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 1.3; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.58
Batch: 560; loss: 1.08; acc: 0.67
Batch: 580; loss: 1.36; acc: 0.56
Batch: 600; loss: 1.32; acc: 0.55
Batch: 620; loss: 1.5; acc: 0.53
Batch: 640; loss: 1.23; acc: 0.53
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 1.34; acc: 0.5
Batch: 700; loss: 1.17; acc: 0.62
Batch: 720; loss: 1.12; acc: 0.66
Batch: 740; loss: 1.2; acc: 0.56
Batch: 760; loss: 1.25; acc: 0.58
Batch: 780; loss: 1.2; acc: 0.58
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.93; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.59
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.21; acc: 0.59
Val Epoch over. val_loss: 1.2036362787720505; val_accuracy: 0.6065883757961783 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.04; acc: 0.59
Batch: 40; loss: 1.48; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.4; acc: 0.53
Batch: 120; loss: 1.46; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.55
Batch: 160; loss: 1.17; acc: 0.56
Batch: 180; loss: 1.14; acc: 0.59
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.3; acc: 0.52
Batch: 240; loss: 1.22; acc: 0.55
Batch: 260; loss: 1.19; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.59
Batch: 300; loss: 1.33; acc: 0.56
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.2; acc: 0.58
Batch: 360; loss: 1.47; acc: 0.55
Batch: 380; loss: 1.17; acc: 0.59
Batch: 400; loss: 1.38; acc: 0.53
Batch: 420; loss: 1.29; acc: 0.56
Batch: 440; loss: 1.34; acc: 0.53
Batch: 460; loss: 1.59; acc: 0.5
Batch: 480; loss: 1.4; acc: 0.55
Batch: 500; loss: 1.36; acc: 0.59
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 1.36; acc: 0.58
Batch: 560; loss: 1.28; acc: 0.56
Batch: 580; loss: 1.23; acc: 0.53
Batch: 600; loss: 1.36; acc: 0.61
Batch: 620; loss: 1.08; acc: 0.59
Batch: 640; loss: 0.95; acc: 0.64
Batch: 660; loss: 1.36; acc: 0.58
Batch: 680; loss: 1.38; acc: 0.48
Batch: 700; loss: 1.13; acc: 0.64
Batch: 720; loss: 1.21; acc: 0.62
Batch: 740; loss: 1.28; acc: 0.53
Batch: 760; loss: 1.41; acc: 0.52
Batch: 780; loss: 1.34; acc: 0.48
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.27; acc: 0.56
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.92; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.29; acc: 0.64
Val Epoch over. val_loss: 1.2008154582066142; val_accuracy: 0.6050955414012739 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.35; acc: 0.58
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.33; acc: 0.58
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 1.33; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.46; acc: 0.5
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.13; acc: 0.58
Batch: 180; loss: 1.45; acc: 0.42
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.31; acc: 0.56
Batch: 260; loss: 1.15; acc: 0.58
Batch: 280; loss: 1.03; acc: 0.64
Batch: 300; loss: 1.32; acc: 0.66
Batch: 320; loss: 0.98; acc: 0.69
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.14; acc: 0.62
Batch: 380; loss: 1.27; acc: 0.62
Batch: 400; loss: 1.22; acc: 0.58
Batch: 420; loss: 1.26; acc: 0.59
Batch: 440; loss: 1.45; acc: 0.52
Batch: 460; loss: 1.41; acc: 0.5
Batch: 480; loss: 0.99; acc: 0.67
Batch: 500; loss: 1.26; acc: 0.55
Batch: 520; loss: 1.3; acc: 0.59
Batch: 540; loss: 1.19; acc: 0.56
Batch: 560; loss: 1.1; acc: 0.59
Batch: 580; loss: 1.35; acc: 0.48
Batch: 600; loss: 1.06; acc: 0.61
Batch: 620; loss: 1.15; acc: 0.58
Batch: 640; loss: 1.32; acc: 0.59
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.13; acc: 0.58
Batch: 700; loss: 1.05; acc: 0.66
Batch: 720; loss: 1.21; acc: 0.55
Batch: 740; loss: 1.41; acc: 0.55
Batch: 760; loss: 1.36; acc: 0.59
Batch: 780; loss: 1.21; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.58 

Batch: 0; loss: 1.27; acc: 0.56
Batch: 20; loss: 1.25; acc: 0.59
Batch: 40; loss: 0.9; acc: 0.64
Batch: 60; loss: 1.3; acc: 0.59
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.58
Val Epoch over. val_loss: 1.189259766013759; val_accuracy: 0.6087778662420382 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.28; acc: 0.52
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.48; acc: 0.47
Batch: 60; loss: 1.31; acc: 0.56
Batch: 80; loss: 1.24; acc: 0.59
Batch: 100; loss: 1.13; acc: 0.59
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.27; acc: 0.58
Batch: 180; loss: 1.51; acc: 0.48
Batch: 200; loss: 1.36; acc: 0.56
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 1.34; acc: 0.62
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.44; acc: 0.55
Batch: 320; loss: 1.47; acc: 0.5
Batch: 340; loss: 1.32; acc: 0.62
Batch: 360; loss: 1.26; acc: 0.53
Batch: 380; loss: 1.4; acc: 0.5
Batch: 400; loss: 1.33; acc: 0.53
Batch: 420; loss: 1.34; acc: 0.53
Batch: 440; loss: 1.24; acc: 0.59
Batch: 460; loss: 1.13; acc: 0.61
Batch: 480; loss: 1.31; acc: 0.53
Batch: 500; loss: 1.0; acc: 0.69
Batch: 520; loss: 1.12; acc: 0.62
Batch: 540; loss: 1.28; acc: 0.61
Batch: 560; loss: 1.47; acc: 0.53
Batch: 580; loss: 1.32; acc: 0.52
Batch: 600; loss: 1.22; acc: 0.59
Batch: 620; loss: 1.27; acc: 0.59
Batch: 640; loss: 1.38; acc: 0.56
Batch: 660; loss: 1.35; acc: 0.59
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.33; acc: 0.55
Batch: 720; loss: 1.3; acc: 0.62
Batch: 740; loss: 1.36; acc: 0.5
Batch: 760; loss: 1.37; acc: 0.56
Batch: 780; loss: 1.42; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.9; acc: 0.67
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.0; acc: 0.72
Batch: 100; loss: 1.24; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.58
Val Epoch over. val_loss: 1.1913964258637397; val_accuracy: 0.607484076433121 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 1.18; acc: 0.58
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.04; acc: 0.56
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.19; acc: 0.61
Batch: 160; loss: 1.2; acc: 0.55
Batch: 180; loss: 1.16; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.59
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 1.21; acc: 0.58
Batch: 260; loss: 1.13; acc: 0.66
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 1.18; acc: 0.64
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 1.03; acc: 0.66
Batch: 380; loss: 0.87; acc: 0.72
Batch: 400; loss: 1.36; acc: 0.61
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 1.25; acc: 0.67
Batch: 460; loss: 1.36; acc: 0.56
Batch: 480; loss: 1.04; acc: 0.7
Batch: 500; loss: 1.11; acc: 0.61
Batch: 520; loss: 1.05; acc: 0.7
Batch: 540; loss: 1.21; acc: 0.58
Batch: 560; loss: 1.23; acc: 0.53
Batch: 580; loss: 1.07; acc: 0.67
Batch: 600; loss: 1.4; acc: 0.61
Batch: 620; loss: 1.21; acc: 0.56
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.35; acc: 0.5
Batch: 680; loss: 0.96; acc: 0.72
Batch: 700; loss: 1.02; acc: 0.7
Batch: 720; loss: 1.42; acc: 0.52
Batch: 740; loss: 1.59; acc: 0.48
Batch: 760; loss: 1.24; acc: 0.56
Batch: 780; loss: 1.29; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.61
Val Epoch over. val_loss: 1.1895674277263082; val_accuracy: 0.6088773885350318 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 1.38; acc: 0.53
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.32; acc: 0.55
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 1.74; acc: 0.45
Batch: 160; loss: 1.18; acc: 0.64
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 1.49; acc: 0.52
Batch: 220; loss: 1.21; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.56
Batch: 280; loss: 1.27; acc: 0.53
Batch: 300; loss: 1.31; acc: 0.66
Batch: 320; loss: 0.98; acc: 0.62
Batch: 340; loss: 1.38; acc: 0.53
Batch: 360; loss: 1.39; acc: 0.53
Batch: 380; loss: 1.19; acc: 0.59
Batch: 400; loss: 1.29; acc: 0.58
Batch: 420; loss: 1.14; acc: 0.64
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 1.3; acc: 0.55
Batch: 480; loss: 1.26; acc: 0.61
Batch: 500; loss: 1.36; acc: 0.48
Batch: 520; loss: 1.3; acc: 0.53
Batch: 540; loss: 1.03; acc: 0.62
Batch: 560; loss: 1.4; acc: 0.55
Batch: 580; loss: 1.39; acc: 0.61
Batch: 600; loss: 1.06; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.62
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 1.31; acc: 0.58
Batch: 700; loss: 1.45; acc: 0.55
Batch: 720; loss: 1.48; acc: 0.48
Batch: 740; loss: 1.51; acc: 0.5
Batch: 760; loss: 1.0; acc: 0.61
Batch: 780; loss: 1.23; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.59
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.31; acc: 0.62
Val Epoch over. val_loss: 1.187601558721749; val_accuracy: 0.6126592356687898 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.59
Batch: 40; loss: 1.15; acc: 0.59
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.45
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.25; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.56
Batch: 160; loss: 1.17; acc: 0.58
Batch: 180; loss: 0.91; acc: 0.7
Batch: 200; loss: 1.46; acc: 0.5
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.56
Batch: 260; loss: 1.07; acc: 0.59
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 1.4; acc: 0.53
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.29; acc: 0.47
Batch: 360; loss: 1.21; acc: 0.53
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.22; acc: 0.61
Batch: 420; loss: 1.28; acc: 0.58
Batch: 440; loss: 1.1; acc: 0.56
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 1.34; acc: 0.55
Batch: 500; loss: 1.21; acc: 0.55
Batch: 520; loss: 1.1; acc: 0.64
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 1.41; acc: 0.61
Batch: 580; loss: 1.31; acc: 0.59
Batch: 600; loss: 1.24; acc: 0.58
Batch: 620; loss: 1.37; acc: 0.55
Batch: 640; loss: 0.96; acc: 0.62
Batch: 660; loss: 1.2; acc: 0.61
Batch: 680; loss: 1.08; acc: 0.62
Batch: 700; loss: 1.11; acc: 0.59
Batch: 720; loss: 1.38; acc: 0.52
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.08; acc: 0.64
Batch: 780; loss: 1.33; acc: 0.55
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 1.17; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.59
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.27; acc: 0.59
Val Epoch over. val_loss: 1.1857992630855294; val_accuracy: 0.613953025477707 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 1.29; acc: 0.52
Batch: 60; loss: 1.44; acc: 0.53
Batch: 80; loss: 1.18; acc: 0.55
Batch: 100; loss: 1.24; acc: 0.56
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 1.03; acc: 0.66
Batch: 160; loss: 0.95; acc: 0.67
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.66
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.04; acc: 0.66
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.15; acc: 0.59
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 0.96; acc: 0.73
Batch: 340; loss: 1.34; acc: 0.55
Batch: 360; loss: 1.35; acc: 0.53
Batch: 380; loss: 1.32; acc: 0.58
Batch: 400; loss: 1.02; acc: 0.66
Batch: 420; loss: 1.27; acc: 0.53
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.48
Batch: 480; loss: 1.32; acc: 0.55
Batch: 500; loss: 1.31; acc: 0.58
Batch: 520; loss: 1.19; acc: 0.56
Batch: 540; loss: 1.19; acc: 0.62
Batch: 560; loss: 1.4; acc: 0.52
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 1.21; acc: 0.59
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 1.58; acc: 0.45
Batch: 660; loss: 1.76; acc: 0.41
Batch: 680; loss: 1.37; acc: 0.55
Batch: 700; loss: 1.39; acc: 0.61
Batch: 720; loss: 1.05; acc: 0.66
Batch: 740; loss: 1.19; acc: 0.56
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 1.25; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1874867875105257; val_accuracy: 0.612062101910828 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.14; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.47
Batch: 60; loss: 1.39; acc: 0.53
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.06; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.64
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.39; acc: 0.53
Batch: 240; loss: 1.07; acc: 0.64
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 1.29; acc: 0.53
Batch: 300; loss: 1.31; acc: 0.66
Batch: 320; loss: 0.96; acc: 0.61
Batch: 340; loss: 1.49; acc: 0.52
Batch: 360; loss: 1.23; acc: 0.56
Batch: 380; loss: 1.28; acc: 0.56
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.35; acc: 0.53
Batch: 440; loss: 1.36; acc: 0.58
Batch: 460; loss: 1.3; acc: 0.58
Batch: 480; loss: 1.3; acc: 0.52
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.12; acc: 0.59
Batch: 540; loss: 1.11; acc: 0.66
Batch: 560; loss: 1.28; acc: 0.56
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 1.42; acc: 0.53
Batch: 620; loss: 1.44; acc: 0.53
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.33; acc: 0.59
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.49; acc: 0.52
Batch: 720; loss: 1.28; acc: 0.55
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.21; acc: 0.67
Batch: 780; loss: 1.51; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 0.9; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.31; acc: 0.59
Val Epoch over. val_loss: 1.1859529428421312; val_accuracy: 0.6153463375796179 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.99; acc: 0.69
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.67
Batch: 100; loss: 1.32; acc: 0.53
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.3; acc: 0.58
Batch: 180; loss: 1.17; acc: 0.58
Batch: 200; loss: 1.35; acc: 0.67
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 1.07; acc: 0.7
Batch: 260; loss: 1.31; acc: 0.52
Batch: 280; loss: 1.3; acc: 0.59
Batch: 300; loss: 1.22; acc: 0.61
Batch: 320; loss: 1.36; acc: 0.59
Batch: 340; loss: 0.92; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.66
Batch: 380; loss: 1.47; acc: 0.56
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 1.25; acc: 0.53
Batch: 440; loss: 1.04; acc: 0.62
Batch: 460; loss: 1.47; acc: 0.47
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.01; acc: 0.69
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.08; acc: 0.67
Batch: 600; loss: 1.33; acc: 0.55
Batch: 620; loss: 1.23; acc: 0.55
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.12; acc: 0.66
Batch: 700; loss: 1.29; acc: 0.58
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.29; acc: 0.58
Batch: 760; loss: 1.11; acc: 0.64
Batch: 780; loss: 1.11; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.26; acc: 0.56
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.64
Batch: 80; loss: 1.06; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 1.32; acc: 0.56
Val Epoch over. val_loss: 1.1906871267944386; val_accuracy: 0.6084792993630573 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.55
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.52
Batch: 100; loss: 1.21; acc: 0.58
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 1.22; acc: 0.59
Batch: 160; loss: 1.24; acc: 0.61
Batch: 180; loss: 1.22; acc: 0.61
Batch: 200; loss: 1.39; acc: 0.56
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 0.96; acc: 0.69
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.21; acc: 0.56
Batch: 320; loss: 1.37; acc: 0.58
Batch: 340; loss: 1.13; acc: 0.67
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.22; acc: 0.59
Batch: 400; loss: 1.28; acc: 0.56
Batch: 420; loss: 1.4; acc: 0.59
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.7
Batch: 480; loss: 1.0; acc: 0.59
Batch: 500; loss: 1.21; acc: 0.61
Batch: 520; loss: 1.22; acc: 0.59
Batch: 540; loss: 1.61; acc: 0.53
Batch: 560; loss: 1.35; acc: 0.59
Batch: 580; loss: 1.34; acc: 0.55
Batch: 600; loss: 1.16; acc: 0.59
Batch: 620; loss: 1.18; acc: 0.59
Batch: 640; loss: 1.4; acc: 0.56
Batch: 660; loss: 1.23; acc: 0.59
Batch: 680; loss: 1.1; acc: 0.64
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 1.45; acc: 0.48
Batch: 740; loss: 1.51; acc: 0.61
Batch: 760; loss: 1.27; acc: 0.56
Batch: 780; loss: 1.32; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.3; acc: 0.61
Val Epoch over. val_loss: 1.1846545391781316; val_accuracy: 0.6125597133757962 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.72
Batch: 20; loss: 1.53; acc: 0.45
Batch: 40; loss: 1.16; acc: 0.61
Batch: 60; loss: 1.02; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.19; acc: 0.56
Batch: 120; loss: 1.28; acc: 0.56
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 1.4; acc: 0.59
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.53
Batch: 220; loss: 1.19; acc: 0.61
Batch: 240; loss: 1.24; acc: 0.53
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.36; acc: 0.52
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.27; acc: 0.58
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 1.3; acc: 0.55
Batch: 380; loss: 1.25; acc: 0.55
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 1.44; acc: 0.55
Batch: 500; loss: 1.41; acc: 0.48
Batch: 520; loss: 1.15; acc: 0.61
Batch: 540; loss: 1.08; acc: 0.62
Batch: 560; loss: 1.26; acc: 0.55
Batch: 580; loss: 1.21; acc: 0.58
Batch: 600; loss: 1.14; acc: 0.62
Batch: 620; loss: 1.02; acc: 0.72
Batch: 640; loss: 1.36; acc: 0.55
Batch: 660; loss: 1.14; acc: 0.62
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.09; acc: 0.62
Batch: 720; loss: 1.13; acc: 0.55
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.24; acc: 0.59
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.87; acc: 0.64
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.26; acc: 0.59
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.58
Val Epoch over. val_loss: 1.1860254012095701; val_accuracy: 0.6124601910828026 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.37; acc: 0.59
Batch: 20; loss: 1.43; acc: 0.47
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 1.16; acc: 0.59
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.69; acc: 0.42
Batch: 160; loss: 1.31; acc: 0.61
Batch: 180; loss: 1.2; acc: 0.62
Batch: 200; loss: 1.14; acc: 0.59
Batch: 220; loss: 1.75; acc: 0.5
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.56
Batch: 280; loss: 1.11; acc: 0.62
Batch: 300; loss: 1.15; acc: 0.59
Batch: 320; loss: 1.34; acc: 0.55
Batch: 340; loss: 1.3; acc: 0.58
Batch: 360; loss: 1.31; acc: 0.53
Batch: 380; loss: 1.14; acc: 0.64
Batch: 400; loss: 1.34; acc: 0.56
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.44; acc: 0.52
Batch: 500; loss: 1.36; acc: 0.58
Batch: 520; loss: 1.3; acc: 0.59
Batch: 540; loss: 1.09; acc: 0.61
Batch: 560; loss: 1.41; acc: 0.55
Batch: 580; loss: 1.07; acc: 0.61
Batch: 600; loss: 1.3; acc: 0.56
Batch: 620; loss: 1.25; acc: 0.52
Batch: 640; loss: 1.54; acc: 0.48
Batch: 660; loss: 1.47; acc: 0.55
Batch: 680; loss: 1.43; acc: 0.48
Batch: 700; loss: 1.12; acc: 0.62
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.4; acc: 0.55
Batch: 760; loss: 1.13; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.61
Val Epoch over. val_loss: 1.1846701510392936; val_accuracy: 0.6134554140127388 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 1.06; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.12; acc: 0.64
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.31; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.48; acc: 0.53
Batch: 180; loss: 1.3; acc: 0.56
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.35; acc: 0.52
Batch: 240; loss: 0.94; acc: 0.75
Batch: 260; loss: 1.29; acc: 0.53
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.3; acc: 0.58
Batch: 320; loss: 1.23; acc: 0.55
Batch: 340; loss: 1.36; acc: 0.52
Batch: 360; loss: 1.28; acc: 0.48
Batch: 380; loss: 1.13; acc: 0.62
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 1.18; acc: 0.62
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 1.25; acc: 0.5
Batch: 480; loss: 1.63; acc: 0.5
Batch: 500; loss: 1.54; acc: 0.59
Batch: 520; loss: 1.21; acc: 0.67
Batch: 540; loss: 1.28; acc: 0.59
Batch: 560; loss: 1.2; acc: 0.56
Batch: 580; loss: 1.17; acc: 0.56
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 1.18; acc: 0.53
Batch: 640; loss: 1.34; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.61
Batch: 700; loss: 1.35; acc: 0.61
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.4; acc: 0.48
Batch: 760; loss: 1.43; acc: 0.55
Batch: 780; loss: 1.19; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1823103355753952; val_accuracy: 0.6152468152866242 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.35; acc: 0.59
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.2; acc: 0.59
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.94; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.67
Batch: 180; loss: 1.56; acc: 0.45
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 1.3; acc: 0.55
Batch: 240; loss: 1.06; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.45; acc: 0.53
Batch: 300; loss: 1.24; acc: 0.55
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.67
Batch: 360; loss: 1.11; acc: 0.64
Batch: 380; loss: 1.26; acc: 0.67
Batch: 400; loss: 1.42; acc: 0.56
Batch: 420; loss: 1.42; acc: 0.48
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.66
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 1.07; acc: 0.64
Batch: 540; loss: 1.4; acc: 0.55
Batch: 560; loss: 1.06; acc: 0.59
Batch: 580; loss: 1.3; acc: 0.61
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.42; acc: 0.55
Batch: 640; loss: 1.32; acc: 0.64
Batch: 660; loss: 1.36; acc: 0.61
Batch: 680; loss: 1.26; acc: 0.59
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.58
Batch: 740; loss: 1.06; acc: 0.62
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.3; acc: 0.58
Val Epoch over. val_loss: 1.1830216160245761; val_accuracy: 0.6137539808917197 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 1.18; acc: 0.58
Batch: 60; loss: 1.04; acc: 0.67
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.0; acc: 0.64
Batch: 120; loss: 1.48; acc: 0.48
Batch: 140; loss: 1.16; acc: 0.59
Batch: 160; loss: 1.42; acc: 0.56
Batch: 180; loss: 1.32; acc: 0.5
Batch: 200; loss: 1.07; acc: 0.69
Batch: 220; loss: 1.56; acc: 0.52
Batch: 240; loss: 1.28; acc: 0.53
Batch: 260; loss: 1.13; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.23; acc: 0.56
Batch: 320; loss: 0.9; acc: 0.7
Batch: 340; loss: 1.36; acc: 0.5
Batch: 360; loss: 1.51; acc: 0.48
Batch: 380; loss: 1.42; acc: 0.56
Batch: 400; loss: 1.44; acc: 0.52
Batch: 420; loss: 1.28; acc: 0.55
Batch: 440; loss: 1.41; acc: 0.56
Batch: 460; loss: 1.41; acc: 0.5
Batch: 480; loss: 1.18; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.01; acc: 0.59
Batch: 540; loss: 1.15; acc: 0.58
Batch: 560; loss: 1.18; acc: 0.62
Batch: 580; loss: 1.36; acc: 0.5
Batch: 600; loss: 1.3; acc: 0.59
Batch: 620; loss: 1.22; acc: 0.58
Batch: 640; loss: 1.01; acc: 0.7
Batch: 660; loss: 1.36; acc: 0.56
Batch: 680; loss: 1.55; acc: 0.34
Batch: 700; loss: 1.4; acc: 0.55
Batch: 720; loss: 1.13; acc: 0.61
Batch: 740; loss: 1.36; acc: 0.59
Batch: 760; loss: 1.25; acc: 0.59
Batch: 780; loss: 1.21; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1847473542401745; val_accuracy: 0.6134554140127388 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.18; acc: 0.62
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 1.39; acc: 0.55
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.31; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.52
Batch: 160; loss: 1.2; acc: 0.59
Batch: 180; loss: 1.14; acc: 0.58
Batch: 200; loss: 1.41; acc: 0.56
Batch: 220; loss: 1.17; acc: 0.59
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.33; acc: 0.56
Batch: 280; loss: 1.39; acc: 0.53
Batch: 300; loss: 1.08; acc: 0.64
Batch: 320; loss: 1.33; acc: 0.59
Batch: 340; loss: 1.09; acc: 0.66
Batch: 360; loss: 1.26; acc: 0.69
Batch: 380; loss: 1.01; acc: 0.62
Batch: 400; loss: 1.31; acc: 0.53
Batch: 420; loss: 1.29; acc: 0.55
Batch: 440; loss: 1.2; acc: 0.59
Batch: 460; loss: 1.24; acc: 0.55
Batch: 480; loss: 1.26; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 1.24; acc: 0.52
Batch: 600; loss: 1.05; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.61
Batch: 640; loss: 1.46; acc: 0.53
Batch: 660; loss: 1.09; acc: 0.61
Batch: 680; loss: 1.55; acc: 0.42
Batch: 700; loss: 1.34; acc: 0.64
Batch: 720; loss: 1.2; acc: 0.58
Batch: 740; loss: 1.3; acc: 0.53
Batch: 760; loss: 1.35; acc: 0.56
Batch: 780; loss: 1.16; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 1.29; acc: 0.55
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.64
Val Epoch over. val_loss: 1.182798504070112; val_accuracy: 0.6156449044585988 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 1.45; acc: 0.53
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.12; acc: 0.62
Batch: 100; loss: 1.02; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.58
Batch: 140; loss: 1.3; acc: 0.59
Batch: 160; loss: 1.23; acc: 0.61
Batch: 180; loss: 1.35; acc: 0.53
Batch: 200; loss: 1.39; acc: 0.61
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 1.34; acc: 0.52
Batch: 280; loss: 1.34; acc: 0.52
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.32; acc: 0.56
Batch: 340; loss: 1.43; acc: 0.5
Batch: 360; loss: 1.06; acc: 0.62
Batch: 380; loss: 1.18; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 1.1; acc: 0.61
Batch: 440; loss: 1.47; acc: 0.53
Batch: 460; loss: 1.01; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.62
Batch: 500; loss: 1.1; acc: 0.62
Batch: 520; loss: 1.29; acc: 0.53
Batch: 540; loss: 1.07; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.59
Batch: 580; loss: 1.11; acc: 0.58
Batch: 600; loss: 1.44; acc: 0.48
Batch: 620; loss: 1.15; acc: 0.53
Batch: 640; loss: 1.17; acc: 0.61
Batch: 660; loss: 1.32; acc: 0.55
Batch: 680; loss: 1.28; acc: 0.64
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.59
Batch: 740; loss: 1.25; acc: 0.53
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.18230288917092; val_accuracy: 0.6149482484076433 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.47; acc: 0.55
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 1.47; acc: 0.45
Batch: 60; loss: 1.59; acc: 0.42
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.58
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.53; acc: 0.52
Batch: 200; loss: 1.34; acc: 0.59
Batch: 220; loss: 1.56; acc: 0.52
Batch: 240; loss: 1.13; acc: 0.61
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.12; acc: 0.58
Batch: 320; loss: 1.37; acc: 0.53
Batch: 340; loss: 1.22; acc: 0.61
Batch: 360; loss: 1.3; acc: 0.56
Batch: 380; loss: 1.2; acc: 0.66
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 1.23; acc: 0.66
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.61
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.27; acc: 0.58
Batch: 540; loss: 1.31; acc: 0.59
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.22; acc: 0.61
Batch: 600; loss: 1.16; acc: 0.66
Batch: 620; loss: 1.46; acc: 0.5
Batch: 640; loss: 1.18; acc: 0.55
Batch: 660; loss: 1.4; acc: 0.47
Batch: 680; loss: 1.5; acc: 0.52
Batch: 700; loss: 1.26; acc: 0.58
Batch: 720; loss: 1.07; acc: 0.64
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.24; acc: 0.56
Batch: 780; loss: 1.26; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1859403313345211; val_accuracy: 0.6134554140127388 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.03; acc: 0.66
Batch: 20; loss: 1.27; acc: 0.56
Batch: 40; loss: 1.19; acc: 0.67
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.56
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.25; acc: 0.56
Batch: 160; loss: 1.14; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.61
Batch: 200; loss: 1.27; acc: 0.66
Batch: 220; loss: 1.36; acc: 0.62
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 1.71; acc: 0.45
Batch: 300; loss: 1.3; acc: 0.55
Batch: 320; loss: 1.43; acc: 0.5
Batch: 340; loss: 1.11; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.59
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.3; acc: 0.55
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 1.46; acc: 0.5
Batch: 460; loss: 1.15; acc: 0.59
Batch: 480; loss: 1.44; acc: 0.48
Batch: 500; loss: 1.12; acc: 0.59
Batch: 520; loss: 1.15; acc: 0.64
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.38; acc: 0.5
Batch: 580; loss: 1.61; acc: 0.44
Batch: 600; loss: 1.24; acc: 0.59
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.12; acc: 0.61
Batch: 660; loss: 1.42; acc: 0.58
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.24; acc: 0.62
Batch: 720; loss: 1.27; acc: 0.56
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.29; acc: 0.64
Batch: 780; loss: 1.27; acc: 0.55
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1813885459474698; val_accuracy: 0.6153463375796179 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.35; acc: 0.56
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 1.05; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.5
Batch: 80; loss: 1.13; acc: 0.61
Batch: 100; loss: 1.22; acc: 0.64
Batch: 120; loss: 1.02; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 1.16; acc: 0.66
Batch: 180; loss: 1.11; acc: 0.59
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.2; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 1.4; acc: 0.48
Batch: 300; loss: 1.27; acc: 0.56
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.21; acc: 0.62
Batch: 360; loss: 1.44; acc: 0.52
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.45; acc: 0.55
Batch: 420; loss: 1.36; acc: 0.59
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 1.2; acc: 0.58
Batch: 480; loss: 1.14; acc: 0.56
Batch: 500; loss: 1.51; acc: 0.48
Batch: 520; loss: 1.21; acc: 0.53
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.5
Batch: 580; loss: 1.25; acc: 0.59
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 1.2; acc: 0.58
Batch: 660; loss: 1.36; acc: 0.59
Batch: 680; loss: 1.25; acc: 0.58
Batch: 700; loss: 1.55; acc: 0.53
Batch: 720; loss: 1.18; acc: 0.64
Batch: 740; loss: 1.32; acc: 0.59
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 1.47; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1823616817498663; val_accuracy: 0.6140525477707006 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 1.19; acc: 0.67
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.44; acc: 0.47
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.66
Batch: 160; loss: 1.43; acc: 0.55
Batch: 180; loss: 1.09; acc: 0.64
Batch: 200; loss: 1.43; acc: 0.56
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.57; acc: 0.47
Batch: 260; loss: 1.32; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.58
Batch: 300; loss: 1.06; acc: 0.67
Batch: 320; loss: 1.11; acc: 0.66
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.53; acc: 0.5
Batch: 380; loss: 1.14; acc: 0.55
Batch: 400; loss: 1.18; acc: 0.62
Batch: 420; loss: 1.27; acc: 0.59
Batch: 440; loss: 1.07; acc: 0.61
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.92; acc: 0.67
Batch: 520; loss: 1.32; acc: 0.55
Batch: 540; loss: 0.94; acc: 0.67
Batch: 560; loss: 1.25; acc: 0.58
Batch: 580; loss: 1.23; acc: 0.5
Batch: 600; loss: 1.04; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.59
Batch: 640; loss: 1.27; acc: 0.56
Batch: 660; loss: 1.26; acc: 0.64
Batch: 680; loss: 1.55; acc: 0.52
Batch: 700; loss: 1.36; acc: 0.56
Batch: 720; loss: 1.27; acc: 0.55
Batch: 740; loss: 1.02; acc: 0.64
Batch: 760; loss: 1.41; acc: 0.58
Batch: 780; loss: 1.24; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.59
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1830232971033472; val_accuracy: 0.6124601910828026 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 1.27; acc: 0.55
Batch: 60; loss: 1.06; acc: 0.67
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.73; acc: 0.45
Batch: 120; loss: 1.15; acc: 0.72
Batch: 140; loss: 1.39; acc: 0.53
Batch: 160; loss: 1.32; acc: 0.55
Batch: 180; loss: 1.13; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.62
Batch: 220; loss: 1.24; acc: 0.59
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.58; acc: 0.45
Batch: 280; loss: 0.91; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.61
Batch: 320; loss: 1.06; acc: 0.64
Batch: 340; loss: 1.12; acc: 0.56
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.25; acc: 0.52
Batch: 400; loss: 1.35; acc: 0.53
Batch: 420; loss: 1.27; acc: 0.48
Batch: 440; loss: 1.34; acc: 0.56
Batch: 460; loss: 1.1; acc: 0.62
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.53
Batch: 520; loss: 1.25; acc: 0.61
Batch: 540; loss: 1.34; acc: 0.56
Batch: 560; loss: 1.36; acc: 0.48
Batch: 580; loss: 1.21; acc: 0.59
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.11; acc: 0.61
Batch: 640; loss: 1.15; acc: 0.58
Batch: 660; loss: 1.27; acc: 0.59
Batch: 680; loss: 1.35; acc: 0.56
Batch: 700; loss: 1.0; acc: 0.64
Batch: 720; loss: 1.34; acc: 0.52
Batch: 740; loss: 1.29; acc: 0.59
Batch: 760; loss: 1.18; acc: 0.59
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.59
Val Epoch over. val_loss: 1.1816978800068996; val_accuracy: 0.6140525477707006 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.58; acc: 0.56
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 1.07; acc: 0.61
Batch: 60; loss: 1.2; acc: 0.61
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.11; acc: 0.62
Batch: 120; loss: 1.17; acc: 0.61
Batch: 140; loss: 1.3; acc: 0.58
Batch: 160; loss: 1.19; acc: 0.58
Batch: 180; loss: 1.34; acc: 0.64
Batch: 200; loss: 1.3; acc: 0.61
Batch: 220; loss: 1.53; acc: 0.48
Batch: 240; loss: 1.2; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 0.96; acc: 0.7
Batch: 300; loss: 1.39; acc: 0.58
Batch: 320; loss: 1.21; acc: 0.56
Batch: 340; loss: 1.34; acc: 0.59
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.27; acc: 0.5
Batch: 400; loss: 1.74; acc: 0.42
Batch: 420; loss: 1.38; acc: 0.5
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.59
Batch: 480; loss: 1.17; acc: 0.59
Batch: 500; loss: 1.46; acc: 0.5
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.27; acc: 0.58
Batch: 580; loss: 1.22; acc: 0.59
Batch: 600; loss: 0.94; acc: 0.72
Batch: 620; loss: 1.34; acc: 0.58
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 1.23; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.58
Batch: 720; loss: 0.88; acc: 0.78
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.45; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.28; acc: 0.58
Val Epoch over. val_loss: 1.181830858349041; val_accuracy: 0.6136544585987261 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.13; acc: 0.58
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.67
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.14; acc: 0.61
Batch: 160; loss: 1.17; acc: 0.64
Batch: 180; loss: 1.16; acc: 0.61
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.28; acc: 0.53
Batch: 240; loss: 1.35; acc: 0.64
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.31; acc: 0.56
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.32; acc: 0.55
Batch: 340; loss: 1.05; acc: 0.7
Batch: 360; loss: 1.25; acc: 0.61
Batch: 380; loss: 1.31; acc: 0.58
Batch: 400; loss: 1.31; acc: 0.52
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 1.36; acc: 0.48
Batch: 460; loss: 1.18; acc: 0.64
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.38; acc: 0.56
Batch: 520; loss: 1.29; acc: 0.62
Batch: 540; loss: 1.05; acc: 0.67
Batch: 560; loss: 1.39; acc: 0.56
Batch: 580; loss: 1.26; acc: 0.59
Batch: 600; loss: 1.4; acc: 0.44
Batch: 620; loss: 1.3; acc: 0.66
Batch: 640; loss: 1.3; acc: 0.62
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.52
Batch: 720; loss: 1.39; acc: 0.53
Batch: 740; loss: 1.42; acc: 0.5
Batch: 760; loss: 1.49; acc: 0.52
Batch: 780; loss: 1.45; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1825448263223004; val_accuracy: 0.6134554140127388 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.31; acc: 0.58
Batch: 20; loss: 1.23; acc: 0.53
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.45
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.28; acc: 0.59
Batch: 180; loss: 1.21; acc: 0.61
Batch: 200; loss: 1.3; acc: 0.52
Batch: 220; loss: 1.28; acc: 0.56
Batch: 240; loss: 1.42; acc: 0.55
Batch: 260; loss: 1.05; acc: 0.59
Batch: 280; loss: 1.53; acc: 0.5
Batch: 300; loss: 1.16; acc: 0.62
Batch: 320; loss: 1.28; acc: 0.58
Batch: 340; loss: 1.34; acc: 0.55
Batch: 360; loss: 0.9; acc: 0.7
Batch: 380; loss: 1.13; acc: 0.64
Batch: 400; loss: 1.28; acc: 0.58
Batch: 420; loss: 1.22; acc: 0.59
Batch: 440; loss: 1.5; acc: 0.41
Batch: 460; loss: 1.14; acc: 0.62
Batch: 480; loss: 1.25; acc: 0.58
Batch: 500; loss: 1.12; acc: 0.61
Batch: 520; loss: 1.31; acc: 0.53
Batch: 540; loss: 1.26; acc: 0.64
Batch: 560; loss: 1.43; acc: 0.48
Batch: 580; loss: 1.1; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.64
Batch: 620; loss: 1.26; acc: 0.52
Batch: 640; loss: 1.22; acc: 0.55
Batch: 660; loss: 1.25; acc: 0.59
Batch: 680; loss: 1.22; acc: 0.61
Batch: 700; loss: 1.25; acc: 0.61
Batch: 720; loss: 1.37; acc: 0.58
Batch: 740; loss: 1.12; acc: 0.66
Batch: 760; loss: 1.08; acc: 0.66
Batch: 780; loss: 0.94; acc: 0.75
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.58
Val Epoch over. val_loss: 1.1817400330191206; val_accuracy: 0.6142515923566879 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.25; acc: 0.56
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.52
Batch: 120; loss: 1.33; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.58
Batch: 160; loss: 1.12; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.44; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.58
Batch: 260; loss: 1.38; acc: 0.53
Batch: 280; loss: 1.32; acc: 0.56
Batch: 300; loss: 1.42; acc: 0.5
Batch: 320; loss: 1.23; acc: 0.59
Batch: 340; loss: 1.2; acc: 0.62
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.35; acc: 0.62
Batch: 400; loss: 1.19; acc: 0.58
Batch: 420; loss: 1.21; acc: 0.55
Batch: 440; loss: 1.1; acc: 0.64
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.02; acc: 0.67
Batch: 500; loss: 1.25; acc: 0.59
Batch: 520; loss: 1.2; acc: 0.61
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.49; acc: 0.52
Batch: 580; loss: 1.16; acc: 0.59
Batch: 600; loss: 1.05; acc: 0.66
Batch: 620; loss: 1.23; acc: 0.5
Batch: 640; loss: 1.28; acc: 0.58
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.43; acc: 0.56
Batch: 700; loss: 1.36; acc: 0.59
Batch: 720; loss: 1.28; acc: 0.59
Batch: 740; loss: 1.13; acc: 0.58
Batch: 760; loss: 1.15; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1810862406803544; val_accuracy: 0.6157444267515924 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.13; acc: 0.58
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.09; acc: 0.61
Batch: 60; loss: 1.14; acc: 0.55
Batch: 80; loss: 1.21; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.56
Batch: 120; loss: 1.34; acc: 0.53
Batch: 140; loss: 1.12; acc: 0.64
Batch: 160; loss: 1.26; acc: 0.59
Batch: 180; loss: 1.16; acc: 0.67
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 1.12; acc: 0.62
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 1.56; acc: 0.55
Batch: 280; loss: 1.31; acc: 0.58
Batch: 300; loss: 1.13; acc: 0.58
Batch: 320; loss: 1.25; acc: 0.61
Batch: 340; loss: 1.43; acc: 0.47
Batch: 360; loss: 1.13; acc: 0.7
Batch: 380; loss: 1.29; acc: 0.56
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.52
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.58
Batch: 480; loss: 1.26; acc: 0.61
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.71; acc: 0.47
Batch: 540; loss: 1.15; acc: 0.66
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.34; acc: 0.61
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 1.32; acc: 0.61
Batch: 640; loss: 1.03; acc: 0.62
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.2; acc: 0.61
Batch: 700; loss: 1.36; acc: 0.53
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.59; acc: 0.5
Batch: 760; loss: 1.39; acc: 0.53
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1817542618247354; val_accuracy: 0.615843949044586 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.28; acc: 0.52
Batch: 20; loss: 1.21; acc: 0.55
Batch: 40; loss: 1.27; acc: 0.59
Batch: 60; loss: 1.23; acc: 0.58
Batch: 80; loss: 1.46; acc: 0.64
Batch: 100; loss: 1.47; acc: 0.56
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.32; acc: 0.56
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.46; acc: 0.53
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.55
Batch: 240; loss: 1.3; acc: 0.56
Batch: 260; loss: 1.26; acc: 0.58
Batch: 280; loss: 1.1; acc: 0.61
Batch: 300; loss: 1.22; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.44; acc: 0.61
Batch: 360; loss: 1.36; acc: 0.62
Batch: 380; loss: 1.39; acc: 0.5
Batch: 400; loss: 1.32; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.45
Batch: 440; loss: 1.08; acc: 0.59
Batch: 460; loss: 1.04; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.3; acc: 0.52
Batch: 520; loss: 1.51; acc: 0.52
Batch: 540; loss: 1.27; acc: 0.62
Batch: 560; loss: 1.48; acc: 0.53
Batch: 580; loss: 1.31; acc: 0.52
Batch: 600; loss: 1.21; acc: 0.58
Batch: 620; loss: 1.3; acc: 0.58
Batch: 640; loss: 1.25; acc: 0.53
Batch: 660; loss: 1.05; acc: 0.64
Batch: 680; loss: 1.39; acc: 0.56
Batch: 700; loss: 1.24; acc: 0.55
Batch: 720; loss: 1.35; acc: 0.5
Batch: 740; loss: 1.27; acc: 0.52
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.09; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1813328364852127; val_accuracy: 0.6143511146496815 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.47
Batch: 80; loss: 1.25; acc: 0.59
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.58
Batch: 160; loss: 1.24; acc: 0.58
Batch: 180; loss: 1.43; acc: 0.62
Batch: 200; loss: 1.21; acc: 0.59
Batch: 220; loss: 1.07; acc: 0.64
Batch: 240; loss: 1.47; acc: 0.45
Batch: 260; loss: 1.5; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.48
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.25; acc: 0.48
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.34; acc: 0.67
Batch: 400; loss: 1.16; acc: 0.59
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.31; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.67
Batch: 480; loss: 1.18; acc: 0.62
Batch: 500; loss: 1.58; acc: 0.45
Batch: 520; loss: 1.05; acc: 0.64
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.13; acc: 0.64
Batch: 580; loss: 1.3; acc: 0.58
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 0.93; acc: 0.67
Batch: 640; loss: 1.14; acc: 0.62
Batch: 660; loss: 1.28; acc: 0.53
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 1.38; acc: 0.52
Batch: 720; loss: 1.19; acc: 0.59
Batch: 740; loss: 1.59; acc: 0.45
Batch: 760; loss: 1.12; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.56
Val Epoch over. val_loss: 1.1828215816977676; val_accuracy: 0.613156847133758 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.1; acc: 0.61
Batch: 40; loss: 1.28; acc: 0.58
Batch: 60; loss: 1.47; acc: 0.52
Batch: 80; loss: 1.0; acc: 0.64
Batch: 100; loss: 1.51; acc: 0.47
Batch: 120; loss: 1.23; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.59
Batch: 160; loss: 1.2; acc: 0.59
Batch: 180; loss: 1.22; acc: 0.56
Batch: 200; loss: 1.47; acc: 0.59
Batch: 220; loss: 1.37; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.5
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.24; acc: 0.61
Batch: 300; loss: 1.48; acc: 0.48
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.0; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.59
Batch: 380; loss: 1.36; acc: 0.55
Batch: 400; loss: 1.51; acc: 0.52
Batch: 420; loss: 1.51; acc: 0.56
Batch: 440; loss: 1.48; acc: 0.44
Batch: 460; loss: 1.21; acc: 0.52
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 1.1; acc: 0.58
Batch: 520; loss: 1.35; acc: 0.55
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 1.31; acc: 0.53
Batch: 580; loss: 1.09; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 1.38; acc: 0.56
Batch: 660; loss: 1.58; acc: 0.5
Batch: 680; loss: 1.35; acc: 0.5
Batch: 700; loss: 1.05; acc: 0.7
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.57; acc: 0.53
Batch: 760; loss: 1.36; acc: 0.58
Batch: 780; loss: 1.23; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1810780459908163; val_accuracy: 0.6154458598726115 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 1.5; acc: 0.5
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.18; acc: 0.56
Batch: 120; loss: 1.17; acc: 0.53
Batch: 140; loss: 1.29; acc: 0.59
Batch: 160; loss: 1.19; acc: 0.58
Batch: 180; loss: 1.26; acc: 0.61
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.34; acc: 0.55
Batch: 240; loss: 1.16; acc: 0.62
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 1.31; acc: 0.56
Batch: 300; loss: 1.16; acc: 0.66
Batch: 320; loss: 1.18; acc: 0.53
Batch: 340; loss: 1.35; acc: 0.59
Batch: 360; loss: 1.41; acc: 0.56
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.08; acc: 0.64
Batch: 420; loss: 1.21; acc: 0.61
Batch: 440; loss: 1.26; acc: 0.58
Batch: 460; loss: 1.27; acc: 0.58
Batch: 480; loss: 1.21; acc: 0.58
Batch: 500; loss: 1.11; acc: 0.61
Batch: 520; loss: 1.54; acc: 0.48
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 0.97; acc: 0.67
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.34; acc: 0.52
Batch: 640; loss: 1.3; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.52
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.1; acc: 0.62
Batch: 720; loss: 1.15; acc: 0.64
Batch: 740; loss: 1.18; acc: 0.61
Batch: 760; loss: 1.15; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1817501331590543; val_accuracy: 0.6129578025477707 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.19; acc: 0.58
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 1.31; acc: 0.56
Batch: 60; loss: 1.33; acc: 0.5
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.55
Batch: 140; loss: 1.13; acc: 0.59
Batch: 160; loss: 1.39; acc: 0.56
Batch: 180; loss: 1.33; acc: 0.66
Batch: 200; loss: 1.42; acc: 0.5
Batch: 220; loss: 1.1; acc: 0.61
Batch: 240; loss: 1.17; acc: 0.56
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 1.11; acc: 0.62
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 0.99; acc: 0.72
Batch: 360; loss: 1.26; acc: 0.64
Batch: 380; loss: 0.93; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.56
Batch: 420; loss: 1.47; acc: 0.48
Batch: 440; loss: 0.9; acc: 0.66
Batch: 460; loss: 1.35; acc: 0.62
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 1.28; acc: 0.64
Batch: 520; loss: 1.14; acc: 0.59
Batch: 540; loss: 1.41; acc: 0.55
Batch: 560; loss: 1.03; acc: 0.69
Batch: 580; loss: 1.29; acc: 0.56
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 1.45; acc: 0.55
Batch: 660; loss: 1.44; acc: 0.45
Batch: 680; loss: 0.97; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.59
Batch: 720; loss: 1.31; acc: 0.64
Batch: 740; loss: 1.3; acc: 0.58
Batch: 760; loss: 1.01; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1817174698137174; val_accuracy: 0.6130573248407644 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.39; acc: 0.58
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.95; acc: 0.7
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 1.02; acc: 0.62
Batch: 100; loss: 1.18; acc: 0.59
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.52
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.48
Batch: 220; loss: 1.08; acc: 0.64
Batch: 240; loss: 1.46; acc: 0.58
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 1.53; acc: 0.56
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.05; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.62
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 1.14; acc: 0.62
Batch: 400; loss: 1.16; acc: 0.62
Batch: 420; loss: 1.52; acc: 0.47
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 1.2; acc: 0.55
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.16; acc: 0.58
Batch: 560; loss: 1.12; acc: 0.69
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.25; acc: 0.58
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.43; acc: 0.56
Batch: 680; loss: 1.24; acc: 0.62
Batch: 700; loss: 1.2; acc: 0.55
Batch: 720; loss: 1.44; acc: 0.56
Batch: 740; loss: 1.12; acc: 0.62
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.31; acc: 0.59
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.181571841999224; val_accuracy: 0.6137539808917197 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.32; acc: 0.53
Batch: 40; loss: 1.45; acc: 0.56
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.38; acc: 0.55
Batch: 100; loss: 1.1; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.35; acc: 0.55
Batch: 160; loss: 1.15; acc: 0.62
Batch: 180; loss: 1.29; acc: 0.53
Batch: 200; loss: 1.22; acc: 0.59
Batch: 220; loss: 1.32; acc: 0.55
Batch: 240; loss: 1.2; acc: 0.62
Batch: 260; loss: 1.48; acc: 0.5
Batch: 280; loss: 1.22; acc: 0.58
Batch: 300; loss: 1.19; acc: 0.59
Batch: 320; loss: 1.21; acc: 0.59
Batch: 340; loss: 1.36; acc: 0.58
Batch: 360; loss: 1.36; acc: 0.56
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.12; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.43; acc: 0.59
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.36; acc: 0.5
Batch: 500; loss: 1.3; acc: 0.64
Batch: 520; loss: 1.53; acc: 0.47
Batch: 540; loss: 1.27; acc: 0.59
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.29; acc: 0.55
Batch: 620; loss: 1.36; acc: 0.53
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.4; acc: 0.48
Batch: 700; loss: 1.18; acc: 0.53
Batch: 720; loss: 1.33; acc: 0.61
Batch: 740; loss: 1.3; acc: 0.61
Batch: 760; loss: 1.19; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.69
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812203439177982; val_accuracy: 0.6143511146496815 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.23; acc: 0.56
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 1.48; acc: 0.53
Batch: 60; loss: 1.34; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.22; acc: 0.58
Batch: 120; loss: 1.27; acc: 0.55
Batch: 140; loss: 1.1; acc: 0.62
Batch: 160; loss: 1.55; acc: 0.52
Batch: 180; loss: 1.34; acc: 0.62
Batch: 200; loss: 1.29; acc: 0.56
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.13; acc: 0.66
Batch: 260; loss: 1.61; acc: 0.45
Batch: 280; loss: 1.18; acc: 0.58
Batch: 300; loss: 1.36; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.67
Batch: 340; loss: 1.04; acc: 0.64
Batch: 360; loss: 0.93; acc: 0.72
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.37; acc: 0.62
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 1.02; acc: 0.67
Batch: 460; loss: 1.02; acc: 0.67
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.33; acc: 0.58
Batch: 520; loss: 1.3; acc: 0.52
Batch: 540; loss: 1.23; acc: 0.62
Batch: 560; loss: 1.14; acc: 0.73
Batch: 580; loss: 1.28; acc: 0.55
Batch: 600; loss: 1.01; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.62
Batch: 640; loss: 0.97; acc: 0.69
Batch: 660; loss: 1.4; acc: 0.55
Batch: 680; loss: 1.27; acc: 0.59
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 1.28; acc: 0.58
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.45
Batch: 780; loss: 1.15; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1814319836865566; val_accuracy: 0.6144506369426752 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.22; acc: 0.55
Batch: 20; loss: 1.48; acc: 0.5
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.54; acc: 0.48
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.17; acc: 0.62
Batch: 160; loss: 1.21; acc: 0.58
Batch: 180; loss: 1.21; acc: 0.67
Batch: 200; loss: 1.55; acc: 0.5
Batch: 220; loss: 1.26; acc: 0.62
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.17; acc: 0.59
Batch: 280; loss: 1.51; acc: 0.53
Batch: 300; loss: 1.56; acc: 0.53
Batch: 320; loss: 1.31; acc: 0.5
Batch: 340; loss: 1.28; acc: 0.55
Batch: 360; loss: 1.0; acc: 0.7
Batch: 380; loss: 1.19; acc: 0.59
Batch: 400; loss: 1.41; acc: 0.58
Batch: 420; loss: 1.27; acc: 0.58
Batch: 440; loss: 1.21; acc: 0.55
Batch: 460; loss: 1.3; acc: 0.55
Batch: 480; loss: 1.38; acc: 0.62
Batch: 500; loss: 1.26; acc: 0.67
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.16; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.38; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.55
Batch: 620; loss: 1.1; acc: 0.59
Batch: 640; loss: 1.51; acc: 0.53
Batch: 660; loss: 1.42; acc: 0.58
Batch: 680; loss: 1.18; acc: 0.62
Batch: 700; loss: 1.3; acc: 0.53
Batch: 720; loss: 1.01; acc: 0.7
Batch: 740; loss: 1.3; acc: 0.64
Batch: 760; loss: 1.37; acc: 0.48
Batch: 780; loss: 1.34; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812812970702056; val_accuracy: 0.6144506369426752 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.17; acc: 0.59
Batch: 60; loss: 1.4; acc: 0.47
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 0.88; acc: 0.66
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.54; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.47
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.31; acc: 0.53
Batch: 260; loss: 1.24; acc: 0.58
Batch: 280; loss: 1.5; acc: 0.48
Batch: 300; loss: 1.29; acc: 0.59
Batch: 320; loss: 1.16; acc: 0.69
Batch: 340; loss: 1.29; acc: 0.53
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.47; acc: 0.5
Batch: 440; loss: 1.31; acc: 0.56
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 1.39; acc: 0.5
Batch: 520; loss: 1.19; acc: 0.61
Batch: 540; loss: 1.22; acc: 0.61
Batch: 560; loss: 1.21; acc: 0.56
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 1.16; acc: 0.58
Batch: 620; loss: 1.18; acc: 0.59
Batch: 640; loss: 1.19; acc: 0.61
Batch: 660; loss: 1.52; acc: 0.5
Batch: 680; loss: 1.29; acc: 0.58
Batch: 700; loss: 1.04; acc: 0.66
Batch: 720; loss: 1.38; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.47; acc: 0.52
Batch: 780; loss: 1.33; acc: 0.58
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.181293975016114; val_accuracy: 0.6146496815286624 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.08; acc: 0.59
Batch: 120; loss: 1.43; acc: 0.41
Batch: 140; loss: 1.27; acc: 0.52
Batch: 160; loss: 1.14; acc: 0.64
Batch: 180; loss: 1.26; acc: 0.53
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 0.9; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 1.17; acc: 0.66
Batch: 280; loss: 1.25; acc: 0.56
Batch: 300; loss: 0.88; acc: 0.72
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 1.15; acc: 0.62
Batch: 360; loss: 1.2; acc: 0.58
Batch: 380; loss: 1.49; acc: 0.53
Batch: 400; loss: 1.3; acc: 0.55
Batch: 420; loss: 1.15; acc: 0.59
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.31; acc: 0.61
Batch: 520; loss: 1.48; acc: 0.53
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.07; acc: 0.64
Batch: 580; loss: 1.25; acc: 0.56
Batch: 600; loss: 1.47; acc: 0.53
Batch: 620; loss: 1.4; acc: 0.58
Batch: 640; loss: 1.22; acc: 0.55
Batch: 660; loss: 1.16; acc: 0.61
Batch: 680; loss: 1.54; acc: 0.5
Batch: 700; loss: 1.27; acc: 0.61
Batch: 720; loss: 1.48; acc: 0.59
Batch: 740; loss: 1.49; acc: 0.52
Batch: 760; loss: 1.27; acc: 0.56
Batch: 780; loss: 1.31; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1815242524359637; val_accuracy: 0.6145501592356688 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.06; acc: 0.61
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.25; acc: 0.5
Batch: 140; loss: 1.44; acc: 0.55
Batch: 160; loss: 1.14; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.52
Batch: 200; loss: 1.16; acc: 0.56
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.22; acc: 0.64
Batch: 260; loss: 1.23; acc: 0.56
Batch: 280; loss: 1.05; acc: 0.64
Batch: 300; loss: 1.14; acc: 0.59
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.32; acc: 0.56
Batch: 360; loss: 1.33; acc: 0.62
Batch: 380; loss: 1.51; acc: 0.5
Batch: 400; loss: 0.9; acc: 0.64
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.62
Batch: 460; loss: 1.14; acc: 0.62
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.44; acc: 0.56
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.28; acc: 0.53
Batch: 560; loss: 1.32; acc: 0.56
Batch: 580; loss: 1.24; acc: 0.58
Batch: 600; loss: 1.05; acc: 0.59
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.42; acc: 0.53
Batch: 660; loss: 0.87; acc: 0.78
Batch: 680; loss: 1.33; acc: 0.61
Batch: 700; loss: 0.97; acc: 0.66
Batch: 720; loss: 1.47; acc: 0.52
Batch: 740; loss: 1.03; acc: 0.7
Batch: 760; loss: 1.45; acc: 0.48
Batch: 780; loss: 1.34; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1815234863074722; val_accuracy: 0.6140525477707006 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.38; acc: 0.61
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.42
Batch: 60; loss: 1.37; acc: 0.56
Batch: 80; loss: 1.3; acc: 0.56
Batch: 100; loss: 1.37; acc: 0.5
Batch: 120; loss: 1.47; acc: 0.44
Batch: 140; loss: 1.36; acc: 0.53
Batch: 160; loss: 1.39; acc: 0.58
Batch: 180; loss: 1.24; acc: 0.53
Batch: 200; loss: 1.18; acc: 0.61
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.24; acc: 0.56
Batch: 280; loss: 1.25; acc: 0.62
Batch: 300; loss: 1.08; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.66
Batch: 340; loss: 1.27; acc: 0.55
Batch: 360; loss: 1.31; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.38; acc: 0.64
Batch: 420; loss: 1.12; acc: 0.62
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.24; acc: 0.59
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.2; acc: 0.56
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.35; acc: 0.58
Batch: 560; loss: 1.25; acc: 0.58
Batch: 580; loss: 1.57; acc: 0.52
Batch: 600; loss: 1.3; acc: 0.64
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.25; acc: 0.59
Batch: 680; loss: 1.06; acc: 0.66
Batch: 700; loss: 1.41; acc: 0.58
Batch: 720; loss: 1.42; acc: 0.5
Batch: 740; loss: 1.31; acc: 0.64
Batch: 760; loss: 1.28; acc: 0.66
Batch: 780; loss: 1.28; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812897586518791; val_accuracy: 0.6138535031847133 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 1.18; acc: 0.66
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.29; acc: 0.55
Batch: 100; loss: 1.39; acc: 0.59
Batch: 120; loss: 1.32; acc: 0.55
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 1.22; acc: 0.59
Batch: 200; loss: 1.09; acc: 0.61
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.13; acc: 0.61
Batch: 260; loss: 1.25; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.56
Batch: 300; loss: 1.31; acc: 0.52
Batch: 320; loss: 1.29; acc: 0.47
Batch: 340; loss: 1.49; acc: 0.55
Batch: 360; loss: 1.12; acc: 0.62
Batch: 380; loss: 0.9; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.66
Batch: 420; loss: 1.28; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.52
Batch: 460; loss: 1.19; acc: 0.61
Batch: 480; loss: 1.38; acc: 0.59
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.13; acc: 0.67
Batch: 540; loss: 1.4; acc: 0.62
Batch: 560; loss: 1.26; acc: 0.55
Batch: 580; loss: 1.28; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.62
Batch: 620; loss: 1.44; acc: 0.53
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.15; acc: 0.66
Batch: 700; loss: 1.37; acc: 0.55
Batch: 720; loss: 1.47; acc: 0.53
Batch: 740; loss: 1.37; acc: 0.53
Batch: 760; loss: 1.15; acc: 0.72
Batch: 780; loss: 1.5; acc: 0.47
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1810369146097996; val_accuracy: 0.6148487261146497 

plots/subspace_training/lenet/2020-01-20 16:25:19/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 3331948
elements in E: 3331950
fraction nonzero: 0.9999993997508966
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.32; acc: 0.05
Batch: 40; loss: 2.32; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.31; acc: 0.11
Batch: 160; loss: 2.31; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.03
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.27; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.08
Batch: 420; loss: 2.28; acc: 0.08
Batch: 440; loss: 2.27; acc: 0.14
Batch: 460; loss: 2.28; acc: 0.06
Batch: 480; loss: 2.28; acc: 0.06
Batch: 500; loss: 2.28; acc: 0.05
Batch: 520; loss: 2.25; acc: 0.19
Batch: 540; loss: 2.27; acc: 0.06
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.26; acc: 0.14
Batch: 600; loss: 2.27; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.16
Batch: 640; loss: 2.27; acc: 0.12
Batch: 660; loss: 2.24; acc: 0.2
Batch: 680; loss: 2.25; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.23; acc: 0.2
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.23; acc: 0.25
Batch: 780; loss: 2.21; acc: 0.34
Train Epoch over. train_loss: 2.28; train_accuracy: 0.13 

Batch: 0; loss: 2.21; acc: 0.36
Batch: 20; loss: 2.2; acc: 0.33
Batch: 40; loss: 2.18; acc: 0.45
Batch: 60; loss: 2.2; acc: 0.42
Batch: 80; loss: 2.21; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.33
Batch: 120; loss: 2.21; acc: 0.36
Batch: 140; loss: 2.19; acc: 0.41
Val Epoch over. val_loss: 2.2144410276109245; val_accuracy: 0.31817277070063693 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.21; acc: 0.44
Batch: 20; loss: 2.21; acc: 0.3
Batch: 40; loss: 2.2; acc: 0.27
Batch: 60; loss: 2.18; acc: 0.42
Batch: 80; loss: 2.18; acc: 0.34
Batch: 100; loss: 2.14; acc: 0.48
Batch: 120; loss: 2.16; acc: 0.33
Batch: 140; loss: 2.11; acc: 0.33
Batch: 160; loss: 2.11; acc: 0.39
Batch: 180; loss: 2.1; acc: 0.31
Batch: 200; loss: 1.95; acc: 0.45
Batch: 220; loss: 1.96; acc: 0.42
Batch: 240; loss: 1.81; acc: 0.47
Batch: 260; loss: 1.67; acc: 0.58
Batch: 280; loss: 1.59; acc: 0.53
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.31; acc: 0.56
Batch: 340; loss: 1.47; acc: 0.52
Batch: 360; loss: 1.46; acc: 0.61
Batch: 380; loss: 1.41; acc: 0.59
Batch: 400; loss: 1.26; acc: 0.56
Batch: 420; loss: 1.26; acc: 0.58
Batch: 440; loss: 1.53; acc: 0.42
Batch: 460; loss: 1.07; acc: 0.61
Batch: 480; loss: 1.18; acc: 0.56
Batch: 500; loss: 1.17; acc: 0.64
Batch: 520; loss: 1.05; acc: 0.72
Batch: 540; loss: 0.98; acc: 0.62
Batch: 560; loss: 0.99; acc: 0.61
Batch: 580; loss: 1.11; acc: 0.69
Batch: 600; loss: 1.08; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.53
Batch: 640; loss: 1.29; acc: 0.61
Batch: 660; loss: 1.02; acc: 0.73
Batch: 680; loss: 0.97; acc: 0.61
Batch: 700; loss: 1.32; acc: 0.58
Batch: 720; loss: 1.22; acc: 0.66
Batch: 740; loss: 1.39; acc: 0.61
Batch: 760; loss: 1.22; acc: 0.59
Batch: 780; loss: 0.9; acc: 0.7
Train Epoch over. train_loss: 1.54; train_accuracy: 0.52 

Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.54; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.66
Batch: 100; loss: 1.0; acc: 0.62
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 0.89; acc: 0.64
Val Epoch over. val_loss: 1.254708520546081; val_accuracy: 0.5931528662420382 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.53
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.11; acc: 0.62
Batch: 140; loss: 0.92; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.62
Batch: 180; loss: 1.19; acc: 0.59
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.41; acc: 0.52
Batch: 240; loss: 0.92; acc: 0.69
Batch: 260; loss: 0.98; acc: 0.61
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 0.74; acc: 0.78
Batch: 320; loss: 1.03; acc: 0.55
Batch: 340; loss: 1.16; acc: 0.62
Batch: 360; loss: 1.09; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.62
Batch: 400; loss: 0.93; acc: 0.7
Batch: 420; loss: 1.31; acc: 0.59
Batch: 440; loss: 0.77; acc: 0.7
Batch: 460; loss: 0.91; acc: 0.7
Batch: 480; loss: 1.0; acc: 0.66
Batch: 500; loss: 1.25; acc: 0.64
Batch: 520; loss: 1.33; acc: 0.59
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.71; acc: 0.75
Batch: 580; loss: 1.11; acc: 0.66
Batch: 600; loss: 0.91; acc: 0.66
Batch: 620; loss: 0.84; acc: 0.7
Batch: 640; loss: 0.86; acc: 0.72
Batch: 660; loss: 1.14; acc: 0.61
Batch: 680; loss: 1.22; acc: 0.66
Batch: 700; loss: 1.3; acc: 0.53
Batch: 720; loss: 0.93; acc: 0.75
Batch: 740; loss: 0.93; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.69
Batch: 780; loss: 0.84; acc: 0.73
Train Epoch over. train_loss: 1.04; train_accuracy: 0.66 

Batch: 0; loss: 1.07; acc: 0.64
Batch: 20; loss: 1.31; acc: 0.5
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 1.2; acc: 0.59
Batch: 140; loss: 0.62; acc: 0.77
Val Epoch over. val_loss: 0.9646452335035725; val_accuracy: 0.6768511146496815 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 1.02; acc: 0.64
Batch: 40; loss: 1.27; acc: 0.66
Batch: 60; loss: 0.95; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.61
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.21; acc: 0.59
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 1.08; acc: 0.67
Batch: 200; loss: 1.05; acc: 0.73
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 0.86; acc: 0.72
Batch: 260; loss: 1.04; acc: 0.67
Batch: 280; loss: 1.02; acc: 0.59
Batch: 300; loss: 1.14; acc: 0.64
Batch: 320; loss: 0.96; acc: 0.69
Batch: 340; loss: 0.98; acc: 0.64
Batch: 360; loss: 0.92; acc: 0.7
Batch: 380; loss: 1.12; acc: 0.58
Batch: 400; loss: 0.9; acc: 0.67
Batch: 420; loss: 1.39; acc: 0.56
Batch: 440; loss: 0.9; acc: 0.69
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.84; acc: 0.7
Batch: 500; loss: 1.0; acc: 0.64
Batch: 520; loss: 1.16; acc: 0.61
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.81; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.67
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 1.25; acc: 0.61
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 0.85; acc: 0.7
Batch: 740; loss: 0.93; acc: 0.64
Batch: 760; loss: 1.04; acc: 0.62
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 1.01; train_accuracy: 0.67 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.31; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.69
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 0.8; acc: 0.7
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 0.78; acc: 0.75
Val Epoch over. val_loss: 0.9932671948603005; val_accuracy: 0.6722730891719745 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 1.1; acc: 0.58
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.98; acc: 0.64
Batch: 100; loss: 1.01; acc: 0.66
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 1.01; acc: 0.62
Batch: 160; loss: 1.08; acc: 0.62
Batch: 180; loss: 1.04; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.67
Batch: 220; loss: 1.21; acc: 0.53
Batch: 240; loss: 0.98; acc: 0.66
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 1.12; acc: 0.66
Batch: 300; loss: 1.06; acc: 0.67
Batch: 320; loss: 0.85; acc: 0.7
Batch: 340; loss: 0.95; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.64
Batch: 380; loss: 0.92; acc: 0.73
Batch: 400; loss: 1.02; acc: 0.72
Batch: 420; loss: 0.73; acc: 0.73
Batch: 440; loss: 1.0; acc: 0.66
Batch: 460; loss: 1.29; acc: 0.59
Batch: 480; loss: 0.84; acc: 0.75
Batch: 500; loss: 0.83; acc: 0.73
Batch: 520; loss: 0.88; acc: 0.73
Batch: 540; loss: 0.92; acc: 0.64
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 0.83; acc: 0.69
Batch: 600; loss: 1.22; acc: 0.61
Batch: 620; loss: 0.99; acc: 0.62
Batch: 640; loss: 0.76; acc: 0.8
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.12; acc: 0.69
Batch: 700; loss: 0.93; acc: 0.67
Batch: 720; loss: 0.82; acc: 0.73
Batch: 740; loss: 1.2; acc: 0.66
Batch: 760; loss: 0.78; acc: 0.73
Batch: 780; loss: 0.91; acc: 0.73
Train Epoch over. train_loss: 1.0; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.59
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 0.86; acc: 0.67
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.64
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 1.28; acc: 0.59
Batch: 140; loss: 0.85; acc: 0.73
Val Epoch over. val_loss: 1.059788109390599; val_accuracy: 0.6451035031847133 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 1.0; acc: 0.66
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.62
Batch: 160; loss: 0.96; acc: 0.69
Batch: 180; loss: 1.1; acc: 0.67
Batch: 200; loss: 1.21; acc: 0.56
Batch: 220; loss: 0.91; acc: 0.64
Batch: 240; loss: 1.02; acc: 0.66
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 0.91; acc: 0.66
Batch: 300; loss: 0.92; acc: 0.69
Batch: 320; loss: 1.01; acc: 0.72
Batch: 340; loss: 0.7; acc: 0.8
Batch: 360; loss: 0.9; acc: 0.66
Batch: 380; loss: 0.93; acc: 0.67
Batch: 400; loss: 1.13; acc: 0.59
Batch: 420; loss: 0.73; acc: 0.73
Batch: 440; loss: 0.94; acc: 0.62
Batch: 460; loss: 0.89; acc: 0.69
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 0.93; acc: 0.62
Batch: 520; loss: 1.16; acc: 0.62
Batch: 540; loss: 0.66; acc: 0.69
Batch: 560; loss: 0.97; acc: 0.62
Batch: 580; loss: 0.87; acc: 0.66
Batch: 600; loss: 1.03; acc: 0.67
Batch: 620; loss: 0.86; acc: 0.73
Batch: 640; loss: 1.09; acc: 0.59
Batch: 660; loss: 0.76; acc: 0.73
Batch: 680; loss: 0.96; acc: 0.72
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 1.34; acc: 0.59
Batch: 740; loss: 0.91; acc: 0.69
Batch: 760; loss: 1.57; acc: 0.48
Batch: 780; loss: 1.22; acc: 0.55
Train Epoch over. train_loss: 0.98; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 0.87; acc: 0.69
Batch: 60; loss: 0.96; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 0.77; acc: 0.7
Val Epoch over. val_loss: 0.998788192014026; val_accuracy: 0.6621218152866242 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.72
Batch: 20; loss: 0.9; acc: 0.64
Batch: 40; loss: 1.0; acc: 0.62
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.69
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.61
Batch: 140; loss: 0.99; acc: 0.67
Batch: 160; loss: 0.87; acc: 0.67
Batch: 180; loss: 1.0; acc: 0.62
Batch: 200; loss: 0.95; acc: 0.66
Batch: 220; loss: 0.76; acc: 0.75
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 1.11; acc: 0.59
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.62
Batch: 340; loss: 0.85; acc: 0.66
Batch: 360; loss: 0.97; acc: 0.72
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 0.98; acc: 0.73
Batch: 420; loss: 0.74; acc: 0.73
Batch: 440; loss: 0.89; acc: 0.72
Batch: 460; loss: 1.28; acc: 0.58
Batch: 480; loss: 0.75; acc: 0.72
Batch: 500; loss: 0.87; acc: 0.73
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 1.17; acc: 0.61
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.97; acc: 0.66
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 1.0; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.67
Batch: 680; loss: 0.97; acc: 0.69
Batch: 700; loss: 0.74; acc: 0.75
Batch: 720; loss: 0.94; acc: 0.69
Batch: 740; loss: 1.07; acc: 0.64
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.75
Train Epoch over. train_loss: 0.96; train_accuracy: 0.68 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.72
Batch: 60; loss: 0.7; acc: 0.7
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.62
Batch: 140; loss: 0.66; acc: 0.78
Val Epoch over. val_loss: 0.9115643677817789; val_accuracy: 0.6994426751592356 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 0.8; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 0.95; acc: 0.69
Batch: 160; loss: 0.84; acc: 0.58
Batch: 180; loss: 1.11; acc: 0.59
Batch: 200; loss: 1.0; acc: 0.64
Batch: 220; loss: 0.83; acc: 0.75
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.11; acc: 0.64
Batch: 280; loss: 0.97; acc: 0.67
Batch: 300; loss: 0.95; acc: 0.75
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 1.02; acc: 0.7
Batch: 360; loss: 0.78; acc: 0.7
Batch: 380; loss: 0.78; acc: 0.69
Batch: 400; loss: 0.87; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.75
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 0.98; acc: 0.69
Batch: 520; loss: 0.98; acc: 0.62
Batch: 540; loss: 0.84; acc: 0.75
Batch: 560; loss: 1.08; acc: 0.69
Batch: 580; loss: 1.02; acc: 0.62
Batch: 600; loss: 1.06; acc: 0.59
Batch: 620; loss: 1.12; acc: 0.58
Batch: 640; loss: 1.09; acc: 0.64
Batch: 660; loss: 0.93; acc: 0.66
Batch: 680; loss: 0.71; acc: 0.73
Batch: 700; loss: 0.82; acc: 0.72
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.96; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.69
Train Epoch over. train_loss: 0.96; train_accuracy: 0.68 

Batch: 0; loss: 0.91; acc: 0.67
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 0.84; acc: 0.69
Batch: 60; loss: 0.92; acc: 0.62
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 1.33; acc: 0.56
Batch: 140; loss: 0.78; acc: 0.7
Val Epoch over. val_loss: 1.030306769784089; val_accuracy: 0.6614251592356688 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.83; acc: 0.62
Batch: 60; loss: 1.13; acc: 0.59
Batch: 80; loss: 0.75; acc: 0.73
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.64
Batch: 160; loss: 0.97; acc: 0.67
Batch: 180; loss: 0.89; acc: 0.67
Batch: 200; loss: 0.94; acc: 0.69
Batch: 220; loss: 0.66; acc: 0.72
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.88; acc: 0.64
Batch: 300; loss: 1.01; acc: 0.66
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 1.26; acc: 0.58
Batch: 360; loss: 1.03; acc: 0.62
Batch: 380; loss: 0.81; acc: 0.75
Batch: 400; loss: 1.06; acc: 0.59
Batch: 420; loss: 1.05; acc: 0.7
Batch: 440; loss: 0.97; acc: 0.7
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 0.76; acc: 0.8
Batch: 500; loss: 0.77; acc: 0.69
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.84; acc: 0.75
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.84; acc: 0.73
Batch: 600; loss: 0.91; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.75
Batch: 680; loss: 0.74; acc: 0.7
Batch: 700; loss: 0.78; acc: 0.7
Batch: 720; loss: 0.82; acc: 0.72
Batch: 740; loss: 0.75; acc: 0.69
Batch: 760; loss: 0.87; acc: 0.7
Batch: 780; loss: 1.0; acc: 0.66
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.97; acc: 0.59
Batch: 20; loss: 1.46; acc: 0.56
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.62
Batch: 100; loss: 0.83; acc: 0.67
Batch: 120; loss: 1.08; acc: 0.62
Batch: 140; loss: 0.77; acc: 0.66
Val Epoch over. val_loss: 1.049511999840949; val_accuracy: 0.652468152866242 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.69
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 0.95; acc: 0.62
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.03; acc: 0.64
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.95; acc: 0.64
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 1.09; acc: 0.67
Batch: 220; loss: 0.91; acc: 0.66
Batch: 240; loss: 0.95; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.77
Batch: 280; loss: 0.88; acc: 0.72
Batch: 300; loss: 0.8; acc: 0.7
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.83; acc: 0.7
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.87; acc: 0.66
Batch: 420; loss: 0.58; acc: 0.78
Batch: 440; loss: 0.74; acc: 0.72
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.88; acc: 0.7
Batch: 500; loss: 0.95; acc: 0.7
Batch: 520; loss: 0.94; acc: 0.7
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 0.88; acc: 0.78
Batch: 580; loss: 0.77; acc: 0.75
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 1.0; acc: 0.67
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.88; acc: 0.61
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.82; acc: 0.7
Batch: 740; loss: 0.94; acc: 0.67
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.81; acc: 0.75
Train Epoch over. train_loss: 0.91; train_accuracy: 0.7 

Batch: 0; loss: 0.94; acc: 0.59
Batch: 20; loss: 1.24; acc: 0.53
Batch: 40; loss: 0.78; acc: 0.72
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.81; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 0.61; acc: 0.77
Val Epoch over. val_loss: 0.931561572156894; val_accuracy: 0.691281847133758 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.03; acc: 0.59
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.96; acc: 0.72
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.8; acc: 0.72
Batch: 200; loss: 0.73; acc: 0.72
Batch: 220; loss: 0.95; acc: 0.7
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 0.76; acc: 0.73
Batch: 280; loss: 0.67; acc: 0.75
Batch: 300; loss: 0.94; acc: 0.67
Batch: 320; loss: 0.87; acc: 0.66
Batch: 340; loss: 1.29; acc: 0.56
Batch: 360; loss: 1.03; acc: 0.67
Batch: 380; loss: 0.71; acc: 0.73
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.67
Batch: 440; loss: 0.82; acc: 0.7
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 0.97; acc: 0.72
Batch: 500; loss: 0.94; acc: 0.7
Batch: 520; loss: 0.66; acc: 0.73
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.96; acc: 0.69
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 0.82; acc: 0.78
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 1.09; acc: 0.59
Batch: 680; loss: 0.83; acc: 0.77
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.96; acc: 0.62
Batch: 780; loss: 0.78; acc: 0.75
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.83
Val Epoch over. val_loss: 0.8138917265044656; val_accuracy: 0.7380573248407644 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.97; acc: 0.69
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.64
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.99; acc: 0.67
Batch: 200; loss: 0.7; acc: 0.72
Batch: 220; loss: 0.87; acc: 0.62
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 0.81; acc: 0.7
Batch: 280; loss: 0.77; acc: 0.78
Batch: 300; loss: 0.95; acc: 0.66
Batch: 320; loss: 0.91; acc: 0.67
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.93; acc: 0.72
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.92; acc: 0.75
Batch: 460; loss: 0.84; acc: 0.7
Batch: 480; loss: 0.78; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.69
Batch: 520; loss: 0.96; acc: 0.67
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 1.01; acc: 0.73
Batch: 600; loss: 0.92; acc: 0.73
Batch: 620; loss: 0.76; acc: 0.73
Batch: 640; loss: 1.03; acc: 0.69
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.56; acc: 0.75
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.93; acc: 0.72
Batch: 780; loss: 0.93; acc: 0.72
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.8357920194887052; val_accuracy: 0.7289012738853503 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.66
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 0.66; acc: 0.72
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 0.82; acc: 0.7
Batch: 180; loss: 0.87; acc: 0.7
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.83; acc: 0.73
Batch: 240; loss: 1.09; acc: 0.7
Batch: 260; loss: 0.87; acc: 0.73
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 1.12; acc: 0.7
Batch: 380; loss: 1.01; acc: 0.64
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 0.88; acc: 0.73
Batch: 440; loss: 0.98; acc: 0.7
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.91; acc: 0.75
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.73
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 0.9; acc: 0.69
Batch: 640; loss: 0.86; acc: 0.7
Batch: 660; loss: 0.87; acc: 0.64
Batch: 680; loss: 1.03; acc: 0.72
Batch: 700; loss: 0.97; acc: 0.67
Batch: 720; loss: 0.9; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 0.73; acc: 0.73
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.91; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.66
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.81
Val Epoch over. val_loss: 0.8505134293987493; val_accuracy: 0.7256170382165605 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.79; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.72
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.7; acc: 0.75
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 1.06; acc: 0.7
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 1.11; acc: 0.61
Batch: 280; loss: 0.7; acc: 0.69
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.91; acc: 0.69
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.77
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.9; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.7
Batch: 500; loss: 0.72; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.66
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.73; acc: 0.7
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 1.04; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.66
Batch: 660; loss: 0.74; acc: 0.75
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.78; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.69
Batch: 740; loss: 1.28; acc: 0.62
Batch: 760; loss: 1.04; acc: 0.64
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.91; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.56
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.65; acc: 0.81
Val Epoch over. val_loss: 0.8203539561693836; val_accuracy: 0.7365644904458599 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.43; acc: 0.67
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.69
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 1.15; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.66
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.9; acc: 0.73
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.75
Batch: 200; loss: 0.81; acc: 0.78
Batch: 220; loss: 0.93; acc: 0.69
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 1.01; acc: 0.67
Batch: 280; loss: 0.86; acc: 0.72
Batch: 300; loss: 0.87; acc: 0.69
Batch: 320; loss: 0.72; acc: 0.7
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.85; acc: 0.73
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 1.15; acc: 0.69
Batch: 460; loss: 1.04; acc: 0.7
Batch: 480; loss: 0.76; acc: 0.81
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.93; acc: 0.66
Batch: 560; loss: 0.98; acc: 0.66
Batch: 580; loss: 0.95; acc: 0.73
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.91; acc: 0.72
Batch: 640; loss: 0.84; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 1.18; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 0.92; acc: 0.69
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.92; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.69; acc: 0.72
Batch: 60; loss: 0.83; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.64; acc: 0.78
Val Epoch over. val_loss: 0.8365322347659214; val_accuracy: 0.7238256369426752 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.98; acc: 0.7
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.97; acc: 0.7
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 0.86; acc: 0.7
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.92; acc: 0.72
Batch: 280; loss: 0.74; acc: 0.73
Batch: 300; loss: 0.82; acc: 0.72
Batch: 320; loss: 0.92; acc: 0.77
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 1.45; acc: 0.58
Batch: 380; loss: 0.95; acc: 0.69
Batch: 400; loss: 0.84; acc: 0.72
Batch: 420; loss: 0.85; acc: 0.75
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 1.07; acc: 0.61
Batch: 480; loss: 1.25; acc: 0.62
Batch: 500; loss: 0.87; acc: 0.67
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 0.84; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.1; acc: 0.67
Batch: 620; loss: 0.92; acc: 0.7
Batch: 640; loss: 1.12; acc: 0.59
Batch: 660; loss: 0.78; acc: 0.72
Batch: 680; loss: 0.92; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.77
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.89; acc: 0.67
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.15; acc: 0.59
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 0.62; acc: 0.83
Val Epoch over. val_loss: 0.8302008418520544; val_accuracy: 0.7311902866242038 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.8; acc: 0.67
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 0.89; acc: 0.73
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.96; acc: 0.7
Batch: 160; loss: 1.0; acc: 0.7
Batch: 180; loss: 0.91; acc: 0.69
Batch: 200; loss: 0.94; acc: 0.66
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.74; acc: 0.7
Batch: 260; loss: 1.06; acc: 0.62
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 1.0; acc: 0.72
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.89; acc: 0.67
Batch: 360; loss: 0.95; acc: 0.7
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 0.93; acc: 0.66
Batch: 420; loss: 1.01; acc: 0.64
Batch: 440; loss: 0.97; acc: 0.69
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.56
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.75
Batch: 540; loss: 0.62; acc: 0.77
Batch: 560; loss: 0.81; acc: 0.78
Batch: 580; loss: 1.16; acc: 0.61
Batch: 600; loss: 0.68; acc: 0.75
Batch: 620; loss: 1.07; acc: 0.56
Batch: 640; loss: 0.92; acc: 0.72
Batch: 660; loss: 1.01; acc: 0.67
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 1.58; acc: 0.61
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.87; acc: 0.66
Batch: 760; loss: 0.79; acc: 0.7
Batch: 780; loss: 0.91; acc: 0.7
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.9; acc: 0.61
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.66; acc: 0.8
Val Epoch over. val_loss: 0.8375848850626855; val_accuracy: 0.7272093949044586 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.91; acc: 0.67
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 1.05; acc: 0.66
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.64
Batch: 160; loss: 0.8; acc: 0.7
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 0.84; acc: 0.78
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 1.11; acc: 0.67
Batch: 320; loss: 0.97; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 0.93; acc: 0.66
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.72
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.78; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.93; acc: 0.7
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.91; acc: 0.73
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 1.22; acc: 0.64
Batch: 640; loss: 0.77; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.77
Batch: 700; loss: 0.98; acc: 0.73
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.84; acc: 0.72
Batch: 760; loss: 0.73; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.64
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.58
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.83
Val Epoch over. val_loss: 0.8166107510685161; val_accuracy: 0.7392515923566879 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.99; acc: 0.59
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.72
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.97; acc: 0.7
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 1.01; acc: 0.75
Batch: 360; loss: 0.79; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.69
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.91; acc: 0.7
Batch: 440; loss: 0.82; acc: 0.7
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.61
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.77; acc: 0.7
Batch: 560; loss: 1.02; acc: 0.73
Batch: 580; loss: 0.8; acc: 0.75
Batch: 600; loss: 0.75; acc: 0.67
Batch: 620; loss: 0.84; acc: 0.75
Batch: 640; loss: 0.87; acc: 0.64
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.83; acc: 0.75
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 1.03; acc: 0.72
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.63; acc: 0.73
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.81
Val Epoch over. val_loss: 0.8067591161864578; val_accuracy: 0.7405453821656051 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.83; acc: 0.64
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.62
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.69
Batch: 160; loss: 0.8; acc: 0.73
Batch: 180; loss: 0.81; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.69
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.91; acc: 0.69
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.96; acc: 0.72
Batch: 320; loss: 0.8; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.86; acc: 0.7
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 1.14; acc: 0.75
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 1.4; acc: 0.64
Batch: 460; loss: 0.96; acc: 0.66
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.9; acc: 0.67
Batch: 520; loss: 0.9; acc: 0.73
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.72
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.66
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 1.01; acc: 0.67
Batch: 700; loss: 1.09; acc: 0.64
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.97; acc: 0.61
Batch: 760; loss: 0.88; acc: 0.66
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.66; acc: 0.72
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.8
Val Epoch over. val_loss: 0.8227234053763615; val_accuracy: 0.734375 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 1.15; acc: 0.61
Batch: 60; loss: 0.84; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.58
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.73; acc: 0.7
Batch: 160; loss: 0.55; acc: 0.8
Batch: 180; loss: 0.94; acc: 0.7
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 0.86; acc: 0.72
Batch: 260; loss: 0.73; acc: 0.77
Batch: 280; loss: 0.9; acc: 0.72
Batch: 300; loss: 0.8; acc: 0.72
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.99; acc: 0.66
Batch: 400; loss: 1.01; acc: 0.66
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.88; acc: 0.72
Batch: 460; loss: 0.9; acc: 0.66
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 0.85; acc: 0.7
Batch: 540; loss: 0.71; acc: 0.77
Batch: 560; loss: 0.89; acc: 0.77
Batch: 580; loss: 0.92; acc: 0.7
Batch: 600; loss: 0.81; acc: 0.69
Batch: 620; loss: 0.82; acc: 0.78
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.93; acc: 0.78
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 0.94; acc: 0.66
Batch: 720; loss: 0.86; acc: 0.72
Batch: 740; loss: 0.93; acc: 0.66
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.91; acc: 0.72
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8011482287744048; val_accuracy: 0.7394506369426752 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.89; acc: 0.67
Batch: 60; loss: 1.04; acc: 0.64
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 0.88; acc: 0.69
Batch: 140; loss: 0.85; acc: 0.67
Batch: 160; loss: 0.93; acc: 0.7
Batch: 180; loss: 0.88; acc: 0.75
Batch: 200; loss: 0.76; acc: 0.72
Batch: 220; loss: 0.88; acc: 0.72
Batch: 240; loss: 1.13; acc: 0.59
Batch: 260; loss: 0.86; acc: 0.75
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 1.25; acc: 0.66
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 0.78; acc: 0.77
Batch: 360; loss: 0.78; acc: 0.72
Batch: 380; loss: 0.92; acc: 0.66
Batch: 400; loss: 0.8; acc: 0.72
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 1.01; acc: 0.72
Batch: 500; loss: 0.73; acc: 0.77
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 1.03; acc: 0.66
Batch: 560; loss: 0.62; acc: 0.75
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 1.27; acc: 0.67
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 1.07; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.59
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.8
Val Epoch over. val_loss: 0.8152576633699381; val_accuracy: 0.7352707006369427 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.84; acc: 0.69
Batch: 40; loss: 1.23; acc: 0.61
Batch: 60; loss: 0.97; acc: 0.67
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 0.86; acc: 0.64
Batch: 140; loss: 0.7; acc: 0.73
Batch: 160; loss: 0.95; acc: 0.7
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.77; acc: 0.73
Batch: 220; loss: 0.87; acc: 0.75
Batch: 240; loss: 0.85; acc: 0.64
Batch: 260; loss: 1.05; acc: 0.67
Batch: 280; loss: 0.81; acc: 0.7
Batch: 300; loss: 0.6; acc: 0.78
Batch: 320; loss: 0.93; acc: 0.73
Batch: 340; loss: 0.79; acc: 0.73
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.75
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.78
Batch: 460; loss: 0.93; acc: 0.69
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.61
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.88; acc: 0.72
Batch: 560; loss: 0.74; acc: 0.75
Batch: 580; loss: 1.05; acc: 0.64
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 0.89; acc: 0.7
Batch: 640; loss: 0.83; acc: 0.67
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.99; acc: 0.73
Batch: 700; loss: 0.75; acc: 0.67
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 0.81; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.95; acc: 0.7
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.69; acc: 0.73
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.78
Val Epoch over. val_loss: 0.8110279211193133; val_accuracy: 0.736265923566879 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 1.08; acc: 0.66
Batch: 40; loss: 0.7; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 0.83; acc: 0.7
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.85; acc: 0.7
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 1.23; acc: 0.52
Batch: 240; loss: 0.85; acc: 0.72
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.94; acc: 0.75
Batch: 300; loss: 1.09; acc: 0.62
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 0.77; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.77
Batch: 380; loss: 0.92; acc: 0.69
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.8; acc: 0.69
Batch: 460; loss: 0.92; acc: 0.73
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.63; acc: 0.77
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.96; acc: 0.7
Batch: 580; loss: 0.9; acc: 0.73
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.73
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.96; acc: 0.66
Batch: 680; loss: 0.8; acc: 0.72
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 1.02; acc: 0.66
Batch: 760; loss: 1.42; acc: 0.61
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.62
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.72; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.83
Val Epoch over. val_loss: 0.8266849438096308; val_accuracy: 0.7321855095541401 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.86; acc: 0.67
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.75
Batch: 160; loss: 0.77; acc: 0.83
Batch: 180; loss: 0.86; acc: 0.69
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 1.04; acc: 0.66
Batch: 240; loss: 0.84; acc: 0.78
Batch: 260; loss: 0.97; acc: 0.66
Batch: 280; loss: 1.0; acc: 0.66
Batch: 300; loss: 0.69; acc: 0.78
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 0.87; acc: 0.66
Batch: 360; loss: 0.92; acc: 0.69
Batch: 380; loss: 0.74; acc: 0.75
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.67
Batch: 460; loss: 0.88; acc: 0.72
Batch: 480; loss: 1.03; acc: 0.61
Batch: 500; loss: 0.82; acc: 0.69
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 0.84; acc: 0.73
Batch: 560; loss: 0.88; acc: 0.66
Batch: 580; loss: 0.62; acc: 0.77
Batch: 600; loss: 1.03; acc: 0.66
Batch: 620; loss: 0.84; acc: 0.72
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.89; acc: 0.69
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.88; acc: 0.66
Batch: 740; loss: 0.88; acc: 0.7
Batch: 760; loss: 1.18; acc: 0.66
Batch: 780; loss: 0.91; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.8038994026411871; val_accuracy: 0.7383558917197452 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.7
Batch: 80; loss: 0.66; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 1.2; acc: 0.69
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.93; acc: 0.67
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 0.9; acc: 0.78
Batch: 240; loss: 1.04; acc: 0.67
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.9; acc: 0.7
Batch: 300; loss: 0.97; acc: 0.77
Batch: 320; loss: 0.92; acc: 0.72
Batch: 340; loss: 0.84; acc: 0.69
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.75
Batch: 400; loss: 1.07; acc: 0.62
Batch: 420; loss: 1.06; acc: 0.66
Batch: 440; loss: 0.74; acc: 0.75
Batch: 460; loss: 0.74; acc: 0.73
Batch: 480; loss: 0.88; acc: 0.7
Batch: 500; loss: 1.02; acc: 0.72
Batch: 520; loss: 0.98; acc: 0.69
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.73
Batch: 600; loss: 0.92; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.69
Batch: 640; loss: 0.76; acc: 0.7
Batch: 660; loss: 0.73; acc: 0.7
Batch: 680; loss: 0.82; acc: 0.69
Batch: 700; loss: 0.8; acc: 0.7
Batch: 720; loss: 0.87; acc: 0.72
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.99; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.7
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.89; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.8041971785247706; val_accuracy: 0.7418391719745223 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 1.14; acc: 0.59
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.73
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 1.01; acc: 0.67
Batch: 200; loss: 1.13; acc: 0.64
Batch: 220; loss: 0.9; acc: 0.67
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.95; acc: 0.7
Batch: 280; loss: 1.04; acc: 0.7
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.87; acc: 0.72
Batch: 360; loss: 0.8; acc: 0.72
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 0.8; acc: 0.75
Batch: 420; loss: 1.05; acc: 0.72
Batch: 440; loss: 0.75; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.67
Batch: 480; loss: 0.55; acc: 0.77
Batch: 500; loss: 0.95; acc: 0.66
Batch: 520; loss: 0.96; acc: 0.72
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 1.14; acc: 0.55
Batch: 600; loss: 0.77; acc: 0.72
Batch: 620; loss: 0.83; acc: 0.73
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 1.03; acc: 0.64
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.8
Val Epoch over. val_loss: 0.8014698281029987; val_accuracy: 0.7419386942675159 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.98; acc: 0.7
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 1.06; acc: 0.62
Batch: 60; loss: 0.92; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.75
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.67
Batch: 240; loss: 0.92; acc: 0.67
Batch: 260; loss: 0.81; acc: 0.73
Batch: 280; loss: 0.77; acc: 0.77
Batch: 300; loss: 0.9; acc: 0.72
Batch: 320; loss: 1.24; acc: 0.61
Batch: 340; loss: 0.87; acc: 0.64
Batch: 360; loss: 0.9; acc: 0.7
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.59; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 1.03; acc: 0.66
Batch: 480; loss: 0.88; acc: 0.69
Batch: 500; loss: 0.84; acc: 0.73
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.66; acc: 0.73
Batch: 580; loss: 1.25; acc: 0.61
Batch: 600; loss: 0.75; acc: 0.75
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.66; acc: 0.75
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 1.23; acc: 0.56
Batch: 760; loss: 0.85; acc: 0.7
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.81
Val Epoch over. val_loss: 0.8118931121507268; val_accuracy: 0.7399482484076433 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.9; acc: 0.7
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.72
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 1.06; acc: 0.61
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 0.94; acc: 0.7
Batch: 320; loss: 0.79; acc: 0.72
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.83; acc: 0.61
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.82; acc: 0.7
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 1.1; acc: 0.64
Batch: 500; loss: 0.64; acc: 0.72
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.79; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.67
Batch: 600; loss: 0.95; acc: 0.66
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 0.89; acc: 0.69
Batch: 660; loss: 1.02; acc: 0.69
Batch: 680; loss: 0.96; acc: 0.69
Batch: 700; loss: 0.73; acc: 0.69
Batch: 720; loss: 0.91; acc: 0.77
Batch: 740; loss: 0.7; acc: 0.75
Batch: 760; loss: 0.8; acc: 0.72
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.803988006654059; val_accuracy: 0.7393511146496815 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.86; acc: 0.66
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 1.02; acc: 0.62
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.7
Batch: 300; loss: 0.79; acc: 0.72
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 1.15; acc: 0.61
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 1.11; acc: 0.66
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.76; acc: 0.66
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 0.91; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.72; acc: 0.72
Batch: 560; loss: 1.05; acc: 0.69
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 1.14; acc: 0.73
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.72
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.73
Batch: 740; loss: 0.9; acc: 0.7
Batch: 760; loss: 0.87; acc: 0.67
Batch: 780; loss: 0.85; acc: 0.69
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.61
Batch: 40; loss: 0.68; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.8028923914690685; val_accuracy: 0.7407444267515924 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.88; acc: 0.72
Batch: 20; loss: 1.05; acc: 0.66
Batch: 40; loss: 0.98; acc: 0.67
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 0.88; acc: 0.73
Batch: 160; loss: 0.86; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 1.0; acc: 0.69
Batch: 220; loss: 0.83; acc: 0.75
Batch: 240; loss: 0.89; acc: 0.73
Batch: 260; loss: 1.02; acc: 0.73
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 1.04; acc: 0.64
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.67
Batch: 380; loss: 0.96; acc: 0.69
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.67; acc: 0.73
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.96; acc: 0.61
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.83; acc: 0.73
Batch: 540; loss: 1.06; acc: 0.59
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.79; acc: 0.67
Batch: 600; loss: 0.8; acc: 0.7
Batch: 620; loss: 0.98; acc: 0.73
Batch: 640; loss: 0.84; acc: 0.72
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 0.92; acc: 0.66
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 0.82; acc: 0.7
Batch: 740; loss: 1.02; acc: 0.61
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.77
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.7993657793968346; val_accuracy: 0.7410429936305732 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 0.96; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.83; acc: 0.77
Batch: 160; loss: 0.74; acc: 0.75
Batch: 180; loss: 0.91; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.78; acc: 0.69
Batch: 280; loss: 0.99; acc: 0.64
Batch: 300; loss: 0.81; acc: 0.75
Batch: 320; loss: 0.81; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.81; acc: 0.72
Batch: 400; loss: 1.0; acc: 0.72
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.97; acc: 0.66
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.84; acc: 0.81
Batch: 500; loss: 0.89; acc: 0.7
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 1.01; acc: 0.67
Batch: 580; loss: 0.85; acc: 0.77
Batch: 600; loss: 0.68; acc: 0.75
Batch: 620; loss: 0.61; acc: 0.73
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.7
Batch: 680; loss: 0.83; acc: 0.7
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.67
Batch: 740; loss: 1.03; acc: 0.73
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8010948442729415; val_accuracy: 0.7415406050955414 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.01; acc: 0.62
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 1.15; acc: 0.69
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.64
Batch: 220; loss: 0.64; acc: 0.72
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 1.01; acc: 0.66
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 0.91; acc: 0.7
Batch: 360; loss: 0.85; acc: 0.73
Batch: 380; loss: 1.01; acc: 0.72
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 0.78; acc: 0.69
Batch: 440; loss: 0.94; acc: 0.7
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.93; acc: 0.7
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 0.96; acc: 0.73
Batch: 540; loss: 0.83; acc: 0.7
Batch: 560; loss: 0.75; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 0.87; acc: 0.67
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 1.2; acc: 0.73
Batch: 660; loss: 0.89; acc: 0.69
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.85; acc: 0.67
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 0.97; acc: 0.73
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.83
Val Epoch over. val_loss: 0.8001846449010691; val_accuracy: 0.743531050955414 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.9; acc: 0.72
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.53
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.73
Batch: 160; loss: 0.96; acc: 0.67
Batch: 180; loss: 1.0; acc: 0.75
Batch: 200; loss: 0.9; acc: 0.72
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.82; acc: 0.77
Batch: 260; loss: 0.61; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.82; acc: 0.78
Batch: 340; loss: 0.92; acc: 0.66
Batch: 360; loss: 0.89; acc: 0.69
Batch: 380; loss: 0.96; acc: 0.67
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.72
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.98; acc: 0.69
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 0.78; acc: 0.66
Batch: 560; loss: 0.91; acc: 0.7
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.93; acc: 0.72
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 1.05; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.78; acc: 0.73
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.8053674737738955; val_accuracy: 0.7403463375796179 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.89; acc: 0.64
Batch: 40; loss: 0.75; acc: 0.69
Batch: 60; loss: 1.12; acc: 0.53
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.77
Batch: 160; loss: 0.92; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.85; acc: 0.73
Batch: 220; loss: 0.95; acc: 0.62
Batch: 240; loss: 1.03; acc: 0.66
Batch: 260; loss: 0.7; acc: 0.72
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.84; acc: 0.7
Batch: 320; loss: 0.76; acc: 0.77
Batch: 340; loss: 0.76; acc: 0.72
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 1.07; acc: 0.67
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.87; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.75
Batch: 480; loss: 0.88; acc: 0.66
Batch: 500; loss: 0.95; acc: 0.69
Batch: 520; loss: 0.93; acc: 0.72
Batch: 540; loss: 0.68; acc: 0.75
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.81; acc: 0.72
Batch: 620; loss: 1.08; acc: 0.67
Batch: 640; loss: 1.04; acc: 0.59
Batch: 660; loss: 0.92; acc: 0.66
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 0.81; acc: 0.78
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.96; acc: 0.72
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.8000313512458923; val_accuracy: 0.7414410828025477 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.64
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.73; acc: 0.73
Batch: 220; loss: 0.81; acc: 0.66
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.87; acc: 0.73
Batch: 340; loss: 0.81; acc: 0.73
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.92; acc: 0.67
Batch: 400; loss: 0.87; acc: 0.77
Batch: 420; loss: 0.99; acc: 0.7
Batch: 440; loss: 0.91; acc: 0.64
Batch: 460; loss: 0.84; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.61
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.85; acc: 0.69
Batch: 540; loss: 1.05; acc: 0.62
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.64
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.7
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 0.82; acc: 0.77
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.75
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.8011715125506091; val_accuracy: 0.7412420382165605 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.06; acc: 0.62
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 1.07; acc: 0.66
Batch: 160; loss: 0.77; acc: 0.75
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 0.65; acc: 0.75
Batch: 260; loss: 0.83; acc: 0.66
Batch: 280; loss: 0.78; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.92; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.61
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.79; acc: 0.73
Batch: 460; loss: 0.75; acc: 0.72
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.73
Batch: 520; loss: 0.73; acc: 0.73
Batch: 540; loss: 1.08; acc: 0.67
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 0.96; acc: 0.64
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 1.05; acc: 0.62
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 1.02; acc: 0.69
Batch: 740; loss: 0.99; acc: 0.7
Batch: 760; loss: 1.0; acc: 0.62
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8020347320729759; val_accuracy: 0.7395501592356688 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 1.05; acc: 0.62
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 1.03; acc: 0.66
Batch: 160; loss: 0.88; acc: 0.73
Batch: 180; loss: 1.02; acc: 0.7
Batch: 200; loss: 0.71; acc: 0.73
Batch: 220; loss: 0.87; acc: 0.73
Batch: 240; loss: 1.3; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.61
Batch: 300; loss: 0.92; acc: 0.67
Batch: 320; loss: 0.86; acc: 0.62
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 1.05; acc: 0.75
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 1.04; acc: 0.66
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 0.93; acc: 0.73
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 1.03; acc: 0.67
Batch: 600; loss: 0.96; acc: 0.7
Batch: 620; loss: 1.32; acc: 0.62
Batch: 640; loss: 0.83; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.93; acc: 0.66
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 1.03; acc: 0.66
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.64
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.802147711917853; val_accuracy: 0.7413415605095541 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 1.07; acc: 0.64
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 0.84; acc: 0.7
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 1.2; acc: 0.53
Batch: 200; loss: 0.89; acc: 0.72
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 1.08; acc: 0.72
Batch: 280; loss: 1.0; acc: 0.69
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.67; acc: 0.73
Batch: 340; loss: 1.08; acc: 0.64
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.96; acc: 0.66
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.78; acc: 0.75
Batch: 440; loss: 1.08; acc: 0.59
Batch: 460; loss: 0.81; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.64
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.77; acc: 0.7
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.79; acc: 0.75
Batch: 580; loss: 0.9; acc: 0.72
Batch: 600; loss: 1.0; acc: 0.73
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 0.86; acc: 0.73
Batch: 700; loss: 0.82; acc: 0.75
Batch: 720; loss: 0.8; acc: 0.69
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.9; acc: 0.69
Batch: 780; loss: 0.79; acc: 0.69
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.8
Val Epoch over. val_loss: 0.8034753696933673; val_accuracy: 0.7407444267515924 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 0.79; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.62
Batch: 80; loss: 0.89; acc: 0.72
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 1.06; acc: 0.66
Batch: 160; loss: 0.92; acc: 0.73
Batch: 180; loss: 0.88; acc: 0.73
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.96; acc: 0.67
Batch: 240; loss: 1.27; acc: 0.64
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.75
Batch: 360; loss: 0.95; acc: 0.64
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.73
Batch: 420; loss: 0.96; acc: 0.66
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 0.86; acc: 0.69
Batch: 480; loss: 1.04; acc: 0.73
Batch: 500; loss: 1.1; acc: 0.62
Batch: 520; loss: 0.99; acc: 0.67
Batch: 540; loss: 0.8; acc: 0.72
Batch: 560; loss: 0.64; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.69
Batch: 600; loss: 0.99; acc: 0.64
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.98; acc: 0.7
Batch: 680; loss: 0.8; acc: 0.69
Batch: 700; loss: 0.88; acc: 0.72
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 1.03; acc: 0.69
Batch: 760; loss: 0.92; acc: 0.72
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.66
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.67; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.8030004674082349; val_accuracy: 0.7387539808917197 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.7
Batch: 100; loss: 0.9; acc: 0.69
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 1.12; acc: 0.69
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.7
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 1.12; acc: 0.62
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.91; acc: 0.64
Batch: 360; loss: 0.84; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.67
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.73
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 1.19; acc: 0.61
Batch: 480; loss: 0.84; acc: 0.75
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.8; acc: 0.7
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 0.78; acc: 0.7
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.75; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.72
Batch: 780; loss: 0.93; acc: 0.73
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.83
Val Epoch over. val_loss: 0.7990142613839192; val_accuracy: 0.743531050955414 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.59
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.69
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.7
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.72
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 1.14; acc: 0.66
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 0.72; acc: 0.8
Batch: 360; loss: 0.89; acc: 0.72
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.79; acc: 0.72
Batch: 420; loss: 0.74; acc: 0.73
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 1.01; acc: 0.69
Batch: 500; loss: 1.06; acc: 0.72
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.88; acc: 0.66
Batch: 560; loss: 0.82; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.61
Batch: 600; loss: 0.78; acc: 0.73
Batch: 620; loss: 0.85; acc: 0.73
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.97; acc: 0.7
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.86; acc: 0.77
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 0.72; acc: 0.73
Batch: 780; loss: 0.74; acc: 0.83
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.800975849104535; val_accuracy: 0.7423367834394905 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.9; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.67
Batch: 120; loss: 0.89; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.69
Batch: 220; loss: 0.9; acc: 0.77
Batch: 240; loss: 0.97; acc: 0.66
Batch: 260; loss: 1.11; acc: 0.64
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.88; acc: 0.73
Batch: 400; loss: 0.88; acc: 0.73
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.99; acc: 0.67
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 1.06; acc: 0.67
Batch: 540; loss: 1.0; acc: 0.7
Batch: 560; loss: 0.75; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 1.05; acc: 0.66
Batch: 640; loss: 0.85; acc: 0.66
Batch: 660; loss: 0.77; acc: 0.73
Batch: 680; loss: 0.91; acc: 0.7
Batch: 700; loss: 0.6; acc: 0.75
Batch: 720; loss: 0.84; acc: 0.73
Batch: 740; loss: 0.88; acc: 0.73
Batch: 760; loss: 0.77; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.7990708009452577; val_accuracy: 0.7429339171974523 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.09; acc: 0.59
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 0.79; acc: 0.69
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.87; acc: 0.66
Batch: 180; loss: 0.91; acc: 0.66
Batch: 200; loss: 0.84; acc: 0.73
Batch: 220; loss: 1.01; acc: 0.69
Batch: 240; loss: 1.03; acc: 0.64
Batch: 260; loss: 0.92; acc: 0.69
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.92; acc: 0.67
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 0.72; acc: 0.69
Batch: 400; loss: 0.79; acc: 0.7
Batch: 420; loss: 1.08; acc: 0.62
Batch: 440; loss: 0.83; acc: 0.67
Batch: 460; loss: 0.73; acc: 0.73
Batch: 480; loss: 1.09; acc: 0.61
Batch: 500; loss: 0.82; acc: 0.67
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.99; acc: 0.62
Batch: 600; loss: 0.98; acc: 0.7
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 0.93; acc: 0.7
Batch: 720; loss: 0.92; acc: 0.67
Batch: 740; loss: 0.85; acc: 0.72
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.799634495548382; val_accuracy: 0.7406449044585988 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.93; acc: 0.69
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.61
Batch: 100; loss: 0.84; acc: 0.69
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.94; acc: 0.67
Batch: 180; loss: 0.85; acc: 0.72
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.62; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.94; acc: 0.7
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 0.91; acc: 0.75
Batch: 360; loss: 0.76; acc: 0.73
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 0.64; acc: 0.75
Batch: 440; loss: 1.02; acc: 0.72
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.73
Batch: 560; loss: 0.89; acc: 0.66
Batch: 580; loss: 0.87; acc: 0.69
Batch: 600; loss: 0.65; acc: 0.78
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 0.98; acc: 0.72
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.81
Val Epoch over. val_loss: 0.7983665158794184; val_accuracy: 0.7429339171974523 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.82; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.67
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 1.04; acc: 0.67
Batch: 180; loss: 0.96; acc: 0.66
Batch: 200; loss: 0.85; acc: 0.67
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 1.13; acc: 0.67
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 1.08; acc: 0.61
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 1.2; acc: 0.67
Batch: 480; loss: 0.75; acc: 0.72
Batch: 500; loss: 0.89; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.97; acc: 0.69
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.92; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.56; acc: 0.78
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.83; acc: 0.69
Batch: 720; loss: 0.77; acc: 0.72
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.79; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.77
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.8003049219489857; val_accuracy: 0.7419386942675159 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.7
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.59
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.72
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.61
Batch: 200; loss: 0.86; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.9; acc: 0.69
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 1.1; acc: 0.64
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.73
Batch: 440; loss: 1.16; acc: 0.64
Batch: 460; loss: 0.91; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.64
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.94; acc: 0.72
Batch: 600; loss: 0.88; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 1.03; acc: 0.69
Batch: 660; loss: 0.88; acc: 0.72
Batch: 680; loss: 0.85; acc: 0.69
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.9; acc: 0.75
Batch: 740; loss: 0.79; acc: 0.7
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.88; acc: 0.75
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.7994098340629772; val_accuracy: 0.742734872611465 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.08; acc: 0.64
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.79; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.73
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.94; acc: 0.73
Batch: 220; loss: 1.06; acc: 0.69
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 0.63; acc: 0.77
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.83; acc: 0.69
Batch: 360; loss: 1.23; acc: 0.61
Batch: 380; loss: 0.99; acc: 0.69
Batch: 400; loss: 0.86; acc: 0.78
Batch: 420; loss: 0.8; acc: 0.73
Batch: 440; loss: 0.84; acc: 0.77
Batch: 460; loss: 0.84; acc: 0.73
Batch: 480; loss: 0.92; acc: 0.69
Batch: 500; loss: 1.06; acc: 0.67
Batch: 520; loss: 0.85; acc: 0.69
Batch: 540; loss: 0.85; acc: 0.72
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.97; acc: 0.67
Batch: 620; loss: 0.95; acc: 0.67
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.91; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.67
Batch: 700; loss: 0.84; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.97; acc: 0.7
Batch: 780; loss: 0.82; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.7995635810171723; val_accuracy: 0.7407444267515924 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.68; acc: 0.72
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.87; acc: 0.75
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 0.96; acc: 0.73
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 1.0; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.89; acc: 0.69
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.94; acc: 0.73
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 0.87; acc: 0.7
Batch: 540; loss: 0.85; acc: 0.75
Batch: 560; loss: 0.93; acc: 0.7
Batch: 580; loss: 0.75; acc: 0.72
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 1.11; acc: 0.58
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.7; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 0.85; acc: 0.66
Batch: 740; loss: 0.87; acc: 0.73
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.83; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.7999211411187603; val_accuracy: 0.7418391719745223 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.8; acc: 0.73
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.87; acc: 0.72
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.9; acc: 0.7
Batch: 280; loss: 0.82; acc: 0.69
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.88; acc: 0.77
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 0.74; acc: 0.77
Batch: 420; loss: 0.88; acc: 0.75
Batch: 440; loss: 0.79; acc: 0.72
Batch: 460; loss: 0.91; acc: 0.67
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.71; acc: 0.72
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.93; acc: 0.66
Batch: 580; loss: 0.85; acc: 0.7
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 0.94; acc: 0.69
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.96; acc: 0.64
Batch: 740; loss: 0.92; acc: 0.75
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.62
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.7992143817008681; val_accuracy: 0.7410429936305732 

plots/subspace_training/lenet/2020-01-20 16:25:19/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 4442598
elements in E: 4442600
fraction nonzero: 0.9999995498131725
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.32; acc: 0.11
Batch: 60; loss: 2.31; acc: 0.09
Batch: 80; loss: 2.31; acc: 0.05
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.29; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.02
Batch: 300; loss: 2.29; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.19
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.12
Batch: 420; loss: 2.28; acc: 0.17
Batch: 440; loss: 2.27; acc: 0.22
Batch: 460; loss: 2.27; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.27; acc: 0.25
Batch: 520; loss: 2.28; acc: 0.16
Batch: 540; loss: 2.27; acc: 0.28
Batch: 560; loss: 2.26; acc: 0.34
Batch: 580; loss: 2.26; acc: 0.17
Batch: 600; loss: 2.25; acc: 0.3
Batch: 620; loss: 2.25; acc: 0.36
Batch: 640; loss: 2.24; acc: 0.34
Batch: 660; loss: 2.24; acc: 0.34
Batch: 680; loss: 2.26; acc: 0.22
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.21; acc: 0.41
Batch: 740; loss: 2.21; acc: 0.31
Batch: 760; loss: 2.19; acc: 0.41
Batch: 780; loss: 2.19; acc: 0.38
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.18; acc: 0.41
Batch: 20; loss: 2.18; acc: 0.39
Batch: 40; loss: 2.12; acc: 0.52
Batch: 60; loss: 2.16; acc: 0.42
Batch: 80; loss: 2.17; acc: 0.39
Batch: 100; loss: 2.17; acc: 0.47
Batch: 120; loss: 2.18; acc: 0.38
Batch: 140; loss: 2.17; acc: 0.34
Val Epoch over. val_loss: 2.1794502097330275; val_accuracy: 0.38554936305732485 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.17; acc: 0.36
Batch: 20; loss: 2.16; acc: 0.42
Batch: 40; loss: 2.12; acc: 0.52
Batch: 60; loss: 2.07; acc: 0.41
Batch: 80; loss: 1.99; acc: 0.45
Batch: 100; loss: 1.93; acc: 0.5
Batch: 120; loss: 1.83; acc: 0.48
Batch: 140; loss: 1.71; acc: 0.45
Batch: 160; loss: 1.52; acc: 0.58
Batch: 180; loss: 1.52; acc: 0.64
Batch: 200; loss: 1.29; acc: 0.67
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.8; acc: 0.84
Batch: 280; loss: 1.01; acc: 0.69
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 1.19; acc: 0.56
Batch: 340; loss: 1.14; acc: 0.61
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 0.86; acc: 0.72
Batch: 400; loss: 0.91; acc: 0.7
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 1.07; acc: 0.64
Batch: 500; loss: 0.84; acc: 0.7
Batch: 520; loss: 0.97; acc: 0.69
Batch: 540; loss: 0.96; acc: 0.72
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 1.0; acc: 0.62
Batch: 600; loss: 0.92; acc: 0.73
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.97; acc: 0.72
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.98; acc: 0.67
Batch: 700; loss: 0.7; acc: 0.78
Batch: 720; loss: 0.95; acc: 0.67
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.82; acc: 0.73
Batch: 780; loss: 0.96; acc: 0.59
Train Epoch over. train_loss: 1.17; train_accuracy: 0.64 

Batch: 0; loss: 1.18; acc: 0.56
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.73; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.9159310401245288; val_accuracy: 0.6957603503184714 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.67
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 0.95; acc: 0.64
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.83; acc: 0.78
Batch: 160; loss: 0.9; acc: 0.72
Batch: 180; loss: 0.73; acc: 0.72
Batch: 200; loss: 0.67; acc: 0.77
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.9; acc: 0.64
Batch: 300; loss: 1.1; acc: 0.7
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 0.99; acc: 0.7
Batch: 360; loss: 0.6; acc: 0.75
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.73
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.75; acc: 0.7
Batch: 460; loss: 0.8; acc: 0.7
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.79; acc: 0.72
Batch: 520; loss: 0.75; acc: 0.72
Batch: 540; loss: 0.78; acc: 0.73
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.83; acc: 0.72
Batch: 600; loss: 0.78; acc: 0.77
Batch: 620; loss: 0.93; acc: 0.73
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 1.06; acc: 0.69
Batch: 680; loss: 0.62; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.75
Batch: 720; loss: 0.76; acc: 0.84
Batch: 740; loss: 0.86; acc: 0.73
Batch: 760; loss: 0.61; acc: 0.73
Batch: 780; loss: 0.64; acc: 0.8
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.64
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.9; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.89593413339299; val_accuracy: 0.7044187898089171 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.63; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.73
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.81; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 1.02; acc: 0.66
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.75
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 0.76; acc: 0.75
Batch: 360; loss: 0.77; acc: 0.69
Batch: 380; loss: 0.83; acc: 0.77
Batch: 400; loss: 0.84; acc: 0.69
Batch: 420; loss: 1.08; acc: 0.75
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.95; acc: 0.69
Batch: 500; loss: 1.03; acc: 0.61
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.87; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.94; acc: 0.75
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.77; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.86; acc: 0.7
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.72
Batch: 780; loss: 0.82; acc: 0.77
Train Epoch over. train_loss: 0.8; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.69
Batch: 20; loss: 1.05; acc: 0.61
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.43; acc: 0.86
Val Epoch over. val_loss: 0.8388949676304106; val_accuracy: 0.7306926751592356 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 1.16; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.72; acc: 0.75
Batch: 180; loss: 1.06; acc: 0.73
Batch: 200; loss: 0.61; acc: 0.78
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.78; acc: 0.7
Batch: 280; loss: 1.13; acc: 0.64
Batch: 300; loss: 0.68; acc: 0.75
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.84; acc: 0.7
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.64; acc: 0.77
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.73; acc: 0.72
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 1.09; acc: 0.73
Batch: 540; loss: 0.78; acc: 0.73
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.9; acc: 0.7
Batch: 620; loss: 0.86; acc: 0.72
Batch: 640; loss: 0.64; acc: 0.77
Batch: 660; loss: 0.81; acc: 0.69
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 0.81; acc: 0.78
Batch: 740; loss: 0.54; acc: 0.78
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.7; acc: 0.75
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.62; acc: 0.66
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.44; acc: 0.56
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 0.93; acc: 0.69
Val Epoch over. val_loss: 1.2663716601718003; val_accuracy: 0.6454020700636943 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.61
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.93; acc: 0.72
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.79; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.78
Batch: 200; loss: 0.97; acc: 0.66
Batch: 220; loss: 0.98; acc: 0.69
Batch: 240; loss: 0.84; acc: 0.75
Batch: 260; loss: 1.12; acc: 0.66
Batch: 280; loss: 0.8; acc: 0.7
Batch: 300; loss: 1.09; acc: 0.67
Batch: 320; loss: 0.76; acc: 0.77
Batch: 340; loss: 0.95; acc: 0.73
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 1.08; acc: 0.72
Batch: 400; loss: 0.74; acc: 0.75
Batch: 420; loss: 0.89; acc: 0.69
Batch: 440; loss: 0.87; acc: 0.7
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.5; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 1.15; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.64
Batch: 620; loss: 0.82; acc: 0.7
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.96; acc: 0.66
Batch: 700; loss: 0.68; acc: 0.72
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.7
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.64
Batch: 140; loss: 0.67; acc: 0.78
Val Epoch over. val_loss: 1.1619531235117821; val_accuracy: 0.6594347133757962 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.64
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 0.59; acc: 0.72
Batch: 80; loss: 1.25; acc: 0.64
Batch: 100; loss: 1.0; acc: 0.72
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.88; acc: 0.67
Batch: 160; loss: 0.8; acc: 0.73
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.78; acc: 0.7
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.84; acc: 0.75
Batch: 260; loss: 0.73; acc: 0.72
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.96; acc: 0.73
Batch: 320; loss: 0.98; acc: 0.66
Batch: 340; loss: 0.95; acc: 0.69
Batch: 360; loss: 0.82; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.9; acc: 0.72
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 1.0; acc: 0.67
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.93; acc: 0.66
Batch: 580; loss: 1.01; acc: 0.64
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 1.03; acc: 0.7
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.75
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.91; acc: 0.73
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.86; acc: 0.7
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.9; acc: 0.66
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.7
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.31; acc: 0.88
Val Epoch over. val_loss: 0.7719871033528808; val_accuracy: 0.7515923566878981 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.98; acc: 0.75
Batch: 200; loss: 0.91; acc: 0.77
Batch: 220; loss: 0.81; acc: 0.81
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.73
Batch: 360; loss: 0.81; acc: 0.69
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 0.73; acc: 0.7
Batch: 440; loss: 0.78; acc: 0.72
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.66
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.9; acc: 0.72
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.8; acc: 0.78
Batch: 660; loss: 0.95; acc: 0.64
Batch: 680; loss: 0.73; acc: 0.73
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.88; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.62
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.77; train_accuracy: 0.75 

Batch: 0; loss: 1.01; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 0.43; acc: 0.88
Val Epoch over. val_loss: 0.9059852557197497; val_accuracy: 0.709593949044586 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.7
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 1.24; acc: 0.58
Batch: 80; loss: 0.83; acc: 0.75
Batch: 100; loss: 0.85; acc: 0.73
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.81; acc: 0.67
Batch: 180; loss: 0.85; acc: 0.73
Batch: 200; loss: 0.9; acc: 0.7
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.82; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.75
Batch: 320; loss: 1.11; acc: 0.69
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.72
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.84; acc: 0.73
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 0.99; acc: 0.64
Batch: 500; loss: 0.65; acc: 0.75
Batch: 520; loss: 0.63; acc: 0.75
Batch: 540; loss: 0.83; acc: 0.73
Batch: 560; loss: 0.9; acc: 0.75
Batch: 580; loss: 0.84; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 1.02; acc: 0.69
Batch: 660; loss: 0.67; acc: 0.77
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 1.23; acc: 0.59
Batch: 780; loss: 0.86; acc: 0.67
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.67
Batch: 140; loss: 0.51; acc: 0.78
Val Epoch over. val_loss: 0.9290418759652763; val_accuracy: 0.7093949044585988 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.75
Batch: 20; loss: 0.76; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.8
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.91; acc: 0.66
Batch: 100; loss: 0.81; acc: 0.72
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.75
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 1.04; acc: 0.62
Batch: 240; loss: 0.87; acc: 0.72
Batch: 260; loss: 0.77; acc: 0.73
Batch: 280; loss: 0.89; acc: 0.64
Batch: 300; loss: 0.72; acc: 0.73
Batch: 320; loss: 0.46; acc: 0.81
Batch: 340; loss: 0.99; acc: 0.72
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.85; acc: 0.77
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.75
Batch: 460; loss: 1.24; acc: 0.59
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.83; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.72
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.86; acc: 0.72
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.69; acc: 0.77
Batch: 680; loss: 0.85; acc: 0.75
Batch: 700; loss: 0.95; acc: 0.73
Batch: 720; loss: 0.85; acc: 0.75
Batch: 740; loss: 0.86; acc: 0.72
Batch: 760; loss: 0.93; acc: 0.67
Batch: 780; loss: 1.13; acc: 0.61
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 0.79; acc: 0.67
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.6958241352609768; val_accuracy: 0.7760748407643312 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.71; acc: 0.73
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.75
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.63; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.8
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.73
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.7; acc: 0.78
Batch: 420; loss: 0.79; acc: 0.73
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.77; acc: 0.7
Batch: 500; loss: 0.72; acc: 0.75
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.77
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 0.84; acc: 0.7
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.81; acc: 0.75
Batch: 760; loss: 0.63; acc: 0.73
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.76; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.69
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.6671966706301756; val_accuracy: 0.7897093949044586 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.67
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.85; acc: 0.7
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 1.27; acc: 0.69
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.72
Batch: 420; loss: 0.69; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.81
Batch: 460; loss: 0.71; acc: 0.73
Batch: 480; loss: 0.88; acc: 0.67
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.93; acc: 0.67
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.84; acc: 0.73
Batch: 600; loss: 0.63; acc: 0.75
Batch: 620; loss: 0.76; acc: 0.73
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.9; acc: 0.72
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 1.06; acc: 0.72
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.91; acc: 0.67
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.7696886626398487; val_accuracy: 0.7575636942675159 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.72
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 0.6; acc: 0.75
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.73; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.78
Batch: 240; loss: 0.87; acc: 0.72
Batch: 260; loss: 0.89; acc: 0.72
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.83; acc: 0.75
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.73; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 0.71; acc: 0.75
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.7; acc: 0.72
Batch: 600; loss: 0.69; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.7
Batch: 640; loss: 0.75; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.7
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.83; acc: 0.64
Batch: 720; loss: 0.8; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.94; acc: 0.69
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.6864620468039422; val_accuracy: 0.7822452229299363 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.85; acc: 0.7
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.77
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 0.87; acc: 0.72
Batch: 400; loss: 0.75; acc: 0.75
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.75
Batch: 520; loss: 0.87; acc: 0.66
Batch: 540; loss: 0.86; acc: 0.72
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 1.06; acc: 0.7
Batch: 620; loss: 0.73; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 0.67; acc: 0.73
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.96; acc: 0.75
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.79; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.67
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.7012769615953895; val_accuracy: 0.7711982484076433 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 0.75; acc: 0.7
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.54; acc: 0.77
Batch: 200; loss: 0.92; acc: 0.69
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.73; acc: 0.77
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.78; acc: 0.73
Batch: 380; loss: 0.7; acc: 0.77
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.74; acc: 0.75
Batch: 480; loss: 0.65; acc: 0.8
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.86; acc: 0.75
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.92; acc: 0.75
Batch: 600; loss: 0.69; acc: 0.77
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.89; acc: 0.67
Batch: 740; loss: 0.89; acc: 0.75
Batch: 760; loss: 0.92; acc: 0.69
Batch: 780; loss: 0.76; acc: 0.7
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.84; acc: 0.66
Batch: 20; loss: 0.83; acc: 0.67
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.54; acc: 0.77
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.7303250295341395; val_accuracy: 0.7687101910828026 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 0.91; acc: 0.69
Batch: 60; loss: 0.82; acc: 0.69
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 0.73; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.92; acc: 0.73
Batch: 320; loss: 0.67; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.85; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.77
Batch: 540; loss: 0.8; acc: 0.7
Batch: 560; loss: 0.53; acc: 0.78
Batch: 580; loss: 0.97; acc: 0.69
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.95; acc: 0.7
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.72
Batch: 700; loss: 0.83; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.98; acc: 0.62
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 1.08; acc: 0.62
Batch: 20; loss: 1.35; acc: 0.61
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.67
Batch: 140; loss: 0.48; acc: 0.81
Val Epoch over. val_loss: 1.007246497520216; val_accuracy: 0.6881966560509554 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.82; acc: 0.72
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.69
Batch: 160; loss: 0.63; acc: 0.73
Batch: 180; loss: 0.56; acc: 0.77
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.65; acc: 0.77
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.86; acc: 0.75
Batch: 300; loss: 0.79; acc: 0.73
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 0.83; acc: 0.83
Batch: 460; loss: 0.95; acc: 0.77
Batch: 480; loss: 0.66; acc: 0.75
Batch: 500; loss: 0.82; acc: 0.83
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.77
Batch: 580; loss: 0.76; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 0.74; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.68; acc: 0.75
Batch: 720; loss: 0.87; acc: 0.75
Batch: 740; loss: 0.97; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.73
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.73
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.6884995874515765; val_accuracy: 0.7797571656050956 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.7
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.78; acc: 0.78
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.66
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.93; acc: 0.75
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.8; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 0.66; acc: 0.8
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.77
Batch: 540; loss: 0.61; acc: 0.75
Batch: 560; loss: 0.78; acc: 0.7
Batch: 580; loss: 0.85; acc: 0.77
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.75
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.83; acc: 0.75
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.82; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.7242576741868523; val_accuracy: 0.768312101910828 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.95; acc: 0.67
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.75
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.69
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.53; acc: 0.8
Batch: 340; loss: 0.81; acc: 0.72
Batch: 360; loss: 0.81; acc: 0.77
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.78
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.75
Batch: 480; loss: 0.77; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.83
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.54; acc: 0.78
Batch: 580; loss: 0.86; acc: 0.73
Batch: 600; loss: 0.78; acc: 0.77
Batch: 620; loss: 0.72; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 1.03; acc: 0.67
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.77
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.75
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.81; acc: 0.69
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.73
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.6978786081834963; val_accuracy: 0.7779657643312102 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.79; acc: 0.72
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.73
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.76; acc: 0.73
Batch: 200; loss: 0.93; acc: 0.66
Batch: 220; loss: 0.64; acc: 0.77
Batch: 240; loss: 0.97; acc: 0.67
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.9; acc: 0.66
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.86; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.8
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.87; acc: 0.7
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.73; acc: 0.73
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 0.91; acc: 0.7
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.93; acc: 0.62
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 0.6; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 0.93; acc: 0.67
Batch: 140; loss: 0.32; acc: 0.88
Val Epoch over. val_loss: 0.7524030759076404; val_accuracy: 0.7553742038216561 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.94; acc: 0.73
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.82; acc: 0.73
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.86; acc: 0.72
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.69
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.95
Batch: 760; loss: 0.76; acc: 0.75
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.75; acc: 0.72
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.6549795413283026; val_accuracy: 0.7959792993630573 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.7
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 1.0; acc: 0.7
Batch: 220; loss: 0.61; acc: 0.72
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.77
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.87; acc: 0.7
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.77
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.76; acc: 0.75
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.75
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.78
Batch: 660; loss: 0.74; acc: 0.73
Batch: 680; loss: 0.86; acc: 0.72
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.61; acc: 0.78
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.83; acc: 0.7
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.6658336379725462; val_accuracy: 0.7905055732484076 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.75
Batch: 180; loss: 0.58; acc: 0.78
Batch: 200; loss: 0.57; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 0.83; acc: 0.75
Batch: 260; loss: 0.63; acc: 0.75
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.75
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.73
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.68; acc: 0.77
Batch: 500; loss: 0.89; acc: 0.78
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.53; acc: 0.81
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 1.14; acc: 0.62
Batch: 600; loss: 0.59; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.72
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.82; acc: 0.69
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.6602254760493139; val_accuracy: 0.791202229299363 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 0.87; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.84; acc: 0.77
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.82; acc: 0.72
Batch: 220; loss: 0.61; acc: 0.73
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.83; acc: 0.75
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.75
Batch: 360; loss: 0.6; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.72
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.78
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.61; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.75
Batch: 620; loss: 0.63; acc: 0.73
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.73
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.72; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.5; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6567983518170702; val_accuracy: 0.7925955414012739 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.72
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.63; acc: 0.78
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 1.0; acc: 0.67
Batch: 280; loss: 0.73; acc: 0.73
Batch: 300; loss: 0.62; acc: 0.77
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.85; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.69
Batch: 420; loss: 0.85; acc: 0.75
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.89; acc: 0.69
Batch: 480; loss: 0.67; acc: 0.73
Batch: 500; loss: 0.61; acc: 0.77
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.76; acc: 0.77
Batch: 600; loss: 0.77; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.74; acc: 0.77
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.78
Batch: 740; loss: 0.91; acc: 0.66
Batch: 760; loss: 0.98; acc: 0.73
Batch: 780; loss: 0.97; acc: 0.69
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.64; acc: 0.75
Batch: 20; loss: 0.78; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.6476281759845224; val_accuracy: 0.7951831210191083 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.84; acc: 0.75
Batch: 180; loss: 0.82; acc: 0.75
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.75
Batch: 240; loss: 0.51; acc: 0.8
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.72
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.78
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.73
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 1.02; acc: 0.67
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.98; acc: 0.73
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.68; acc: 0.77
Batch: 600; loss: 0.64; acc: 0.75
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.72
Batch: 660; loss: 0.99; acc: 0.72
Batch: 680; loss: 0.65; acc: 0.75
Batch: 700; loss: 0.58; acc: 0.8
Batch: 720; loss: 0.88; acc: 0.69
Batch: 740; loss: 0.75; acc: 0.75
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.71; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.69
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.6575342970099419; val_accuracy: 0.7921974522292994 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.74; acc: 0.73
Batch: 60; loss: 0.76; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.77; acc: 0.69
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.73
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.97; acc: 0.75
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.84; acc: 0.66
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.77
Batch: 400; loss: 0.55; acc: 0.8
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.94; acc: 0.69
Batch: 480; loss: 0.86; acc: 0.78
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.95; acc: 0.7
Batch: 540; loss: 0.77; acc: 0.75
Batch: 560; loss: 0.54; acc: 0.8
Batch: 580; loss: 0.77; acc: 0.7
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.73; acc: 0.7
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.75
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.97; acc: 0.62
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.6582238643792024; val_accuracy: 0.7939888535031847 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.8; acc: 0.73
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.7
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.77
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.8
Batch: 240; loss: 0.66; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.82; acc: 0.8
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 0.6; acc: 0.77
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.78
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.75
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.8; acc: 0.73
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.96; acc: 0.66
Batch: 740; loss: 0.53; acc: 0.78
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.6736198633339754; val_accuracy: 0.7847332802547771 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.73; acc: 0.7
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.95; acc: 0.67
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.82; acc: 0.69
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.74; acc: 0.73
Batch: 420; loss: 0.84; acc: 0.7
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.7
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.84; acc: 0.72
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.77
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.77
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.73; acc: 0.77
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.71; acc: 0.73
Batch: 20; loss: 0.8; acc: 0.7
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.6587005625864503; val_accuracy: 0.7927945859872612 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.7
Batch: 100; loss: 0.63; acc: 0.72
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.75
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 0.83; acc: 0.7
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.77
Batch: 400; loss: 0.59; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.94; acc: 0.64
Batch: 580; loss: 0.81; acc: 0.73
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.93; acc: 0.7
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.73
Batch: 740; loss: 0.76; acc: 0.72
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.82; acc: 0.72
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.6529453804918156; val_accuracy: 0.7900079617834395 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.67; acc: 0.73
Batch: 200; loss: 0.56; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.86; acc: 0.73
Batch: 280; loss: 0.78; acc: 0.72
Batch: 300; loss: 0.74; acc: 0.75
Batch: 320; loss: 0.93; acc: 0.69
Batch: 340; loss: 0.71; acc: 0.7
Batch: 360; loss: 0.92; acc: 0.75
Batch: 380; loss: 0.83; acc: 0.73
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.89; acc: 0.7
Batch: 520; loss: 0.82; acc: 0.73
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.91; acc: 0.67
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.8; acc: 0.78
Batch: 640; loss: 0.66; acc: 0.77
Batch: 660; loss: 0.84; acc: 0.72
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.6423374409698377; val_accuracy: 0.7975716560509554 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.77; acc: 0.72
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.94; acc: 0.69
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.72
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.62; acc: 0.73
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.77
Batch: 360; loss: 0.72; acc: 0.8
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 0.62; acc: 0.75
Batch: 420; loss: 0.97; acc: 0.64
Batch: 440; loss: 0.73; acc: 0.73
Batch: 460; loss: 0.83; acc: 0.69
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.85; acc: 0.7
Batch: 540; loss: 0.84; acc: 0.75
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.73
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.82; acc: 0.73
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.89; acc: 0.7
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.7
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6423826296428207; val_accuracy: 0.7973726114649682 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.54; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.78
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.95; acc: 0.73
Batch: 300; loss: 0.79; acc: 0.72
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.75
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.77
Batch: 560; loss: 0.71; acc: 0.73
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.76; acc: 0.75
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.86; acc: 0.7
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.77
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.6416636416866521; val_accuracy: 0.7967754777070064 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.75
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.72
Batch: 280; loss: 0.49; acc: 0.8
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.83; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.77
Batch: 360; loss: 0.77; acc: 0.73
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.76; acc: 0.78
Batch: 520; loss: 0.75; acc: 0.72
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.77; acc: 0.73
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.77
Batch: 640; loss: 0.69; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.72
Batch: 680; loss: 0.9; acc: 0.72
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 0.71; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.6444016440658812; val_accuracy: 0.7953821656050956 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.77; acc: 0.7
Batch: 180; loss: 0.68; acc: 0.75
Batch: 200; loss: 0.85; acc: 0.8
Batch: 220; loss: 0.56; acc: 0.77
Batch: 240; loss: 0.78; acc: 0.75
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.75
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.93; acc: 0.78
Batch: 440; loss: 0.56; acc: 0.78
Batch: 460; loss: 0.95; acc: 0.73
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.74; acc: 0.72
Batch: 540; loss: 0.89; acc: 0.69
Batch: 560; loss: 0.99; acc: 0.73
Batch: 580; loss: 0.98; acc: 0.69
Batch: 600; loss: 0.82; acc: 0.7
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.73
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.62; acc: 0.77
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.69
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.89
Val Epoch over. val_loss: 0.6461482160979775; val_accuracy: 0.7959792993630573 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.69
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.78; acc: 0.72
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.51; acc: 0.78
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.81; acc: 0.77
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.63; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.54; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.72; acc: 0.75
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.67; acc: 0.75
Batch: 700; loss: 0.56; acc: 0.78
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.82; acc: 0.75
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6419128799324583; val_accuracy: 0.7982683121019108 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.77
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.76; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.81; acc: 0.78
Batch: 380; loss: 0.71; acc: 0.72
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.85; acc: 0.75
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.74; acc: 0.73
Batch: 520; loss: 0.82; acc: 0.78
Batch: 540; loss: 0.69; acc: 0.77
Batch: 560; loss: 0.8; acc: 0.75
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 1.03; acc: 0.69
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.96; acc: 0.64
Batch: 680; loss: 0.71; acc: 0.75
Batch: 700; loss: 0.5; acc: 0.8
Batch: 720; loss: 0.49; acc: 0.81
Batch: 740; loss: 0.77; acc: 0.77
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.73
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.6437978819487201; val_accuracy: 0.7936902866242038 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.61; acc: 0.75
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.75
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.87; acc: 0.72
Batch: 260; loss: 0.7; acc: 0.73
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.78
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.72; acc: 0.75
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.66; acc: 0.77
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.87; acc: 0.75
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.46; acc: 0.8
Batch: 700; loss: 0.91; acc: 0.78
Batch: 720; loss: 0.62; acc: 0.77
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.84; acc: 0.73
Batch: 780; loss: 0.77; acc: 0.72
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.83; acc: 0.67
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.75
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.6479799233993907; val_accuracy: 0.794984076433121 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.76; acc: 0.69
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.9; acc: 0.75
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 1.03; acc: 0.62
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.61; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.72
Batch: 480; loss: 0.65; acc: 0.75
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.74; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.71; acc: 0.73
Batch: 620; loss: 0.91; acc: 0.72
Batch: 640; loss: 0.79; acc: 0.72
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.7
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.99; acc: 0.7
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.644004017968846; val_accuracy: 0.79796974522293 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.81; acc: 0.78
Batch: 180; loss: 0.83; acc: 0.72
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.77
Batch: 300; loss: 0.73; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.77; acc: 0.77
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.8; acc: 0.7
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 1.04; acc: 0.66
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 0.56; acc: 0.77
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.67
Batch: 740; loss: 0.73; acc: 0.75
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.69
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.6451275705531904; val_accuracy: 0.798765923566879 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.8; acc: 0.73
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.84; acc: 0.67
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 0.87; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.77
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.94; acc: 0.75
Batch: 380; loss: 0.8; acc: 0.73
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.79; acc: 0.72
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.71; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.75
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.83; acc: 0.73
Batch: 600; loss: 0.79; acc: 0.72
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.76; acc: 0.73
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.78; acc: 0.77
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.6418165135535465; val_accuracy: 0.7970740445859873 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.75
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.83
Batch: 160; loss: 0.78; acc: 0.73
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.93; acc: 0.77
Batch: 280; loss: 0.67; acc: 0.77
Batch: 300; loss: 0.81; acc: 0.72
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.7
Batch: 380; loss: 1.03; acc: 0.75
Batch: 400; loss: 0.94; acc: 0.77
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.7
Batch: 540; loss: 0.66; acc: 0.77
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.45; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.7; acc: 0.75
Batch: 640; loss: 0.63; acc: 0.78
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.42; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6418961147024373; val_accuracy: 0.7989649681528662 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.72
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.84; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.78
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.7
Batch: 240; loss: 0.83; acc: 0.75
Batch: 260; loss: 0.61; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.88; acc: 0.73
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.61; acc: 0.77
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.9; acc: 0.78
Batch: 540; loss: 0.73; acc: 0.78
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.72
Batch: 640; loss: 0.76; acc: 0.75
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.73; acc: 0.73
Batch: 700; loss: 0.92; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.8
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.75
Batch: 20; loss: 0.78; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.6423736751838854; val_accuracy: 0.7972730891719745 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 0.98; acc: 0.69
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.84; acc: 0.69
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.73; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.67
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.78
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.77
Batch: 500; loss: 0.67; acc: 0.77
Batch: 520; loss: 0.84; acc: 0.73
Batch: 540; loss: 0.6; acc: 0.8
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.77; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.77
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.67; acc: 0.73
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6417078068301936; val_accuracy: 0.7969745222929936 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.74; acc: 0.73
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.77
Batch: 200; loss: 1.12; acc: 0.72
Batch: 220; loss: 0.88; acc: 0.75
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.75; acc: 0.75
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.77
Batch: 480; loss: 0.69; acc: 0.75
Batch: 500; loss: 0.61; acc: 0.77
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.76; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.8
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.72
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.73; acc: 0.77
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.9; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 1.02; acc: 0.7
Batch: 780; loss: 0.71; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.75
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.6426326980826201; val_accuracy: 0.7959792993630573 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.67
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.78; acc: 0.73
Batch: 160; loss: 0.69; acc: 0.73
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.92; acc: 0.7
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.8
Batch: 260; loss: 0.56; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.7
Batch: 300; loss: 0.61; acc: 0.78
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.73
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.73
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.74; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.75
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.96; acc: 0.67
Batch: 500; loss: 0.91; acc: 0.7
Batch: 520; loss: 0.68; acc: 0.73
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.71; acc: 0.78
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 1.05; acc: 0.66
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6425007183081025; val_accuracy: 0.7963773885350318 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.72
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.8; acc: 0.77
Batch: 360; loss: 0.95; acc: 0.7
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.75
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.89; acc: 0.72
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.75
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.93; acc: 0.73
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.83; acc: 0.75
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.6427441857233169; val_accuracy: 0.7965764331210191 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.92; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.84; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.77
Batch: 280; loss: 0.66; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.78; acc: 0.73
Batch: 360; loss: 0.83; acc: 0.72
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.77
Batch: 420; loss: 0.63; acc: 0.78
Batch: 440; loss: 0.81; acc: 0.8
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.75; acc: 0.7
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.72
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6415402936707636; val_accuracy: 0.7985668789808917 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.74; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.59; acc: 0.77
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.89; acc: 0.73
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.87; acc: 0.78
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.86; acc: 0.72
Batch: 300; loss: 0.75; acc: 0.75
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.8; acc: 0.77
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.8
Batch: 480; loss: 0.79; acc: 0.67
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 1.07; acc: 0.72
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.6418984894919547; val_accuracy: 0.7972730891719745 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 1.09; acc: 0.69
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.88; acc: 0.73
Batch: 160; loss: 0.99; acc: 0.66
Batch: 180; loss: 0.75; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.73
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.75; acc: 0.7
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.75; acc: 0.77
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.93; acc: 0.77
Batch: 440; loss: 0.68; acc: 0.77
Batch: 460; loss: 0.84; acc: 0.69
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.92; acc: 0.75
Batch: 540; loss: 0.49; acc: 0.81
Batch: 560; loss: 0.64; acc: 0.78
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.81; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.93; acc: 0.75
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.75
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.6427931353734557; val_accuracy: 0.7973726114649682 

plots/subspace_training/lenet/2020-01-20 16:25:19/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5553248
elements in E: 5553250
fraction nonzero: 0.999999639850538
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.03
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.28; acc: 0.08
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.26; acc: 0.17
Batch: 260; loss: 2.28; acc: 0.09
Batch: 280; loss: 2.26; acc: 0.22
Batch: 300; loss: 2.27; acc: 0.17
Batch: 320; loss: 2.27; acc: 0.16
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.26; acc: 0.23
Batch: 380; loss: 2.25; acc: 0.27
Batch: 400; loss: 2.25; acc: 0.22
Batch: 420; loss: 2.25; acc: 0.2
Batch: 440; loss: 2.24; acc: 0.22
Batch: 460; loss: 2.2; acc: 0.36
Batch: 480; loss: 2.22; acc: 0.25
Batch: 500; loss: 2.21; acc: 0.33
Batch: 520; loss: 2.17; acc: 0.27
Batch: 540; loss: 2.15; acc: 0.36
Batch: 560; loss: 2.14; acc: 0.28
Batch: 580; loss: 2.1; acc: 0.36
Batch: 600; loss: 2.09; acc: 0.36
Batch: 620; loss: 2.04; acc: 0.42
Batch: 640; loss: 2.02; acc: 0.31
Batch: 660; loss: 1.88; acc: 0.47
Batch: 680; loss: 1.92; acc: 0.38
Batch: 700; loss: 1.79; acc: 0.53
Batch: 720; loss: 1.6; acc: 0.55
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.34; acc: 0.59
Train Epoch over. train_loss: 2.14; train_accuracy: 0.25 

Batch: 0; loss: 1.31; acc: 0.62
Batch: 20; loss: 1.43; acc: 0.41
Batch: 40; loss: 1.14; acc: 0.69
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.34; acc: 0.64
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 1.25; acc: 0.53
Val Epoch over. val_loss: 1.3296164441260563; val_accuracy: 0.5711584394904459 

Traceback (most recent call last):
  File "ddim_vs_acc.py", line 98, in <module>
    main()
  File "ddim_vs_acc.py", line 27, in main
    train_loss, train_accuracy, val_loss, val_accuracy = train_model_once(ARGS)
  File "/home/llang/thesis-intrinsic-dimension/classify_mnist.py", line 55, in train_model_once
    train_loss, train_acc, val_loss, val_acc, subspace_distance = train_epoch(model,train_loader,val_loader,optimizer,criterion,ARGS)
  File "/home/llang/thesis-intrinsic-dimension/train_helpers.py", line 19, in train_epoch
    subspace_distance = optimizer.compute_subspace_distance()
  File "/home/llang/thesis-intrinsic-dimension/optimizers.py", line 130, in compute_subspace_distance
    difference_vector = p_D - diff_D
RuntimeError: CUDA out of memory. Tried to allocate 7.35 GiB (GPU 0; 10.92 GiB total capacity; 44.04 MiB already allocated; 3.00 GiB free; 7.33 GiB cached)
/var/spool/slurm-llnl/slurmd/job4387145/slurm_script: line 26: --print_freq=20: command not found
