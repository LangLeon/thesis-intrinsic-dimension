model : lenet
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : False
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-19 22:21:20
nonzero elements in E: 2173
elements in E: 444260
fraction nonzero: 0.004891279881150678
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.08
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.09
Batch: 200; loss: 2.32; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.32; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.19
Batch: 320; loss: 2.32; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.17
Batch: 360; loss: 2.29; acc: 0.08
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.31; acc: 0.14
Batch: 420; loss: 2.3; acc: 0.2
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.31; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.32; acc: 0.06
Batch: 540; loss: 2.31; acc: 0.08
Batch: 560; loss: 2.3; acc: 0.05
Batch: 580; loss: 2.32; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.31; acc: 0.11
Batch: 660; loss: 2.28; acc: 0.23
Batch: 680; loss: 2.31; acc: 0.05
Batch: 700; loss: 2.3; acc: 0.16
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.32; acc: 0.05
Batch: 760; loss: 2.3; acc: 0.06
Batch: 780; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.05
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.32; acc: 0.03
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.09
Val Epoch over. val_loss: 2.3014404849641643; val_accuracy: 0.09872611464968153 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.19
Batch: 80; loss: 2.31; acc: 0.03
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.31; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.32; acc: 0.02
Batch: 340; loss: 2.31; acc: 0.06
Batch: 360; loss: 2.31; acc: 0.06
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.28; acc: 0.19
Batch: 460; loss: 2.29; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.05
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.29; acc: 0.11
Batch: 540; loss: 2.31; acc: 0.05
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.08
Batch: 640; loss: 2.31; acc: 0.03
Batch: 660; loss: 2.3; acc: 0.08
Batch: 680; loss: 2.32; acc: 0.05
Batch: 700; loss: 2.29; acc: 0.11
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.3; acc: 0.12
Batch: 780; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.31; acc: 0.03
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.09
Val Epoch over. val_loss: 2.2957883412670936; val_accuracy: 0.09922372611464968 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.11
Batch: 40; loss: 2.29; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.31; acc: 0.06
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.12
Batch: 220; loss: 2.31; acc: 0.08
Batch: 240; loss: 2.31; acc: 0.05
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.29; acc: 0.11
Batch: 300; loss: 2.28; acc: 0.12
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.05
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.28; acc: 0.16
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.28; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.12
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.29; acc: 0.19
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.12
Batch: 680; loss: 2.28; acc: 0.12
Batch: 700; loss: 2.28; acc: 0.08
Batch: 720; loss: 2.3; acc: 0.08
Batch: 740; loss: 2.3; acc: 0.08
Batch: 760; loss: 2.28; acc: 0.08
Batch: 780; loss: 2.28; acc: 0.16
Train Epoch over. train_loss: 2.29; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.05
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.29; acc: 0.06
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.03
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.28; acc: 0.09
Val Epoch over. val_loss: 2.2895428208029194; val_accuracy: 0.1009156050955414 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.19
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.29; acc: 0.05
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.05
Batch: 200; loss: 2.28; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.29; acc: 0.05
Batch: 300; loss: 2.31; acc: 0.05
Batch: 320; loss: 2.27; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.12
Batch: 360; loss: 2.29; acc: 0.08
Batch: 380; loss: 2.29; acc: 0.16
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.29; acc: 0.12
Batch: 440; loss: 2.28; acc: 0.17
Batch: 460; loss: 2.29; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.06
Batch: 500; loss: 2.29; acc: 0.17
Batch: 520; loss: 2.28; acc: 0.11
Batch: 540; loss: 2.3; acc: 0.08
Batch: 560; loss: 2.29; acc: 0.19
Batch: 580; loss: 2.29; acc: 0.16
Batch: 600; loss: 2.29; acc: 0.06
Batch: 620; loss: 2.27; acc: 0.19
Batch: 640; loss: 2.29; acc: 0.06
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.28; acc: 0.08
Batch: 700; loss: 2.27; acc: 0.16
Batch: 720; loss: 2.28; acc: 0.17
Batch: 740; loss: 2.28; acc: 0.25
Batch: 760; loss: 2.28; acc: 0.14
Batch: 780; loss: 2.29; acc: 0.12
Train Epoch over. train_loss: 2.29; train_accuracy: 0.12 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.27; acc: 0.2
Batch: 40; loss: 2.29; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.19
Batch: 100; loss: 2.29; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.28; acc: 0.12
Val Epoch over. val_loss: 2.2841906334943833; val_accuracy: 0.14689490445859874 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.2
Batch: 40; loss: 2.27; acc: 0.17
Batch: 60; loss: 2.26; acc: 0.2
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.19
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.27; acc: 0.16
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.26; acc: 0.22
Batch: 280; loss: 2.29; acc: 0.16
Batch: 300; loss: 2.29; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.03
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.19
Batch: 380; loss: 2.28; acc: 0.17
Batch: 400; loss: 2.28; acc: 0.08
Batch: 420; loss: 2.29; acc: 0.12
Batch: 440; loss: 2.28; acc: 0.27
Batch: 460; loss: 2.28; acc: 0.19
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.28; acc: 0.19
Batch: 520; loss: 2.29; acc: 0.2
Batch: 540; loss: 2.27; acc: 0.17
Batch: 560; loss: 2.27; acc: 0.17
Batch: 580; loss: 2.26; acc: 0.22
Batch: 600; loss: 2.28; acc: 0.19
Batch: 620; loss: 2.28; acc: 0.23
Batch: 640; loss: 2.29; acc: 0.16
Batch: 660; loss: 2.28; acc: 0.2
Batch: 680; loss: 2.28; acc: 0.25
Batch: 700; loss: 2.28; acc: 0.22
Batch: 720; loss: 2.28; acc: 0.3
Batch: 740; loss: 2.29; acc: 0.19
Batch: 760; loss: 2.28; acc: 0.2
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.28; train_accuracy: 0.17 

Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.25; acc: 0.27
Batch: 40; loss: 2.27; acc: 0.22
Batch: 60; loss: 2.26; acc: 0.2
Batch: 80; loss: 2.26; acc: 0.22
Batch: 100; loss: 2.28; acc: 0.19
Batch: 120; loss: 2.26; acc: 0.22
Batch: 140; loss: 2.27; acc: 0.2
Val Epoch over. val_loss: 2.278840707365874; val_accuracy: 0.1933718152866242 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.23
Batch: 20; loss: 2.26; acc: 0.25
Batch: 40; loss: 2.29; acc: 0.19
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.26; acc: 0.27
Batch: 100; loss: 2.26; acc: 0.23
Batch: 120; loss: 2.28; acc: 0.25
Batch: 140; loss: 2.26; acc: 0.19
Batch: 160; loss: 2.31; acc: 0.16
Batch: 180; loss: 2.28; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.2
Batch: 220; loss: 2.29; acc: 0.09
Batch: 240; loss: 2.26; acc: 0.14
Batch: 260; loss: 2.26; acc: 0.2
Batch: 280; loss: 2.29; acc: 0.19
Batch: 300; loss: 2.29; acc: 0.17
Batch: 320; loss: 2.27; acc: 0.33
Batch: 340; loss: 2.28; acc: 0.17
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.28; acc: 0.22
Batch: 400; loss: 2.29; acc: 0.16
Batch: 420; loss: 2.27; acc: 0.27
Batch: 440; loss: 2.29; acc: 0.2
Batch: 460; loss: 2.29; acc: 0.16
Batch: 480; loss: 2.31; acc: 0.12
Batch: 500; loss: 2.26; acc: 0.25
Batch: 520; loss: 2.27; acc: 0.2
Batch: 540; loss: 2.32; acc: 0.12
Batch: 560; loss: 2.23; acc: 0.3
Batch: 580; loss: 2.29; acc: 0.16
Batch: 600; loss: 2.3; acc: 0.16
Batch: 620; loss: 2.28; acc: 0.14
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.28; acc: 0.14
Batch: 680; loss: 2.24; acc: 0.25
Batch: 700; loss: 2.28; acc: 0.23
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.27; acc: 0.14
Batch: 760; loss: 2.3; acc: 0.2
Batch: 780; loss: 2.28; acc: 0.22
Train Epoch over. train_loss: 2.28; train_accuracy: 0.2 

Batch: 0; loss: 2.27; acc: 0.17
Batch: 20; loss: 2.24; acc: 0.25
Batch: 40; loss: 2.25; acc: 0.28
Batch: 60; loss: 2.24; acc: 0.27
Batch: 80; loss: 2.26; acc: 0.16
Batch: 100; loss: 2.26; acc: 0.3
Batch: 120; loss: 2.23; acc: 0.34
Batch: 140; loss: 2.27; acc: 0.25
Val Epoch over. val_loss: 2.269776019321126; val_accuracy: 0.1913813694267516 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.19
Batch: 40; loss: 2.26; acc: 0.19
Batch: 60; loss: 2.26; acc: 0.27
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.25; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.24; acc: 0.3
Batch: 160; loss: 2.3; acc: 0.16
Batch: 180; loss: 2.28; acc: 0.22
Batch: 200; loss: 2.27; acc: 0.12
Batch: 220; loss: 2.25; acc: 0.17
Batch: 240; loss: 2.27; acc: 0.2
Batch: 260; loss: 2.28; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.22
Batch: 300; loss: 2.28; acc: 0.09
Batch: 320; loss: 2.27; acc: 0.19
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.29; acc: 0.11
Batch: 380; loss: 2.26; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.24; acc: 0.19
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.24; acc: 0.25
Batch: 480; loss: 2.26; acc: 0.14
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.28; acc: 0.11
Batch: 560; loss: 2.25; acc: 0.16
Batch: 580; loss: 2.28; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.25; acc: 0.17
Batch: 640; loss: 2.27; acc: 0.17
Batch: 660; loss: 2.27; acc: 0.08
Batch: 680; loss: 2.24; acc: 0.17
Batch: 700; loss: 2.24; acc: 0.17
Batch: 720; loss: 2.25; acc: 0.19
Batch: 740; loss: 2.28; acc: 0.09
Batch: 760; loss: 2.24; acc: 0.14
Batch: 780; loss: 2.23; acc: 0.2
Train Epoch over. train_loss: 2.26; train_accuracy: 0.16 

Batch: 0; loss: 2.25; acc: 0.17
Batch: 20; loss: 2.21; acc: 0.16
Batch: 40; loss: 2.24; acc: 0.17
Batch: 60; loss: 2.22; acc: 0.19
Batch: 80; loss: 2.26; acc: 0.09
Batch: 100; loss: 2.23; acc: 0.23
Batch: 120; loss: 2.19; acc: 0.3
Batch: 140; loss: 2.26; acc: 0.14
Val Epoch over. val_loss: 2.254388751497694; val_accuracy: 0.1351512738853503 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.19
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.24; acc: 0.17
Batch: 80; loss: 2.26; acc: 0.06
Batch: 100; loss: 2.23; acc: 0.23
Batch: 120; loss: 2.25; acc: 0.09
Batch: 140; loss: 2.24; acc: 0.14
Batch: 160; loss: 2.27; acc: 0.11
Batch: 180; loss: 2.26; acc: 0.09
Batch: 200; loss: 2.26; acc: 0.14
Batch: 220; loss: 2.24; acc: 0.16
Batch: 240; loss: 2.26; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.25; acc: 0.14
Batch: 300; loss: 2.25; acc: 0.12
Batch: 320; loss: 2.22; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.09
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.11
Batch: 440; loss: 2.22; acc: 0.25
Batch: 460; loss: 2.23; acc: 0.19
Batch: 480; loss: 2.22; acc: 0.17
Batch: 500; loss: 2.29; acc: 0.09
Batch: 520; loss: 2.25; acc: 0.11
Batch: 540; loss: 2.33; acc: 0.06
Batch: 560; loss: 2.26; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.19
Batch: 600; loss: 2.25; acc: 0.11
Batch: 620; loss: 2.21; acc: 0.17
Batch: 640; loss: 2.2; acc: 0.2
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.21; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.23; acc: 0.19
Batch: 760; loss: 2.19; acc: 0.25
Batch: 780; loss: 2.21; acc: 0.2
Train Epoch over. train_loss: 2.24; train_accuracy: 0.15 

Batch: 0; loss: 2.25; acc: 0.19
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.19; acc: 0.25
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.24; acc: 0.11
Batch: 100; loss: 2.19; acc: 0.25
Batch: 120; loss: 2.13; acc: 0.3
Batch: 140; loss: 2.19; acc: 0.17
Val Epoch over. val_loss: 2.2149741102935403; val_accuracy: 0.18859474522292993 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.26; acc: 0.14
Batch: 60; loss: 2.24; acc: 0.23
Batch: 80; loss: 2.24; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.15; acc: 0.3
Batch: 160; loss: 2.2; acc: 0.16
Batch: 180; loss: 2.17; acc: 0.16
Batch: 200; loss: 2.21; acc: 0.14
Batch: 220; loss: 2.29; acc: 0.16
Batch: 240; loss: 2.3; acc: 0.03
Batch: 260; loss: 2.25; acc: 0.12
Batch: 280; loss: 2.24; acc: 0.11
Batch: 300; loss: 2.22; acc: 0.22
Batch: 320; loss: 2.23; acc: 0.2
Batch: 340; loss: 2.27; acc: 0.17
Batch: 360; loss: 2.22; acc: 0.16
Batch: 380; loss: 2.23; acc: 0.14
Batch: 400; loss: 2.31; acc: 0.12
Batch: 420; loss: 2.21; acc: 0.2
Batch: 440; loss: 2.17; acc: 0.17
Batch: 460; loss: 2.13; acc: 0.22
Batch: 480; loss: 2.18; acc: 0.2
Batch: 500; loss: 2.24; acc: 0.19
Batch: 520; loss: 2.24; acc: 0.12
Batch: 540; loss: 2.27; acc: 0.23
Batch: 560; loss: 2.26; acc: 0.12
Batch: 580; loss: 2.23; acc: 0.2
Batch: 600; loss: 2.21; acc: 0.25
Batch: 620; loss: 2.18; acc: 0.22
Batch: 640; loss: 2.18; acc: 0.22
Batch: 660; loss: 2.2; acc: 0.25
Batch: 680; loss: 2.16; acc: 0.28
Batch: 700; loss: 2.24; acc: 0.22
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.17; acc: 0.17
Batch: 760; loss: 2.19; acc: 0.17
Batch: 780; loss: 2.24; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.2
Batch: 20; loss: 2.17; acc: 0.28
Batch: 40; loss: 2.15; acc: 0.28
Batch: 60; loss: 2.18; acc: 0.22
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.18; acc: 0.25
Batch: 120; loss: 2.1; acc: 0.28
Batch: 140; loss: 2.16; acc: 0.22
Val Epoch over. val_loss: 2.202954927067848; val_accuracy: 0.19058519108280256 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.2; acc: 0.22
Batch: 20; loss: 2.25; acc: 0.19
Batch: 40; loss: 2.13; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.22
Batch: 80; loss: 2.19; acc: 0.2
Batch: 100; loss: 2.2; acc: 0.19
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.21; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.29; acc: 0.09
Batch: 220; loss: 2.12; acc: 0.33
Batch: 240; loss: 2.15; acc: 0.14
Batch: 260; loss: 2.2; acc: 0.22
Batch: 280; loss: 2.19; acc: 0.14
Batch: 300; loss: 2.13; acc: 0.23
Batch: 320; loss: 2.22; acc: 0.17
Batch: 340; loss: 2.24; acc: 0.12
Batch: 360; loss: 2.25; acc: 0.2
Batch: 380; loss: 2.22; acc: 0.22
Batch: 400; loss: 2.14; acc: 0.22
Batch: 420; loss: 2.17; acc: 0.23
Batch: 440; loss: 2.15; acc: 0.19
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.25; acc: 0.17
Batch: 500; loss: 2.16; acc: 0.22
Batch: 520; loss: 2.22; acc: 0.19
Batch: 540; loss: 2.22; acc: 0.14
Batch: 560; loss: 2.17; acc: 0.25
Batch: 580; loss: 2.2; acc: 0.19
Batch: 600; loss: 2.15; acc: 0.19
Batch: 620; loss: 2.22; acc: 0.2
Batch: 640; loss: 2.21; acc: 0.23
Batch: 660; loss: 2.24; acc: 0.14
Batch: 680; loss: 2.19; acc: 0.22
Batch: 700; loss: 2.16; acc: 0.14
Batch: 720; loss: 2.25; acc: 0.17
Batch: 740; loss: 2.2; acc: 0.17
Batch: 760; loss: 2.22; acc: 0.25
Batch: 780; loss: 2.29; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.2
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.12; acc: 0.31
Batch: 60; loss: 2.16; acc: 0.22
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.08; acc: 0.28
Batch: 140; loss: 2.14; acc: 0.22
Val Epoch over. val_loss: 2.1997583641368115; val_accuracy: 0.1904856687898089 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.24; acc: 0.14
Batch: 20; loss: 2.22; acc: 0.12
Batch: 40; loss: 2.26; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.22
Batch: 80; loss: 2.14; acc: 0.23
Batch: 100; loss: 2.28; acc: 0.06
Batch: 120; loss: 2.18; acc: 0.16
Batch: 140; loss: 2.14; acc: 0.25
Batch: 160; loss: 2.31; acc: 0.19
Batch: 180; loss: 2.21; acc: 0.14
Batch: 200; loss: 2.14; acc: 0.25
Batch: 220; loss: 2.2; acc: 0.16
Batch: 240; loss: 2.26; acc: 0.17
Batch: 260; loss: 2.15; acc: 0.23
Batch: 280; loss: 2.21; acc: 0.14
Batch: 300; loss: 2.15; acc: 0.17
Batch: 320; loss: 2.26; acc: 0.17
Batch: 340; loss: 2.26; acc: 0.23
Batch: 360; loss: 2.23; acc: 0.19
Batch: 380; loss: 2.2; acc: 0.17
Batch: 400; loss: 2.24; acc: 0.11
Batch: 420; loss: 2.27; acc: 0.19
Batch: 440; loss: 2.27; acc: 0.11
Batch: 460; loss: 2.13; acc: 0.2
Batch: 480; loss: 2.24; acc: 0.12
Batch: 500; loss: 2.18; acc: 0.22
Batch: 520; loss: 2.26; acc: 0.16
Batch: 540; loss: 2.23; acc: 0.16
Batch: 560; loss: 2.31; acc: 0.17
Batch: 580; loss: 2.17; acc: 0.19
Batch: 600; loss: 2.17; acc: 0.22
Batch: 620; loss: 2.21; acc: 0.17
Batch: 640; loss: 2.29; acc: 0.14
Batch: 660; loss: 2.27; acc: 0.27
Batch: 680; loss: 2.24; acc: 0.19
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.3; acc: 0.17
Batch: 740; loss: 2.29; acc: 0.09
Batch: 760; loss: 2.27; acc: 0.2
Batch: 780; loss: 2.24; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.12; acc: 0.27
Batch: 60; loss: 2.16; acc: 0.23
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.28
Batch: 140; loss: 2.14; acc: 0.23
Val Epoch over. val_loss: 2.199986898215713; val_accuracy: 0.18949044585987262 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.26; acc: 0.11
Batch: 20; loss: 2.22; acc: 0.2
Batch: 40; loss: 2.21; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.22
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.14; acc: 0.28
Batch: 120; loss: 2.22; acc: 0.14
Batch: 140; loss: 2.16; acc: 0.22
Batch: 160; loss: 2.08; acc: 0.3
Batch: 180; loss: 2.19; acc: 0.2
Batch: 200; loss: 2.25; acc: 0.16
Batch: 220; loss: 2.28; acc: 0.14
Batch: 240; loss: 2.11; acc: 0.23
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.14; acc: 0.2
Batch: 300; loss: 2.21; acc: 0.2
Batch: 320; loss: 2.25; acc: 0.12
Batch: 340; loss: 2.15; acc: 0.17
Batch: 360; loss: 2.27; acc: 0.16
Batch: 380; loss: 2.23; acc: 0.12
Batch: 400; loss: 2.21; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.27
Batch: 440; loss: 2.22; acc: 0.19
Batch: 460; loss: 2.36; acc: 0.11
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.26; acc: 0.19
Batch: 520; loss: 2.22; acc: 0.19
Batch: 540; loss: 2.22; acc: 0.2
Batch: 560; loss: 2.2; acc: 0.19
Batch: 580; loss: 2.18; acc: 0.17
Batch: 600; loss: 2.24; acc: 0.16
Batch: 620; loss: 2.29; acc: 0.12
Batch: 640; loss: 2.24; acc: 0.2
Batch: 660; loss: 2.21; acc: 0.2
Batch: 680; loss: 2.23; acc: 0.19
Batch: 700; loss: 2.17; acc: 0.2
Batch: 720; loss: 2.16; acc: 0.19
Batch: 740; loss: 2.25; acc: 0.16
Batch: 760; loss: 2.28; acc: 0.12
Batch: 780; loss: 2.22; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.12; acc: 0.28
Batch: 60; loss: 2.16; acc: 0.23
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.25
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.23
Val Epoch over. val_loss: 2.1997029857271038; val_accuracy: 0.1887937898089172 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.29; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.33; acc: 0.11
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.15; acc: 0.2
Batch: 100; loss: 2.19; acc: 0.17
Batch: 120; loss: 2.25; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.18; acc: 0.19
Batch: 180; loss: 2.26; acc: 0.19
Batch: 200; loss: 2.26; acc: 0.17
Batch: 220; loss: 2.17; acc: 0.22
Batch: 240; loss: 2.18; acc: 0.2
Batch: 260; loss: 2.16; acc: 0.17
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.19; acc: 0.17
Batch: 320; loss: 2.01; acc: 0.28
Batch: 340; loss: 2.18; acc: 0.16
Batch: 360; loss: 2.26; acc: 0.17
Batch: 380; loss: 2.19; acc: 0.23
Batch: 400; loss: 2.17; acc: 0.25
Batch: 420; loss: 2.24; acc: 0.17
Batch: 440; loss: 2.2; acc: 0.22
Batch: 460; loss: 2.35; acc: 0.14
Batch: 480; loss: 2.35; acc: 0.12
Batch: 500; loss: 2.14; acc: 0.22
Batch: 520; loss: 2.26; acc: 0.17
Batch: 540; loss: 2.21; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.17
Batch: 580; loss: 2.15; acc: 0.2
Batch: 600; loss: 2.25; acc: 0.14
Batch: 620; loss: 2.12; acc: 0.19
Batch: 640; loss: 2.16; acc: 0.22
Batch: 660; loss: 2.22; acc: 0.14
Batch: 680; loss: 2.3; acc: 0.14
Batch: 700; loss: 2.26; acc: 0.11
Batch: 720; loss: 2.25; acc: 0.19
Batch: 740; loss: 2.24; acc: 0.17
Batch: 760; loss: 2.17; acc: 0.16
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.22; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.12; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.25
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1996495009987216; val_accuracy: 0.18829617834394904 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.12; acc: 0.3
Batch: 20; loss: 2.26; acc: 0.11
Batch: 40; loss: 2.08; acc: 0.28
Batch: 60; loss: 2.22; acc: 0.2
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.21; acc: 0.17
Batch: 120; loss: 2.2; acc: 0.14
Batch: 140; loss: 2.18; acc: 0.22
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.24; acc: 0.11
Batch: 200; loss: 2.25; acc: 0.19
Batch: 220; loss: 2.18; acc: 0.22
Batch: 240; loss: 2.18; acc: 0.22
Batch: 260; loss: 2.13; acc: 0.25
Batch: 280; loss: 2.23; acc: 0.25
Batch: 300; loss: 2.22; acc: 0.12
Batch: 320; loss: 2.07; acc: 0.28
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.18; acc: 0.2
Batch: 380; loss: 2.21; acc: 0.17
Batch: 400; loss: 2.28; acc: 0.14
Batch: 420; loss: 2.12; acc: 0.22
Batch: 440; loss: 2.37; acc: 0.09
Batch: 460; loss: 2.25; acc: 0.11
Batch: 480; loss: 2.19; acc: 0.19
Batch: 500; loss: 2.18; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.17
Batch: 540; loss: 2.26; acc: 0.14
Batch: 560; loss: 2.24; acc: 0.14
Batch: 580; loss: 2.22; acc: 0.17
Batch: 600; loss: 2.23; acc: 0.17
Batch: 620; loss: 2.26; acc: 0.12
Batch: 640; loss: 2.19; acc: 0.17
Batch: 660; loss: 2.19; acc: 0.2
Batch: 680; loss: 2.21; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.16
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.23; acc: 0.16
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.15; acc: 0.22
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.25
Batch: 120; loss: 2.07; acc: 0.28
Batch: 140; loss: 2.13; acc: 0.23
Val Epoch over. val_loss: 2.1993438483803134; val_accuracy: 0.1877985668789809 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 2.15; acc: 0.23
Batch: 60; loss: 2.26; acc: 0.19
Batch: 80; loss: 2.1; acc: 0.23
Batch: 100; loss: 2.12; acc: 0.3
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.24; acc: 0.19
Batch: 180; loss: 2.14; acc: 0.19
Batch: 200; loss: 2.19; acc: 0.14
Batch: 220; loss: 2.21; acc: 0.14
Batch: 240; loss: 2.31; acc: 0.14
Batch: 260; loss: 2.26; acc: 0.14
Batch: 280; loss: 2.24; acc: 0.17
Batch: 300; loss: 2.17; acc: 0.23
Batch: 320; loss: 2.25; acc: 0.16
Batch: 340; loss: 2.25; acc: 0.16
Batch: 360; loss: 2.22; acc: 0.22
Batch: 380; loss: 2.1; acc: 0.19
Batch: 400; loss: 2.19; acc: 0.16
Batch: 420; loss: 2.31; acc: 0.08
Batch: 440; loss: 2.37; acc: 0.11
Batch: 460; loss: 2.22; acc: 0.14
Batch: 480; loss: 2.19; acc: 0.16
Batch: 500; loss: 2.27; acc: 0.19
Batch: 520; loss: 2.34; acc: 0.17
Batch: 540; loss: 2.2; acc: 0.17
Batch: 560; loss: 2.25; acc: 0.19
Batch: 580; loss: 2.29; acc: 0.12
Batch: 600; loss: 2.17; acc: 0.22
Batch: 620; loss: 2.2; acc: 0.19
Batch: 640; loss: 2.24; acc: 0.23
Batch: 660; loss: 2.21; acc: 0.17
Batch: 680; loss: 2.15; acc: 0.25
Batch: 700; loss: 2.21; acc: 0.17
Batch: 720; loss: 2.23; acc: 0.11
Batch: 740; loss: 2.09; acc: 0.25
Batch: 760; loss: 2.3; acc: 0.09
Batch: 780; loss: 2.18; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.23
Batch: 120; loss: 2.07; acc: 0.28
Batch: 140; loss: 2.13; acc: 0.23
Val Epoch over. val_loss: 2.1992537610849756; val_accuracy: 0.18809713375796178 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.19
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.17
Batch: 60; loss: 2.18; acc: 0.17
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.11; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.22; acc: 0.17
Batch: 160; loss: 2.22; acc: 0.19
Batch: 180; loss: 2.26; acc: 0.11
Batch: 200; loss: 2.14; acc: 0.22
Batch: 220; loss: 2.1; acc: 0.22
Batch: 240; loss: 2.29; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.15; acc: 0.19
Batch: 300; loss: 2.22; acc: 0.12
Batch: 320; loss: 2.32; acc: 0.12
Batch: 340; loss: 2.22; acc: 0.17
Batch: 360; loss: 2.23; acc: 0.12
Batch: 380; loss: 2.2; acc: 0.23
Batch: 400; loss: 2.09; acc: 0.22
Batch: 420; loss: 2.25; acc: 0.12
Batch: 440; loss: 2.24; acc: 0.12
Batch: 460; loss: 2.23; acc: 0.16
Batch: 480; loss: 2.11; acc: 0.19
Batch: 500; loss: 2.22; acc: 0.12
Batch: 520; loss: 2.22; acc: 0.2
Batch: 540; loss: 2.2; acc: 0.19
Batch: 560; loss: 2.21; acc: 0.23
Batch: 580; loss: 2.24; acc: 0.11
Batch: 600; loss: 2.15; acc: 0.22
Batch: 620; loss: 2.18; acc: 0.16
Batch: 640; loss: 2.17; acc: 0.25
Batch: 660; loss: 2.22; acc: 0.22
Batch: 680; loss: 2.22; acc: 0.17
Batch: 700; loss: 2.17; acc: 0.2
Batch: 720; loss: 2.18; acc: 0.16
Batch: 740; loss: 2.26; acc: 0.11
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.19; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.2
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.11; acc: 0.3
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.23
Val Epoch over. val_loss: 2.199203939194892; val_accuracy: 0.1845143312101911 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.14
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.21; acc: 0.2
Batch: 80; loss: 2.2; acc: 0.23
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.24; acc: 0.17
Batch: 140; loss: 2.18; acc: 0.19
Batch: 160; loss: 2.18; acc: 0.27
Batch: 180; loss: 2.19; acc: 0.19
Batch: 200; loss: 2.27; acc: 0.16
Batch: 220; loss: 2.23; acc: 0.14
Batch: 240; loss: 2.21; acc: 0.19
Batch: 260; loss: 2.21; acc: 0.19
Batch: 280; loss: 2.26; acc: 0.16
Batch: 300; loss: 2.25; acc: 0.16
Batch: 320; loss: 2.15; acc: 0.22
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.09; acc: 0.2
Batch: 380; loss: 2.15; acc: 0.19
Batch: 400; loss: 2.29; acc: 0.08
Batch: 420; loss: 2.27; acc: 0.12
Batch: 440; loss: 2.11; acc: 0.23
Batch: 460; loss: 2.22; acc: 0.17
Batch: 480; loss: 2.14; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.22
Batch: 520; loss: 2.13; acc: 0.25
Batch: 540; loss: 2.2; acc: 0.2
Batch: 560; loss: 2.21; acc: 0.22
Batch: 580; loss: 2.21; acc: 0.19
Batch: 600; loss: 2.15; acc: 0.19
Batch: 620; loss: 2.2; acc: 0.23
Batch: 640; loss: 2.29; acc: 0.17
Batch: 660; loss: 2.22; acc: 0.17
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.12; acc: 0.33
Batch: 720; loss: 2.18; acc: 0.23
Batch: 740; loss: 2.11; acc: 0.22
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.25; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.2
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.12
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199495749868405; val_accuracy: 0.1872014331210191 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.32; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.17
Batch: 40; loss: 2.23; acc: 0.17
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.13; acc: 0.17
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.2; acc: 0.22
Batch: 180; loss: 2.19; acc: 0.14
Batch: 200; loss: 2.22; acc: 0.16
Batch: 220; loss: 2.14; acc: 0.25
Batch: 240; loss: 2.15; acc: 0.16
Batch: 260; loss: 2.23; acc: 0.16
Batch: 280; loss: 2.15; acc: 0.19
Batch: 300; loss: 2.19; acc: 0.23
Batch: 320; loss: 2.2; acc: 0.17
Batch: 340; loss: 2.23; acc: 0.16
Batch: 360; loss: 2.21; acc: 0.28
Batch: 380; loss: 2.26; acc: 0.17
Batch: 400; loss: 2.19; acc: 0.16
Batch: 420; loss: 2.29; acc: 0.11
Batch: 440; loss: 2.27; acc: 0.16
Batch: 460; loss: 2.25; acc: 0.09
Batch: 480; loss: 2.22; acc: 0.16
Batch: 500; loss: 2.22; acc: 0.2
Batch: 520; loss: 2.05; acc: 0.3
Batch: 540; loss: 2.11; acc: 0.22
Batch: 560; loss: 2.19; acc: 0.14
Batch: 580; loss: 2.2; acc: 0.17
Batch: 600; loss: 2.08; acc: 0.23
Batch: 620; loss: 2.2; acc: 0.19
Batch: 640; loss: 2.28; acc: 0.11
Batch: 660; loss: 2.15; acc: 0.14
Batch: 680; loss: 2.19; acc: 0.2
Batch: 700; loss: 2.18; acc: 0.16
Batch: 720; loss: 2.15; acc: 0.27
Batch: 740; loss: 2.26; acc: 0.16
Batch: 760; loss: 2.3; acc: 0.12
Batch: 780; loss: 2.27; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.06; acc: 0.31
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1996101285241973; val_accuracy: 0.18660429936305734 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.14
Batch: 20; loss: 2.18; acc: 0.22
Batch: 40; loss: 2.14; acc: 0.25
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.26; acc: 0.22
Batch: 100; loss: 2.18; acc: 0.25
Batch: 120; loss: 2.23; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.2
Batch: 160; loss: 2.18; acc: 0.22
Batch: 180; loss: 2.19; acc: 0.17
Batch: 200; loss: 2.23; acc: 0.22
Batch: 220; loss: 2.22; acc: 0.14
Batch: 240; loss: 2.18; acc: 0.19
Batch: 260; loss: 2.16; acc: 0.19
Batch: 280; loss: 2.22; acc: 0.09
Batch: 300; loss: 2.11; acc: 0.27
Batch: 320; loss: 2.1; acc: 0.23
Batch: 340; loss: 2.19; acc: 0.16
Batch: 360; loss: 2.26; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.17
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.23; acc: 0.19
Batch: 440; loss: 2.3; acc: 0.16
Batch: 460; loss: 2.17; acc: 0.2
Batch: 480; loss: 2.19; acc: 0.19
Batch: 500; loss: 2.2; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.17; acc: 0.2
Batch: 560; loss: 2.05; acc: 0.3
Batch: 580; loss: 2.22; acc: 0.19
Batch: 600; loss: 2.19; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.23
Batch: 640; loss: 2.26; acc: 0.19
Batch: 660; loss: 2.24; acc: 0.19
Batch: 680; loss: 2.14; acc: 0.17
Batch: 700; loss: 2.14; acc: 0.25
Batch: 720; loss: 2.14; acc: 0.27
Batch: 740; loss: 2.15; acc: 0.2
Batch: 760; loss: 2.29; acc: 0.08
Batch: 780; loss: 2.23; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.11; acc: 0.3
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1990613800704857; val_accuracy: 0.18600716560509553 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.24; acc: 0.16
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.24; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.2
Batch: 100; loss: 2.12; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.3; acc: 0.08
Batch: 160; loss: 2.13; acc: 0.2
Batch: 180; loss: 2.18; acc: 0.2
Batch: 200; loss: 2.19; acc: 0.17
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.12; acc: 0.22
Batch: 280; loss: 2.14; acc: 0.2
Batch: 300; loss: 2.26; acc: 0.2
Batch: 320; loss: 2.25; acc: 0.09
Batch: 340; loss: 2.15; acc: 0.25
Batch: 360; loss: 2.16; acc: 0.27
Batch: 380; loss: 2.17; acc: 0.16
Batch: 400; loss: 2.07; acc: 0.23
Batch: 420; loss: 2.25; acc: 0.12
Batch: 440; loss: 2.18; acc: 0.25
Batch: 460; loss: 2.27; acc: 0.14
Batch: 480; loss: 2.23; acc: 0.19
Batch: 500; loss: 2.15; acc: 0.27
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.26; acc: 0.14
Batch: 560; loss: 2.13; acc: 0.27
Batch: 580; loss: 2.21; acc: 0.14
Batch: 600; loss: 2.17; acc: 0.19
Batch: 620; loss: 2.19; acc: 0.19
Batch: 640; loss: 2.26; acc: 0.22
Batch: 660; loss: 2.2; acc: 0.16
Batch: 680; loss: 2.17; acc: 0.23
Batch: 700; loss: 2.22; acc: 0.19
Batch: 720; loss: 2.23; acc: 0.2
Batch: 740; loss: 2.23; acc: 0.14
Batch: 760; loss: 2.21; acc: 0.22
Batch: 780; loss: 2.21; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.12
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199549140444227; val_accuracy: 0.18680334394904458 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.14; acc: 0.31
Batch: 80; loss: 2.13; acc: 0.31
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.11; acc: 0.2
Batch: 140; loss: 2.18; acc: 0.19
Batch: 160; loss: 2.32; acc: 0.16
Batch: 180; loss: 2.17; acc: 0.2
Batch: 200; loss: 2.21; acc: 0.16
Batch: 220; loss: 2.2; acc: 0.19
Batch: 240; loss: 2.2; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.14
Batch: 280; loss: 2.21; acc: 0.22
Batch: 300; loss: 2.25; acc: 0.2
Batch: 320; loss: 2.28; acc: 0.12
Batch: 340; loss: 2.25; acc: 0.16
Batch: 360; loss: 2.31; acc: 0.14
Batch: 380; loss: 2.2; acc: 0.16
Batch: 400; loss: 2.19; acc: 0.23
Batch: 420; loss: 2.19; acc: 0.22
Batch: 440; loss: 2.15; acc: 0.19
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.21; acc: 0.2
Batch: 500; loss: 2.19; acc: 0.14
Batch: 520; loss: 2.24; acc: 0.19
Batch: 540; loss: 2.2; acc: 0.23
Batch: 560; loss: 2.09; acc: 0.3
Batch: 580; loss: 2.2; acc: 0.17
Batch: 600; loss: 2.3; acc: 0.14
Batch: 620; loss: 2.19; acc: 0.2
Batch: 640; loss: 2.2; acc: 0.17
Batch: 660; loss: 2.2; acc: 0.2
Batch: 680; loss: 2.22; acc: 0.12
Batch: 700; loss: 2.26; acc: 0.17
Batch: 720; loss: 2.12; acc: 0.27
Batch: 740; loss: 2.22; acc: 0.09
Batch: 760; loss: 2.23; acc: 0.17
Batch: 780; loss: 2.28; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199358628813628; val_accuracy: 0.1877985668789809 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.31; acc: 0.14
Batch: 20; loss: 2.32; acc: 0.14
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.12
Batch: 80; loss: 2.11; acc: 0.27
Batch: 100; loss: 2.11; acc: 0.22
Batch: 120; loss: 2.11; acc: 0.17
Batch: 140; loss: 2.33; acc: 0.14
Batch: 160; loss: 2.26; acc: 0.14
Batch: 180; loss: 2.21; acc: 0.17
Batch: 200; loss: 2.16; acc: 0.19
Batch: 220; loss: 2.24; acc: 0.2
Batch: 240; loss: 2.27; acc: 0.16
Batch: 260; loss: 2.27; acc: 0.06
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.18; acc: 0.16
Batch: 320; loss: 2.19; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.27
Batch: 360; loss: 2.35; acc: 0.11
Batch: 380; loss: 2.22; acc: 0.12
Batch: 400; loss: 2.16; acc: 0.19
Batch: 420; loss: 2.17; acc: 0.19
Batch: 440; loss: 2.17; acc: 0.19
Batch: 460; loss: 2.32; acc: 0.12
Batch: 480; loss: 2.26; acc: 0.17
Batch: 500; loss: 2.22; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.2
Batch: 540; loss: 2.24; acc: 0.19
Batch: 560; loss: 2.28; acc: 0.14
Batch: 580; loss: 2.24; acc: 0.14
Batch: 600; loss: 2.12; acc: 0.27
Batch: 620; loss: 2.21; acc: 0.25
Batch: 640; loss: 2.26; acc: 0.19
Batch: 660; loss: 2.33; acc: 0.16
Batch: 680; loss: 2.26; acc: 0.12
Batch: 700; loss: 2.23; acc: 0.11
Batch: 720; loss: 2.13; acc: 0.23
Batch: 740; loss: 2.2; acc: 0.2
Batch: 760; loss: 2.21; acc: 0.23
Batch: 780; loss: 2.23; acc: 0.23
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.28
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199446429112914; val_accuracy: 0.18799761146496816 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.09; acc: 0.22
Batch: 40; loss: 2.22; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.12
Batch: 80; loss: 2.19; acc: 0.11
Batch: 100; loss: 2.24; acc: 0.2
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.15; acc: 0.25
Batch: 160; loss: 2.26; acc: 0.2
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.22; acc: 0.2
Batch: 220; loss: 2.21; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.14
Batch: 260; loss: 2.18; acc: 0.19
Batch: 280; loss: 2.27; acc: 0.2
Batch: 300; loss: 2.3; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.22; acc: 0.19
Batch: 380; loss: 2.32; acc: 0.16
Batch: 400; loss: 2.19; acc: 0.23
Batch: 420; loss: 2.17; acc: 0.17
Batch: 440; loss: 2.21; acc: 0.23
Batch: 460; loss: 2.2; acc: 0.23
Batch: 480; loss: 2.22; acc: 0.19
Batch: 500; loss: 2.19; acc: 0.14
Batch: 520; loss: 2.23; acc: 0.19
Batch: 540; loss: 2.27; acc: 0.11
Batch: 560; loss: 2.15; acc: 0.16
Batch: 580; loss: 2.28; acc: 0.17
Batch: 600; loss: 2.13; acc: 0.19
Batch: 620; loss: 2.15; acc: 0.23
Batch: 640; loss: 2.21; acc: 0.19
Batch: 660; loss: 2.26; acc: 0.09
Batch: 680; loss: 2.25; acc: 0.16
Batch: 700; loss: 2.23; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.23
Batch: 740; loss: 2.24; acc: 0.25
Batch: 760; loss: 2.25; acc: 0.17
Batch: 780; loss: 2.19; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.25
Batch: 120; loss: 2.06; acc: 0.28
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1994903695051837; val_accuracy: 0.1884952229299363 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.2; acc: 0.22
Batch: 20; loss: 2.21; acc: 0.16
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.32; acc: 0.12
Batch: 80; loss: 2.26; acc: 0.14
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.19; acc: 0.12
Batch: 160; loss: 2.26; acc: 0.16
Batch: 180; loss: 2.29; acc: 0.12
Batch: 200; loss: 2.27; acc: 0.11
Batch: 220; loss: 2.2; acc: 0.19
Batch: 240; loss: 2.19; acc: 0.17
Batch: 260; loss: 2.24; acc: 0.17
Batch: 280; loss: 2.17; acc: 0.28
Batch: 300; loss: 2.19; acc: 0.06
Batch: 320; loss: 2.27; acc: 0.16
Batch: 340; loss: 2.31; acc: 0.19
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.26; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.14
Batch: 420; loss: 2.15; acc: 0.23
Batch: 440; loss: 2.23; acc: 0.16
Batch: 460; loss: 2.19; acc: 0.16
Batch: 480; loss: 2.28; acc: 0.16
Batch: 500; loss: 2.24; acc: 0.12
Batch: 520; loss: 2.28; acc: 0.17
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.2; acc: 0.22
Batch: 580; loss: 2.18; acc: 0.19
Batch: 600; loss: 2.25; acc: 0.09
Batch: 620; loss: 2.29; acc: 0.16
Batch: 640; loss: 2.17; acc: 0.16
Batch: 660; loss: 2.15; acc: 0.19
Batch: 680; loss: 2.18; acc: 0.22
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.19; acc: 0.17
Batch: 740; loss: 2.18; acc: 0.2
Batch: 760; loss: 2.22; acc: 0.16
Batch: 780; loss: 2.35; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199427636565676; val_accuracy: 0.18690286624203822 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.1; acc: 0.2
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.14
Batch: 100; loss: 2.21; acc: 0.17
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.26; acc: 0.2
Batch: 160; loss: 2.22; acc: 0.12
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.2; acc: 0.14
Batch: 220; loss: 2.18; acc: 0.16
Batch: 240; loss: 2.2; acc: 0.19
Batch: 260; loss: 2.2; acc: 0.14
Batch: 280; loss: 2.24; acc: 0.17
Batch: 300; loss: 2.16; acc: 0.2
Batch: 320; loss: 2.31; acc: 0.16
Batch: 340; loss: 2.15; acc: 0.23
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.23; acc: 0.12
Batch: 420; loss: 2.25; acc: 0.16
Batch: 440; loss: 2.22; acc: 0.19
Batch: 460; loss: 2.16; acc: 0.2
Batch: 480; loss: 2.18; acc: 0.17
Batch: 500; loss: 2.34; acc: 0.11
Batch: 520; loss: 2.25; acc: 0.12
Batch: 540; loss: 2.26; acc: 0.12
Batch: 560; loss: 2.2; acc: 0.2
Batch: 580; loss: 2.18; acc: 0.06
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.23; acc: 0.22
Batch: 640; loss: 2.16; acc: 0.17
Batch: 660; loss: 2.26; acc: 0.16
Batch: 680; loss: 2.23; acc: 0.22
Batch: 700; loss: 2.18; acc: 0.23
Batch: 720; loss: 2.21; acc: 0.11
Batch: 740; loss: 2.25; acc: 0.11
Batch: 760; loss: 2.16; acc: 0.22
Batch: 780; loss: 2.15; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.28
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199267114043995; val_accuracy: 0.18700238853503184 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.2; acc: 0.23
Batch: 40; loss: 2.24; acc: 0.09
Batch: 60; loss: 2.21; acc: 0.19
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.32; acc: 0.16
Batch: 120; loss: 2.23; acc: 0.2
Batch: 140; loss: 2.26; acc: 0.2
Batch: 160; loss: 2.16; acc: 0.27
Batch: 180; loss: 2.21; acc: 0.23
Batch: 200; loss: 2.2; acc: 0.17
Batch: 220; loss: 2.16; acc: 0.17
Batch: 240; loss: 2.14; acc: 0.17
Batch: 260; loss: 2.22; acc: 0.14
Batch: 280; loss: 2.16; acc: 0.22
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.19; acc: 0.16
Batch: 340; loss: 2.22; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.06
Batch: 380; loss: 2.15; acc: 0.17
Batch: 400; loss: 2.16; acc: 0.22
Batch: 420; loss: 2.21; acc: 0.19
Batch: 440; loss: 2.09; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.16
Batch: 480; loss: 2.22; acc: 0.17
Batch: 500; loss: 2.12; acc: 0.25
Batch: 520; loss: 2.19; acc: 0.16
Batch: 540; loss: 2.22; acc: 0.25
Batch: 560; loss: 2.08; acc: 0.23
Batch: 580; loss: 2.23; acc: 0.16
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.18; acc: 0.17
Batch: 640; loss: 2.2; acc: 0.16
Batch: 660; loss: 2.17; acc: 0.23
Batch: 680; loss: 2.09; acc: 0.23
Batch: 700; loss: 2.19; acc: 0.19
Batch: 720; loss: 2.21; acc: 0.17
Batch: 740; loss: 2.28; acc: 0.08
Batch: 760; loss: 2.14; acc: 0.23
Batch: 780; loss: 2.23; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.23
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199251108108812; val_accuracy: 0.18730095541401273 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.18; acc: 0.12
Batch: 20; loss: 2.2; acc: 0.16
Batch: 40; loss: 2.21; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.12
Batch: 80; loss: 2.19; acc: 0.22
Batch: 100; loss: 2.08; acc: 0.2
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.33; acc: 0.12
Batch: 180; loss: 2.16; acc: 0.23
Batch: 200; loss: 2.19; acc: 0.19
Batch: 220; loss: 2.34; acc: 0.09
Batch: 240; loss: 2.25; acc: 0.14
Batch: 260; loss: 2.23; acc: 0.16
Batch: 280; loss: 2.16; acc: 0.28
Batch: 300; loss: 2.12; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.17
Batch: 340; loss: 2.19; acc: 0.2
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.15; acc: 0.25
Batch: 400; loss: 2.32; acc: 0.09
Batch: 420; loss: 2.25; acc: 0.12
Batch: 440; loss: 2.11; acc: 0.33
Batch: 460; loss: 2.27; acc: 0.12
Batch: 480; loss: 2.12; acc: 0.22
Batch: 500; loss: 2.23; acc: 0.17
Batch: 520; loss: 2.25; acc: 0.19
Batch: 540; loss: 2.26; acc: 0.08
Batch: 560; loss: 2.21; acc: 0.19
Batch: 580; loss: 2.19; acc: 0.19
Batch: 600; loss: 2.14; acc: 0.25
Batch: 620; loss: 2.19; acc: 0.17
Batch: 640; loss: 2.13; acc: 0.2
Batch: 660; loss: 2.18; acc: 0.2
Batch: 680; loss: 2.16; acc: 0.27
Batch: 700; loss: 2.16; acc: 0.16
Batch: 720; loss: 2.33; acc: 0.09
Batch: 740; loss: 2.23; acc: 0.17
Batch: 760; loss: 2.14; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.11; acc: 0.3
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.31
Batch: 140; loss: 2.13; acc: 0.23
Val Epoch over. val_loss: 2.1989453491891267; val_accuracy: 0.18680334394904458 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.18; acc: 0.23
Batch: 40; loss: 2.13; acc: 0.28
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.2; acc: 0.27
Batch: 100; loss: 2.23; acc: 0.19
Batch: 120; loss: 2.26; acc: 0.08
Batch: 140; loss: 2.21; acc: 0.16
Batch: 160; loss: 2.2; acc: 0.22
Batch: 180; loss: 2.25; acc: 0.14
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.21; acc: 0.17
Batch: 240; loss: 2.18; acc: 0.28
Batch: 260; loss: 2.19; acc: 0.17
Batch: 280; loss: 2.25; acc: 0.14
Batch: 300; loss: 2.24; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.2
Batch: 340; loss: 2.23; acc: 0.16
Batch: 360; loss: 2.2; acc: 0.09
Batch: 380; loss: 2.22; acc: 0.16
Batch: 400; loss: 2.21; acc: 0.2
Batch: 420; loss: 2.24; acc: 0.16
Batch: 440; loss: 2.13; acc: 0.3
Batch: 460; loss: 2.28; acc: 0.08
Batch: 480; loss: 2.22; acc: 0.12
Batch: 500; loss: 2.18; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.25
Batch: 540; loss: 2.14; acc: 0.16
Batch: 560; loss: 2.18; acc: 0.14
Batch: 580; loss: 2.26; acc: 0.2
Batch: 600; loss: 2.26; acc: 0.09
Batch: 620; loss: 2.24; acc: 0.14
Batch: 640; loss: 2.27; acc: 0.11
Batch: 660; loss: 2.19; acc: 0.23
Batch: 680; loss: 2.15; acc: 0.23
Batch: 700; loss: 2.16; acc: 0.17
Batch: 720; loss: 2.24; acc: 0.12
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.11; acc: 0.25
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1992586843527047; val_accuracy: 0.1875 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.19; acc: 0.23
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.23; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.05
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.21; acc: 0.2
Batch: 140; loss: 2.18; acc: 0.2
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.19; acc: 0.19
Batch: 200; loss: 2.25; acc: 0.09
Batch: 220; loss: 2.24; acc: 0.11
Batch: 240; loss: 2.12; acc: 0.22
Batch: 260; loss: 2.15; acc: 0.22
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.24; acc: 0.11
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.08; acc: 0.17
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.11; acc: 0.27
Batch: 420; loss: 2.25; acc: 0.12
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.27; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.28; acc: 0.12
Batch: 520; loss: 2.27; acc: 0.08
Batch: 540; loss: 2.2; acc: 0.19
Batch: 560; loss: 2.28; acc: 0.2
Batch: 580; loss: 2.35; acc: 0.14
Batch: 600; loss: 2.28; acc: 0.19
Batch: 620; loss: 2.2; acc: 0.12
Batch: 640; loss: 2.19; acc: 0.22
Batch: 660; loss: 2.19; acc: 0.2
Batch: 680; loss: 2.2; acc: 0.17
Batch: 700; loss: 2.23; acc: 0.12
Batch: 720; loss: 2.22; acc: 0.2
Batch: 740; loss: 2.19; acc: 0.14
Batch: 760; loss: 2.25; acc: 0.12
Batch: 780; loss: 2.2; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199441460287495; val_accuracy: 0.18710191082802546 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.24; acc: 0.16
Batch: 20; loss: 2.2; acc: 0.2
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.25; acc: 0.25
Batch: 80; loss: 2.14; acc: 0.22
Batch: 100; loss: 2.2; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.17
Batch: 140; loss: 2.13; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.22
Batch: 180; loss: 2.22; acc: 0.12
Batch: 200; loss: 2.09; acc: 0.16
Batch: 220; loss: 2.25; acc: 0.19
Batch: 240; loss: 2.22; acc: 0.14
Batch: 260; loss: 2.12; acc: 0.25
Batch: 280; loss: 2.24; acc: 0.17
Batch: 300; loss: 2.2; acc: 0.2
Batch: 320; loss: 2.21; acc: 0.17
Batch: 340; loss: 2.25; acc: 0.12
Batch: 360; loss: 2.16; acc: 0.14
Batch: 380; loss: 2.25; acc: 0.12
Batch: 400; loss: 2.24; acc: 0.2
Batch: 420; loss: 2.2; acc: 0.2
Batch: 440; loss: 2.23; acc: 0.19
Batch: 460; loss: 2.22; acc: 0.19
Batch: 480; loss: 2.22; acc: 0.16
Batch: 500; loss: 2.25; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.22
Batch: 540; loss: 2.2; acc: 0.14
Batch: 560; loss: 2.33; acc: 0.2
Batch: 580; loss: 2.22; acc: 0.12
Batch: 600; loss: 2.07; acc: 0.25
Batch: 620; loss: 2.31; acc: 0.12
Batch: 640; loss: 2.22; acc: 0.2
Batch: 660; loss: 2.25; acc: 0.14
Batch: 680; loss: 2.31; acc: 0.06
Batch: 700; loss: 2.27; acc: 0.17
Batch: 720; loss: 2.2; acc: 0.2
Batch: 740; loss: 2.17; acc: 0.16
Batch: 760; loss: 2.23; acc: 0.19
Batch: 780; loss: 2.25; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.12
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.23
Val Epoch over. val_loss: 2.1993362599877035; val_accuracy: 0.18740047770700638 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.22; acc: 0.19
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.12; acc: 0.23
Batch: 60; loss: 2.2; acc: 0.27
Batch: 80; loss: 2.19; acc: 0.17
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.19
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.24; acc: 0.17
Batch: 220; loss: 2.23; acc: 0.22
Batch: 240; loss: 2.22; acc: 0.2
Batch: 260; loss: 2.25; acc: 0.2
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.34; acc: 0.11
Batch: 320; loss: 2.19; acc: 0.16
Batch: 340; loss: 2.19; acc: 0.16
Batch: 360; loss: 2.26; acc: 0.17
Batch: 380; loss: 2.18; acc: 0.14
Batch: 400; loss: 2.13; acc: 0.25
Batch: 420; loss: 2.23; acc: 0.16
Batch: 440; loss: 2.31; acc: 0.16
Batch: 460; loss: 2.24; acc: 0.16
Batch: 480; loss: 2.13; acc: 0.22
Batch: 500; loss: 2.2; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.12
Batch: 540; loss: 2.23; acc: 0.11
Batch: 560; loss: 2.26; acc: 0.16
Batch: 580; loss: 2.2; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.16
Batch: 620; loss: 2.23; acc: 0.11
Batch: 640; loss: 2.25; acc: 0.16
Batch: 660; loss: 2.22; acc: 0.17
Batch: 680; loss: 2.26; acc: 0.14
Batch: 700; loss: 2.3; acc: 0.14
Batch: 720; loss: 2.28; acc: 0.2
Batch: 740; loss: 2.15; acc: 0.2
Batch: 760; loss: 2.27; acc: 0.09
Batch: 780; loss: 2.23; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.23
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993771221987; val_accuracy: 0.1872014331210191 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.14
Batch: 20; loss: 2.22; acc: 0.16
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.28; acc: 0.19
Batch: 80; loss: 2.19; acc: 0.22
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.06; acc: 0.25
Batch: 140; loss: 2.22; acc: 0.19
Batch: 160; loss: 2.23; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.12
Batch: 200; loss: 2.27; acc: 0.14
Batch: 220; loss: 2.19; acc: 0.2
Batch: 240; loss: 2.22; acc: 0.17
Batch: 260; loss: 2.21; acc: 0.2
Batch: 280; loss: 2.13; acc: 0.23
Batch: 300; loss: 2.19; acc: 0.19
Batch: 320; loss: 2.07; acc: 0.31
Batch: 340; loss: 2.2; acc: 0.16
Batch: 360; loss: 2.16; acc: 0.23
Batch: 380; loss: 2.24; acc: 0.09
Batch: 400; loss: 2.17; acc: 0.16
Batch: 420; loss: 2.12; acc: 0.27
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.25; acc: 0.2
Batch: 480; loss: 2.37; acc: 0.12
Batch: 500; loss: 2.33; acc: 0.09
Batch: 520; loss: 2.2; acc: 0.16
Batch: 540; loss: 2.18; acc: 0.19
Batch: 560; loss: 2.37; acc: 0.09
Batch: 580; loss: 2.18; acc: 0.14
Batch: 600; loss: 2.24; acc: 0.17
Batch: 620; loss: 2.26; acc: 0.12
Batch: 640; loss: 2.2; acc: 0.25
Batch: 660; loss: 2.29; acc: 0.19
Batch: 680; loss: 2.2; acc: 0.16
Batch: 700; loss: 2.29; acc: 0.14
Batch: 720; loss: 2.21; acc: 0.11
Batch: 740; loss: 2.21; acc: 0.17
Batch: 760; loss: 2.22; acc: 0.16
Batch: 780; loss: 2.19; acc: 0.2
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.19937787390059; val_accuracy: 0.18759952229299362 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.2; acc: 0.19
Batch: 20; loss: 2.29; acc: 0.17
Batch: 40; loss: 2.27; acc: 0.19
Batch: 60; loss: 2.17; acc: 0.16
Batch: 80; loss: 2.21; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.2
Batch: 120; loss: 2.22; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.19; acc: 0.14
Batch: 180; loss: 2.15; acc: 0.2
Batch: 200; loss: 2.26; acc: 0.16
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.19; acc: 0.17
Batch: 260; loss: 2.18; acc: 0.17
Batch: 280; loss: 2.21; acc: 0.23
Batch: 300; loss: 2.13; acc: 0.22
Batch: 320; loss: 2.16; acc: 0.14
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.25; acc: 0.22
Batch: 380; loss: 2.23; acc: 0.16
Batch: 400; loss: 2.23; acc: 0.19
Batch: 420; loss: 2.25; acc: 0.22
Batch: 440; loss: 2.35; acc: 0.14
Batch: 460; loss: 2.33; acc: 0.16
Batch: 480; loss: 2.23; acc: 0.16
Batch: 500; loss: 2.21; acc: 0.16
Batch: 520; loss: 2.34; acc: 0.14
Batch: 540; loss: 2.14; acc: 0.23
Batch: 560; loss: 2.29; acc: 0.17
Batch: 580; loss: 2.15; acc: 0.19
Batch: 600; loss: 2.29; acc: 0.09
Batch: 620; loss: 2.11; acc: 0.25
Batch: 640; loss: 2.24; acc: 0.16
Batch: 660; loss: 2.19; acc: 0.14
Batch: 680; loss: 2.23; acc: 0.17
Batch: 700; loss: 2.14; acc: 0.2
Batch: 720; loss: 2.29; acc: 0.09
Batch: 740; loss: 2.15; acc: 0.14
Batch: 760; loss: 2.25; acc: 0.16
Batch: 780; loss: 2.23; acc: 0.09
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.19937047077592; val_accuracy: 0.1872014331210191 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.29; acc: 0.11
Batch: 20; loss: 2.24; acc: 0.11
Batch: 40; loss: 2.17; acc: 0.23
Batch: 60; loss: 2.18; acc: 0.22
Batch: 80; loss: 2.34; acc: 0.08
Batch: 100; loss: 2.24; acc: 0.16
Batch: 120; loss: 2.19; acc: 0.16
Batch: 140; loss: 2.17; acc: 0.23
Batch: 160; loss: 2.21; acc: 0.22
Batch: 180; loss: 2.17; acc: 0.19
Batch: 200; loss: 2.27; acc: 0.16
Batch: 220; loss: 2.1; acc: 0.22
Batch: 240; loss: 2.26; acc: 0.19
Batch: 260; loss: 2.12; acc: 0.22
Batch: 280; loss: 2.19; acc: 0.19
Batch: 300; loss: 2.23; acc: 0.14
Batch: 320; loss: 2.19; acc: 0.22
Batch: 340; loss: 2.27; acc: 0.2
Batch: 360; loss: 2.23; acc: 0.14
Batch: 380; loss: 2.26; acc: 0.17
Batch: 400; loss: 2.13; acc: 0.22
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.26; acc: 0.19
Batch: 460; loss: 2.18; acc: 0.17
Batch: 480; loss: 2.24; acc: 0.11
Batch: 500; loss: 2.32; acc: 0.12
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.19; acc: 0.2
Batch: 560; loss: 2.32; acc: 0.14
Batch: 580; loss: 2.23; acc: 0.17
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.18; acc: 0.17
Batch: 640; loss: 2.28; acc: 0.14
Batch: 660; loss: 2.14; acc: 0.19
Batch: 680; loss: 2.24; acc: 0.16
Batch: 700; loss: 2.25; acc: 0.11
Batch: 720; loss: 2.23; acc: 0.17
Batch: 740; loss: 2.22; acc: 0.16
Batch: 760; loss: 2.22; acc: 0.25
Batch: 780; loss: 2.17; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993463221628953; val_accuracy: 0.18740047770700638 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.2; acc: 0.14
Batch: 20; loss: 2.22; acc: 0.16
Batch: 40; loss: 2.32; acc: 0.16
Batch: 60; loss: 2.19; acc: 0.17
Batch: 80; loss: 2.19; acc: 0.12
Batch: 100; loss: 2.22; acc: 0.19
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.18; acc: 0.23
Batch: 160; loss: 2.25; acc: 0.14
Batch: 180; loss: 2.21; acc: 0.19
Batch: 200; loss: 2.13; acc: 0.28
Batch: 220; loss: 2.13; acc: 0.23
Batch: 240; loss: 2.29; acc: 0.06
Batch: 260; loss: 2.15; acc: 0.25
Batch: 280; loss: 2.16; acc: 0.22
Batch: 300; loss: 2.24; acc: 0.17
Batch: 320; loss: 2.28; acc: 0.11
Batch: 340; loss: 2.32; acc: 0.09
Batch: 360; loss: 2.12; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.16
Batch: 400; loss: 2.31; acc: 0.06
Batch: 420; loss: 2.2; acc: 0.16
Batch: 440; loss: 2.23; acc: 0.19
Batch: 460; loss: 2.28; acc: 0.14
Batch: 480; loss: 2.25; acc: 0.16
Batch: 500; loss: 2.26; acc: 0.12
Batch: 520; loss: 2.19; acc: 0.14
Batch: 540; loss: 2.27; acc: 0.11
Batch: 560; loss: 2.27; acc: 0.2
Batch: 580; loss: 2.27; acc: 0.16
Batch: 600; loss: 2.22; acc: 0.16
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.17; acc: 0.19
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.37; acc: 0.09
Batch: 700; loss: 2.28; acc: 0.16
Batch: 720; loss: 2.31; acc: 0.12
Batch: 740; loss: 2.18; acc: 0.19
Batch: 760; loss: 2.09; acc: 0.28
Batch: 780; loss: 2.11; acc: 0.25
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1994183200180153; val_accuracy: 0.18730095541401273 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.14; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.2
Batch: 60; loss: 2.11; acc: 0.27
Batch: 80; loss: 2.24; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.22; acc: 0.14
Batch: 180; loss: 2.19; acc: 0.16
Batch: 200; loss: 2.33; acc: 0.14
Batch: 220; loss: 2.21; acc: 0.17
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.16
Batch: 300; loss: 2.24; acc: 0.16
Batch: 320; loss: 2.2; acc: 0.17
Batch: 340; loss: 2.18; acc: 0.22
Batch: 360; loss: 2.22; acc: 0.14
Batch: 380; loss: 2.21; acc: 0.19
Batch: 400; loss: 2.12; acc: 0.2
Batch: 420; loss: 2.26; acc: 0.16
Batch: 440; loss: 2.22; acc: 0.16
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.1; acc: 0.28
Batch: 500; loss: 2.19; acc: 0.19
Batch: 520; loss: 2.24; acc: 0.22
Batch: 540; loss: 2.17; acc: 0.17
Batch: 560; loss: 2.22; acc: 0.16
Batch: 580; loss: 2.26; acc: 0.16
Batch: 600; loss: 2.2; acc: 0.17
Batch: 620; loss: 2.2; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 2.25; acc: 0.12
Batch: 680; loss: 2.19; acc: 0.22
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.31; acc: 0.12
Batch: 740; loss: 2.2; acc: 0.2
Batch: 760; loss: 2.18; acc: 0.27
Batch: 780; loss: 2.19; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1992942816132954; val_accuracy: 0.1875 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.2; acc: 0.14
Batch: 40; loss: 2.16; acc: 0.17
Batch: 60; loss: 2.26; acc: 0.16
Batch: 80; loss: 2.19; acc: 0.19
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.17; acc: 0.16
Batch: 140; loss: 2.2; acc: 0.12
Batch: 160; loss: 2.16; acc: 0.2
Batch: 180; loss: 2.22; acc: 0.09
Batch: 200; loss: 2.21; acc: 0.14
Batch: 220; loss: 2.2; acc: 0.12
Batch: 240; loss: 2.19; acc: 0.16
Batch: 260; loss: 2.27; acc: 0.22
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.22; acc: 0.12
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.21; acc: 0.19
Batch: 360; loss: 2.25; acc: 0.11
Batch: 380; loss: 2.12; acc: 0.23
Batch: 400; loss: 2.16; acc: 0.2
Batch: 420; loss: 2.24; acc: 0.16
Batch: 440; loss: 2.2; acc: 0.17
Batch: 460; loss: 2.21; acc: 0.19
Batch: 480; loss: 2.26; acc: 0.14
Batch: 500; loss: 2.26; acc: 0.17
Batch: 520; loss: 2.25; acc: 0.17
Batch: 540; loss: 2.2; acc: 0.19
Batch: 560; loss: 2.2; acc: 0.16
Batch: 580; loss: 2.18; acc: 0.2
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.22; acc: 0.14
Batch: 640; loss: 2.07; acc: 0.3
Batch: 660; loss: 2.15; acc: 0.2
Batch: 680; loss: 2.24; acc: 0.2
Batch: 700; loss: 2.27; acc: 0.17
Batch: 720; loss: 2.16; acc: 0.17
Batch: 740; loss: 2.2; acc: 0.16
Batch: 760; loss: 2.28; acc: 0.16
Batch: 780; loss: 2.25; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199291457036498; val_accuracy: 0.18710191082802546 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.2
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.27; acc: 0.2
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.17; acc: 0.23
Batch: 140; loss: 2.22; acc: 0.2
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.24; acc: 0.12
Batch: 200; loss: 2.17; acc: 0.14
Batch: 220; loss: 2.16; acc: 0.17
Batch: 240; loss: 2.22; acc: 0.19
Batch: 260; loss: 2.24; acc: 0.22
Batch: 280; loss: 2.22; acc: 0.16
Batch: 300; loss: 2.17; acc: 0.19
Batch: 320; loss: 2.26; acc: 0.11
Batch: 340; loss: 2.21; acc: 0.19
Batch: 360; loss: 2.22; acc: 0.14
Batch: 380; loss: 2.17; acc: 0.25
Batch: 400; loss: 2.27; acc: 0.17
Batch: 420; loss: 2.19; acc: 0.23
Batch: 440; loss: 2.27; acc: 0.11
Batch: 460; loss: 2.15; acc: 0.19
Batch: 480; loss: 2.24; acc: 0.16
Batch: 500; loss: 2.29; acc: 0.16
Batch: 520; loss: 2.25; acc: 0.17
Batch: 540; loss: 2.31; acc: 0.16
Batch: 560; loss: 2.26; acc: 0.11
Batch: 580; loss: 2.12; acc: 0.34
Batch: 600; loss: 2.16; acc: 0.22
Batch: 620; loss: 2.23; acc: 0.16
Batch: 640; loss: 2.27; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.16
Batch: 720; loss: 2.21; acc: 0.16
Batch: 740; loss: 2.17; acc: 0.16
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.24; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993501808992617; val_accuracy: 0.18730095541401273 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.22; acc: 0.17
Batch: 20; loss: 2.18; acc: 0.22
Batch: 40; loss: 2.25; acc: 0.12
Batch: 60; loss: 2.14; acc: 0.2
Batch: 80; loss: 2.21; acc: 0.17
Batch: 100; loss: 2.27; acc: 0.08
Batch: 120; loss: 2.21; acc: 0.25
Batch: 140; loss: 2.14; acc: 0.17
Batch: 160; loss: 2.23; acc: 0.17
Batch: 180; loss: 2.28; acc: 0.09
Batch: 200; loss: 2.16; acc: 0.16
Batch: 220; loss: 2.11; acc: 0.2
Batch: 240; loss: 2.19; acc: 0.17
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.23; acc: 0.19
Batch: 300; loss: 2.22; acc: 0.16
Batch: 320; loss: 2.26; acc: 0.12
Batch: 340; loss: 2.25; acc: 0.19
Batch: 360; loss: 2.32; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.09
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.22; acc: 0.12
Batch: 440; loss: 2.18; acc: 0.14
Batch: 460; loss: 2.06; acc: 0.23
Batch: 480; loss: 2.18; acc: 0.23
Batch: 500; loss: 2.33; acc: 0.06
Batch: 520; loss: 2.18; acc: 0.16
Batch: 540; loss: 2.17; acc: 0.2
Batch: 560; loss: 2.27; acc: 0.19
Batch: 580; loss: 2.06; acc: 0.31
Batch: 600; loss: 2.23; acc: 0.17
Batch: 620; loss: 2.22; acc: 0.12
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.24; acc: 0.19
Batch: 680; loss: 2.19; acc: 0.22
Batch: 700; loss: 2.23; acc: 0.17
Batch: 720; loss: 2.13; acc: 0.27
Batch: 740; loss: 2.19; acc: 0.23
Batch: 760; loss: 2.27; acc: 0.16
Batch: 780; loss: 2.21; acc: 0.23
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993019292309026; val_accuracy: 0.18710191082802546 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.14; acc: 0.19
Batch: 20; loss: 2.26; acc: 0.19
Batch: 40; loss: 2.17; acc: 0.2
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.23; acc: 0.17
Batch: 100; loss: 2.27; acc: 0.16
Batch: 120; loss: 2.21; acc: 0.17
Batch: 140; loss: 2.19; acc: 0.25
Batch: 160; loss: 2.24; acc: 0.14
Batch: 180; loss: 2.25; acc: 0.12
Batch: 200; loss: 2.15; acc: 0.28
Batch: 220; loss: 2.21; acc: 0.25
Batch: 240; loss: 2.2; acc: 0.17
Batch: 260; loss: 2.19; acc: 0.19
Batch: 280; loss: 2.19; acc: 0.2
Batch: 300; loss: 2.22; acc: 0.14
Batch: 320; loss: 2.26; acc: 0.14
Batch: 340; loss: 2.19; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.22; acc: 0.17
Batch: 400; loss: 2.27; acc: 0.14
Batch: 420; loss: 2.21; acc: 0.16
Batch: 440; loss: 2.26; acc: 0.19
Batch: 460; loss: 2.19; acc: 0.16
Batch: 480; loss: 2.2; acc: 0.23
Batch: 500; loss: 2.18; acc: 0.16
Batch: 520; loss: 2.2; acc: 0.25
Batch: 540; loss: 2.23; acc: 0.19
Batch: 560; loss: 2.26; acc: 0.14
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.19; acc: 0.22
Batch: 620; loss: 2.2; acc: 0.19
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 2.24; acc: 0.19
Batch: 680; loss: 2.25; acc: 0.14
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.16; acc: 0.16
Batch: 740; loss: 2.31; acc: 0.11
Batch: 760; loss: 2.21; acc: 0.19
Batch: 780; loss: 2.16; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1994249835895125; val_accuracy: 0.18700238853503184 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.17
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.22; acc: 0.09
Batch: 60; loss: 2.11; acc: 0.28
Batch: 80; loss: 2.19; acc: 0.17
Batch: 100; loss: 2.18; acc: 0.2
Batch: 120; loss: 2.17; acc: 0.25
Batch: 140; loss: 2.16; acc: 0.22
Batch: 160; loss: 2.31; acc: 0.12
Batch: 180; loss: 2.09; acc: 0.28
Batch: 200; loss: 2.2; acc: 0.19
Batch: 220; loss: 2.35; acc: 0.09
Batch: 240; loss: 2.26; acc: 0.2
Batch: 260; loss: 2.22; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.19
Batch: 300; loss: 2.27; acc: 0.17
Batch: 320; loss: 2.22; acc: 0.22
Batch: 340; loss: 2.21; acc: 0.2
Batch: 360; loss: 2.23; acc: 0.19
Batch: 380; loss: 2.21; acc: 0.14
Batch: 400; loss: 2.2; acc: 0.11
Batch: 420; loss: 2.16; acc: 0.17
Batch: 440; loss: 2.23; acc: 0.11
Batch: 460; loss: 2.26; acc: 0.11
Batch: 480; loss: 2.13; acc: 0.2
Batch: 500; loss: 2.19; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.16
Batch: 540; loss: 2.31; acc: 0.14
Batch: 560; loss: 2.17; acc: 0.2
Batch: 580; loss: 2.23; acc: 0.12
Batch: 600; loss: 2.23; acc: 0.12
Batch: 620; loss: 2.2; acc: 0.14
Batch: 640; loss: 2.18; acc: 0.22
Batch: 660; loss: 2.26; acc: 0.12
Batch: 680; loss: 2.16; acc: 0.25
Batch: 700; loss: 2.14; acc: 0.14
Batch: 720; loss: 2.22; acc: 0.2
Batch: 740; loss: 2.25; acc: 0.12
Batch: 760; loss: 2.18; acc: 0.17
Batch: 780; loss: 2.23; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993861471771434; val_accuracy: 0.18690286624203822 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.21; acc: 0.16
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.19; acc: 0.25
Batch: 100; loss: 2.19; acc: 0.23
Batch: 120; loss: 2.24; acc: 0.09
Batch: 140; loss: 2.13; acc: 0.23
Batch: 160; loss: 2.18; acc: 0.14
Batch: 180; loss: 2.24; acc: 0.17
Batch: 200; loss: 2.25; acc: 0.23
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.23; acc: 0.16
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.22; acc: 0.16
Batch: 300; loss: 2.22; acc: 0.2
Batch: 320; loss: 2.26; acc: 0.11
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.16
Batch: 380; loss: 2.21; acc: 0.12
Batch: 400; loss: 2.2; acc: 0.14
Batch: 420; loss: 2.26; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.19
Batch: 460; loss: 2.26; acc: 0.19
Batch: 480; loss: 2.24; acc: 0.16
Batch: 500; loss: 2.21; acc: 0.09
Batch: 520; loss: 2.24; acc: 0.14
Batch: 540; loss: 2.24; acc: 0.16
Batch: 560; loss: 2.18; acc: 0.19
Batch: 580; loss: 2.23; acc: 0.16
Batch: 600; loss: 2.22; acc: 0.17
Batch: 620; loss: 2.3; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.3
Batch: 660; loss: 2.27; acc: 0.16
Batch: 680; loss: 2.2; acc: 0.2
Batch: 700; loss: 2.16; acc: 0.16
Batch: 720; loss: 2.12; acc: 0.22
Batch: 740; loss: 2.29; acc: 0.14
Batch: 760; loss: 2.24; acc: 0.16
Batch: 780; loss: 2.18; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993705679656594; val_accuracy: 0.1872014331210191 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.24; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.23
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.14; acc: 0.2
Batch: 80; loss: 2.2; acc: 0.22
Batch: 100; loss: 2.24; acc: 0.16
Batch: 120; loss: 2.18; acc: 0.2
Batch: 140; loss: 2.14; acc: 0.22
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.27; acc: 0.11
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.32; acc: 0.12
Batch: 240; loss: 2.17; acc: 0.2
Batch: 260; loss: 2.33; acc: 0.09
Batch: 280; loss: 2.18; acc: 0.16
Batch: 300; loss: 2.25; acc: 0.14
Batch: 320; loss: 2.18; acc: 0.17
Batch: 340; loss: 2.2; acc: 0.17
Batch: 360; loss: 2.2; acc: 0.22
Batch: 380; loss: 2.14; acc: 0.12
Batch: 400; loss: 2.17; acc: 0.2
Batch: 420; loss: 2.38; acc: 0.05
Batch: 440; loss: 2.16; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.17
Batch: 480; loss: 2.24; acc: 0.16
Batch: 500; loss: 2.2; acc: 0.11
Batch: 520; loss: 2.13; acc: 0.22
Batch: 540; loss: 2.3; acc: 0.08
Batch: 560; loss: 2.27; acc: 0.2
Batch: 580; loss: 2.17; acc: 0.2
Batch: 600; loss: 2.19; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.27; acc: 0.16
Batch: 660; loss: 2.21; acc: 0.17
Batch: 680; loss: 2.15; acc: 0.25
Batch: 700; loss: 2.18; acc: 0.16
Batch: 720; loss: 2.15; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.16
Batch: 760; loss: 2.25; acc: 0.16
Batch: 780; loss: 2.29; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993540548215247; val_accuracy: 0.18700238853503184 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.27; acc: 0.11
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.12; acc: 0.19
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.21; acc: 0.14
Batch: 140; loss: 2.27; acc: 0.22
Batch: 160; loss: 2.13; acc: 0.27
Batch: 180; loss: 2.13; acc: 0.2
Batch: 200; loss: 2.23; acc: 0.14
Batch: 220; loss: 2.26; acc: 0.12
Batch: 240; loss: 2.22; acc: 0.12
Batch: 260; loss: 2.18; acc: 0.17
Batch: 280; loss: 2.27; acc: 0.19
Batch: 300; loss: 2.18; acc: 0.19
Batch: 320; loss: 2.18; acc: 0.2
Batch: 340; loss: 2.26; acc: 0.17
Batch: 360; loss: 2.15; acc: 0.28
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.29; acc: 0.14
Batch: 420; loss: 2.14; acc: 0.22
Batch: 440; loss: 2.25; acc: 0.12
Batch: 460; loss: 2.24; acc: 0.14
Batch: 480; loss: 2.19; acc: 0.17
Batch: 500; loss: 2.15; acc: 0.2
Batch: 520; loss: 2.23; acc: 0.12
Batch: 540; loss: 2.16; acc: 0.22
Batch: 560; loss: 2.23; acc: 0.16
Batch: 580; loss: 2.14; acc: 0.2
Batch: 600; loss: 2.2; acc: 0.09
Batch: 620; loss: 2.21; acc: 0.19
Batch: 640; loss: 2.13; acc: 0.25
Batch: 660; loss: 2.14; acc: 0.22
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.16; acc: 0.25
Batch: 740; loss: 2.2; acc: 0.2
Batch: 760; loss: 2.28; acc: 0.14
Batch: 780; loss: 2.2; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993615475429853; val_accuracy: 0.18710191082802546 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.22; acc: 0.2
Batch: 20; loss: 2.09; acc: 0.23
Batch: 40; loss: 2.25; acc: 0.12
Batch: 60; loss: 2.23; acc: 0.17
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.13; acc: 0.25
Batch: 140; loss: 2.27; acc: 0.09
Batch: 160; loss: 2.19; acc: 0.2
Batch: 180; loss: 2.16; acc: 0.17
Batch: 200; loss: 2.15; acc: 0.23
Batch: 220; loss: 2.11; acc: 0.3
Batch: 240; loss: 2.26; acc: 0.19
Batch: 260; loss: 2.2; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.25
Batch: 300; loss: 2.17; acc: 0.25
Batch: 320; loss: 2.27; acc: 0.12
Batch: 340; loss: 2.13; acc: 0.3
Batch: 360; loss: 2.14; acc: 0.23
Batch: 380; loss: 2.13; acc: 0.2
Batch: 400; loss: 2.2; acc: 0.25
Batch: 420; loss: 2.21; acc: 0.12
Batch: 440; loss: 2.28; acc: 0.12
Batch: 460; loss: 2.21; acc: 0.14
Batch: 480; loss: 2.18; acc: 0.22
Batch: 500; loss: 2.23; acc: 0.14
Batch: 520; loss: 2.18; acc: 0.19
Batch: 540; loss: 2.21; acc: 0.16
Batch: 560; loss: 2.32; acc: 0.09
Batch: 580; loss: 2.26; acc: 0.22
Batch: 600; loss: 2.27; acc: 0.17
Batch: 620; loss: 2.19; acc: 0.22
Batch: 640; loss: 2.25; acc: 0.2
Batch: 660; loss: 2.14; acc: 0.23
Batch: 680; loss: 2.27; acc: 0.17
Batch: 700; loss: 2.32; acc: 0.12
Batch: 720; loss: 2.29; acc: 0.12
Batch: 740; loss: 2.2; acc: 0.12
Batch: 760; loss: 2.25; acc: 0.16
Batch: 780; loss: 2.24; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993474125102828; val_accuracy: 0.18730095541401273 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.19; acc: 0.12
Batch: 20; loss: 2.18; acc: 0.2
Batch: 40; loss: 2.25; acc: 0.11
Batch: 60; loss: 2.23; acc: 0.2
Batch: 80; loss: 2.2; acc: 0.23
Batch: 100; loss: 2.31; acc: 0.09
Batch: 120; loss: 2.22; acc: 0.16
Batch: 140; loss: 2.16; acc: 0.22
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.18; acc: 0.17
Batch: 200; loss: 2.12; acc: 0.25
Batch: 220; loss: 2.21; acc: 0.12
Batch: 240; loss: 2.22; acc: 0.17
Batch: 260; loss: 2.25; acc: 0.17
Batch: 280; loss: 2.19; acc: 0.14
Batch: 300; loss: 2.21; acc: 0.17
Batch: 320; loss: 2.22; acc: 0.16
Batch: 340; loss: 2.22; acc: 0.14
Batch: 360; loss: 2.26; acc: 0.19
Batch: 380; loss: 2.23; acc: 0.2
Batch: 400; loss: 2.23; acc: 0.14
Batch: 420; loss: 2.21; acc: 0.16
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.25; acc: 0.12
Batch: 480; loss: 2.24; acc: 0.19
Batch: 500; loss: 2.28; acc: 0.16
Batch: 520; loss: 2.24; acc: 0.2
Batch: 540; loss: 2.25; acc: 0.16
Batch: 560; loss: 2.32; acc: 0.09
Batch: 580; loss: 2.24; acc: 0.12
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.17
Batch: 640; loss: 2.19; acc: 0.19
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.18; acc: 0.27
Batch: 700; loss: 2.18; acc: 0.22
Batch: 720; loss: 2.24; acc: 0.2
Batch: 740; loss: 2.22; acc: 0.16
Batch: 760; loss: 2.25; acc: 0.12
Batch: 780; loss: 2.26; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.199352675942099; val_accuracy: 0.18710191082802546 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.18; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.32; acc: 0.17
Batch: 60; loss: 2.28; acc: 0.09
Batch: 80; loss: 2.15; acc: 0.19
Batch: 100; loss: 2.27; acc: 0.17
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.18; acc: 0.2
Batch: 160; loss: 2.26; acc: 0.16
Batch: 180; loss: 2.26; acc: 0.14
Batch: 200; loss: 2.32; acc: 0.19
Batch: 220; loss: 2.12; acc: 0.27
Batch: 240; loss: 2.22; acc: 0.22
Batch: 260; loss: 2.24; acc: 0.19
Batch: 280; loss: 2.24; acc: 0.17
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.28; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.2
Batch: 360; loss: 2.16; acc: 0.19
Batch: 380; loss: 2.25; acc: 0.14
Batch: 400; loss: 2.18; acc: 0.22
Batch: 420; loss: 2.16; acc: 0.14
Batch: 440; loss: 2.22; acc: 0.11
Batch: 460; loss: 2.18; acc: 0.22
Batch: 480; loss: 2.27; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.2
Batch: 520; loss: 2.31; acc: 0.09
Batch: 540; loss: 2.19; acc: 0.14
Batch: 560; loss: 2.24; acc: 0.16
Batch: 580; loss: 2.21; acc: 0.22
Batch: 600; loss: 2.16; acc: 0.19
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.21; acc: 0.22
Batch: 660; loss: 2.28; acc: 0.11
Batch: 680; loss: 2.07; acc: 0.28
Batch: 700; loss: 2.33; acc: 0.16
Batch: 720; loss: 2.34; acc: 0.09
Batch: 740; loss: 2.15; acc: 0.2
Batch: 760; loss: 2.34; acc: 0.12
Batch: 780; loss: 2.31; acc: 0.09
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993364604415406; val_accuracy: 0.18740047770700638 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.14; acc: 0.23
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.09
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.01; acc: 0.27
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.26; acc: 0.17
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.22; acc: 0.14
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.2; acc: 0.23
Batch: 240; loss: 2.2; acc: 0.25
Batch: 260; loss: 2.14; acc: 0.23
Batch: 280; loss: 2.19; acc: 0.19
Batch: 300; loss: 2.18; acc: 0.12
Batch: 320; loss: 2.15; acc: 0.28
Batch: 340; loss: 2.15; acc: 0.2
Batch: 360; loss: 2.17; acc: 0.19
Batch: 380; loss: 2.12; acc: 0.25
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.24; acc: 0.19
Batch: 440; loss: 2.22; acc: 0.17
Batch: 460; loss: 2.32; acc: 0.14
Batch: 480; loss: 2.2; acc: 0.23
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.21; acc: 0.22
Batch: 540; loss: 2.24; acc: 0.16
Batch: 560; loss: 2.26; acc: 0.14
Batch: 580; loss: 2.23; acc: 0.19
Batch: 600; loss: 2.19; acc: 0.23
Batch: 620; loss: 2.25; acc: 0.2
Batch: 640; loss: 2.21; acc: 0.17
Batch: 660; loss: 2.21; acc: 0.14
Batch: 680; loss: 2.18; acc: 0.25
Batch: 700; loss: 2.22; acc: 0.14
Batch: 720; loss: 2.22; acc: 0.14
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.2; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.07; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993333518884746; val_accuracy: 0.18740047770700638 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.23; acc: 0.19
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.23; acc: 0.2
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.21; acc: 0.19
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.19
Batch: 140; loss: 2.11; acc: 0.3
Batch: 160; loss: 2.2; acc: 0.12
Batch: 180; loss: 2.24; acc: 0.22
Batch: 200; loss: 2.16; acc: 0.2
Batch: 220; loss: 2.34; acc: 0.09
Batch: 240; loss: 2.14; acc: 0.23
Batch: 260; loss: 2.19; acc: 0.2
Batch: 280; loss: 2.14; acc: 0.28
Batch: 300; loss: 2.23; acc: 0.16
Batch: 320; loss: 2.22; acc: 0.22
Batch: 340; loss: 2.19; acc: 0.22
Batch: 360; loss: 2.24; acc: 0.2
Batch: 380; loss: 2.22; acc: 0.2
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.21; acc: 0.2
Batch: 440; loss: 2.21; acc: 0.16
Batch: 460; loss: 2.18; acc: 0.2
Batch: 480; loss: 2.17; acc: 0.2
Batch: 500; loss: 2.29; acc: 0.17
Batch: 520; loss: 2.18; acc: 0.16
Batch: 540; loss: 2.19; acc: 0.17
Batch: 560; loss: 2.22; acc: 0.16
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.18; acc: 0.19
Batch: 620; loss: 2.19; acc: 0.2
Batch: 640; loss: 2.18; acc: 0.17
Batch: 660; loss: 2.26; acc: 0.16
Batch: 680; loss: 2.14; acc: 0.17
Batch: 700; loss: 2.12; acc: 0.27
Batch: 720; loss: 2.28; acc: 0.14
Batch: 740; loss: 2.17; acc: 0.22
Batch: 760; loss: 2.28; acc: 0.14
Batch: 780; loss: 2.23; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993653060524325; val_accuracy: 0.18690286624203822 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.23; acc: 0.14
Batch: 20; loss: 2.21; acc: 0.19
Batch: 40; loss: 2.32; acc: 0.14
Batch: 60; loss: 2.23; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.25; acc: 0.19
Batch: 120; loss: 2.25; acc: 0.16
Batch: 140; loss: 2.17; acc: 0.3
Batch: 160; loss: 2.28; acc: 0.19
Batch: 180; loss: 2.25; acc: 0.09
Batch: 200; loss: 2.27; acc: 0.22
Batch: 220; loss: 2.24; acc: 0.17
Batch: 240; loss: 2.18; acc: 0.19
Batch: 260; loss: 2.28; acc: 0.11
Batch: 280; loss: 2.2; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.16
Batch: 320; loss: 2.2; acc: 0.17
Batch: 340; loss: 2.27; acc: 0.11
Batch: 360; loss: 2.2; acc: 0.14
Batch: 380; loss: 2.33; acc: 0.11
Batch: 400; loss: 2.16; acc: 0.22
Batch: 420; loss: 2.1; acc: 0.31
Batch: 440; loss: 2.27; acc: 0.16
Batch: 460; loss: 2.2; acc: 0.22
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.24; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.14
Batch: 540; loss: 2.25; acc: 0.12
Batch: 560; loss: 2.24; acc: 0.19
Batch: 580; loss: 2.22; acc: 0.16
Batch: 600; loss: 2.27; acc: 0.12
Batch: 620; loss: 2.19; acc: 0.19
Batch: 640; loss: 2.25; acc: 0.09
Batch: 660; loss: 2.2; acc: 0.2
Batch: 680; loss: 2.28; acc: 0.12
Batch: 700; loss: 2.14; acc: 0.23
Batch: 720; loss: 2.14; acc: 0.19
Batch: 740; loss: 2.21; acc: 0.17
Batch: 760; loss: 2.11; acc: 0.2
Batch: 780; loss: 2.09; acc: 0.23
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.16; acc: 0.22
Batch: 120; loss: 2.06; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.25
Val Epoch over. val_loss: 2.1993476296686065; val_accuracy: 0.1872014331210191 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5158
elements in E: 1110650
fraction nonzero: 0.004644127312834826
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.32; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.19
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.3; acc: 0.08
Batch: 560; loss: 2.3; acc: 0.05
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.22
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.3; acc: 0.12
Batch: 720; loss: 2.29; acc: 0.14
Batch: 740; loss: 2.31; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.31; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.12
Val Epoch over. val_loss: 2.290404871011236; val_accuracy: 0.13793789808917198 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.22
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.16
Batch: 180; loss: 2.3; acc: 0.17
Batch: 200; loss: 2.29; acc: 0.14
Batch: 220; loss: 2.29; acc: 0.2
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.29; acc: 0.06
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.31; acc: 0.02
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.29; acc: 0.11
Batch: 380; loss: 2.28; acc: 0.19
Batch: 400; loss: 2.28; acc: 0.14
Batch: 420; loss: 2.29; acc: 0.09
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.27; acc: 0.17
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.26; acc: 0.19
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.28; acc: 0.11
Batch: 600; loss: 2.28; acc: 0.19
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.09
Batch: 680; loss: 2.28; acc: 0.11
Batch: 700; loss: 2.27; acc: 0.17
Batch: 720; loss: 2.27; acc: 0.17
Batch: 740; loss: 2.27; acc: 0.16
Batch: 760; loss: 2.28; acc: 0.11
Batch: 780; loss: 2.27; acc: 0.03
Train Epoch over. train_loss: 2.28; train_accuracy: 0.13 

Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.11
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.17
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.26; acc: 0.12
Val Epoch over. val_loss: 2.2724259658983557; val_accuracy: 0.11126592356687898 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.08
Batch: 20; loss: 2.26; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.06
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.09
Batch: 160; loss: 2.27; acc: 0.09
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.26; acc: 0.14
Batch: 240; loss: 2.27; acc: 0.14
Batch: 260; loss: 2.25; acc: 0.23
Batch: 280; loss: 2.26; acc: 0.17
Batch: 300; loss: 2.26; acc: 0.12
Batch: 320; loss: 2.27; acc: 0.14
Batch: 340; loss: 2.24; acc: 0.2
Batch: 360; loss: 2.24; acc: 0.2
Batch: 380; loss: 2.24; acc: 0.14
Batch: 400; loss: 2.29; acc: 0.08
Batch: 420; loss: 2.24; acc: 0.23
Batch: 440; loss: 2.24; acc: 0.08
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.28; acc: 0.06
Batch: 500; loss: 2.24; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.14
Batch: 540; loss: 2.24; acc: 0.14
Batch: 560; loss: 2.22; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.16
Batch: 600; loss: 2.24; acc: 0.16
Batch: 620; loss: 2.21; acc: 0.16
Batch: 640; loss: 2.25; acc: 0.14
Batch: 660; loss: 2.2; acc: 0.17
Batch: 680; loss: 2.15; acc: 0.19
Batch: 700; loss: 2.15; acc: 0.23
Batch: 720; loss: 2.14; acc: 0.23
Batch: 740; loss: 2.15; acc: 0.16
Batch: 760; loss: 2.14; acc: 0.23
Batch: 780; loss: 2.12; acc: 0.28
Train Epoch over. train_loss: 2.24; train_accuracy: 0.15 

Batch: 0; loss: 2.1; acc: 0.3
Batch: 20; loss: 2.12; acc: 0.25
Batch: 40; loss: 2.1; acc: 0.28
Batch: 60; loss: 2.09; acc: 0.33
Batch: 80; loss: 2.05; acc: 0.23
Batch: 100; loss: 2.11; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.25
Batch: 140; loss: 2.09; acc: 0.22
Val Epoch over. val_loss: 2.1339698703425705; val_accuracy: 0.21009156050955413 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.15; acc: 0.22
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.08; acc: 0.3
Batch: 60; loss: 2.06; acc: 0.3
Batch: 80; loss: 2.12; acc: 0.23
Batch: 100; loss: 1.97; acc: 0.31
Batch: 120; loss: 2.11; acc: 0.14
Batch: 140; loss: 2.01; acc: 0.28
Batch: 160; loss: 2.04; acc: 0.33
Batch: 180; loss: 2.13; acc: 0.16
Batch: 200; loss: 2.01; acc: 0.25
Batch: 220; loss: 2.11; acc: 0.23
Batch: 240; loss: 1.95; acc: 0.3
Batch: 260; loss: 2.03; acc: 0.25
Batch: 280; loss: 2.02; acc: 0.3
Batch: 300; loss: 2.05; acc: 0.28
Batch: 320; loss: 2.01; acc: 0.3
Batch: 340; loss: 2.08; acc: 0.25
Batch: 360; loss: 1.98; acc: 0.22
Batch: 380; loss: 1.96; acc: 0.38
Batch: 400; loss: 1.82; acc: 0.33
Batch: 420; loss: 2.0; acc: 0.3
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 2.0; acc: 0.34
Batch: 480; loss: 2.03; acc: 0.33
Batch: 500; loss: 1.86; acc: 0.38
Batch: 520; loss: 1.87; acc: 0.31
Batch: 540; loss: 1.85; acc: 0.38
Batch: 560; loss: 2.05; acc: 0.27
Batch: 580; loss: 2.04; acc: 0.27
Batch: 600; loss: 1.99; acc: 0.3
Batch: 620; loss: 1.9; acc: 0.3
Batch: 640; loss: 1.92; acc: 0.36
Batch: 660; loss: 2.18; acc: 0.2
Batch: 680; loss: 1.85; acc: 0.38
Batch: 700; loss: 1.8; acc: 0.34
Batch: 720; loss: 1.93; acc: 0.27
Batch: 740; loss: 1.88; acc: 0.33
Batch: 760; loss: 1.79; acc: 0.41
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.98; train_accuracy: 0.29 

Batch: 0; loss: 1.77; acc: 0.33
Batch: 20; loss: 1.82; acc: 0.39
Batch: 40; loss: 1.59; acc: 0.47
Batch: 60; loss: 1.84; acc: 0.36
Batch: 80; loss: 1.81; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.36
Batch: 120; loss: 2.0; acc: 0.28
Batch: 140; loss: 1.77; acc: 0.28
Val Epoch over. val_loss: 1.847914940232684; val_accuracy: 0.3414609872611465 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.9; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.86; acc: 0.39
Batch: 60; loss: 1.8; acc: 0.39
Batch: 80; loss: 1.89; acc: 0.31
Batch: 100; loss: 2.01; acc: 0.28
Batch: 120; loss: 1.83; acc: 0.41
Batch: 140; loss: 1.71; acc: 0.36
Batch: 160; loss: 1.87; acc: 0.31
Batch: 180; loss: 1.81; acc: 0.31
Batch: 200; loss: 1.91; acc: 0.31
Batch: 220; loss: 1.83; acc: 0.36
Batch: 240; loss: 1.8; acc: 0.38
Batch: 260; loss: 1.86; acc: 0.31
Batch: 280; loss: 1.89; acc: 0.28
Batch: 300; loss: 1.77; acc: 0.44
Batch: 320; loss: 1.73; acc: 0.44
Batch: 340; loss: 1.71; acc: 0.34
Batch: 360; loss: 1.79; acc: 0.36
Batch: 380; loss: 1.88; acc: 0.38
Batch: 400; loss: 1.53; acc: 0.52
Batch: 420; loss: 1.84; acc: 0.34
Batch: 440; loss: 1.87; acc: 0.34
Batch: 460; loss: 1.79; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.31
Batch: 500; loss: 1.86; acc: 0.39
Batch: 520; loss: 1.77; acc: 0.41
Batch: 540; loss: 1.74; acc: 0.41
Batch: 560; loss: 1.79; acc: 0.41
Batch: 580; loss: 1.78; acc: 0.33
Batch: 600; loss: 1.75; acc: 0.34
Batch: 620; loss: 1.8; acc: 0.33
Batch: 640; loss: 1.75; acc: 0.45
Batch: 660; loss: 1.89; acc: 0.38
Batch: 680; loss: 1.69; acc: 0.44
Batch: 700; loss: 1.7; acc: 0.42
Batch: 720; loss: 1.75; acc: 0.34
Batch: 740; loss: 1.71; acc: 0.38
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.95; acc: 0.28
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.41
Batch: 40; loss: 1.53; acc: 0.48
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.69; acc: 0.39
Batch: 100; loss: 1.75; acc: 0.3
Batch: 120; loss: 2.0; acc: 0.34
Batch: 140; loss: 1.68; acc: 0.42
Val Epoch over. val_loss: 1.7407711736715523; val_accuracy: 0.3770899681528662 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.85; acc: 0.36
Batch: 20; loss: 1.86; acc: 0.41
Batch: 40; loss: 1.78; acc: 0.38
Batch: 60; loss: 1.62; acc: 0.42
Batch: 80; loss: 1.68; acc: 0.39
Batch: 100; loss: 1.61; acc: 0.42
Batch: 120; loss: 1.75; acc: 0.34
Batch: 140; loss: 1.6; acc: 0.45
Batch: 160; loss: 1.68; acc: 0.41
Batch: 180; loss: 1.64; acc: 0.33
Batch: 200; loss: 1.89; acc: 0.3
Batch: 220; loss: 1.89; acc: 0.31
Batch: 240; loss: 1.63; acc: 0.41
Batch: 260; loss: 1.65; acc: 0.36
Batch: 280; loss: 1.72; acc: 0.36
Batch: 300; loss: 1.81; acc: 0.3
Batch: 320; loss: 1.65; acc: 0.36
Batch: 340; loss: 1.68; acc: 0.45
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.38
Batch: 400; loss: 1.89; acc: 0.36
Batch: 420; loss: 1.67; acc: 0.45
Batch: 440; loss: 1.6; acc: 0.38
Batch: 460; loss: 1.86; acc: 0.28
Batch: 480; loss: 1.8; acc: 0.34
Batch: 500; loss: 1.64; acc: 0.38
Batch: 520; loss: 1.55; acc: 0.45
Batch: 540; loss: 1.95; acc: 0.25
Batch: 560; loss: 1.8; acc: 0.36
Batch: 580; loss: 1.72; acc: 0.36
Batch: 600; loss: 1.78; acc: 0.39
Batch: 620; loss: 1.57; acc: 0.48
Batch: 640; loss: 1.72; acc: 0.36
Batch: 660; loss: 1.73; acc: 0.36
Batch: 680; loss: 1.69; acc: 0.44
Batch: 700; loss: 1.68; acc: 0.38
Batch: 720; loss: 1.62; acc: 0.48
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.53; acc: 0.45
Batch: 780; loss: 1.68; acc: 0.38
Train Epoch over. train_loss: 1.75; train_accuracy: 0.37 

Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.71; acc: 0.36
Batch: 80; loss: 1.69; acc: 0.38
Batch: 100; loss: 1.77; acc: 0.38
Batch: 120; loss: 2.06; acc: 0.27
Batch: 140; loss: 1.64; acc: 0.41
Val Epoch over. val_loss: 1.7290737499856645; val_accuracy: 0.37669187898089174 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.91; acc: 0.34
Batch: 20; loss: 1.86; acc: 0.31
Batch: 40; loss: 1.76; acc: 0.34
Batch: 60; loss: 1.49; acc: 0.53
Batch: 80; loss: 1.88; acc: 0.34
Batch: 100; loss: 1.6; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.3
Batch: 140; loss: 1.61; acc: 0.47
Batch: 160; loss: 1.78; acc: 0.33
Batch: 180; loss: 1.76; acc: 0.42
Batch: 200; loss: 1.66; acc: 0.41
Batch: 220; loss: 1.6; acc: 0.34
Batch: 240; loss: 1.92; acc: 0.34
Batch: 260; loss: 1.68; acc: 0.41
Batch: 280; loss: 1.73; acc: 0.5
Batch: 300; loss: 1.74; acc: 0.45
Batch: 320; loss: 1.63; acc: 0.42
Batch: 340; loss: 1.6; acc: 0.36
Batch: 360; loss: 1.81; acc: 0.36
Batch: 380; loss: 1.72; acc: 0.39
Batch: 400; loss: 1.93; acc: 0.33
Batch: 420; loss: 1.63; acc: 0.52
Batch: 440; loss: 1.67; acc: 0.41
Batch: 460; loss: 1.83; acc: 0.27
Batch: 480; loss: 1.87; acc: 0.27
Batch: 500; loss: 1.54; acc: 0.48
Batch: 520; loss: 1.67; acc: 0.34
Batch: 540; loss: 1.83; acc: 0.34
Batch: 560; loss: 1.87; acc: 0.38
Batch: 580; loss: 1.83; acc: 0.31
Batch: 600; loss: 1.69; acc: 0.41
Batch: 620; loss: 1.71; acc: 0.36
Batch: 640; loss: 1.68; acc: 0.39
Batch: 660; loss: 1.7; acc: 0.45
Batch: 680; loss: 1.53; acc: 0.53
Batch: 700; loss: 1.83; acc: 0.36
Batch: 720; loss: 1.83; acc: 0.33
Batch: 740; loss: 1.91; acc: 0.23
Batch: 760; loss: 1.73; acc: 0.42
Batch: 780; loss: 1.9; acc: 0.33
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.44
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.71; acc: 0.34
Batch: 100; loss: 1.78; acc: 0.33
Batch: 120; loss: 2.06; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7290549741429129; val_accuracy: 0.3782842356687898 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.87; acc: 0.31
Batch: 20; loss: 1.89; acc: 0.31
Batch: 40; loss: 1.79; acc: 0.41
Batch: 60; loss: 1.62; acc: 0.42
Batch: 80; loss: 1.88; acc: 0.34
Batch: 100; loss: 1.69; acc: 0.42
Batch: 120; loss: 1.71; acc: 0.34
Batch: 140; loss: 1.84; acc: 0.33
Batch: 160; loss: 1.84; acc: 0.33
Batch: 180; loss: 1.69; acc: 0.42
Batch: 200; loss: 1.54; acc: 0.45
Batch: 220; loss: 1.54; acc: 0.44
Batch: 240; loss: 1.91; acc: 0.27
Batch: 260; loss: 1.65; acc: 0.44
Batch: 280; loss: 1.68; acc: 0.45
Batch: 300; loss: 1.75; acc: 0.36
Batch: 320; loss: 1.86; acc: 0.36
Batch: 340; loss: 1.67; acc: 0.47
Batch: 360; loss: 1.66; acc: 0.41
Batch: 380; loss: 1.85; acc: 0.3
Batch: 400; loss: 1.84; acc: 0.31
Batch: 420; loss: 1.82; acc: 0.33
Batch: 440; loss: 1.73; acc: 0.42
Batch: 460; loss: 1.96; acc: 0.33
Batch: 480; loss: 1.67; acc: 0.41
Batch: 500; loss: 1.96; acc: 0.25
Batch: 520; loss: 1.75; acc: 0.41
Batch: 540; loss: 1.68; acc: 0.39
Batch: 560; loss: 1.85; acc: 0.28
Batch: 580; loss: 1.84; acc: 0.36
Batch: 600; loss: 1.81; acc: 0.31
Batch: 620; loss: 1.64; acc: 0.33
Batch: 640; loss: 1.61; acc: 0.41
Batch: 660; loss: 1.71; acc: 0.3
Batch: 680; loss: 1.76; acc: 0.45
Batch: 700; loss: 1.96; acc: 0.27
Batch: 720; loss: 1.67; acc: 0.39
Batch: 740; loss: 1.97; acc: 0.3
Batch: 760; loss: 1.91; acc: 0.25
Batch: 780; loss: 1.87; acc: 0.33
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.72; acc: 0.38
Batch: 80; loss: 1.71; acc: 0.38
Batch: 100; loss: 1.8; acc: 0.33
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.62; acc: 0.39
Val Epoch over. val_loss: 1.730155076950219; val_accuracy: 0.37649283439490444 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.41
Batch: 20; loss: 1.9; acc: 0.27
Batch: 40; loss: 1.59; acc: 0.39
Batch: 60; loss: 1.8; acc: 0.28
Batch: 80; loss: 1.54; acc: 0.47
Batch: 100; loss: 1.76; acc: 0.41
Batch: 120; loss: 1.84; acc: 0.3
Batch: 140; loss: 1.66; acc: 0.39
Batch: 160; loss: 1.65; acc: 0.38
Batch: 180; loss: 1.75; acc: 0.44
Batch: 200; loss: 1.77; acc: 0.38
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.69; acc: 0.42
Batch: 260; loss: 1.59; acc: 0.47
Batch: 280; loss: 1.63; acc: 0.45
Batch: 300; loss: 1.71; acc: 0.39
Batch: 320; loss: 1.68; acc: 0.38
Batch: 340; loss: 1.78; acc: 0.3
Batch: 360; loss: 1.72; acc: 0.52
Batch: 380; loss: 1.73; acc: 0.41
Batch: 400; loss: 1.74; acc: 0.41
Batch: 420; loss: 1.69; acc: 0.41
Batch: 440; loss: 1.85; acc: 0.34
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.62; acc: 0.47
Batch: 500; loss: 1.82; acc: 0.31
Batch: 520; loss: 1.98; acc: 0.22
Batch: 540; loss: 1.88; acc: 0.27
Batch: 560; loss: 1.82; acc: 0.33
Batch: 580; loss: 1.89; acc: 0.34
Batch: 600; loss: 1.84; acc: 0.41
Batch: 620; loss: 1.65; acc: 0.39
Batch: 640; loss: 1.82; acc: 0.41
Batch: 660; loss: 1.8; acc: 0.33
Batch: 680; loss: 1.8; acc: 0.42
Batch: 700; loss: 1.6; acc: 0.33
Batch: 720; loss: 1.88; acc: 0.27
Batch: 740; loss: 1.75; acc: 0.38
Batch: 760; loss: 1.7; acc: 0.42
Batch: 780; loss: 1.7; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.52; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.8; acc: 0.3
Batch: 120; loss: 2.09; acc: 0.25
Batch: 140; loss: 1.62; acc: 0.41
Val Epoch over. val_loss: 1.729498998374696; val_accuracy: 0.37738853503184716 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.79; acc: 0.41
Batch: 60; loss: 1.64; acc: 0.42
Batch: 80; loss: 1.79; acc: 0.34
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.74; acc: 0.38
Batch: 140; loss: 1.72; acc: 0.36
Batch: 160; loss: 1.73; acc: 0.38
Batch: 180; loss: 1.65; acc: 0.42
Batch: 200; loss: 1.71; acc: 0.34
Batch: 220; loss: 1.61; acc: 0.47
Batch: 240; loss: 1.71; acc: 0.34
Batch: 260; loss: 1.84; acc: 0.31
Batch: 280; loss: 1.66; acc: 0.53
Batch: 300; loss: 1.68; acc: 0.42
Batch: 320; loss: 1.89; acc: 0.28
Batch: 340; loss: 1.84; acc: 0.33
Batch: 360; loss: 1.73; acc: 0.42
Batch: 380; loss: 1.53; acc: 0.5
Batch: 400; loss: 1.76; acc: 0.33
Batch: 420; loss: 1.64; acc: 0.39
Batch: 440; loss: 1.62; acc: 0.45
Batch: 460; loss: 1.69; acc: 0.31
Batch: 480; loss: 1.9; acc: 0.3
Batch: 500; loss: 1.63; acc: 0.36
Batch: 520; loss: 1.8; acc: 0.33
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.76; acc: 0.39
Batch: 580; loss: 2.01; acc: 0.28
Batch: 600; loss: 1.68; acc: 0.41
Batch: 620; loss: 1.51; acc: 0.5
Batch: 640; loss: 1.75; acc: 0.39
Batch: 660; loss: 1.64; acc: 0.44
Batch: 680; loss: 1.98; acc: 0.31
Batch: 700; loss: 1.86; acc: 0.3
Batch: 720; loss: 1.72; acc: 0.33
Batch: 740; loss: 1.77; acc: 0.38
Batch: 760; loss: 1.57; acc: 0.47
Batch: 780; loss: 1.56; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.44
Batch: 20; loss: 1.61; acc: 0.39
Batch: 40; loss: 1.52; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.38
Batch: 80; loss: 1.71; acc: 0.36
Batch: 100; loss: 1.78; acc: 0.31
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.62; acc: 0.41
Val Epoch over. val_loss: 1.7310437374054246; val_accuracy: 0.37778662420382164 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.89; acc: 0.28
Batch: 20; loss: 1.71; acc: 0.33
Batch: 40; loss: 1.8; acc: 0.36
Batch: 60; loss: 1.76; acc: 0.47
Batch: 80; loss: 1.77; acc: 0.39
Batch: 100; loss: 1.85; acc: 0.27
Batch: 120; loss: 1.77; acc: 0.31
Batch: 140; loss: 1.88; acc: 0.3
Batch: 160; loss: 1.76; acc: 0.33
Batch: 180; loss: 1.76; acc: 0.3
Batch: 200; loss: 1.56; acc: 0.44
Batch: 220; loss: 1.8; acc: 0.38
Batch: 240; loss: 1.98; acc: 0.28
Batch: 260; loss: 1.71; acc: 0.44
Batch: 280; loss: 1.73; acc: 0.34
Batch: 300; loss: 1.77; acc: 0.36
Batch: 320; loss: 1.66; acc: 0.36
Batch: 340; loss: 1.65; acc: 0.42
Batch: 360; loss: 1.64; acc: 0.45
Batch: 380; loss: 1.9; acc: 0.33
Batch: 400; loss: 1.92; acc: 0.31
Batch: 420; loss: 1.95; acc: 0.31
Batch: 440; loss: 1.94; acc: 0.27
Batch: 460; loss: 1.76; acc: 0.33
Batch: 480; loss: 1.75; acc: 0.44
Batch: 500; loss: 1.73; acc: 0.33
Batch: 520; loss: 1.7; acc: 0.42
Batch: 540; loss: 1.48; acc: 0.5
Batch: 560; loss: 1.65; acc: 0.45
Batch: 580; loss: 1.8; acc: 0.31
Batch: 600; loss: 1.63; acc: 0.45
Batch: 620; loss: 1.48; acc: 0.42
Batch: 640; loss: 1.86; acc: 0.33
Batch: 660; loss: 1.82; acc: 0.38
Batch: 680; loss: 1.64; acc: 0.39
Batch: 700; loss: 1.81; acc: 0.36
Batch: 720; loss: 1.68; acc: 0.39
Batch: 740; loss: 1.7; acc: 0.31
Batch: 760; loss: 1.79; acc: 0.39
Batch: 780; loss: 1.79; acc: 0.27
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.39
Batch: 40; loss: 1.52; acc: 0.48
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.28
Batch: 120; loss: 2.07; acc: 0.25
Batch: 140; loss: 1.62; acc: 0.41
Val Epoch over. val_loss: 1.7302876521068014; val_accuracy: 0.37718949044585987 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.7; acc: 0.36
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.75; acc: 0.34
Batch: 60; loss: 1.71; acc: 0.38
Batch: 80; loss: 1.65; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.39
Batch: 120; loss: 1.81; acc: 0.3
Batch: 140; loss: 1.82; acc: 0.33
Batch: 160; loss: 1.63; acc: 0.45
Batch: 180; loss: 1.69; acc: 0.44
Batch: 200; loss: 1.74; acc: 0.36
Batch: 220; loss: 1.64; acc: 0.42
Batch: 240; loss: 1.88; acc: 0.28
Batch: 260; loss: 1.65; acc: 0.41
Batch: 280; loss: 1.69; acc: 0.38
Batch: 300; loss: 1.79; acc: 0.45
Batch: 320; loss: 1.74; acc: 0.28
Batch: 340; loss: 1.81; acc: 0.36
Batch: 360; loss: 1.75; acc: 0.39
Batch: 380; loss: 1.85; acc: 0.39
Batch: 400; loss: 1.76; acc: 0.36
Batch: 420; loss: 1.85; acc: 0.25
Batch: 440; loss: 1.51; acc: 0.52
Batch: 460; loss: 1.82; acc: 0.41
Batch: 480; loss: 1.9; acc: 0.33
Batch: 500; loss: 1.77; acc: 0.41
Batch: 520; loss: 1.71; acc: 0.38
Batch: 540; loss: 1.61; acc: 0.42
Batch: 560; loss: 2.02; acc: 0.28
Batch: 580; loss: 1.76; acc: 0.34
Batch: 600; loss: 1.87; acc: 0.31
Batch: 620; loss: 1.67; acc: 0.39
Batch: 640; loss: 1.83; acc: 0.31
Batch: 660; loss: 1.55; acc: 0.45
Batch: 680; loss: 1.78; acc: 0.39
Batch: 700; loss: 1.74; acc: 0.41
Batch: 720; loss: 1.68; acc: 0.39
Batch: 740; loss: 1.81; acc: 0.38
Batch: 760; loss: 1.77; acc: 0.39
Batch: 780; loss: 1.76; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.36
Batch: 40; loss: 1.52; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.39
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.25
Batch: 140; loss: 1.62; acc: 0.41
Val Epoch over. val_loss: 1.7290343243605013; val_accuracy: 0.38007563694267515 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.9; acc: 0.33
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.62; acc: 0.39
Batch: 60; loss: 1.74; acc: 0.38
Batch: 80; loss: 1.69; acc: 0.38
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.84; acc: 0.31
Batch: 140; loss: 1.66; acc: 0.44
Batch: 160; loss: 1.69; acc: 0.36
Batch: 180; loss: 1.8; acc: 0.36
Batch: 200; loss: 1.7; acc: 0.42
Batch: 220; loss: 1.64; acc: 0.41
Batch: 240; loss: 1.72; acc: 0.39
Batch: 260; loss: 1.73; acc: 0.42
Batch: 280; loss: 1.75; acc: 0.31
Batch: 300; loss: 1.7; acc: 0.42
Batch: 320; loss: 1.65; acc: 0.39
Batch: 340; loss: 1.72; acc: 0.34
Batch: 360; loss: 1.74; acc: 0.3
Batch: 380; loss: 1.6; acc: 0.45
Batch: 400; loss: 1.82; acc: 0.36
Batch: 420; loss: 1.72; acc: 0.41
Batch: 440; loss: 1.79; acc: 0.38
Batch: 460; loss: 1.76; acc: 0.42
Batch: 480; loss: 1.87; acc: 0.38
Batch: 500; loss: 1.9; acc: 0.25
Batch: 520; loss: 1.8; acc: 0.33
Batch: 540; loss: 1.85; acc: 0.33
Batch: 560; loss: 1.81; acc: 0.34
Batch: 580; loss: 1.45; acc: 0.47
Batch: 600; loss: 1.94; acc: 0.3
Batch: 620; loss: 1.88; acc: 0.33
Batch: 640; loss: 1.77; acc: 0.28
Batch: 660; loss: 1.88; acc: 0.3
Batch: 680; loss: 1.91; acc: 0.41
Batch: 700; loss: 1.79; acc: 0.38
Batch: 720; loss: 1.86; acc: 0.27
Batch: 740; loss: 1.68; acc: 0.38
Batch: 760; loss: 1.79; acc: 0.36
Batch: 780; loss: 1.65; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.61; acc: 0.38
Batch: 40; loss: 1.5; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.28
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7295853719589815; val_accuracy: 0.38007563694267515 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.58; acc: 0.38
Batch: 20; loss: 1.76; acc: 0.38
Batch: 40; loss: 1.53; acc: 0.47
Batch: 60; loss: 1.83; acc: 0.39
Batch: 80; loss: 1.62; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.42
Batch: 120; loss: 1.64; acc: 0.42
Batch: 140; loss: 1.82; acc: 0.36
Batch: 160; loss: 1.64; acc: 0.42
Batch: 180; loss: 1.76; acc: 0.38
Batch: 200; loss: 1.66; acc: 0.36
Batch: 220; loss: 1.79; acc: 0.36
Batch: 240; loss: 1.83; acc: 0.34
Batch: 260; loss: 1.79; acc: 0.36
Batch: 280; loss: 1.84; acc: 0.33
Batch: 300; loss: 1.65; acc: 0.5
Batch: 320; loss: 1.7; acc: 0.36
Batch: 340; loss: 1.68; acc: 0.41
Batch: 360; loss: 1.68; acc: 0.41
Batch: 380; loss: 1.8; acc: 0.36
Batch: 400; loss: 1.63; acc: 0.47
Batch: 420; loss: 1.7; acc: 0.31
Batch: 440; loss: 1.74; acc: 0.38
Batch: 460; loss: 1.8; acc: 0.34
Batch: 480; loss: 1.82; acc: 0.28
Batch: 500; loss: 1.77; acc: 0.41
Batch: 520; loss: 1.9; acc: 0.28
Batch: 540; loss: 1.58; acc: 0.42
Batch: 560; loss: 1.93; acc: 0.36
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.78; acc: 0.47
Batch: 620; loss: 1.72; acc: 0.33
Batch: 640; loss: 1.64; acc: 0.44
Batch: 660; loss: 1.77; acc: 0.33
Batch: 680; loss: 1.91; acc: 0.23
Batch: 700; loss: 1.66; acc: 0.42
Batch: 720; loss: 1.57; acc: 0.44
Batch: 740; loss: 1.7; acc: 0.36
Batch: 760; loss: 1.66; acc: 0.44
Batch: 780; loss: 1.72; acc: 0.38
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.36
Batch: 40; loss: 1.52; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.41
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.39
Val Epoch over. val_loss: 1.7293629950019205; val_accuracy: 0.3794785031847134 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.84; acc: 0.3
Batch: 20; loss: 1.66; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.28
Batch: 60; loss: 1.66; acc: 0.42
Batch: 80; loss: 1.71; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.44
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.61; acc: 0.36
Batch: 160; loss: 1.91; acc: 0.31
Batch: 180; loss: 1.79; acc: 0.38
Batch: 200; loss: 1.67; acc: 0.41
Batch: 220; loss: 1.89; acc: 0.34
Batch: 240; loss: 1.93; acc: 0.28
Batch: 260; loss: 1.67; acc: 0.33
Batch: 280; loss: 1.77; acc: 0.3
Batch: 300; loss: 1.65; acc: 0.44
Batch: 320; loss: 1.73; acc: 0.42
Batch: 340; loss: 1.8; acc: 0.44
Batch: 360; loss: 1.58; acc: 0.41
Batch: 380; loss: 1.74; acc: 0.41
Batch: 400; loss: 1.61; acc: 0.44
Batch: 420; loss: 1.82; acc: 0.31
Batch: 440; loss: 1.66; acc: 0.44
Batch: 460; loss: 1.8; acc: 0.28
Batch: 480; loss: 1.92; acc: 0.3
Batch: 500; loss: 1.88; acc: 0.3
Batch: 520; loss: 1.64; acc: 0.38
Batch: 540; loss: 1.74; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.42
Batch: 580; loss: 1.59; acc: 0.45
Batch: 600; loss: 1.59; acc: 0.47
Batch: 620; loss: 1.61; acc: 0.48
Batch: 640; loss: 1.74; acc: 0.33
Batch: 660; loss: 1.42; acc: 0.58
Batch: 680; loss: 1.81; acc: 0.34
Batch: 700; loss: 2.0; acc: 0.22
Batch: 720; loss: 1.81; acc: 0.34
Batch: 740; loss: 1.63; acc: 0.48
Batch: 760; loss: 1.59; acc: 0.42
Batch: 780; loss: 1.98; acc: 0.25
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.38
Batch: 40; loss: 1.5; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7291397175211816; val_accuracy: 0.3781847133757962 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.61; acc: 0.45
Batch: 20; loss: 1.72; acc: 0.38
Batch: 40; loss: 1.76; acc: 0.33
Batch: 60; loss: 1.93; acc: 0.27
Batch: 80; loss: 1.77; acc: 0.33
Batch: 100; loss: 1.76; acc: 0.36
Batch: 120; loss: 1.7; acc: 0.33
Batch: 140; loss: 1.75; acc: 0.36
Batch: 160; loss: 1.67; acc: 0.38
Batch: 180; loss: 1.66; acc: 0.42
Batch: 200; loss: 1.62; acc: 0.38
Batch: 220; loss: 1.79; acc: 0.33
Batch: 240; loss: 1.54; acc: 0.42
Batch: 260; loss: 1.76; acc: 0.36
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.68; acc: 0.38
Batch: 320; loss: 1.79; acc: 0.38
Batch: 340; loss: 1.95; acc: 0.28
Batch: 360; loss: 1.87; acc: 0.33
Batch: 380; loss: 1.53; acc: 0.47
Batch: 400; loss: 1.73; acc: 0.33
Batch: 420; loss: 1.52; acc: 0.47
Batch: 440; loss: 1.71; acc: 0.42
Batch: 460; loss: 1.75; acc: 0.31
Batch: 480; loss: 1.84; acc: 0.41
Batch: 500; loss: 1.67; acc: 0.45
Batch: 520; loss: 1.75; acc: 0.41
Batch: 540; loss: 1.66; acc: 0.39
Batch: 560; loss: 1.77; acc: 0.33
Batch: 580; loss: 2.08; acc: 0.25
Batch: 600; loss: 1.84; acc: 0.3
Batch: 620; loss: 1.66; acc: 0.41
Batch: 640; loss: 1.78; acc: 0.34
Batch: 660; loss: 1.58; acc: 0.48
Batch: 680; loss: 1.84; acc: 0.34
Batch: 700; loss: 1.72; acc: 0.38
Batch: 720; loss: 1.67; acc: 0.34
Batch: 740; loss: 1.67; acc: 0.47
Batch: 760; loss: 1.6; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.3
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.52; acc: 0.48
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.33
Batch: 100; loss: 1.79; acc: 0.3
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7297095098313253; val_accuracy: 0.37748805732484075 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.73; acc: 0.36
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.76; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.45
Batch: 80; loss: 1.76; acc: 0.34
Batch: 100; loss: 1.77; acc: 0.3
Batch: 120; loss: 1.62; acc: 0.36
Batch: 140; loss: 1.78; acc: 0.33
Batch: 160; loss: 1.88; acc: 0.45
Batch: 180; loss: 1.9; acc: 0.31
Batch: 200; loss: 1.62; acc: 0.47
Batch: 220; loss: 1.81; acc: 0.36
Batch: 240; loss: 2.0; acc: 0.41
Batch: 260; loss: 1.71; acc: 0.41
Batch: 280; loss: 1.64; acc: 0.44
Batch: 300; loss: 1.72; acc: 0.34
Batch: 320; loss: 1.98; acc: 0.34
Batch: 340; loss: 1.71; acc: 0.38
Batch: 360; loss: 1.76; acc: 0.38
Batch: 380; loss: 1.82; acc: 0.39
Batch: 400; loss: 1.8; acc: 0.36
Batch: 420; loss: 1.69; acc: 0.39
Batch: 440; loss: 1.65; acc: 0.33
Batch: 460; loss: 1.75; acc: 0.33
Batch: 480; loss: 1.61; acc: 0.41
Batch: 500; loss: 1.7; acc: 0.44
Batch: 520; loss: 1.6; acc: 0.45
Batch: 540; loss: 1.86; acc: 0.31
Batch: 560; loss: 1.72; acc: 0.42
Batch: 580; loss: 1.76; acc: 0.34
Batch: 600; loss: 1.65; acc: 0.44
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.6; acc: 0.45
Batch: 660; loss: 1.65; acc: 0.39
Batch: 680; loss: 1.9; acc: 0.36
Batch: 700; loss: 1.77; acc: 0.38
Batch: 720; loss: 1.57; acc: 0.42
Batch: 740; loss: 1.7; acc: 0.42
Batch: 760; loss: 1.72; acc: 0.36
Batch: 780; loss: 1.74; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.5; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.3
Batch: 120; loss: 2.08; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.42
Val Epoch over. val_loss: 1.7288633577383248; val_accuracy: 0.3794785031847134 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.82; acc: 0.33
Batch: 20; loss: 1.57; acc: 0.42
Batch: 40; loss: 1.78; acc: 0.34
Batch: 60; loss: 1.76; acc: 0.33
Batch: 80; loss: 1.78; acc: 0.34
Batch: 100; loss: 1.65; acc: 0.39
Batch: 120; loss: 1.59; acc: 0.39
Batch: 140; loss: 1.97; acc: 0.36
Batch: 160; loss: 1.74; acc: 0.38
Batch: 180; loss: 1.78; acc: 0.33
Batch: 200; loss: 1.64; acc: 0.5
Batch: 220; loss: 1.87; acc: 0.28
Batch: 240; loss: 1.85; acc: 0.36
Batch: 260; loss: 1.75; acc: 0.38
Batch: 280; loss: 1.74; acc: 0.33
Batch: 300; loss: 1.67; acc: 0.39
Batch: 320; loss: 1.73; acc: 0.39
Batch: 340; loss: 1.52; acc: 0.45
Batch: 360; loss: 1.78; acc: 0.34
Batch: 380; loss: 1.69; acc: 0.47
Batch: 400; loss: 1.5; acc: 0.53
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.66; acc: 0.36
Batch: 460; loss: 1.81; acc: 0.3
Batch: 480; loss: 1.5; acc: 0.47
Batch: 500; loss: 1.9; acc: 0.33
Batch: 520; loss: 1.83; acc: 0.31
Batch: 540; loss: 1.89; acc: 0.3
Batch: 560; loss: 2.1; acc: 0.25
Batch: 580; loss: 1.91; acc: 0.31
Batch: 600; loss: 1.93; acc: 0.33
Batch: 620; loss: 1.77; acc: 0.34
Batch: 640; loss: 1.74; acc: 0.39
Batch: 660; loss: 1.77; acc: 0.38
Batch: 680; loss: 1.82; acc: 0.34
Batch: 700; loss: 1.74; acc: 0.39
Batch: 720; loss: 1.82; acc: 0.33
Batch: 740; loss: 1.99; acc: 0.33
Batch: 760; loss: 1.57; acc: 0.39
Batch: 780; loss: 1.72; acc: 0.34
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.77; acc: 0.3
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7300540003806921; val_accuracy: 0.3785828025477707 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.57; acc: 0.36
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.92; acc: 0.36
Batch: 60; loss: 1.73; acc: 0.38
Batch: 80; loss: 1.73; acc: 0.36
Batch: 100; loss: 1.78; acc: 0.36
Batch: 120; loss: 1.56; acc: 0.52
Batch: 140; loss: 1.78; acc: 0.34
Batch: 160; loss: 1.83; acc: 0.38
Batch: 180; loss: 1.72; acc: 0.38
Batch: 200; loss: 1.57; acc: 0.39
Batch: 220; loss: 1.84; acc: 0.3
Batch: 240; loss: 1.78; acc: 0.31
Batch: 260; loss: 1.56; acc: 0.42
Batch: 280; loss: 1.45; acc: 0.5
Batch: 300; loss: 1.75; acc: 0.33
Batch: 320; loss: 1.74; acc: 0.45
Batch: 340; loss: 1.74; acc: 0.41
Batch: 360; loss: 1.78; acc: 0.39
Batch: 380; loss: 1.92; acc: 0.31
Batch: 400; loss: 1.66; acc: 0.39
Batch: 420; loss: 1.81; acc: 0.34
Batch: 440; loss: 1.65; acc: 0.38
Batch: 460; loss: 1.63; acc: 0.41
Batch: 480; loss: 1.77; acc: 0.3
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.87; acc: 0.31
Batch: 540; loss: 1.59; acc: 0.47
Batch: 560; loss: 1.79; acc: 0.38
Batch: 580; loss: 1.63; acc: 0.41
Batch: 600; loss: 1.62; acc: 0.47
Batch: 620; loss: 1.65; acc: 0.45
Batch: 640; loss: 1.78; acc: 0.34
Batch: 660; loss: 1.73; acc: 0.44
Batch: 680; loss: 1.65; acc: 0.39
Batch: 700; loss: 1.74; acc: 0.38
Batch: 720; loss: 1.63; acc: 0.38
Batch: 740; loss: 1.92; acc: 0.33
Batch: 760; loss: 1.75; acc: 0.38
Batch: 780; loss: 1.88; acc: 0.33
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.78; acc: 0.3
Batch: 120; loss: 2.06; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7293823251299039; val_accuracy: 0.37878184713375795 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.61; acc: 0.33
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.62; acc: 0.36
Batch: 60; loss: 1.74; acc: 0.36
Batch: 80; loss: 1.83; acc: 0.34
Batch: 100; loss: 1.86; acc: 0.28
Batch: 120; loss: 1.78; acc: 0.41
Batch: 140; loss: 1.79; acc: 0.42
Batch: 160; loss: 1.77; acc: 0.3
Batch: 180; loss: 1.66; acc: 0.41
Batch: 200; loss: 1.68; acc: 0.36
Batch: 220; loss: 1.76; acc: 0.39
Batch: 240; loss: 1.69; acc: 0.42
Batch: 260; loss: 1.96; acc: 0.3
Batch: 280; loss: 1.63; acc: 0.34
Batch: 300; loss: 1.79; acc: 0.34
Batch: 320; loss: 1.87; acc: 0.27
Batch: 340; loss: 1.72; acc: 0.39
Batch: 360; loss: 1.64; acc: 0.41
Batch: 380; loss: 1.82; acc: 0.31
Batch: 400; loss: 1.88; acc: 0.31
Batch: 420; loss: 1.73; acc: 0.45
Batch: 440; loss: 1.72; acc: 0.36
Batch: 460; loss: 2.1; acc: 0.25
Batch: 480; loss: 1.87; acc: 0.23
Batch: 500; loss: 1.84; acc: 0.34
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.81; acc: 0.41
Batch: 560; loss: 1.64; acc: 0.47
Batch: 580; loss: 1.7; acc: 0.3
Batch: 600; loss: 1.79; acc: 0.33
Batch: 620; loss: 1.81; acc: 0.38
Batch: 640; loss: 1.91; acc: 0.28
Batch: 660; loss: 1.81; acc: 0.39
Batch: 680; loss: 1.69; acc: 0.36
Batch: 700; loss: 1.75; acc: 0.3
Batch: 720; loss: 1.67; acc: 0.36
Batch: 740; loss: 1.67; acc: 0.42
Batch: 760; loss: 1.67; acc: 0.41
Batch: 780; loss: 1.82; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.39
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.71; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.73097042824812; val_accuracy: 0.38007563694267515 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 1.72; acc: 0.39
Batch: 40; loss: 1.53; acc: 0.48
Batch: 60; loss: 1.67; acc: 0.36
Batch: 80; loss: 1.55; acc: 0.45
Batch: 100; loss: 1.74; acc: 0.38
Batch: 120; loss: 1.89; acc: 0.36
Batch: 140; loss: 1.69; acc: 0.45
Batch: 160; loss: 1.73; acc: 0.34
Batch: 180; loss: 1.7; acc: 0.36
Batch: 200; loss: 1.73; acc: 0.38
Batch: 220; loss: 1.76; acc: 0.38
Batch: 240; loss: 1.79; acc: 0.38
Batch: 260; loss: 1.66; acc: 0.38
Batch: 280; loss: 1.68; acc: 0.38
Batch: 300; loss: 1.56; acc: 0.5
Batch: 320; loss: 1.76; acc: 0.41
Batch: 340; loss: 1.7; acc: 0.38
Batch: 360; loss: 1.87; acc: 0.34
Batch: 380; loss: 1.77; acc: 0.36
Batch: 400; loss: 1.88; acc: 0.39
Batch: 420; loss: 1.88; acc: 0.36
Batch: 440; loss: 1.68; acc: 0.34
Batch: 460; loss: 1.62; acc: 0.31
Batch: 480; loss: 1.76; acc: 0.36
Batch: 500; loss: 1.94; acc: 0.34
Batch: 520; loss: 1.74; acc: 0.33
Batch: 540; loss: 1.76; acc: 0.39
Batch: 560; loss: 1.77; acc: 0.34
Batch: 580; loss: 1.88; acc: 0.34
Batch: 600; loss: 1.91; acc: 0.28
Batch: 620; loss: 1.68; acc: 0.33
Batch: 640; loss: 1.79; acc: 0.38
Batch: 660; loss: 1.61; acc: 0.42
Batch: 680; loss: 1.66; acc: 0.48
Batch: 700; loss: 1.72; acc: 0.45
Batch: 720; loss: 1.59; acc: 0.39
Batch: 740; loss: 1.7; acc: 0.41
Batch: 760; loss: 1.76; acc: 0.38
Batch: 780; loss: 1.73; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.34
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.3
Batch: 120; loss: 2.08; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.39
Val Epoch over. val_loss: 1.7288663759353056; val_accuracy: 0.37768710191082805 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.85; acc: 0.31
Batch: 20; loss: 1.5; acc: 0.42
Batch: 40; loss: 1.84; acc: 0.31
Batch: 60; loss: 1.66; acc: 0.44
Batch: 80; loss: 1.74; acc: 0.39
Batch: 100; loss: 1.84; acc: 0.36
Batch: 120; loss: 1.8; acc: 0.39
Batch: 140; loss: 2.03; acc: 0.3
Batch: 160; loss: 1.6; acc: 0.38
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.79; acc: 0.36
Batch: 220; loss: 1.77; acc: 0.41
Batch: 240; loss: 1.76; acc: 0.33
Batch: 260; loss: 1.7; acc: 0.39
Batch: 280; loss: 1.58; acc: 0.48
Batch: 300; loss: 1.81; acc: 0.28
Batch: 320; loss: 1.85; acc: 0.36
Batch: 340; loss: 1.73; acc: 0.41
Batch: 360; loss: 1.62; acc: 0.42
Batch: 380; loss: 1.8; acc: 0.39
Batch: 400; loss: 1.59; acc: 0.39
Batch: 420; loss: 1.63; acc: 0.41
Batch: 440; loss: 1.71; acc: 0.39
Batch: 460; loss: 1.62; acc: 0.47
Batch: 480; loss: 1.78; acc: 0.38
Batch: 500; loss: 1.98; acc: 0.34
Batch: 520; loss: 1.64; acc: 0.42
Batch: 540; loss: 2.1; acc: 0.33
Batch: 560; loss: 1.58; acc: 0.45
Batch: 580; loss: 1.71; acc: 0.48
Batch: 600; loss: 1.64; acc: 0.39
Batch: 620; loss: 1.63; acc: 0.48
Batch: 640; loss: 1.84; acc: 0.42
Batch: 660; loss: 1.65; acc: 0.44
Batch: 680; loss: 1.63; acc: 0.42
Batch: 700; loss: 1.72; acc: 0.39
Batch: 720; loss: 1.66; acc: 0.39
Batch: 740; loss: 1.77; acc: 0.42
Batch: 760; loss: 1.95; acc: 0.27
Batch: 780; loss: 1.68; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728897021834258; val_accuracy: 0.3783837579617834 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.94; acc: 0.3
Batch: 20; loss: 1.54; acc: 0.44
Batch: 40; loss: 1.81; acc: 0.31
Batch: 60; loss: 1.77; acc: 0.31
Batch: 80; loss: 1.78; acc: 0.44
Batch: 100; loss: 1.87; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.36
Batch: 140; loss: 1.77; acc: 0.31
Batch: 160; loss: 1.77; acc: 0.33
Batch: 180; loss: 1.72; acc: 0.36
Batch: 200; loss: 1.7; acc: 0.44
Batch: 220; loss: 1.74; acc: 0.38
Batch: 240; loss: 1.62; acc: 0.44
Batch: 260; loss: 1.73; acc: 0.38
Batch: 280; loss: 1.67; acc: 0.42
Batch: 300; loss: 1.91; acc: 0.38
Batch: 320; loss: 1.85; acc: 0.34
Batch: 340; loss: 1.79; acc: 0.34
Batch: 360; loss: 1.78; acc: 0.38
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.64; acc: 0.44
Batch: 420; loss: 1.82; acc: 0.39
Batch: 440; loss: 1.63; acc: 0.42
Batch: 460; loss: 1.71; acc: 0.38
Batch: 480; loss: 1.56; acc: 0.42
Batch: 500; loss: 1.99; acc: 0.34
Batch: 520; loss: 1.88; acc: 0.34
Batch: 540; loss: 1.95; acc: 0.33
Batch: 560; loss: 1.78; acc: 0.34
Batch: 580; loss: 1.71; acc: 0.44
Batch: 600; loss: 1.53; acc: 0.44
Batch: 620; loss: 1.67; acc: 0.45
Batch: 640; loss: 1.67; acc: 0.39
Batch: 660; loss: 1.65; acc: 0.36
Batch: 680; loss: 1.59; acc: 0.44
Batch: 700; loss: 1.7; acc: 0.36
Batch: 720; loss: 1.98; acc: 0.28
Batch: 740; loss: 1.66; acc: 0.45
Batch: 760; loss: 1.95; acc: 0.3
Batch: 780; loss: 1.89; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.78; acc: 0.28
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288197354905923; val_accuracy: 0.37927945859872614 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.87; acc: 0.3
Batch: 20; loss: 1.77; acc: 0.39
Batch: 40; loss: 1.61; acc: 0.41
Batch: 60; loss: 1.92; acc: 0.28
Batch: 80; loss: 1.93; acc: 0.25
Batch: 100; loss: 1.69; acc: 0.39
Batch: 120; loss: 1.72; acc: 0.36
Batch: 140; loss: 1.77; acc: 0.31
Batch: 160; loss: 1.7; acc: 0.44
Batch: 180; loss: 1.83; acc: 0.3
Batch: 200; loss: 1.77; acc: 0.39
Batch: 220; loss: 1.72; acc: 0.42
Batch: 240; loss: 1.91; acc: 0.22
Batch: 260; loss: 1.6; acc: 0.47
Batch: 280; loss: 1.92; acc: 0.27
Batch: 300; loss: 1.66; acc: 0.34
Batch: 320; loss: 1.85; acc: 0.33
Batch: 340; loss: 1.96; acc: 0.33
Batch: 360; loss: 1.93; acc: 0.33
Batch: 380; loss: 1.76; acc: 0.42
Batch: 400; loss: 1.76; acc: 0.41
Batch: 420; loss: 1.72; acc: 0.39
Batch: 440; loss: 1.63; acc: 0.44
Batch: 460; loss: 1.62; acc: 0.38
Batch: 480; loss: 1.74; acc: 0.33
Batch: 500; loss: 1.78; acc: 0.34
Batch: 520; loss: 1.75; acc: 0.39
Batch: 540; loss: 1.84; acc: 0.34
Batch: 560; loss: 1.75; acc: 0.41
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.86; acc: 0.39
Batch: 620; loss: 1.68; acc: 0.38
Batch: 640; loss: 1.75; acc: 0.33
Batch: 660; loss: 1.75; acc: 0.38
Batch: 680; loss: 1.62; acc: 0.42
Batch: 700; loss: 1.63; acc: 0.31
Batch: 720; loss: 1.89; acc: 0.38
Batch: 740; loss: 1.67; acc: 0.33
Batch: 760; loss: 1.84; acc: 0.3
Batch: 780; loss: 1.82; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728746275233615; val_accuracy: 0.3791799363057325 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.62; acc: 0.39
Batch: 80; loss: 1.64; acc: 0.44
Batch: 100; loss: 1.72; acc: 0.42
Batch: 120; loss: 1.93; acc: 0.23
Batch: 140; loss: 1.84; acc: 0.33
Batch: 160; loss: 1.68; acc: 0.34
Batch: 180; loss: 1.66; acc: 0.33
Batch: 200; loss: 2.14; acc: 0.25
Batch: 220; loss: 1.88; acc: 0.34
Batch: 240; loss: 1.78; acc: 0.33
Batch: 260; loss: 1.89; acc: 0.3
Batch: 280; loss: 1.6; acc: 0.47
Batch: 300; loss: 1.58; acc: 0.41
Batch: 320; loss: 1.75; acc: 0.36
Batch: 340; loss: 1.67; acc: 0.47
Batch: 360; loss: 1.72; acc: 0.38
Batch: 380; loss: 2.05; acc: 0.31
Batch: 400; loss: 1.83; acc: 0.33
Batch: 420; loss: 1.63; acc: 0.39
Batch: 440; loss: 1.7; acc: 0.36
Batch: 460; loss: 1.78; acc: 0.36
Batch: 480; loss: 1.72; acc: 0.42
Batch: 500; loss: 1.73; acc: 0.38
Batch: 520; loss: 1.87; acc: 0.31
Batch: 540; loss: 1.88; acc: 0.3
Batch: 560; loss: 1.75; acc: 0.45
Batch: 580; loss: 1.78; acc: 0.41
Batch: 600; loss: 1.78; acc: 0.34
Batch: 620; loss: 1.78; acc: 0.39
Batch: 640; loss: 1.61; acc: 0.39
Batch: 660; loss: 1.71; acc: 0.39
Batch: 680; loss: 1.84; acc: 0.41
Batch: 700; loss: 1.99; acc: 0.25
Batch: 720; loss: 2.0; acc: 0.33
Batch: 740; loss: 1.78; acc: 0.44
Batch: 760; loss: 1.85; acc: 0.38
Batch: 780; loss: 1.69; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7289423912194124; val_accuracy: 0.37798566878980894 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.82; acc: 0.34
Batch: 20; loss: 1.79; acc: 0.45
Batch: 40; loss: 1.75; acc: 0.38
Batch: 60; loss: 1.96; acc: 0.36
Batch: 80; loss: 1.77; acc: 0.39
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.34
Batch: 140; loss: 1.68; acc: 0.41
Batch: 160; loss: 1.61; acc: 0.45
Batch: 180; loss: 1.56; acc: 0.5
Batch: 200; loss: 1.72; acc: 0.39
Batch: 220; loss: 1.65; acc: 0.42
Batch: 240; loss: 1.68; acc: 0.41
Batch: 260; loss: 1.92; acc: 0.23
Batch: 280; loss: 1.55; acc: 0.42
Batch: 300; loss: 1.79; acc: 0.39
Batch: 320; loss: 1.7; acc: 0.34
Batch: 340; loss: 1.69; acc: 0.38
Batch: 360; loss: 1.93; acc: 0.31
Batch: 380; loss: 1.79; acc: 0.42
Batch: 400; loss: 1.64; acc: 0.34
Batch: 420; loss: 1.85; acc: 0.33
Batch: 440; loss: 1.89; acc: 0.33
Batch: 460; loss: 1.61; acc: 0.34
Batch: 480; loss: 1.7; acc: 0.3
Batch: 500; loss: 1.5; acc: 0.48
Batch: 520; loss: 1.61; acc: 0.39
Batch: 540; loss: 1.67; acc: 0.36
Batch: 560; loss: 1.73; acc: 0.33
Batch: 580; loss: 1.61; acc: 0.42
Batch: 600; loss: 1.65; acc: 0.34
Batch: 620; loss: 1.64; acc: 0.44
Batch: 640; loss: 1.7; acc: 0.42
Batch: 660; loss: 1.7; acc: 0.41
Batch: 680; loss: 1.62; acc: 0.44
Batch: 700; loss: 1.59; acc: 0.36
Batch: 720; loss: 1.96; acc: 0.28
Batch: 740; loss: 1.95; acc: 0.23
Batch: 760; loss: 1.62; acc: 0.45
Batch: 780; loss: 1.71; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.78; acc: 0.3
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728810936022716; val_accuracy: 0.37898089171974525 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.71; acc: 0.42
Batch: 20; loss: 1.64; acc: 0.39
Batch: 40; loss: 1.75; acc: 0.33
Batch: 60; loss: 1.88; acc: 0.3
Batch: 80; loss: 1.51; acc: 0.45
Batch: 100; loss: 1.78; acc: 0.3
Batch: 120; loss: 1.8; acc: 0.27
Batch: 140; loss: 1.83; acc: 0.31
Batch: 160; loss: 1.69; acc: 0.42
Batch: 180; loss: 1.87; acc: 0.34
Batch: 200; loss: 1.89; acc: 0.3
Batch: 220; loss: 1.87; acc: 0.38
Batch: 240; loss: 1.77; acc: 0.38
Batch: 260; loss: 1.75; acc: 0.38
Batch: 280; loss: 1.84; acc: 0.38
Batch: 300; loss: 1.64; acc: 0.5
Batch: 320; loss: 1.86; acc: 0.3
Batch: 340; loss: 1.7; acc: 0.41
Batch: 360; loss: 1.74; acc: 0.39
Batch: 380; loss: 1.78; acc: 0.38
Batch: 400; loss: 1.96; acc: 0.33
Batch: 420; loss: 1.8; acc: 0.39
Batch: 440; loss: 1.86; acc: 0.31
Batch: 460; loss: 1.65; acc: 0.45
Batch: 480; loss: 1.85; acc: 0.36
Batch: 500; loss: 1.7; acc: 0.41
Batch: 520; loss: 1.97; acc: 0.31
Batch: 540; loss: 1.78; acc: 0.38
Batch: 560; loss: 1.54; acc: 0.47
Batch: 580; loss: 1.88; acc: 0.31
Batch: 600; loss: 1.74; acc: 0.38
Batch: 620; loss: 1.98; acc: 0.2
Batch: 640; loss: 1.82; acc: 0.34
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.7; acc: 0.34
Batch: 700; loss: 1.82; acc: 0.34
Batch: 720; loss: 1.69; acc: 0.3
Batch: 740; loss: 1.58; acc: 0.36
Batch: 760; loss: 1.88; acc: 0.36
Batch: 780; loss: 1.74; acc: 0.48
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.28
Batch: 120; loss: 2.07; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.39
Val Epoch over. val_loss: 1.7288643015418084; val_accuracy: 0.3791799363057325 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.76; acc: 0.33
Batch: 40; loss: 1.81; acc: 0.36
Batch: 60; loss: 1.75; acc: 0.36
Batch: 80; loss: 1.83; acc: 0.33
Batch: 100; loss: 1.76; acc: 0.36
Batch: 120; loss: 1.7; acc: 0.34
Batch: 140; loss: 1.86; acc: 0.22
Batch: 160; loss: 1.54; acc: 0.42
Batch: 180; loss: 1.52; acc: 0.45
Batch: 200; loss: 1.73; acc: 0.36
Batch: 220; loss: 1.86; acc: 0.41
Batch: 240; loss: 1.75; acc: 0.33
Batch: 260; loss: 1.61; acc: 0.45
Batch: 280; loss: 1.75; acc: 0.34
Batch: 300; loss: 1.81; acc: 0.33
Batch: 320; loss: 1.82; acc: 0.44
Batch: 340; loss: 1.71; acc: 0.44
Batch: 360; loss: 1.74; acc: 0.38
Batch: 380; loss: 1.59; acc: 0.42
Batch: 400; loss: 1.66; acc: 0.41
Batch: 420; loss: 1.84; acc: 0.27
Batch: 440; loss: 1.86; acc: 0.3
Batch: 460; loss: 1.74; acc: 0.41
Batch: 480; loss: 1.79; acc: 0.41
Batch: 500; loss: 1.92; acc: 0.27
Batch: 520; loss: 1.65; acc: 0.38
Batch: 540; loss: 1.81; acc: 0.38
Batch: 560; loss: 1.73; acc: 0.36
Batch: 580; loss: 1.86; acc: 0.31
Batch: 600; loss: 1.7; acc: 0.34
Batch: 620; loss: 1.74; acc: 0.34
Batch: 640; loss: 1.65; acc: 0.39
Batch: 660; loss: 1.66; acc: 0.34
Batch: 680; loss: 1.76; acc: 0.39
Batch: 700; loss: 1.84; acc: 0.41
Batch: 720; loss: 1.84; acc: 0.34
Batch: 740; loss: 1.75; acc: 0.34
Batch: 760; loss: 1.51; acc: 0.48
Batch: 780; loss: 1.73; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.52; acc: 0.48
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.39
Val Epoch over. val_loss: 1.7286665378862125; val_accuracy: 0.37878184713375795 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.82; acc: 0.38
Batch: 20; loss: 1.76; acc: 0.31
Batch: 40; loss: 1.67; acc: 0.44
Batch: 60; loss: 1.77; acc: 0.39
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.63; acc: 0.41
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.81; acc: 0.33
Batch: 160; loss: 1.97; acc: 0.3
Batch: 180; loss: 1.75; acc: 0.39
Batch: 200; loss: 1.59; acc: 0.44
Batch: 220; loss: 1.62; acc: 0.39
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.85; acc: 0.34
Batch: 280; loss: 1.95; acc: 0.28
Batch: 300; loss: 1.51; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.42
Batch: 340; loss: 1.87; acc: 0.33
Batch: 360; loss: 1.7; acc: 0.36
Batch: 380; loss: 1.66; acc: 0.38
Batch: 400; loss: 1.69; acc: 0.33
Batch: 420; loss: 1.71; acc: 0.44
Batch: 440; loss: 1.75; acc: 0.39
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 1.49; acc: 0.44
Batch: 500; loss: 1.75; acc: 0.3
Batch: 520; loss: 1.72; acc: 0.41
Batch: 540; loss: 1.51; acc: 0.52
Batch: 560; loss: 1.88; acc: 0.36
Batch: 580; loss: 1.85; acc: 0.3
Batch: 600; loss: 1.66; acc: 0.39
Batch: 620; loss: 1.8; acc: 0.42
Batch: 640; loss: 1.58; acc: 0.42
Batch: 660; loss: 1.78; acc: 0.3
Batch: 680; loss: 1.72; acc: 0.39
Batch: 700; loss: 1.99; acc: 0.27
Batch: 720; loss: 1.89; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.3
Batch: 760; loss: 1.77; acc: 0.33
Batch: 780; loss: 1.72; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.52; acc: 0.48
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.41
Batch: 100; loss: 1.79; acc: 0.28
Batch: 120; loss: 2.07; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.42
Val Epoch over. val_loss: 1.7289371308247754; val_accuracy: 0.37848328025477707 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.71; acc: 0.41
Batch: 40; loss: 1.77; acc: 0.36
Batch: 60; loss: 1.63; acc: 0.42
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.87; acc: 0.3
Batch: 120; loss: 2.02; acc: 0.3
Batch: 140; loss: 1.67; acc: 0.42
Batch: 160; loss: 1.77; acc: 0.34
Batch: 180; loss: 1.55; acc: 0.44
Batch: 200; loss: 1.59; acc: 0.38
Batch: 220; loss: 1.87; acc: 0.3
Batch: 240; loss: 1.73; acc: 0.34
Batch: 260; loss: 1.72; acc: 0.34
Batch: 280; loss: 1.76; acc: 0.31
Batch: 300; loss: 1.83; acc: 0.42
Batch: 320; loss: 1.9; acc: 0.31
Batch: 340; loss: 1.69; acc: 0.34
Batch: 360; loss: 1.84; acc: 0.28
Batch: 380; loss: 1.58; acc: 0.39
Batch: 400; loss: 1.74; acc: 0.38
Batch: 420; loss: 1.81; acc: 0.42
Batch: 440; loss: 1.63; acc: 0.5
Batch: 460; loss: 1.66; acc: 0.41
Batch: 480; loss: 1.58; acc: 0.34
Batch: 500; loss: 1.71; acc: 0.36
Batch: 520; loss: 1.78; acc: 0.31
Batch: 540; loss: 1.68; acc: 0.41
Batch: 560; loss: 1.63; acc: 0.42
Batch: 580; loss: 1.74; acc: 0.38
Batch: 600; loss: 1.78; acc: 0.34
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.74; acc: 0.31
Batch: 660; loss: 1.6; acc: 0.44
Batch: 680; loss: 1.92; acc: 0.25
Batch: 700; loss: 1.47; acc: 0.47
Batch: 720; loss: 1.61; acc: 0.39
Batch: 740; loss: 1.63; acc: 0.45
Batch: 760; loss: 1.8; acc: 0.39
Batch: 780; loss: 1.91; acc: 0.3
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287674914499758; val_accuracy: 0.3794785031847134 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.83; acc: 0.31
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.67; acc: 0.39
Batch: 60; loss: 1.91; acc: 0.34
Batch: 80; loss: 1.52; acc: 0.48
Batch: 100; loss: 1.81; acc: 0.3
Batch: 120; loss: 1.72; acc: 0.39
Batch: 140; loss: 1.75; acc: 0.3
Batch: 160; loss: 1.74; acc: 0.33
Batch: 180; loss: 1.78; acc: 0.33
Batch: 200; loss: 1.62; acc: 0.41
Batch: 220; loss: 1.74; acc: 0.39
Batch: 240; loss: 1.6; acc: 0.44
Batch: 260; loss: 1.77; acc: 0.38
Batch: 280; loss: 1.75; acc: 0.33
Batch: 300; loss: 1.75; acc: 0.41
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 1.65; acc: 0.34
Batch: 360; loss: 1.62; acc: 0.44
Batch: 380; loss: 1.75; acc: 0.36
Batch: 400; loss: 1.71; acc: 0.41
Batch: 420; loss: 1.79; acc: 0.34
Batch: 440; loss: 1.74; acc: 0.36
Batch: 460; loss: 1.61; acc: 0.44
Batch: 480; loss: 1.6; acc: 0.48
Batch: 500; loss: 1.73; acc: 0.38
Batch: 520; loss: 1.65; acc: 0.38
Batch: 540; loss: 1.88; acc: 0.27
Batch: 560; loss: 1.74; acc: 0.42
Batch: 580; loss: 1.89; acc: 0.34
Batch: 600; loss: 1.87; acc: 0.31
Batch: 620; loss: 1.62; acc: 0.34
Batch: 640; loss: 1.89; acc: 0.33
Batch: 660; loss: 1.58; acc: 0.45
Batch: 680; loss: 1.95; acc: 0.25
Batch: 700; loss: 1.91; acc: 0.31
Batch: 720; loss: 1.8; acc: 0.39
Batch: 740; loss: 1.62; acc: 0.41
Batch: 760; loss: 1.88; acc: 0.34
Batch: 780; loss: 1.7; acc: 0.38
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728714942172834; val_accuracy: 0.3788813694267516 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.76; acc: 0.39
Batch: 40; loss: 1.79; acc: 0.28
Batch: 60; loss: 1.56; acc: 0.34
Batch: 80; loss: 1.73; acc: 0.41
Batch: 100; loss: 1.64; acc: 0.45
Batch: 120; loss: 1.91; acc: 0.33
Batch: 140; loss: 1.77; acc: 0.38
Batch: 160; loss: 1.7; acc: 0.47
Batch: 180; loss: 1.75; acc: 0.36
Batch: 200; loss: 1.78; acc: 0.3
Batch: 220; loss: 1.61; acc: 0.39
Batch: 240; loss: 1.92; acc: 0.25
Batch: 260; loss: 1.82; acc: 0.34
Batch: 280; loss: 1.76; acc: 0.34
Batch: 300; loss: 1.72; acc: 0.42
Batch: 320; loss: 1.62; acc: 0.36
Batch: 340; loss: 1.72; acc: 0.45
Batch: 360; loss: 1.75; acc: 0.38
Batch: 380; loss: 1.66; acc: 0.38
Batch: 400; loss: 1.9; acc: 0.38
Batch: 420; loss: 1.94; acc: 0.25
Batch: 440; loss: 1.62; acc: 0.41
Batch: 460; loss: 1.75; acc: 0.44
Batch: 480; loss: 1.66; acc: 0.44
Batch: 500; loss: 1.87; acc: 0.34
Batch: 520; loss: 1.73; acc: 0.42
Batch: 540; loss: 1.84; acc: 0.36
Batch: 560; loss: 1.92; acc: 0.41
Batch: 580; loss: 1.91; acc: 0.38
Batch: 600; loss: 1.72; acc: 0.39
Batch: 620; loss: 1.73; acc: 0.39
Batch: 640; loss: 1.73; acc: 0.38
Batch: 660; loss: 1.79; acc: 0.34
Batch: 680; loss: 1.79; acc: 0.39
Batch: 700; loss: 1.74; acc: 0.34
Batch: 720; loss: 1.74; acc: 0.41
Batch: 740; loss: 1.57; acc: 0.48
Batch: 760; loss: 1.62; acc: 0.5
Batch: 780; loss: 1.79; acc: 0.34
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287314780958138; val_accuracy: 0.3788813694267516 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.96; acc: 0.3
Batch: 20; loss: 1.9; acc: 0.33
Batch: 40; loss: 1.79; acc: 0.34
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.75; acc: 0.38
Batch: 100; loss: 1.83; acc: 0.31
Batch: 120; loss: 1.75; acc: 0.34
Batch: 140; loss: 1.77; acc: 0.3
Batch: 160; loss: 1.67; acc: 0.36
Batch: 180; loss: 1.69; acc: 0.39
Batch: 200; loss: 1.85; acc: 0.38
Batch: 220; loss: 1.73; acc: 0.38
Batch: 240; loss: 1.72; acc: 0.33
Batch: 260; loss: 1.68; acc: 0.44
Batch: 280; loss: 1.74; acc: 0.42
Batch: 300; loss: 1.65; acc: 0.45
Batch: 320; loss: 1.65; acc: 0.44
Batch: 340; loss: 1.63; acc: 0.38
Batch: 360; loss: 1.66; acc: 0.45
Batch: 380; loss: 1.64; acc: 0.33
Batch: 400; loss: 1.78; acc: 0.38
Batch: 420; loss: 1.62; acc: 0.45
Batch: 440; loss: 1.98; acc: 0.28
Batch: 460; loss: 1.52; acc: 0.47
Batch: 480; loss: 1.69; acc: 0.31
Batch: 500; loss: 1.93; acc: 0.27
Batch: 520; loss: 1.82; acc: 0.39
Batch: 540; loss: 1.71; acc: 0.42
Batch: 560; loss: 1.88; acc: 0.44
Batch: 580; loss: 1.62; acc: 0.42
Batch: 600; loss: 1.79; acc: 0.31
Batch: 620; loss: 1.8; acc: 0.28
Batch: 640; loss: 1.67; acc: 0.41
Batch: 660; loss: 1.66; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.45
Batch: 700; loss: 1.72; acc: 0.36
Batch: 720; loss: 1.7; acc: 0.47
Batch: 740; loss: 1.68; acc: 0.41
Batch: 760; loss: 1.63; acc: 0.42
Batch: 780; loss: 1.79; acc: 0.34
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7286889720114933; val_accuracy: 0.37937898089171973 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.71; acc: 0.38
Batch: 20; loss: 1.78; acc: 0.3
Batch: 40; loss: 1.71; acc: 0.41
Batch: 60; loss: 1.99; acc: 0.25
Batch: 80; loss: 1.79; acc: 0.39
Batch: 100; loss: 2.06; acc: 0.3
Batch: 120; loss: 1.67; acc: 0.41
Batch: 140; loss: 1.65; acc: 0.42
Batch: 160; loss: 1.96; acc: 0.28
Batch: 180; loss: 1.74; acc: 0.38
Batch: 200; loss: 1.76; acc: 0.41
Batch: 220; loss: 1.76; acc: 0.39
Batch: 240; loss: 1.89; acc: 0.38
Batch: 260; loss: 1.7; acc: 0.41
Batch: 280; loss: 1.91; acc: 0.31
Batch: 300; loss: 1.6; acc: 0.41
Batch: 320; loss: 1.72; acc: 0.38
Batch: 340; loss: 1.74; acc: 0.31
Batch: 360; loss: 1.56; acc: 0.52
Batch: 380; loss: 1.79; acc: 0.36
Batch: 400; loss: 1.53; acc: 0.45
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.82; acc: 0.41
Batch: 460; loss: 1.86; acc: 0.31
Batch: 480; loss: 1.75; acc: 0.38
Batch: 500; loss: 1.51; acc: 0.61
Batch: 520; loss: 1.57; acc: 0.52
Batch: 540; loss: 1.6; acc: 0.47
Batch: 560; loss: 1.82; acc: 0.33
Batch: 580; loss: 1.7; acc: 0.41
Batch: 600; loss: 1.71; acc: 0.31
Batch: 620; loss: 1.67; acc: 0.38
Batch: 640; loss: 1.76; acc: 0.41
Batch: 660; loss: 1.75; acc: 0.47
Batch: 680; loss: 1.94; acc: 0.31
Batch: 700; loss: 1.74; acc: 0.41
Batch: 720; loss: 1.81; acc: 0.41
Batch: 740; loss: 1.69; acc: 0.33
Batch: 760; loss: 1.85; acc: 0.28
Batch: 780; loss: 1.77; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287703798075391; val_accuracy: 0.379578025477707 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.6; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.77; acc: 0.41
Batch: 80; loss: 1.64; acc: 0.42
Batch: 100; loss: 1.71; acc: 0.47
Batch: 120; loss: 1.67; acc: 0.41
Batch: 140; loss: 1.5; acc: 0.47
Batch: 160; loss: 1.76; acc: 0.28
Batch: 180; loss: 1.75; acc: 0.41
Batch: 200; loss: 1.8; acc: 0.41
Batch: 220; loss: 1.68; acc: 0.39
Batch: 240; loss: 1.73; acc: 0.44
Batch: 260; loss: 1.73; acc: 0.34
Batch: 280; loss: 1.69; acc: 0.38
Batch: 300; loss: 1.78; acc: 0.38
Batch: 320; loss: 1.63; acc: 0.47
Batch: 340; loss: 1.86; acc: 0.38
Batch: 360; loss: 1.79; acc: 0.33
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.98; acc: 0.36
Batch: 420; loss: 1.57; acc: 0.44
Batch: 440; loss: 1.75; acc: 0.36
Batch: 460; loss: 1.64; acc: 0.38
Batch: 480; loss: 1.73; acc: 0.39
Batch: 500; loss: 1.68; acc: 0.42
Batch: 520; loss: 1.82; acc: 0.36
Batch: 540; loss: 1.87; acc: 0.28
Batch: 560; loss: 1.58; acc: 0.45
Batch: 580; loss: 1.6; acc: 0.36
Batch: 600; loss: 1.73; acc: 0.36
Batch: 620; loss: 1.64; acc: 0.41
Batch: 640; loss: 1.68; acc: 0.44
Batch: 660; loss: 1.81; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.41
Batch: 700; loss: 1.83; acc: 0.28
Batch: 720; loss: 1.75; acc: 0.38
Batch: 740; loss: 1.78; acc: 0.31
Batch: 760; loss: 1.67; acc: 0.41
Batch: 780; loss: 1.54; acc: 0.44
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287938685933495; val_accuracy: 0.37898089171974525 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.76; acc: 0.3
Batch: 20; loss: 1.62; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.38
Batch: 60; loss: 1.73; acc: 0.39
Batch: 80; loss: 1.59; acc: 0.45
Batch: 100; loss: 1.79; acc: 0.34
Batch: 120; loss: 1.9; acc: 0.27
Batch: 140; loss: 1.89; acc: 0.28
Batch: 160; loss: 1.74; acc: 0.44
Batch: 180; loss: 1.76; acc: 0.42
Batch: 200; loss: 1.82; acc: 0.36
Batch: 220; loss: 1.77; acc: 0.47
Batch: 240; loss: 1.69; acc: 0.39
Batch: 260; loss: 1.85; acc: 0.38
Batch: 280; loss: 1.8; acc: 0.34
Batch: 300; loss: 1.7; acc: 0.41
Batch: 320; loss: 1.55; acc: 0.42
Batch: 340; loss: 1.63; acc: 0.45
Batch: 360; loss: 1.71; acc: 0.41
Batch: 380; loss: 1.69; acc: 0.44
Batch: 400; loss: 1.63; acc: 0.48
Batch: 420; loss: 1.59; acc: 0.38
Batch: 440; loss: 1.67; acc: 0.44
Batch: 460; loss: 2.04; acc: 0.3
Batch: 480; loss: 1.82; acc: 0.38
Batch: 500; loss: 1.74; acc: 0.39
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.82; acc: 0.33
Batch: 560; loss: 1.78; acc: 0.3
Batch: 580; loss: 1.61; acc: 0.41
Batch: 600; loss: 1.93; acc: 0.33
Batch: 620; loss: 1.67; acc: 0.44
Batch: 640; loss: 1.57; acc: 0.44
Batch: 660; loss: 1.74; acc: 0.31
Batch: 680; loss: 1.72; acc: 0.39
Batch: 700; loss: 1.92; acc: 0.3
Batch: 720; loss: 1.59; acc: 0.44
Batch: 740; loss: 1.65; acc: 0.42
Batch: 760; loss: 1.59; acc: 0.41
Batch: 780; loss: 1.66; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728755133167194; val_accuracy: 0.3783837579617834 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.97; acc: 0.31
Batch: 20; loss: 1.52; acc: 0.5
Batch: 40; loss: 1.61; acc: 0.41
Batch: 60; loss: 1.72; acc: 0.38
Batch: 80; loss: 1.67; acc: 0.55
Batch: 100; loss: 1.66; acc: 0.41
Batch: 120; loss: 1.8; acc: 0.3
Batch: 140; loss: 1.76; acc: 0.42
Batch: 160; loss: 1.67; acc: 0.33
Batch: 180; loss: 1.76; acc: 0.36
Batch: 200; loss: 1.77; acc: 0.41
Batch: 220; loss: 1.58; acc: 0.44
Batch: 240; loss: 1.85; acc: 0.36
Batch: 260; loss: 1.63; acc: 0.42
Batch: 280; loss: 2.01; acc: 0.31
Batch: 300; loss: 1.84; acc: 0.38
Batch: 320; loss: 1.53; acc: 0.5
Batch: 340; loss: 1.71; acc: 0.41
Batch: 360; loss: 1.69; acc: 0.42
Batch: 380; loss: 1.5; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.33
Batch: 420; loss: 1.66; acc: 0.33
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.75; acc: 0.38
Batch: 480; loss: 1.67; acc: 0.44
Batch: 500; loss: 1.82; acc: 0.28
Batch: 520; loss: 1.68; acc: 0.45
Batch: 540; loss: 1.64; acc: 0.48
Batch: 560; loss: 1.71; acc: 0.33
Batch: 580; loss: 1.71; acc: 0.38
Batch: 600; loss: 1.76; acc: 0.31
Batch: 620; loss: 1.92; acc: 0.31
Batch: 640; loss: 1.52; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.38
Batch: 680; loss: 1.56; acc: 0.42
Batch: 700; loss: 1.44; acc: 0.47
Batch: 720; loss: 1.73; acc: 0.36
Batch: 740; loss: 1.62; acc: 0.39
Batch: 760; loss: 1.61; acc: 0.52
Batch: 780; loss: 1.65; acc: 0.45
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287368425138436; val_accuracy: 0.3785828025477707 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.9; acc: 0.28
Batch: 20; loss: 1.82; acc: 0.28
Batch: 40; loss: 1.85; acc: 0.3
Batch: 60; loss: 1.79; acc: 0.45
Batch: 80; loss: 1.74; acc: 0.3
Batch: 100; loss: 1.99; acc: 0.25
Batch: 120; loss: 1.56; acc: 0.39
Batch: 140; loss: 1.76; acc: 0.41
Batch: 160; loss: 1.76; acc: 0.42
Batch: 180; loss: 1.74; acc: 0.34
Batch: 200; loss: 1.76; acc: 0.31
Batch: 220; loss: 1.78; acc: 0.34
Batch: 240; loss: 1.88; acc: 0.31
Batch: 260; loss: 1.74; acc: 0.39
Batch: 280; loss: 1.67; acc: 0.48
Batch: 300; loss: 1.93; acc: 0.31
Batch: 320; loss: 2.12; acc: 0.2
Batch: 340; loss: 1.68; acc: 0.39
Batch: 360; loss: 1.71; acc: 0.38
Batch: 380; loss: 1.7; acc: 0.41
Batch: 400; loss: 1.72; acc: 0.31
Batch: 420; loss: 1.77; acc: 0.31
Batch: 440; loss: 1.69; acc: 0.42
Batch: 460; loss: 1.86; acc: 0.34
Batch: 480; loss: 1.79; acc: 0.39
Batch: 500; loss: 1.79; acc: 0.41
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.84; acc: 0.36
Batch: 560; loss: 1.78; acc: 0.34
Batch: 580; loss: 1.85; acc: 0.3
Batch: 600; loss: 1.83; acc: 0.33
Batch: 620; loss: 1.62; acc: 0.42
Batch: 640; loss: 1.64; acc: 0.39
Batch: 660; loss: 1.75; acc: 0.28
Batch: 680; loss: 1.7; acc: 0.39
Batch: 700; loss: 1.77; acc: 0.34
Batch: 720; loss: 1.63; acc: 0.36
Batch: 740; loss: 1.81; acc: 0.42
Batch: 760; loss: 1.94; acc: 0.31
Batch: 780; loss: 1.57; acc: 0.55
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728857927261644; val_accuracy: 0.3791799363057325 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.67; acc: 0.39
Batch: 20; loss: 1.69; acc: 0.36
Batch: 40; loss: 1.76; acc: 0.39
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.79; acc: 0.36
Batch: 100; loss: 2.03; acc: 0.25
Batch: 120; loss: 1.5; acc: 0.5
Batch: 140; loss: 1.61; acc: 0.41
Batch: 160; loss: 1.95; acc: 0.25
Batch: 180; loss: 1.58; acc: 0.39
Batch: 200; loss: 1.62; acc: 0.45
Batch: 220; loss: 1.7; acc: 0.34
Batch: 240; loss: 1.7; acc: 0.42
Batch: 260; loss: 1.67; acc: 0.39
Batch: 280; loss: 1.68; acc: 0.39
Batch: 300; loss: 1.68; acc: 0.45
Batch: 320; loss: 1.76; acc: 0.34
Batch: 340; loss: 1.85; acc: 0.28
Batch: 360; loss: 1.62; acc: 0.44
Batch: 380; loss: 1.77; acc: 0.31
Batch: 400; loss: 1.57; acc: 0.41
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.71; acc: 0.3
Batch: 460; loss: 1.63; acc: 0.45
Batch: 480; loss: 1.86; acc: 0.42
Batch: 500; loss: 1.5; acc: 0.48
Batch: 520; loss: 1.69; acc: 0.33
Batch: 540; loss: 1.68; acc: 0.36
Batch: 560; loss: 1.82; acc: 0.41
Batch: 580; loss: 1.56; acc: 0.45
Batch: 600; loss: 1.89; acc: 0.34
Batch: 620; loss: 1.64; acc: 0.41
Batch: 640; loss: 1.42; acc: 0.5
Batch: 660; loss: 1.59; acc: 0.5
Batch: 680; loss: 1.76; acc: 0.34
Batch: 700; loss: 1.76; acc: 0.3
Batch: 720; loss: 1.78; acc: 0.36
Batch: 740; loss: 1.83; acc: 0.31
Batch: 760; loss: 1.48; acc: 0.48
Batch: 780; loss: 1.94; acc: 0.28
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.34
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.25
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728889054553524; val_accuracy: 0.3785828025477707 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.81; acc: 0.34
Batch: 20; loss: 1.7; acc: 0.38
Batch: 40; loss: 1.62; acc: 0.44
Batch: 60; loss: 1.62; acc: 0.47
Batch: 80; loss: 1.84; acc: 0.33
Batch: 100; loss: 1.8; acc: 0.34
Batch: 120; loss: 1.69; acc: 0.34
Batch: 140; loss: 1.37; acc: 0.58
Batch: 160; loss: 1.8; acc: 0.42
Batch: 180; loss: 1.78; acc: 0.33
Batch: 200; loss: 1.86; acc: 0.41
Batch: 220; loss: 1.62; acc: 0.39
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.89; acc: 0.34
Batch: 280; loss: 1.77; acc: 0.33
Batch: 300; loss: 1.84; acc: 0.39
Batch: 320; loss: 1.79; acc: 0.36
Batch: 340; loss: 1.73; acc: 0.38
Batch: 360; loss: 1.78; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.38
Batch: 400; loss: 1.9; acc: 0.33
Batch: 420; loss: 1.8; acc: 0.31
Batch: 440; loss: 1.82; acc: 0.3
Batch: 460; loss: 1.56; acc: 0.36
Batch: 480; loss: 1.72; acc: 0.42
Batch: 500; loss: 1.85; acc: 0.33
Batch: 520; loss: 1.54; acc: 0.5
Batch: 540; loss: 1.81; acc: 0.39
Batch: 560; loss: 1.63; acc: 0.39
Batch: 580; loss: 1.94; acc: 0.27
Batch: 600; loss: 1.76; acc: 0.39
Batch: 620; loss: 1.65; acc: 0.38
Batch: 640; loss: 1.69; acc: 0.42
Batch: 660; loss: 1.9; acc: 0.25
Batch: 680; loss: 1.62; acc: 0.47
Batch: 700; loss: 1.54; acc: 0.47
Batch: 720; loss: 1.82; acc: 0.36
Batch: 740; loss: 1.77; acc: 0.34
Batch: 760; loss: 1.74; acc: 0.38
Batch: 780; loss: 1.77; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.38
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288627366351474; val_accuracy: 0.37908041401273884 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 1.68; acc: 0.36
Batch: 40; loss: 1.89; acc: 0.33
Batch: 60; loss: 1.94; acc: 0.3
Batch: 80; loss: 1.83; acc: 0.36
Batch: 100; loss: 1.46; acc: 0.58
Batch: 120; loss: 1.64; acc: 0.31
Batch: 140; loss: 1.82; acc: 0.41
Batch: 160; loss: 1.86; acc: 0.34
Batch: 180; loss: 1.61; acc: 0.36
Batch: 200; loss: 1.62; acc: 0.41
Batch: 220; loss: 1.88; acc: 0.34
Batch: 240; loss: 1.77; acc: 0.42
Batch: 260; loss: 1.81; acc: 0.33
Batch: 280; loss: 1.73; acc: 0.41
Batch: 300; loss: 1.72; acc: 0.39
Batch: 320; loss: 1.61; acc: 0.39
Batch: 340; loss: 1.79; acc: 0.28
Batch: 360; loss: 1.71; acc: 0.38
Batch: 380; loss: 1.55; acc: 0.48
Batch: 400; loss: 1.63; acc: 0.41
Batch: 420; loss: 1.54; acc: 0.44
Batch: 440; loss: 1.73; acc: 0.36
Batch: 460; loss: 1.79; acc: 0.3
Batch: 480; loss: 1.66; acc: 0.36
Batch: 500; loss: 1.75; acc: 0.36
Batch: 520; loss: 2.06; acc: 0.33
Batch: 540; loss: 1.65; acc: 0.33
Batch: 560; loss: 1.58; acc: 0.47
Batch: 580; loss: 1.84; acc: 0.33
Batch: 600; loss: 1.82; acc: 0.39
Batch: 620; loss: 1.76; acc: 0.38
Batch: 640; loss: 1.71; acc: 0.39
Batch: 660; loss: 1.59; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.38
Batch: 700; loss: 1.54; acc: 0.5
Batch: 720; loss: 1.54; acc: 0.42
Batch: 740; loss: 1.78; acc: 0.39
Batch: 760; loss: 1.84; acc: 0.28
Batch: 780; loss: 1.9; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287502410305533; val_accuracy: 0.3786823248407643 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.74; acc: 0.39
Batch: 40; loss: 1.62; acc: 0.45
Batch: 60; loss: 1.7; acc: 0.44
Batch: 80; loss: 1.78; acc: 0.45
Batch: 100; loss: 1.68; acc: 0.41
Batch: 120; loss: 1.6; acc: 0.44
Batch: 140; loss: 1.64; acc: 0.42
Batch: 160; loss: 1.82; acc: 0.31
Batch: 180; loss: 1.59; acc: 0.39
Batch: 200; loss: 1.72; acc: 0.33
Batch: 220; loss: 1.88; acc: 0.33
Batch: 240; loss: 1.89; acc: 0.41
Batch: 260; loss: 1.68; acc: 0.36
Batch: 280; loss: 1.81; acc: 0.36
Batch: 300; loss: 1.81; acc: 0.34
Batch: 320; loss: 1.72; acc: 0.34
Batch: 340; loss: 1.87; acc: 0.3
Batch: 360; loss: 1.64; acc: 0.42
Batch: 380; loss: 1.8; acc: 0.38
Batch: 400; loss: 1.74; acc: 0.3
Batch: 420; loss: 1.86; acc: 0.31
Batch: 440; loss: 1.41; acc: 0.48
Batch: 460; loss: 1.61; acc: 0.48
Batch: 480; loss: 1.94; acc: 0.34
Batch: 500; loss: 1.83; acc: 0.33
Batch: 520; loss: 1.55; acc: 0.44
Batch: 540; loss: 1.75; acc: 0.39
Batch: 560; loss: 1.98; acc: 0.34
Batch: 580; loss: 1.67; acc: 0.34
Batch: 600; loss: 1.76; acc: 0.31
Batch: 620; loss: 1.84; acc: 0.34
Batch: 640; loss: 1.8; acc: 0.36
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.72; acc: 0.38
Batch: 700; loss: 1.61; acc: 0.45
Batch: 720; loss: 1.78; acc: 0.33
Batch: 740; loss: 1.78; acc: 0.38
Batch: 760; loss: 1.67; acc: 0.41
Batch: 780; loss: 1.72; acc: 0.42
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287844214469763; val_accuracy: 0.3783837579617834 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.36
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.64; acc: 0.42
Batch: 60; loss: 1.83; acc: 0.45
Batch: 80; loss: 1.52; acc: 0.42
Batch: 100; loss: 1.53; acc: 0.47
Batch: 120; loss: 1.81; acc: 0.36
Batch: 140; loss: 1.66; acc: 0.33
Batch: 160; loss: 1.7; acc: 0.44
Batch: 180; loss: 1.68; acc: 0.39
Batch: 200; loss: 1.91; acc: 0.33
Batch: 220; loss: 1.84; acc: 0.41
Batch: 240; loss: 1.63; acc: 0.38
Batch: 260; loss: 1.67; acc: 0.45
Batch: 280; loss: 1.82; acc: 0.36
Batch: 300; loss: 1.7; acc: 0.33
Batch: 320; loss: 1.72; acc: 0.3
Batch: 340; loss: 1.61; acc: 0.48
Batch: 360; loss: 1.75; acc: 0.44
Batch: 380; loss: 1.68; acc: 0.39
Batch: 400; loss: 1.5; acc: 0.5
Batch: 420; loss: 1.79; acc: 0.41
Batch: 440; loss: 1.76; acc: 0.36
Batch: 460; loss: 1.78; acc: 0.36
Batch: 480; loss: 1.68; acc: 0.41
Batch: 500; loss: 1.77; acc: 0.36
Batch: 520; loss: 1.88; acc: 0.3
Batch: 540; loss: 1.6; acc: 0.44
Batch: 560; loss: 1.69; acc: 0.41
Batch: 580; loss: 1.94; acc: 0.34
Batch: 600; loss: 1.88; acc: 0.33
Batch: 620; loss: 1.86; acc: 0.3
Batch: 640; loss: 1.74; acc: 0.38
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.84; acc: 0.34
Batch: 700; loss: 1.77; acc: 0.38
Batch: 720; loss: 1.68; acc: 0.41
Batch: 740; loss: 1.72; acc: 0.41
Batch: 760; loss: 1.69; acc: 0.39
Batch: 780; loss: 1.77; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7287767495319342; val_accuracy: 0.3783837579617834 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.44
Batch: 20; loss: 1.62; acc: 0.41
Batch: 40; loss: 1.74; acc: 0.31
Batch: 60; loss: 1.8; acc: 0.33
Batch: 80; loss: 1.74; acc: 0.31
Batch: 100; loss: 1.83; acc: 0.34
Batch: 120; loss: 1.85; acc: 0.34
Batch: 140; loss: 1.63; acc: 0.39
Batch: 160; loss: 1.54; acc: 0.45
Batch: 180; loss: 1.77; acc: 0.27
Batch: 200; loss: 1.82; acc: 0.38
Batch: 220; loss: 1.52; acc: 0.5
Batch: 240; loss: 1.63; acc: 0.42
Batch: 260; loss: 1.98; acc: 0.33
Batch: 280; loss: 1.69; acc: 0.38
Batch: 300; loss: 1.94; acc: 0.34
Batch: 320; loss: 2.01; acc: 0.33
Batch: 340; loss: 1.71; acc: 0.39
Batch: 360; loss: 1.81; acc: 0.34
Batch: 380; loss: 1.68; acc: 0.3
Batch: 400; loss: 1.94; acc: 0.23
Batch: 420; loss: 1.7; acc: 0.38
Batch: 440; loss: 1.88; acc: 0.38
Batch: 460; loss: 1.72; acc: 0.38
Batch: 480; loss: 1.69; acc: 0.39
Batch: 500; loss: 1.81; acc: 0.45
Batch: 520; loss: 1.78; acc: 0.34
Batch: 540; loss: 1.88; acc: 0.33
Batch: 560; loss: 1.85; acc: 0.31
Batch: 580; loss: 1.74; acc: 0.39
Batch: 600; loss: 1.77; acc: 0.41
Batch: 620; loss: 1.75; acc: 0.38
Batch: 640; loss: 1.8; acc: 0.39
Batch: 660; loss: 1.9; acc: 0.34
Batch: 680; loss: 1.65; acc: 0.38
Batch: 700; loss: 1.72; acc: 0.28
Batch: 720; loss: 1.76; acc: 0.38
Batch: 740; loss: 1.81; acc: 0.42
Batch: 760; loss: 1.8; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.38
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288179215352246; val_accuracy: 0.3782842356687898 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.51; acc: 0.53
Batch: 60; loss: 1.82; acc: 0.28
Batch: 80; loss: 1.87; acc: 0.3
Batch: 100; loss: 1.78; acc: 0.38
Batch: 120; loss: 1.89; acc: 0.38
Batch: 140; loss: 1.73; acc: 0.38
Batch: 160; loss: 1.61; acc: 0.39
Batch: 180; loss: 1.83; acc: 0.36
Batch: 200; loss: 1.63; acc: 0.42
Batch: 220; loss: 1.66; acc: 0.45
Batch: 240; loss: 1.73; acc: 0.39
Batch: 260; loss: 1.62; acc: 0.33
Batch: 280; loss: 1.64; acc: 0.36
Batch: 300; loss: 1.82; acc: 0.31
Batch: 320; loss: 1.8; acc: 0.28
Batch: 340; loss: 1.58; acc: 0.47
Batch: 360; loss: 1.68; acc: 0.42
Batch: 380; loss: 1.69; acc: 0.41
Batch: 400; loss: 1.9; acc: 0.39
Batch: 420; loss: 1.79; acc: 0.34
Batch: 440; loss: 1.93; acc: 0.34
Batch: 460; loss: 1.88; acc: 0.36
Batch: 480; loss: 1.63; acc: 0.45
Batch: 500; loss: 1.78; acc: 0.41
Batch: 520; loss: 1.92; acc: 0.34
Batch: 540; loss: 1.69; acc: 0.38
Batch: 560; loss: 1.68; acc: 0.34
Batch: 580; loss: 1.68; acc: 0.34
Batch: 600; loss: 1.74; acc: 0.45
Batch: 620; loss: 1.98; acc: 0.33
Batch: 640; loss: 1.74; acc: 0.31
Batch: 660; loss: 1.7; acc: 0.41
Batch: 680; loss: 1.68; acc: 0.38
Batch: 700; loss: 1.98; acc: 0.27
Batch: 720; loss: 1.65; acc: 0.41
Batch: 740; loss: 1.63; acc: 0.39
Batch: 760; loss: 1.81; acc: 0.36
Batch: 780; loss: 1.74; acc: 0.39
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288029748163405; val_accuracy: 0.3788813694267516 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.02; acc: 0.25
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.67; acc: 0.38
Batch: 60; loss: 1.83; acc: 0.28
Batch: 80; loss: 2.03; acc: 0.25
Batch: 100; loss: 1.74; acc: 0.33
Batch: 120; loss: 1.66; acc: 0.42
Batch: 140; loss: 1.73; acc: 0.39
Batch: 160; loss: 1.91; acc: 0.3
Batch: 180; loss: 1.72; acc: 0.38
Batch: 200; loss: 1.73; acc: 0.39
Batch: 220; loss: 1.74; acc: 0.38
Batch: 240; loss: 2.03; acc: 0.19
Batch: 260; loss: 1.77; acc: 0.38
Batch: 280; loss: 1.85; acc: 0.36
Batch: 300; loss: 1.65; acc: 0.38
Batch: 320; loss: 1.74; acc: 0.45
Batch: 340; loss: 1.78; acc: 0.39
Batch: 360; loss: 1.8; acc: 0.34
Batch: 380; loss: 1.46; acc: 0.5
Batch: 400; loss: 1.71; acc: 0.39
Batch: 420; loss: 1.92; acc: 0.27
Batch: 440; loss: 1.93; acc: 0.36
Batch: 460; loss: 1.8; acc: 0.38
Batch: 480; loss: 1.75; acc: 0.38
Batch: 500; loss: 1.84; acc: 0.31
Batch: 520; loss: 1.77; acc: 0.33
Batch: 540; loss: 1.81; acc: 0.33
Batch: 560; loss: 1.81; acc: 0.39
Batch: 580; loss: 1.66; acc: 0.47
Batch: 600; loss: 1.73; acc: 0.41
Batch: 620; loss: 1.78; acc: 0.41
Batch: 640; loss: 1.78; acc: 0.34
Batch: 660; loss: 1.73; acc: 0.41
Batch: 680; loss: 1.84; acc: 0.3
Batch: 700; loss: 1.67; acc: 0.39
Batch: 720; loss: 1.63; acc: 0.42
Batch: 740; loss: 1.57; acc: 0.47
Batch: 760; loss: 1.7; acc: 0.41
Batch: 780; loss: 1.91; acc: 0.34
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288337133492633; val_accuracy: 0.37878184713375795 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.75; acc: 0.33
Batch: 60; loss: 1.72; acc: 0.45
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.86; acc: 0.34
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.72; acc: 0.39
Batch: 160; loss: 1.83; acc: 0.41
Batch: 180; loss: 1.67; acc: 0.44
Batch: 200; loss: 1.62; acc: 0.44
Batch: 220; loss: 1.78; acc: 0.3
Batch: 240; loss: 1.79; acc: 0.36
Batch: 260; loss: 1.77; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.33
Batch: 300; loss: 1.65; acc: 0.34
Batch: 320; loss: 1.71; acc: 0.42
Batch: 340; loss: 1.5; acc: 0.48
Batch: 360; loss: 1.56; acc: 0.48
Batch: 380; loss: 1.82; acc: 0.38
Batch: 400; loss: 1.91; acc: 0.27
Batch: 420; loss: 1.88; acc: 0.39
Batch: 440; loss: 1.63; acc: 0.39
Batch: 460; loss: 1.74; acc: 0.39
Batch: 480; loss: 1.59; acc: 0.44
Batch: 500; loss: 2.03; acc: 0.27
Batch: 520; loss: 1.79; acc: 0.38
Batch: 540; loss: 1.89; acc: 0.44
Batch: 560; loss: 1.93; acc: 0.3
Batch: 580; loss: 1.65; acc: 0.38
Batch: 600; loss: 1.81; acc: 0.42
Batch: 620; loss: 1.54; acc: 0.41
Batch: 640; loss: 1.86; acc: 0.31
Batch: 660; loss: 1.4; acc: 0.53
Batch: 680; loss: 1.73; acc: 0.44
Batch: 700; loss: 1.7; acc: 0.38
Batch: 720; loss: 1.93; acc: 0.3
Batch: 740; loss: 1.86; acc: 0.31
Batch: 760; loss: 1.93; acc: 0.25
Batch: 780; loss: 1.65; acc: 0.45
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.728784250605638; val_accuracy: 0.3788813694267516 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.72; acc: 0.39
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.7; acc: 0.39
Batch: 60; loss: 1.92; acc: 0.31
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.63; acc: 0.38
Batch: 120; loss: 1.8; acc: 0.38
Batch: 140; loss: 1.68; acc: 0.36
Batch: 160; loss: 1.68; acc: 0.47
Batch: 180; loss: 1.97; acc: 0.33
Batch: 200; loss: 1.88; acc: 0.23
Batch: 220; loss: 1.86; acc: 0.25
Batch: 240; loss: 1.64; acc: 0.38
Batch: 260; loss: 1.68; acc: 0.41
Batch: 280; loss: 1.9; acc: 0.28
Batch: 300; loss: 1.74; acc: 0.38
Batch: 320; loss: 1.71; acc: 0.36
Batch: 340; loss: 1.94; acc: 0.31
Batch: 360; loss: 1.86; acc: 0.28
Batch: 380; loss: 1.72; acc: 0.36
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.83; acc: 0.41
Batch: 440; loss: 1.83; acc: 0.38
Batch: 460; loss: 1.83; acc: 0.3
Batch: 480; loss: 1.61; acc: 0.39
Batch: 500; loss: 1.6; acc: 0.44
Batch: 520; loss: 1.85; acc: 0.27
Batch: 540; loss: 1.85; acc: 0.34
Batch: 560; loss: 1.75; acc: 0.38
Batch: 580; loss: 1.66; acc: 0.36
Batch: 600; loss: 1.75; acc: 0.42
Batch: 620; loss: 1.85; acc: 0.34
Batch: 640; loss: 1.64; acc: 0.38
Batch: 660; loss: 2.02; acc: 0.36
Batch: 680; loss: 2.05; acc: 0.27
Batch: 700; loss: 1.89; acc: 0.23
Batch: 720; loss: 1.9; acc: 0.36
Batch: 740; loss: 1.79; acc: 0.39
Batch: 760; loss: 1.81; acc: 0.41
Batch: 780; loss: 1.62; acc: 0.36
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.41
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288073468360172; val_accuracy: 0.37878184713375795 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.84; acc: 0.33
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.7; acc: 0.3
Batch: 60; loss: 1.63; acc: 0.48
Batch: 80; loss: 1.8; acc: 0.39
Batch: 100; loss: 1.85; acc: 0.39
Batch: 120; loss: 1.65; acc: 0.38
Batch: 140; loss: 1.57; acc: 0.47
Batch: 160; loss: 1.69; acc: 0.34
Batch: 180; loss: 1.81; acc: 0.42
Batch: 200; loss: 1.74; acc: 0.34
Batch: 220; loss: 1.88; acc: 0.33
Batch: 240; loss: 1.79; acc: 0.34
Batch: 260; loss: 1.6; acc: 0.39
Batch: 280; loss: 1.84; acc: 0.44
Batch: 300; loss: 1.67; acc: 0.36
Batch: 320; loss: 1.67; acc: 0.39
Batch: 340; loss: 1.82; acc: 0.38
Batch: 360; loss: 1.7; acc: 0.34
Batch: 380; loss: 1.71; acc: 0.41
Batch: 400; loss: 1.61; acc: 0.41
Batch: 420; loss: 1.71; acc: 0.38
Batch: 440; loss: 1.84; acc: 0.38
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.83; acc: 0.31
Batch: 500; loss: 1.75; acc: 0.31
Batch: 520; loss: 1.65; acc: 0.39
Batch: 540; loss: 1.82; acc: 0.44
Batch: 560; loss: 1.54; acc: 0.42
Batch: 580; loss: 1.84; acc: 0.36
Batch: 600; loss: 1.75; acc: 0.36
Batch: 620; loss: 1.7; acc: 0.38
Batch: 640; loss: 1.79; acc: 0.34
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.61; acc: 0.52
Batch: 700; loss: 1.63; acc: 0.42
Batch: 720; loss: 1.8; acc: 0.22
Batch: 740; loss: 1.86; acc: 0.33
Batch: 760; loss: 1.65; acc: 0.41
Batch: 780; loss: 1.71; acc: 0.33
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288520867657509; val_accuracy: 0.3786823248407643 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.82; acc: 0.33
Batch: 20; loss: 1.42; acc: 0.48
Batch: 40; loss: 1.76; acc: 0.41
Batch: 60; loss: 1.59; acc: 0.45
Batch: 80; loss: 1.76; acc: 0.34
Batch: 100; loss: 1.81; acc: 0.33
Batch: 120; loss: 1.9; acc: 0.25
Batch: 140; loss: 1.94; acc: 0.34
Batch: 160; loss: 1.71; acc: 0.33
Batch: 180; loss: 1.78; acc: 0.28
Batch: 200; loss: 1.69; acc: 0.42
Batch: 220; loss: 1.71; acc: 0.39
Batch: 240; loss: 1.82; acc: 0.33
Batch: 260; loss: 1.76; acc: 0.39
Batch: 280; loss: 1.73; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.39
Batch: 320; loss: 1.58; acc: 0.47
Batch: 340; loss: 1.82; acc: 0.28
Batch: 360; loss: 1.38; acc: 0.55
Batch: 380; loss: 1.7; acc: 0.41
Batch: 400; loss: 1.87; acc: 0.3
Batch: 420; loss: 1.75; acc: 0.42
Batch: 440; loss: 1.63; acc: 0.45
Batch: 460; loss: 1.65; acc: 0.44
Batch: 480; loss: 1.78; acc: 0.38
Batch: 500; loss: 1.95; acc: 0.28
Batch: 520; loss: 1.75; acc: 0.36
Batch: 540; loss: 1.75; acc: 0.34
Batch: 560; loss: 1.67; acc: 0.41
Batch: 580; loss: 1.75; acc: 0.34
Batch: 600; loss: 1.76; acc: 0.45
Batch: 620; loss: 1.68; acc: 0.34
Batch: 640; loss: 1.95; acc: 0.33
Batch: 660; loss: 1.83; acc: 0.28
Batch: 680; loss: 1.67; acc: 0.41
Batch: 700; loss: 1.55; acc: 0.48
Batch: 720; loss: 1.67; acc: 0.38
Batch: 740; loss: 1.73; acc: 0.3
Batch: 760; loss: 1.7; acc: 0.42
Batch: 780; loss: 1.8; acc: 0.31
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.42
Batch: 80; loss: 1.7; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.27
Batch: 120; loss: 2.07; acc: 0.27
Batch: 140; loss: 1.61; acc: 0.41
Val Epoch over. val_loss: 1.7288427049187338; val_accuracy: 0.37898089171974525 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 10570
elements in E: 2221300
fraction nonzero: 0.004758474767028317
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.2
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.29; acc: 0.16
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.08
Batch: 480; loss: 2.29; acc: 0.09
Batch: 500; loss: 2.29; acc: 0.06
Batch: 520; loss: 2.3; acc: 0.02
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.08
Batch: 580; loss: 2.31; acc: 0.06
Batch: 600; loss: 2.28; acc: 0.06
Batch: 620; loss: 2.29; acc: 0.08
Batch: 640; loss: 2.29; acc: 0.09
Batch: 660; loss: 2.27; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.14
Batch: 700; loss: 2.28; acc: 0.12
Batch: 720; loss: 2.26; acc: 0.09
Batch: 740; loss: 2.29; acc: 0.06
Batch: 760; loss: 2.27; acc: 0.16
Batch: 780; loss: 2.26; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.1 

Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.26; acc: 0.14
Batch: 80; loss: 2.26; acc: 0.17
Batch: 100; loss: 2.29; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.09
Val Epoch over. val_loss: 2.270501689546427; val_accuracy: 0.11395302547770701 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.05
Batch: 20; loss: 2.26; acc: 0.14
Batch: 40; loss: 2.26; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.03
Batch: 80; loss: 2.28; acc: 0.05
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.24; acc: 0.25
Batch: 140; loss: 2.26; acc: 0.17
Batch: 160; loss: 2.24; acc: 0.19
Batch: 180; loss: 2.24; acc: 0.23
Batch: 200; loss: 2.23; acc: 0.28
Batch: 220; loss: 2.22; acc: 0.27
Batch: 240; loss: 2.25; acc: 0.28
Batch: 260; loss: 2.23; acc: 0.33
Batch: 280; loss: 2.25; acc: 0.2
Batch: 300; loss: 2.23; acc: 0.33
Batch: 320; loss: 2.27; acc: 0.2
Batch: 340; loss: 2.22; acc: 0.23
Batch: 360; loss: 2.25; acc: 0.17
Batch: 380; loss: 2.19; acc: 0.31
Batch: 400; loss: 2.2; acc: 0.25
Batch: 420; loss: 2.21; acc: 0.25
Batch: 440; loss: 2.19; acc: 0.33
Batch: 460; loss: 2.18; acc: 0.23
Batch: 480; loss: 2.15; acc: 0.25
Batch: 500; loss: 2.2; acc: 0.17
Batch: 520; loss: 2.14; acc: 0.28
Batch: 540; loss: 2.19; acc: 0.23
Batch: 560; loss: 2.17; acc: 0.19
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.14; acc: 0.23
Batch: 620; loss: 2.15; acc: 0.16
Batch: 640; loss: 2.1; acc: 0.33
Batch: 660; loss: 2.16; acc: 0.11
Batch: 680; loss: 2.14; acc: 0.19
Batch: 700; loss: 2.04; acc: 0.25
Batch: 720; loss: 2.02; acc: 0.36
Batch: 740; loss: 2.04; acc: 0.22
Batch: 760; loss: 2.1; acc: 0.22
Batch: 780; loss: 2.14; acc: 0.14
Train Epoch over. train_loss: 2.19; train_accuracy: 0.21 

Batch: 0; loss: 2.1; acc: 0.16
Batch: 20; loss: 2.1; acc: 0.25
Batch: 40; loss: 1.96; acc: 0.28
Batch: 60; loss: 1.98; acc: 0.31
Batch: 80; loss: 2.04; acc: 0.3
Batch: 100; loss: 1.92; acc: 0.36
Batch: 120; loss: 2.05; acc: 0.27
Batch: 140; loss: 1.9; acc: 0.33
Val Epoch over. val_loss: 2.0146119883106013; val_accuracy: 0.27697054140127386 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.02; acc: 0.23
Batch: 20; loss: 2.01; acc: 0.22
Batch: 40; loss: 2.03; acc: 0.25
Batch: 60; loss: 2.04; acc: 0.27
Batch: 80; loss: 1.98; acc: 0.27
Batch: 100; loss: 1.91; acc: 0.34
Batch: 120; loss: 1.82; acc: 0.41
Batch: 140; loss: 1.95; acc: 0.42
Batch: 160; loss: 1.83; acc: 0.34
Batch: 180; loss: 1.9; acc: 0.39
Batch: 200; loss: 1.89; acc: 0.34
Batch: 220; loss: 1.81; acc: 0.44
Batch: 240; loss: 1.79; acc: 0.47
Batch: 260; loss: 1.83; acc: 0.36
Batch: 280; loss: 1.82; acc: 0.41
Batch: 300; loss: 1.91; acc: 0.25
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.89; acc: 0.47
Batch: 360; loss: 1.94; acc: 0.33
Batch: 380; loss: 1.87; acc: 0.42
Batch: 400; loss: 1.73; acc: 0.45
Batch: 420; loss: 1.86; acc: 0.41
Batch: 440; loss: 1.82; acc: 0.3
Batch: 460; loss: 1.9; acc: 0.34
Batch: 480; loss: 1.7; acc: 0.34
Batch: 500; loss: 1.77; acc: 0.36
Batch: 520; loss: 1.63; acc: 0.53
Batch: 540; loss: 1.59; acc: 0.55
Batch: 560; loss: 1.58; acc: 0.47
Batch: 580; loss: 1.51; acc: 0.53
Batch: 600; loss: 1.65; acc: 0.38
Batch: 620; loss: 1.83; acc: 0.41
Batch: 640; loss: 1.69; acc: 0.47
Batch: 660; loss: 1.79; acc: 0.41
Batch: 680; loss: 1.48; acc: 0.55
Batch: 700; loss: 1.3; acc: 0.56
Batch: 720; loss: 1.6; acc: 0.48
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.59; acc: 0.45
Batch: 780; loss: 1.61; acc: 0.52
Train Epoch over. train_loss: 1.78; train_accuracy: 0.39 

Batch: 0; loss: 1.7; acc: 0.34
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.22; acc: 0.55
Batch: 60; loss: 1.31; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.62
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.63; acc: 0.45
Batch: 140; loss: 1.3; acc: 0.55
Val Epoch over. val_loss: 1.5312250207184226; val_accuracy: 0.49442675159235666 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.52
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.58; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.45
Batch: 80; loss: 1.63; acc: 0.42
Batch: 100; loss: 1.58; acc: 0.42
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.4; acc: 0.45
Batch: 160; loss: 1.49; acc: 0.48
Batch: 180; loss: 1.46; acc: 0.47
Batch: 200; loss: 1.53; acc: 0.52
Batch: 220; loss: 1.35; acc: 0.56
Batch: 240; loss: 1.41; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.47
Batch: 280; loss: 1.65; acc: 0.45
Batch: 300; loss: 1.37; acc: 0.55
Batch: 320; loss: 1.48; acc: 0.53
Batch: 340; loss: 1.48; acc: 0.44
Batch: 360; loss: 1.43; acc: 0.53
Batch: 380; loss: 1.63; acc: 0.47
Batch: 400; loss: 1.61; acc: 0.44
Batch: 420; loss: 1.42; acc: 0.53
Batch: 440; loss: 1.3; acc: 0.55
Batch: 460; loss: 1.37; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.41
Batch: 500; loss: 1.37; acc: 0.52
Batch: 520; loss: 1.45; acc: 0.52
Batch: 540; loss: 1.62; acc: 0.56
Batch: 560; loss: 1.66; acc: 0.5
Batch: 580; loss: 1.42; acc: 0.61
Batch: 600; loss: 1.32; acc: 0.59
Batch: 620; loss: 1.49; acc: 0.53
Batch: 640; loss: 1.47; acc: 0.58
Batch: 660; loss: 1.79; acc: 0.47
Batch: 680; loss: 1.58; acc: 0.48
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.57; acc: 0.48
Batch: 740; loss: 1.42; acc: 0.52
Batch: 760; loss: 1.42; acc: 0.55
Batch: 780; loss: 1.16; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.5 

Batch: 0; loss: 1.65; acc: 0.41
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.22; acc: 0.53
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.21; acc: 0.58
Batch: 100; loss: 1.21; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.1; acc: 0.66
Val Epoch over. val_loss: 1.3863047167753717; val_accuracy: 0.5447850318471338 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 1.63; acc: 0.42
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.16; acc: 0.61
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.51; acc: 0.53
Batch: 160; loss: 1.35; acc: 0.56
Batch: 180; loss: 1.49; acc: 0.53
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.51; acc: 0.45
Batch: 240; loss: 1.34; acc: 0.55
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 1.32; acc: 0.56
Batch: 300; loss: 1.43; acc: 0.56
Batch: 320; loss: 1.37; acc: 0.53
Batch: 340; loss: 1.32; acc: 0.58
Batch: 360; loss: 1.54; acc: 0.44
Batch: 380; loss: 1.36; acc: 0.58
Batch: 400; loss: 1.33; acc: 0.59
Batch: 420; loss: 1.27; acc: 0.56
Batch: 440; loss: 1.65; acc: 0.47
Batch: 460; loss: 1.27; acc: 0.62
Batch: 480; loss: 1.34; acc: 0.61
Batch: 500; loss: 1.38; acc: 0.52
Batch: 520; loss: 1.21; acc: 0.7
Batch: 540; loss: 1.56; acc: 0.5
Batch: 560; loss: 1.22; acc: 0.53
Batch: 580; loss: 1.53; acc: 0.5
Batch: 600; loss: 1.24; acc: 0.62
Batch: 620; loss: 1.37; acc: 0.53
Batch: 640; loss: 1.43; acc: 0.52
Batch: 660; loss: 1.51; acc: 0.42
Batch: 680; loss: 1.3; acc: 0.5
Batch: 700; loss: 1.31; acc: 0.59
Batch: 720; loss: 1.47; acc: 0.5
Batch: 740; loss: 1.63; acc: 0.45
Batch: 760; loss: 1.29; acc: 0.52
Batch: 780; loss: 1.46; acc: 0.62
Train Epoch over. train_loss: 1.38; train_accuracy: 0.55 

Batch: 0; loss: 1.64; acc: 0.41
Batch: 20; loss: 2.13; acc: 0.38
Batch: 40; loss: 1.21; acc: 0.59
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 1.24; acc: 0.55
Batch: 120; loss: 1.86; acc: 0.44
Batch: 140; loss: 1.05; acc: 0.67
Val Epoch over. val_loss: 1.3574424591034082; val_accuracy: 0.5557324840764332 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 1.61; acc: 0.47
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.61
Batch: 100; loss: 1.2; acc: 0.53
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.23; acc: 0.55
Batch: 160; loss: 1.35; acc: 0.56
Batch: 180; loss: 1.32; acc: 0.58
Batch: 200; loss: 1.32; acc: 0.59
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.3; acc: 0.59
Batch: 260; loss: 1.37; acc: 0.56
Batch: 280; loss: 0.99; acc: 0.75
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.29; acc: 0.59
Batch: 340; loss: 1.42; acc: 0.48
Batch: 360; loss: 1.31; acc: 0.44
Batch: 380; loss: 1.56; acc: 0.45
Batch: 400; loss: 1.61; acc: 0.5
Batch: 420; loss: 1.38; acc: 0.52
Batch: 440; loss: 1.25; acc: 0.62
Batch: 460; loss: 1.3; acc: 0.58
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.16; acc: 0.59
Batch: 520; loss: 1.3; acc: 0.61
Batch: 540; loss: 1.54; acc: 0.48
Batch: 560; loss: 1.65; acc: 0.5
Batch: 580; loss: 1.29; acc: 0.53
Batch: 600; loss: 1.33; acc: 0.56
Batch: 620; loss: 1.54; acc: 0.52
Batch: 640; loss: 1.25; acc: 0.62
Batch: 660; loss: 1.08; acc: 0.62
Batch: 680; loss: 1.38; acc: 0.53
Batch: 700; loss: 1.15; acc: 0.64
Batch: 720; loss: 1.56; acc: 0.52
Batch: 740; loss: 1.48; acc: 0.59
Batch: 760; loss: 1.24; acc: 0.58
Batch: 780; loss: 1.25; acc: 0.59
Train Epoch over. train_loss: 1.36; train_accuracy: 0.56 

Batch: 0; loss: 1.55; acc: 0.42
Batch: 20; loss: 1.76; acc: 0.44
Batch: 40; loss: 1.05; acc: 0.67
Batch: 60; loss: 1.16; acc: 0.66
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.59
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.05; acc: 0.58
Val Epoch over. val_loss: 1.3118581604805721; val_accuracy: 0.5672770700636943 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.53
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 1.54; acc: 0.48
Batch: 60; loss: 1.2; acc: 0.64
Batch: 80; loss: 1.27; acc: 0.61
Batch: 100; loss: 1.34; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.22; acc: 0.58
Batch: 160; loss: 1.19; acc: 0.61
Batch: 180; loss: 1.55; acc: 0.5
Batch: 200; loss: 1.26; acc: 0.64
Batch: 220; loss: 1.35; acc: 0.56
Batch: 240; loss: 1.26; acc: 0.56
Batch: 260; loss: 1.47; acc: 0.48
Batch: 280; loss: 1.28; acc: 0.53
Batch: 300; loss: 1.15; acc: 0.64
Batch: 320; loss: 1.57; acc: 0.47
Batch: 340; loss: 1.37; acc: 0.55
Batch: 360; loss: 1.52; acc: 0.47
Batch: 380; loss: 1.51; acc: 0.52
Batch: 400; loss: 1.39; acc: 0.55
Batch: 420; loss: 1.25; acc: 0.55
Batch: 440; loss: 1.18; acc: 0.59
Batch: 460; loss: 1.38; acc: 0.53
Batch: 480; loss: 1.29; acc: 0.52
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.43; acc: 0.61
Batch: 540; loss: 1.26; acc: 0.55
Batch: 560; loss: 1.57; acc: 0.53
Batch: 580; loss: 1.41; acc: 0.56
Batch: 600; loss: 1.43; acc: 0.55
Batch: 620; loss: 1.61; acc: 0.5
Batch: 640; loss: 1.48; acc: 0.53
Batch: 660; loss: 1.31; acc: 0.58
Batch: 680; loss: 1.35; acc: 0.52
Batch: 700; loss: 1.18; acc: 0.56
Batch: 720; loss: 1.51; acc: 0.47
Batch: 740; loss: 1.23; acc: 0.58
Batch: 760; loss: 1.16; acc: 0.64
Batch: 780; loss: 1.26; acc: 0.62
Train Epoch over. train_loss: 1.34; train_accuracy: 0.56 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.0; acc: 0.7
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.62
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 0.95; acc: 0.72
Val Epoch over. val_loss: 1.2713356056031149; val_accuracy: 0.5862858280254777 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.53
Batch: 20; loss: 1.51; acc: 0.45
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.23; acc: 0.56
Batch: 120; loss: 1.37; acc: 0.56
Batch: 140; loss: 1.28; acc: 0.55
Batch: 160; loss: 1.06; acc: 0.62
Batch: 180; loss: 1.3; acc: 0.55
Batch: 200; loss: 1.23; acc: 0.66
Batch: 220; loss: 1.28; acc: 0.64
Batch: 240; loss: 0.99; acc: 0.69
Batch: 260; loss: 1.3; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.61
Batch: 300; loss: 1.26; acc: 0.61
Batch: 320; loss: 1.26; acc: 0.56
Batch: 340; loss: 1.19; acc: 0.58
Batch: 360; loss: 1.21; acc: 0.58
Batch: 380; loss: 1.08; acc: 0.67
Batch: 400; loss: 1.39; acc: 0.59
Batch: 420; loss: 1.39; acc: 0.53
Batch: 440; loss: 1.35; acc: 0.58
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.09; acc: 0.64
Batch: 500; loss: 1.47; acc: 0.56
Batch: 520; loss: 1.25; acc: 0.58
Batch: 540; loss: 1.36; acc: 0.62
Batch: 560; loss: 1.13; acc: 0.55
Batch: 580; loss: 1.17; acc: 0.59
Batch: 600; loss: 1.28; acc: 0.59
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.14; acc: 0.55
Batch: 660; loss: 1.35; acc: 0.5
Batch: 680; loss: 1.1; acc: 0.59
Batch: 700; loss: 1.57; acc: 0.52
Batch: 720; loss: 1.31; acc: 0.58
Batch: 740; loss: 1.33; acc: 0.53
Batch: 760; loss: 1.44; acc: 0.5
Batch: 780; loss: 1.34; acc: 0.55
Train Epoch over. train_loss: 1.3; train_accuracy: 0.57 

Batch: 0; loss: 1.63; acc: 0.42
Batch: 20; loss: 1.59; acc: 0.48
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 1.0; acc: 0.62
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.58; acc: 0.48
Batch: 140; loss: 1.01; acc: 0.66
Val Epoch over. val_loss: 1.2719316501526317; val_accuracy: 0.582703025477707 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.55; acc: 0.53
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.28; acc: 0.62
Batch: 120; loss: 1.57; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.5
Batch: 160; loss: 1.2; acc: 0.58
Batch: 180; loss: 1.07; acc: 0.7
Batch: 200; loss: 1.27; acc: 0.53
Batch: 220; loss: 1.04; acc: 0.66
Batch: 240; loss: 1.29; acc: 0.58
Batch: 260; loss: 1.29; acc: 0.59
Batch: 280; loss: 1.09; acc: 0.58
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.28; acc: 0.56
Batch: 340; loss: 1.67; acc: 0.47
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 1.23; acc: 0.59
Batch: 400; loss: 1.44; acc: 0.53
Batch: 420; loss: 1.09; acc: 0.66
Batch: 440; loss: 1.28; acc: 0.62
Batch: 460; loss: 1.35; acc: 0.52
Batch: 480; loss: 1.47; acc: 0.55
Batch: 500; loss: 1.37; acc: 0.58
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.74; acc: 0.42
Batch: 560; loss: 1.4; acc: 0.47
Batch: 580; loss: 1.56; acc: 0.56
Batch: 600; loss: 1.33; acc: 0.52
Batch: 620; loss: 1.38; acc: 0.52
Batch: 640; loss: 1.29; acc: 0.53
Batch: 660; loss: 1.33; acc: 0.59
Batch: 680; loss: 1.38; acc: 0.52
Batch: 700; loss: 1.2; acc: 0.5
Batch: 720; loss: 1.14; acc: 0.69
Batch: 740; loss: 1.2; acc: 0.59
Batch: 760; loss: 1.1; acc: 0.61
Batch: 780; loss: 1.42; acc: 0.58
Train Epoch over. train_loss: 1.29; train_accuracy: 0.57 

Batch: 0; loss: 1.52; acc: 0.44
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 1.07; acc: 0.59
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 0.91; acc: 0.72
Val Epoch over. val_loss: 1.2319417170658233; val_accuracy: 0.5974323248407644 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.5
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 1.49; acc: 0.48
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.31; acc: 0.53
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 1.25; acc: 0.59
Batch: 160; loss: 1.17; acc: 0.64
Batch: 180; loss: 1.39; acc: 0.53
Batch: 200; loss: 1.07; acc: 0.69
Batch: 220; loss: 1.26; acc: 0.58
Batch: 240; loss: 1.55; acc: 0.47
Batch: 260; loss: 1.25; acc: 0.58
Batch: 280; loss: 0.9; acc: 0.72
Batch: 300; loss: 1.1; acc: 0.59
Batch: 320; loss: 1.49; acc: 0.48
Batch: 340; loss: 1.37; acc: 0.56
Batch: 360; loss: 1.42; acc: 0.59
Batch: 380; loss: 1.32; acc: 0.53
Batch: 400; loss: 1.38; acc: 0.55
Batch: 420; loss: 1.1; acc: 0.62
Batch: 440; loss: 1.45; acc: 0.62
Batch: 460; loss: 1.2; acc: 0.59
Batch: 480; loss: 1.41; acc: 0.52
Batch: 500; loss: 1.25; acc: 0.61
Batch: 520; loss: 1.24; acc: 0.55
Batch: 540; loss: 1.08; acc: 0.67
Batch: 560; loss: 1.3; acc: 0.55
Batch: 580; loss: 1.41; acc: 0.48
Batch: 600; loss: 1.38; acc: 0.52
Batch: 620; loss: 1.3; acc: 0.56
Batch: 640; loss: 1.2; acc: 0.61
Batch: 660; loss: 1.31; acc: 0.5
Batch: 680; loss: 1.43; acc: 0.55
Batch: 700; loss: 1.29; acc: 0.61
Batch: 720; loss: 1.27; acc: 0.52
Batch: 740; loss: 1.21; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.52
Batch: 780; loss: 1.1; acc: 0.58
Train Epoch over. train_loss: 1.29; train_accuracy: 0.57 

Batch: 0; loss: 1.49; acc: 0.48
Batch: 20; loss: 1.45; acc: 0.48
Batch: 40; loss: 0.92; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.61
Batch: 80; loss: 0.97; acc: 0.66
Batch: 100; loss: 1.11; acc: 0.58
Batch: 120; loss: 1.45; acc: 0.52
Batch: 140; loss: 0.94; acc: 0.72
Val Epoch over. val_loss: 1.2628372983568033; val_accuracy: 0.5769307324840764 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.4; acc: 0.52
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 1.23; acc: 0.58
Batch: 60; loss: 1.17; acc: 0.62
Batch: 80; loss: 1.29; acc: 0.58
Batch: 100; loss: 1.31; acc: 0.59
Batch: 120; loss: 1.13; acc: 0.56
Batch: 140; loss: 1.35; acc: 0.55
Batch: 160; loss: 1.48; acc: 0.55
Batch: 180; loss: 1.33; acc: 0.55
Batch: 200; loss: 1.16; acc: 0.59
Batch: 220; loss: 0.95; acc: 0.66
Batch: 240; loss: 1.28; acc: 0.56
Batch: 260; loss: 1.34; acc: 0.53
Batch: 280; loss: 1.35; acc: 0.62
Batch: 300; loss: 1.28; acc: 0.59
Batch: 320; loss: 1.36; acc: 0.56
Batch: 340; loss: 1.17; acc: 0.56
Batch: 360; loss: 1.29; acc: 0.58
Batch: 380; loss: 1.38; acc: 0.58
Batch: 400; loss: 1.15; acc: 0.64
Batch: 420; loss: 1.39; acc: 0.53
Batch: 440; loss: 1.26; acc: 0.55
Batch: 460; loss: 1.1; acc: 0.72
Batch: 480; loss: 1.04; acc: 0.61
Batch: 500; loss: 1.09; acc: 0.61
Batch: 520; loss: 1.29; acc: 0.56
Batch: 540; loss: 1.1; acc: 0.59
Batch: 560; loss: 1.42; acc: 0.52
Batch: 580; loss: 1.25; acc: 0.53
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.29; acc: 0.59
Batch: 640; loss: 1.25; acc: 0.58
Batch: 660; loss: 1.28; acc: 0.61
Batch: 680; loss: 1.44; acc: 0.52
Batch: 700; loss: 1.46; acc: 0.53
Batch: 720; loss: 1.09; acc: 0.69
Batch: 740; loss: 1.47; acc: 0.55
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.39; acc: 0.53
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.41
Batch: 20; loss: 1.4; acc: 0.55
Batch: 40; loss: 0.91; acc: 0.75
Batch: 60; loss: 1.08; acc: 0.62
Batch: 80; loss: 0.96; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.61
Batch: 120; loss: 1.49; acc: 0.48
Batch: 140; loss: 0.91; acc: 0.72
Val Epoch over. val_loss: 1.2355731106867456; val_accuracy: 0.5876791401273885 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.34; acc: 0.52
Batch: 40; loss: 1.47; acc: 0.48
Batch: 60; loss: 1.13; acc: 0.61
Batch: 80; loss: 1.27; acc: 0.56
Batch: 100; loss: 1.34; acc: 0.56
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.61
Batch: 160; loss: 1.25; acc: 0.5
Batch: 180; loss: 1.26; acc: 0.61
Batch: 200; loss: 1.36; acc: 0.56
Batch: 220; loss: 1.56; acc: 0.58
Batch: 240; loss: 1.45; acc: 0.47
Batch: 260; loss: 1.21; acc: 0.5
Batch: 280; loss: 1.36; acc: 0.62
Batch: 300; loss: 1.04; acc: 0.62
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.41; acc: 0.53
Batch: 360; loss: 1.39; acc: 0.56
Batch: 380; loss: 1.53; acc: 0.55
Batch: 400; loss: 1.37; acc: 0.53
Batch: 420; loss: 1.5; acc: 0.47
Batch: 440; loss: 1.35; acc: 0.52
Batch: 460; loss: 1.45; acc: 0.53
Batch: 480; loss: 1.41; acc: 0.61
Batch: 500; loss: 1.41; acc: 0.53
Batch: 520; loss: 1.38; acc: 0.52
Batch: 540; loss: 1.29; acc: 0.56
Batch: 560; loss: 1.53; acc: 0.5
Batch: 580; loss: 1.35; acc: 0.5
Batch: 600; loss: 1.2; acc: 0.58
Batch: 620; loss: 1.36; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.47
Batch: 660; loss: 1.36; acc: 0.53
Batch: 680; loss: 1.05; acc: 0.64
Batch: 700; loss: 1.3; acc: 0.59
Batch: 720; loss: 1.29; acc: 0.62
Batch: 740; loss: 1.45; acc: 0.56
Batch: 760; loss: 1.47; acc: 0.52
Batch: 780; loss: 1.23; acc: 0.62
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.44
Batch: 20; loss: 1.46; acc: 0.52
Batch: 40; loss: 0.88; acc: 0.75
Batch: 60; loss: 1.06; acc: 0.62
Batch: 80; loss: 0.96; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 0.91; acc: 0.75
Val Epoch over. val_loss: 1.2316174116104273; val_accuracy: 0.5931528662420382 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.3; acc: 0.56
Batch: 20; loss: 1.4; acc: 0.61
Batch: 40; loss: 1.24; acc: 0.61
Batch: 60; loss: 1.24; acc: 0.61
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 1.28; acc: 0.56
Batch: 140; loss: 1.11; acc: 0.64
Batch: 160; loss: 1.38; acc: 0.56
Batch: 180; loss: 1.35; acc: 0.55
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 1.34; acc: 0.56
Batch: 240; loss: 1.17; acc: 0.61
Batch: 260; loss: 0.97; acc: 0.69
Batch: 280; loss: 1.58; acc: 0.39
Batch: 300; loss: 1.06; acc: 0.64
Batch: 320; loss: 1.07; acc: 0.69
Batch: 340; loss: 1.31; acc: 0.58
Batch: 360; loss: 1.36; acc: 0.55
Batch: 380; loss: 1.41; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.62
Batch: 420; loss: 1.35; acc: 0.52
Batch: 440; loss: 1.35; acc: 0.58
Batch: 460; loss: 1.31; acc: 0.55
Batch: 480; loss: 1.51; acc: 0.55
Batch: 500; loss: 1.44; acc: 0.48
Batch: 520; loss: 1.19; acc: 0.61
Batch: 540; loss: 1.39; acc: 0.58
Batch: 560; loss: 1.32; acc: 0.64
Batch: 580; loss: 1.15; acc: 0.59
Batch: 600; loss: 1.28; acc: 0.55
Batch: 620; loss: 1.27; acc: 0.59
Batch: 640; loss: 1.25; acc: 0.61
Batch: 660; loss: 1.41; acc: 0.5
Batch: 680; loss: 1.37; acc: 0.53
Batch: 700; loss: 1.15; acc: 0.7
Batch: 720; loss: 1.28; acc: 0.52
Batch: 740; loss: 1.17; acc: 0.61
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.2; acc: 0.55
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.45
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.95; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.67
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 0.91; acc: 0.69
Val Epoch over. val_loss: 1.2303771471521656; val_accuracy: 0.5900676751592356 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 1.2; acc: 0.58
Batch: 60; loss: 1.36; acc: 0.52
Batch: 80; loss: 1.34; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.47
Batch: 120; loss: 1.18; acc: 0.62
Batch: 140; loss: 1.44; acc: 0.47
Batch: 160; loss: 1.56; acc: 0.47
Batch: 180; loss: 1.6; acc: 0.58
Batch: 200; loss: 1.36; acc: 0.52
Batch: 220; loss: 1.05; acc: 0.69
Batch: 240; loss: 1.32; acc: 0.67
Batch: 260; loss: 1.03; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.61
Batch: 300; loss: 1.29; acc: 0.56
Batch: 320; loss: 1.1; acc: 0.61
Batch: 340; loss: 1.44; acc: 0.53
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 1.29; acc: 0.61
Batch: 400; loss: 1.14; acc: 0.61
Batch: 420; loss: 1.4; acc: 0.55
Batch: 440; loss: 1.3; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.36; acc: 0.58
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 1.39; acc: 0.58
Batch: 540; loss: 1.13; acc: 0.64
Batch: 560; loss: 1.27; acc: 0.55
Batch: 580; loss: 1.12; acc: 0.64
Batch: 600; loss: 1.42; acc: 0.52
Batch: 620; loss: 1.32; acc: 0.55
Batch: 640; loss: 1.16; acc: 0.64
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.42; acc: 0.45
Batch: 700; loss: 1.31; acc: 0.61
Batch: 720; loss: 1.11; acc: 0.62
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.15; acc: 0.72
Batch: 780; loss: 1.28; acc: 0.55
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.42
Batch: 20; loss: 1.39; acc: 0.56
Batch: 40; loss: 0.89; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.95; acc: 0.7
Batch: 100; loss: 1.03; acc: 0.67
Batch: 120; loss: 1.53; acc: 0.52
Batch: 140; loss: 0.86; acc: 0.73
Val Epoch over. val_loss: 1.2270375892614862; val_accuracy: 0.5945461783439491 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.23; acc: 0.53
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 1.06; acc: 0.64
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.24; acc: 0.58
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.11; acc: 0.61
Batch: 140; loss: 1.25; acc: 0.59
Batch: 160; loss: 1.47; acc: 0.53
Batch: 180; loss: 1.16; acc: 0.62
Batch: 200; loss: 1.16; acc: 0.66
Batch: 220; loss: 1.37; acc: 0.53
Batch: 240; loss: 1.04; acc: 0.69
Batch: 260; loss: 1.17; acc: 0.69
Batch: 280; loss: 1.24; acc: 0.5
Batch: 300; loss: 1.14; acc: 0.7
Batch: 320; loss: 1.37; acc: 0.56
Batch: 340; loss: 1.36; acc: 0.61
Batch: 360; loss: 1.31; acc: 0.53
Batch: 380; loss: 1.29; acc: 0.55
Batch: 400; loss: 1.17; acc: 0.58
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.24; acc: 0.58
Batch: 460; loss: 1.27; acc: 0.61
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 1.34; acc: 0.53
Batch: 520; loss: 1.4; acc: 0.42
Batch: 540; loss: 1.34; acc: 0.61
Batch: 560; loss: 1.0; acc: 0.66
Batch: 580; loss: 1.27; acc: 0.56
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.28; acc: 0.64
Batch: 640; loss: 1.38; acc: 0.59
Batch: 660; loss: 0.98; acc: 0.66
Batch: 680; loss: 1.42; acc: 0.5
Batch: 700; loss: 1.42; acc: 0.5
Batch: 720; loss: 1.13; acc: 0.66
Batch: 740; loss: 1.29; acc: 0.52
Batch: 760; loss: 1.11; acc: 0.61
Batch: 780; loss: 1.49; acc: 0.5
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.49; acc: 0.47
Batch: 20; loss: 1.41; acc: 0.5
Batch: 40; loss: 0.91; acc: 0.77
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 0.93; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.229200347973283; val_accuracy: 0.5888734076433121 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.15; acc: 0.61
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 1.18; acc: 0.59
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.29; acc: 0.59
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 1.17; acc: 0.56
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.19; acc: 0.59
Batch: 180; loss: 1.21; acc: 0.56
Batch: 200; loss: 0.9; acc: 0.69
Batch: 220; loss: 1.43; acc: 0.53
Batch: 240; loss: 1.21; acc: 0.55
Batch: 260; loss: 1.15; acc: 0.7
Batch: 280; loss: 1.32; acc: 0.52
Batch: 300; loss: 1.4; acc: 0.56
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.59; acc: 0.38
Batch: 360; loss: 1.16; acc: 0.62
Batch: 380; loss: 1.29; acc: 0.56
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.05; acc: 0.67
Batch: 440; loss: 1.7; acc: 0.52
Batch: 460; loss: 1.37; acc: 0.59
Batch: 480; loss: 1.22; acc: 0.58
Batch: 500; loss: 1.21; acc: 0.58
Batch: 520; loss: 1.6; acc: 0.5
Batch: 540; loss: 1.15; acc: 0.56
Batch: 560; loss: 1.23; acc: 0.62
Batch: 580; loss: 1.35; acc: 0.48
Batch: 600; loss: 1.11; acc: 0.62
Batch: 620; loss: 1.34; acc: 0.55
Batch: 640; loss: 1.22; acc: 0.52
Batch: 660; loss: 1.47; acc: 0.53
Batch: 680; loss: 1.22; acc: 0.67
Batch: 700; loss: 1.17; acc: 0.62
Batch: 720; loss: 1.24; acc: 0.56
Batch: 740; loss: 1.29; acc: 0.52
Batch: 760; loss: 1.4; acc: 0.5
Batch: 780; loss: 1.04; acc: 0.66
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.39; acc: 0.55
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.95; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.5
Batch: 140; loss: 0.92; acc: 0.72
Val Epoch over. val_loss: 1.229781829627456; val_accuracy: 0.5904657643312102 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.07; acc: 0.64
Batch: 40; loss: 1.41; acc: 0.52
Batch: 60; loss: 1.41; acc: 0.56
Batch: 80; loss: 1.36; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.5
Batch: 120; loss: 1.28; acc: 0.56
Batch: 140; loss: 1.4; acc: 0.53
Batch: 160; loss: 1.21; acc: 0.53
Batch: 180; loss: 1.45; acc: 0.53
Batch: 200; loss: 1.34; acc: 0.59
Batch: 220; loss: 1.18; acc: 0.69
Batch: 240; loss: 1.35; acc: 0.55
Batch: 260; loss: 1.16; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.61
Batch: 300; loss: 1.16; acc: 0.55
Batch: 320; loss: 1.31; acc: 0.58
Batch: 340; loss: 1.54; acc: 0.53
Batch: 360; loss: 1.39; acc: 0.52
Batch: 380; loss: 1.05; acc: 0.67
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 1.36; acc: 0.62
Batch: 440; loss: 1.01; acc: 0.66
Batch: 460; loss: 1.47; acc: 0.47
Batch: 480; loss: 1.08; acc: 0.58
Batch: 500; loss: 1.31; acc: 0.64
Batch: 520; loss: 1.03; acc: 0.66
Batch: 540; loss: 1.26; acc: 0.53
Batch: 560; loss: 1.24; acc: 0.5
Batch: 580; loss: 1.29; acc: 0.53
Batch: 600; loss: 1.34; acc: 0.58
Batch: 620; loss: 1.38; acc: 0.58
Batch: 640; loss: 1.18; acc: 0.52
Batch: 660; loss: 1.27; acc: 0.58
Batch: 680; loss: 1.34; acc: 0.56
Batch: 700; loss: 1.22; acc: 0.59
Batch: 720; loss: 1.45; acc: 0.58
Batch: 740; loss: 1.05; acc: 0.62
Batch: 760; loss: 1.32; acc: 0.61
Batch: 780; loss: 1.27; acc: 0.48
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.45
Batch: 20; loss: 1.4; acc: 0.58
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.04; acc: 0.62
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.05; acc: 0.69
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 0.88; acc: 0.72
Val Epoch over. val_loss: 1.2250540567811128; val_accuracy: 0.5905652866242038 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 1.32; acc: 0.53
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.25; acc: 0.58
Batch: 100; loss: 1.33; acc: 0.53
Batch: 120; loss: 1.44; acc: 0.58
Batch: 140; loss: 1.58; acc: 0.58
Batch: 160; loss: 1.26; acc: 0.64
Batch: 180; loss: 1.11; acc: 0.58
Batch: 200; loss: 1.39; acc: 0.66
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.39; acc: 0.52
Batch: 260; loss: 1.39; acc: 0.67
Batch: 280; loss: 1.02; acc: 0.61
Batch: 300; loss: 0.98; acc: 0.69
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.16; acc: 0.59
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 1.19; acc: 0.59
Batch: 400; loss: 1.21; acc: 0.61
Batch: 420; loss: 1.62; acc: 0.52
Batch: 440; loss: 1.34; acc: 0.52
Batch: 460; loss: 1.52; acc: 0.41
Batch: 480; loss: 1.28; acc: 0.5
Batch: 500; loss: 1.55; acc: 0.5
Batch: 520; loss: 1.08; acc: 0.62
Batch: 540; loss: 1.39; acc: 0.52
Batch: 560; loss: 1.38; acc: 0.56
Batch: 580; loss: 1.48; acc: 0.48
Batch: 600; loss: 1.55; acc: 0.45
Batch: 620; loss: 1.05; acc: 0.69
Batch: 640; loss: 1.48; acc: 0.55
Batch: 660; loss: 1.2; acc: 0.55
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.2; acc: 0.58
Batch: 720; loss: 1.2; acc: 0.59
Batch: 740; loss: 1.74; acc: 0.52
Batch: 760; loss: 1.07; acc: 0.61
Batch: 780; loss: 1.17; acc: 0.55
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.42
Batch: 20; loss: 1.44; acc: 0.53
Batch: 40; loss: 0.89; acc: 0.75
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 0.95; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.57; acc: 0.5
Batch: 140; loss: 0.9; acc: 0.69
Val Epoch over. val_loss: 1.2278070700396397; val_accuracy: 0.5934514331210191 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.59
Batch: 20; loss: 1.27; acc: 0.67
Batch: 40; loss: 1.58; acc: 0.44
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.45
Batch: 100; loss: 1.16; acc: 0.59
Batch: 120; loss: 1.51; acc: 0.5
Batch: 140; loss: 1.29; acc: 0.5
Batch: 160; loss: 1.15; acc: 0.59
Batch: 180; loss: 1.24; acc: 0.58
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.4; acc: 0.58
Batch: 240; loss: 1.6; acc: 0.52
Batch: 260; loss: 1.24; acc: 0.58
Batch: 280; loss: 0.94; acc: 0.64
Batch: 300; loss: 1.32; acc: 0.58
Batch: 320; loss: 1.14; acc: 0.67
Batch: 340; loss: 1.17; acc: 0.58
Batch: 360; loss: 1.46; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.44
Batch: 400; loss: 1.44; acc: 0.61
Batch: 420; loss: 1.5; acc: 0.44
Batch: 440; loss: 1.21; acc: 0.53
Batch: 460; loss: 1.35; acc: 0.59
Batch: 480; loss: 1.02; acc: 0.66
Batch: 500; loss: 1.12; acc: 0.61
Batch: 520; loss: 1.4; acc: 0.55
Batch: 540; loss: 0.98; acc: 0.67
Batch: 560; loss: 1.14; acc: 0.62
Batch: 580; loss: 1.19; acc: 0.59
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 1.34; acc: 0.59
Batch: 640; loss: 1.22; acc: 0.59
Batch: 660; loss: 1.26; acc: 0.64
Batch: 680; loss: 1.0; acc: 0.61
Batch: 700; loss: 1.27; acc: 0.61
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 1.38; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.21; acc: 0.56
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.54; acc: 0.44
Batch: 20; loss: 1.42; acc: 0.52
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.95; acc: 0.66
Batch: 100; loss: 1.08; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.5
Batch: 140; loss: 0.93; acc: 0.69
Val Epoch over. val_loss: 1.2393477505939021; val_accuracy: 0.5878781847133758 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.04; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 1.32; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.37; acc: 0.48
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.56
Batch: 160; loss: 1.28; acc: 0.58
Batch: 180; loss: 1.16; acc: 0.59
Batch: 200; loss: 1.31; acc: 0.48
Batch: 220; loss: 1.53; acc: 0.56
Batch: 240; loss: 1.43; acc: 0.55
Batch: 260; loss: 1.18; acc: 0.61
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 1.48; acc: 0.55
Batch: 320; loss: 1.49; acc: 0.45
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.38; acc: 0.52
Batch: 380; loss: 1.27; acc: 0.58
Batch: 400; loss: 1.26; acc: 0.58
Batch: 420; loss: 1.57; acc: 0.48
Batch: 440; loss: 1.51; acc: 0.53
Batch: 460; loss: 1.44; acc: 0.5
Batch: 480; loss: 1.65; acc: 0.42
Batch: 500; loss: 1.11; acc: 0.62
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.69
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.17; acc: 0.59
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.3; acc: 0.55
Batch: 640; loss: 1.17; acc: 0.56
Batch: 660; loss: 1.3; acc: 0.58
Batch: 680; loss: 1.07; acc: 0.64
Batch: 700; loss: 1.15; acc: 0.61
Batch: 720; loss: 1.29; acc: 0.67
Batch: 740; loss: 1.1; acc: 0.61
Batch: 760; loss: 1.34; acc: 0.56
Batch: 780; loss: 1.18; acc: 0.64
Train Epoch over. train_loss: 1.28; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.41
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.61
Batch: 80; loss: 0.95; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.2343994864992276; val_accuracy: 0.59375 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.23; acc: 0.59
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 1.17; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.5
Batch: 80; loss: 1.37; acc: 0.52
Batch: 100; loss: 1.29; acc: 0.52
Batch: 120; loss: 1.6; acc: 0.5
Batch: 140; loss: 1.22; acc: 0.55
Batch: 160; loss: 1.36; acc: 0.45
Batch: 180; loss: 1.2; acc: 0.64
Batch: 200; loss: 1.4; acc: 0.53
Batch: 220; loss: 1.24; acc: 0.62
Batch: 240; loss: 1.22; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.59
Batch: 280; loss: 1.52; acc: 0.52
Batch: 300; loss: 1.38; acc: 0.47
Batch: 320; loss: 1.22; acc: 0.55
Batch: 340; loss: 0.99; acc: 0.61
Batch: 360; loss: 1.52; acc: 0.44
Batch: 380; loss: 1.29; acc: 0.61
Batch: 400; loss: 1.23; acc: 0.58
Batch: 420; loss: 1.27; acc: 0.61
Batch: 440; loss: 1.25; acc: 0.56
Batch: 460; loss: 1.3; acc: 0.5
Batch: 480; loss: 1.45; acc: 0.59
Batch: 500; loss: 1.53; acc: 0.55
Batch: 520; loss: 1.49; acc: 0.5
Batch: 540; loss: 1.27; acc: 0.67
Batch: 560; loss: 1.37; acc: 0.5
Batch: 580; loss: 1.29; acc: 0.61
Batch: 600; loss: 1.18; acc: 0.55
Batch: 620; loss: 1.33; acc: 0.53
Batch: 640; loss: 1.36; acc: 0.5
Batch: 660; loss: 1.16; acc: 0.56
Batch: 680; loss: 1.4; acc: 0.59
Batch: 700; loss: 1.19; acc: 0.61
Batch: 720; loss: 1.23; acc: 0.62
Batch: 740; loss: 1.37; acc: 0.47
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.26; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.41
Batch: 20; loss: 1.4; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.62
Batch: 80; loss: 0.94; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.67
Batch: 120; loss: 1.59; acc: 0.48
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.2271152525950388; val_accuracy: 0.5893710191082803 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.13; acc: 0.67
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.59
Batch: 120; loss: 1.19; acc: 0.58
Batch: 140; loss: 1.43; acc: 0.44
Batch: 160; loss: 0.98; acc: 0.67
Batch: 180; loss: 1.12; acc: 0.62
Batch: 200; loss: 1.16; acc: 0.64
Batch: 220; loss: 1.15; acc: 0.58
Batch: 240; loss: 1.32; acc: 0.56
Batch: 260; loss: 1.3; acc: 0.52
Batch: 280; loss: 1.21; acc: 0.62
Batch: 300; loss: 1.38; acc: 0.48
Batch: 320; loss: 1.32; acc: 0.55
Batch: 340; loss: 1.2; acc: 0.56
Batch: 360; loss: 1.29; acc: 0.62
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.15; acc: 0.58
Batch: 420; loss: 0.93; acc: 0.69
Batch: 440; loss: 1.11; acc: 0.64
Batch: 460; loss: 1.39; acc: 0.59
Batch: 480; loss: 1.22; acc: 0.69
Batch: 500; loss: 1.35; acc: 0.58
Batch: 520; loss: 1.28; acc: 0.58
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 1.3; acc: 0.53
Batch: 580; loss: 1.21; acc: 0.55
Batch: 600; loss: 1.02; acc: 0.67
Batch: 620; loss: 1.35; acc: 0.55
Batch: 640; loss: 1.37; acc: 0.59
Batch: 660; loss: 1.4; acc: 0.5
Batch: 680; loss: 1.18; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.59
Batch: 740; loss: 1.57; acc: 0.48
Batch: 760; loss: 1.34; acc: 0.55
Batch: 780; loss: 1.13; acc: 0.67
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.41
Batch: 20; loss: 1.39; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.06; acc: 0.66
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.67
Batch: 120; loss: 1.54; acc: 0.48
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.226288666391069; val_accuracy: 0.5968351910828026 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.36; acc: 0.5
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.43; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.61
Batch: 100; loss: 1.31; acc: 0.59
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.69
Batch: 160; loss: 1.28; acc: 0.62
Batch: 180; loss: 1.35; acc: 0.55
Batch: 200; loss: 1.49; acc: 0.55
Batch: 220; loss: 1.32; acc: 0.55
Batch: 240; loss: 1.18; acc: 0.67
Batch: 260; loss: 1.36; acc: 0.5
Batch: 280; loss: 1.13; acc: 0.59
Batch: 300; loss: 1.59; acc: 0.48
Batch: 320; loss: 1.42; acc: 0.55
Batch: 340; loss: 1.4; acc: 0.55
Batch: 360; loss: 1.46; acc: 0.58
Batch: 380; loss: 1.09; acc: 0.64
Batch: 400; loss: 1.12; acc: 0.72
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.36; acc: 0.59
Batch: 460; loss: 1.23; acc: 0.64
Batch: 480; loss: 1.23; acc: 0.56
Batch: 500; loss: 1.33; acc: 0.56
Batch: 520; loss: 1.17; acc: 0.66
Batch: 540; loss: 1.64; acc: 0.5
Batch: 560; loss: 0.98; acc: 0.64
Batch: 580; loss: 1.34; acc: 0.56
Batch: 600; loss: 1.03; acc: 0.64
Batch: 620; loss: 1.14; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.59
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.33; acc: 0.5
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.33; acc: 0.56
Batch: 740; loss: 1.31; acc: 0.55
Batch: 760; loss: 1.51; acc: 0.55
Batch: 780; loss: 1.33; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.41
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.62
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.57; acc: 0.5
Batch: 140; loss: 0.91; acc: 0.69
Val Epoch over. val_loss: 1.2274962230852455; val_accuracy: 0.5898686305732485 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.37; acc: 0.58
Batch: 20; loss: 1.25; acc: 0.69
Batch: 40; loss: 1.13; acc: 0.61
Batch: 60; loss: 1.32; acc: 0.5
Batch: 80; loss: 1.46; acc: 0.52
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.59
Batch: 160; loss: 1.41; acc: 0.56
Batch: 180; loss: 1.47; acc: 0.52
Batch: 200; loss: 1.5; acc: 0.5
Batch: 220; loss: 1.42; acc: 0.58
Batch: 240; loss: 1.07; acc: 0.61
Batch: 260; loss: 1.13; acc: 0.66
Batch: 280; loss: 1.39; acc: 0.55
Batch: 300; loss: 1.0; acc: 0.67
Batch: 320; loss: 1.23; acc: 0.59
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.49; acc: 0.52
Batch: 380; loss: 1.37; acc: 0.59
Batch: 400; loss: 1.17; acc: 0.64
Batch: 420; loss: 1.34; acc: 0.52
Batch: 440; loss: 1.13; acc: 0.59
Batch: 460; loss: 1.04; acc: 0.69
Batch: 480; loss: 1.19; acc: 0.58
Batch: 500; loss: 1.11; acc: 0.64
Batch: 520; loss: 1.24; acc: 0.59
Batch: 540; loss: 1.52; acc: 0.48
Batch: 560; loss: 1.38; acc: 0.59
Batch: 580; loss: 1.37; acc: 0.64
Batch: 600; loss: 1.37; acc: 0.59
Batch: 620; loss: 1.47; acc: 0.5
Batch: 640; loss: 1.31; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.58
Batch: 680; loss: 1.31; acc: 0.61
Batch: 700; loss: 1.24; acc: 0.62
Batch: 720; loss: 1.39; acc: 0.56
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.4; acc: 0.44
Batch: 780; loss: 1.16; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.42
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.57; acc: 0.52
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.226655418326141; val_accuracy: 0.5927547770700637 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.16; acc: 0.59
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 1.12; acc: 0.67
Batch: 60; loss: 1.16; acc: 0.56
Batch: 80; loss: 1.28; acc: 0.55
Batch: 100; loss: 1.5; acc: 0.55
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.45
Batch: 160; loss: 1.21; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.58
Batch: 200; loss: 1.38; acc: 0.5
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.39; acc: 0.61
Batch: 260; loss: 1.36; acc: 0.5
Batch: 280; loss: 1.26; acc: 0.55
Batch: 300; loss: 1.32; acc: 0.53
Batch: 320; loss: 1.25; acc: 0.53
Batch: 340; loss: 1.11; acc: 0.61
Batch: 360; loss: 1.29; acc: 0.59
Batch: 380; loss: 1.31; acc: 0.52
Batch: 400; loss: 1.28; acc: 0.53
Batch: 420; loss: 1.38; acc: 0.55
Batch: 440; loss: 1.32; acc: 0.53
Batch: 460; loss: 1.31; acc: 0.58
Batch: 480; loss: 0.94; acc: 0.7
Batch: 500; loss: 1.34; acc: 0.53
Batch: 520; loss: 1.38; acc: 0.56
Batch: 540; loss: 1.26; acc: 0.55
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.05; acc: 0.61
Batch: 600; loss: 1.19; acc: 0.62
Batch: 620; loss: 1.15; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.59
Batch: 660; loss: 1.17; acc: 0.62
Batch: 680; loss: 1.36; acc: 0.47
Batch: 700; loss: 1.22; acc: 0.58
Batch: 720; loss: 1.4; acc: 0.62
Batch: 740; loss: 1.27; acc: 0.59
Batch: 760; loss: 1.28; acc: 0.55
Batch: 780; loss: 1.2; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.45
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 0.89; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.67
Batch: 120; loss: 1.58; acc: 0.48
Batch: 140; loss: 0.91; acc: 0.7
Val Epoch over. val_loss: 1.2279791471305166; val_accuracy: 0.5912619426751592 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 1.19; acc: 0.59
Batch: 60; loss: 1.29; acc: 0.55
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.52
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 1.39; acc: 0.58
Batch: 160; loss: 1.28; acc: 0.61
Batch: 180; loss: 1.16; acc: 0.56
Batch: 200; loss: 1.17; acc: 0.62
Batch: 220; loss: 1.41; acc: 0.58
Batch: 240; loss: 1.18; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.55
Batch: 280; loss: 1.03; acc: 0.66
Batch: 300; loss: 1.13; acc: 0.62
Batch: 320; loss: 1.42; acc: 0.5
Batch: 340; loss: 1.35; acc: 0.59
Batch: 360; loss: 1.52; acc: 0.44
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.18; acc: 0.59
Batch: 420; loss: 0.9; acc: 0.8
Batch: 440; loss: 1.26; acc: 0.64
Batch: 460; loss: 1.33; acc: 0.55
Batch: 480; loss: 0.97; acc: 0.59
Batch: 500; loss: 1.25; acc: 0.58
Batch: 520; loss: 1.63; acc: 0.41
Batch: 540; loss: 1.24; acc: 0.56
Batch: 560; loss: 1.18; acc: 0.58
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.21; acc: 0.66
Batch: 620; loss: 1.16; acc: 0.62
Batch: 640; loss: 1.38; acc: 0.55
Batch: 660; loss: 1.33; acc: 0.56
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 1.01; acc: 0.64
Batch: 720; loss: 1.42; acc: 0.52
Batch: 740; loss: 1.27; acc: 0.56
Batch: 760; loss: 1.46; acc: 0.48
Batch: 780; loss: 1.08; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 0.91; acc: 0.69
Val Epoch over. val_loss: 1.2261849854402482; val_accuracy: 0.5915605095541401 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.24; acc: 0.5
Batch: 60; loss: 1.18; acc: 0.61
Batch: 80; loss: 1.26; acc: 0.53
Batch: 100; loss: 1.17; acc: 0.52
Batch: 120; loss: 1.31; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.44
Batch: 160; loss: 1.26; acc: 0.58
Batch: 180; loss: 1.11; acc: 0.69
Batch: 200; loss: 1.46; acc: 0.48
Batch: 220; loss: 1.41; acc: 0.5
Batch: 240; loss: 1.5; acc: 0.53
Batch: 260; loss: 1.13; acc: 0.64
Batch: 280; loss: 1.34; acc: 0.53
Batch: 300; loss: 1.32; acc: 0.56
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.32; acc: 0.52
Batch: 360; loss: 1.24; acc: 0.56
Batch: 380; loss: 1.39; acc: 0.52
Batch: 400; loss: 1.58; acc: 0.5
Batch: 420; loss: 1.24; acc: 0.55
Batch: 440; loss: 1.29; acc: 0.58
Batch: 460; loss: 1.75; acc: 0.47
Batch: 480; loss: 1.25; acc: 0.59
Batch: 500; loss: 1.15; acc: 0.53
Batch: 520; loss: 1.54; acc: 0.47
Batch: 540; loss: 1.08; acc: 0.64
Batch: 560; loss: 1.09; acc: 0.62
Batch: 580; loss: 1.39; acc: 0.48
Batch: 600; loss: 1.31; acc: 0.53
Batch: 620; loss: 1.12; acc: 0.61
Batch: 640; loss: 1.31; acc: 0.45
Batch: 660; loss: 1.39; acc: 0.62
Batch: 680; loss: 1.23; acc: 0.59
Batch: 700; loss: 1.34; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 0.94; acc: 0.69
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.15; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.67
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 0.9; acc: 0.72
Val Epoch over. val_loss: 1.227027643638052; val_accuracy: 0.5922571656050956 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.47; acc: 0.48
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.3; acc: 0.58
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 1.23; acc: 0.56
Batch: 120; loss: 1.12; acc: 0.58
Batch: 140; loss: 1.64; acc: 0.39
Batch: 160; loss: 1.3; acc: 0.61
Batch: 180; loss: 1.19; acc: 0.62
Batch: 200; loss: 1.35; acc: 0.53
Batch: 220; loss: 1.46; acc: 0.55
Batch: 240; loss: 1.02; acc: 0.62
Batch: 260; loss: 1.22; acc: 0.58
Batch: 280; loss: 1.48; acc: 0.45
Batch: 300; loss: 1.35; acc: 0.56
Batch: 320; loss: 1.48; acc: 0.56
Batch: 340; loss: 1.2; acc: 0.62
Batch: 360; loss: 1.27; acc: 0.58
Batch: 380; loss: 1.4; acc: 0.5
Batch: 400; loss: 0.96; acc: 0.67
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 1.2; acc: 0.66
Batch: 460; loss: 1.24; acc: 0.59
Batch: 480; loss: 1.04; acc: 0.67
Batch: 500; loss: 1.32; acc: 0.61
Batch: 520; loss: 1.28; acc: 0.61
Batch: 540; loss: 1.28; acc: 0.56
Batch: 560; loss: 1.43; acc: 0.55
Batch: 580; loss: 1.54; acc: 0.44
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 1.42; acc: 0.53
Batch: 640; loss: 1.35; acc: 0.55
Batch: 660; loss: 0.98; acc: 0.7
Batch: 680; loss: 1.03; acc: 0.62
Batch: 700; loss: 1.45; acc: 0.58
Batch: 720; loss: 1.26; acc: 0.61
Batch: 740; loss: 1.34; acc: 0.56
Batch: 760; loss: 1.14; acc: 0.64
Batch: 780; loss: 1.21; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.2253068275512404; val_accuracy: 0.5942476114649682 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.28; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.52
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 1.14; acc: 0.62
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 1.39; acc: 0.52
Batch: 160; loss: 1.32; acc: 0.55
Batch: 180; loss: 1.21; acc: 0.53
Batch: 200; loss: 1.33; acc: 0.55
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.29; acc: 0.58
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.61
Batch: 300; loss: 1.1; acc: 0.62
Batch: 320; loss: 1.33; acc: 0.56
Batch: 340; loss: 1.12; acc: 0.58
Batch: 360; loss: 0.97; acc: 0.69
Batch: 380; loss: 1.31; acc: 0.61
Batch: 400; loss: 1.23; acc: 0.59
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.37; acc: 0.55
Batch: 460; loss: 1.26; acc: 0.52
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 1.1; acc: 0.58
Batch: 520; loss: 1.27; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.61
Batch: 560; loss: 1.35; acc: 0.52
Batch: 580; loss: 1.25; acc: 0.59
Batch: 600; loss: 1.54; acc: 0.48
Batch: 620; loss: 1.28; acc: 0.53
Batch: 640; loss: 1.12; acc: 0.67
Batch: 660; loss: 1.05; acc: 0.56
Batch: 680; loss: 1.05; acc: 0.7
Batch: 700; loss: 1.48; acc: 0.53
Batch: 720; loss: 1.35; acc: 0.55
Batch: 740; loss: 1.24; acc: 0.56
Batch: 760; loss: 1.44; acc: 0.52
Batch: 780; loss: 1.13; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.49; acc: 0.45
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 0.88; acc: 0.7
Val Epoch over. val_loss: 1.2244367827275755; val_accuracy: 0.59484474522293 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.55; acc: 0.48
Batch: 20; loss: 1.34; acc: 0.56
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.13; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.56; acc: 0.39
Batch: 140; loss: 1.48; acc: 0.55
Batch: 160; loss: 1.21; acc: 0.61
Batch: 180; loss: 1.5; acc: 0.44
Batch: 200; loss: 1.02; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.52
Batch: 240; loss: 1.41; acc: 0.58
Batch: 260; loss: 1.42; acc: 0.53
Batch: 280; loss: 1.34; acc: 0.58
Batch: 300; loss: 1.37; acc: 0.55
Batch: 320; loss: 1.31; acc: 0.61
Batch: 340; loss: 1.34; acc: 0.55
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.41; acc: 0.55
Batch: 400; loss: 1.23; acc: 0.56
Batch: 420; loss: 1.26; acc: 0.59
Batch: 440; loss: 1.2; acc: 0.61
Batch: 460; loss: 1.16; acc: 0.61
Batch: 480; loss: 1.36; acc: 0.53
Batch: 500; loss: 1.05; acc: 0.61
Batch: 520; loss: 1.02; acc: 0.69
Batch: 540; loss: 1.21; acc: 0.66
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 1.37; acc: 0.53
Batch: 600; loss: 1.19; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.58
Batch: 640; loss: 1.55; acc: 0.52
Batch: 660; loss: 1.27; acc: 0.56
Batch: 680; loss: 1.35; acc: 0.62
Batch: 700; loss: 1.18; acc: 0.61
Batch: 720; loss: 1.2; acc: 0.56
Batch: 740; loss: 1.13; acc: 0.72
Batch: 760; loss: 1.41; acc: 0.58
Batch: 780; loss: 1.2; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.67
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 0.9; acc: 0.72
Val Epoch over. val_loss: 1.2264456042818204; val_accuracy: 0.5922571656050956 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.11; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 1.06; acc: 0.62
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.08; acc: 0.66
Batch: 100; loss: 1.13; acc: 0.55
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.55
Batch: 160; loss: 1.33; acc: 0.5
Batch: 180; loss: 1.12; acc: 0.66
Batch: 200; loss: 1.33; acc: 0.56
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.29; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.58
Batch: 300; loss: 1.21; acc: 0.62
Batch: 320; loss: 1.16; acc: 0.61
Batch: 340; loss: 1.14; acc: 0.62
Batch: 360; loss: 1.14; acc: 0.62
Batch: 380; loss: 1.41; acc: 0.52
Batch: 400; loss: 1.28; acc: 0.53
Batch: 420; loss: 1.33; acc: 0.59
Batch: 440; loss: 1.49; acc: 0.55
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.42; acc: 0.56
Batch: 500; loss: 1.48; acc: 0.53
Batch: 520; loss: 1.34; acc: 0.61
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.03; acc: 0.66
Batch: 580; loss: 1.27; acc: 0.52
Batch: 600; loss: 1.39; acc: 0.47
Batch: 620; loss: 1.29; acc: 0.52
Batch: 640; loss: 1.32; acc: 0.52
Batch: 660; loss: 1.17; acc: 0.59
Batch: 680; loss: 1.41; acc: 0.58
Batch: 700; loss: 1.13; acc: 0.64
Batch: 720; loss: 1.19; acc: 0.56
Batch: 740; loss: 1.3; acc: 0.56
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 1.22; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.42; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 0.89; acc: 0.72
Val Epoch over. val_loss: 1.2235754420802851; val_accuracy: 0.5947452229299363 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.02; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 1.26; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.31; acc: 0.56
Batch: 160; loss: 1.39; acc: 0.55
Batch: 180; loss: 1.19; acc: 0.55
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.18; acc: 0.59
Batch: 240; loss: 1.29; acc: 0.55
Batch: 260; loss: 1.26; acc: 0.53
Batch: 280; loss: 1.24; acc: 0.55
Batch: 300; loss: 1.31; acc: 0.58
Batch: 320; loss: 1.45; acc: 0.5
Batch: 340; loss: 1.22; acc: 0.55
Batch: 360; loss: 1.27; acc: 0.62
Batch: 380; loss: 1.25; acc: 0.59
Batch: 400; loss: 1.23; acc: 0.55
Batch: 420; loss: 1.38; acc: 0.52
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.35; acc: 0.59
Batch: 480; loss: 1.31; acc: 0.59
Batch: 500; loss: 1.2; acc: 0.59
Batch: 520; loss: 1.29; acc: 0.56
Batch: 540; loss: 1.21; acc: 0.61
Batch: 560; loss: 1.36; acc: 0.56
Batch: 580; loss: 1.34; acc: 0.58
Batch: 600; loss: 1.4; acc: 0.45
Batch: 620; loss: 1.34; acc: 0.56
Batch: 640; loss: 1.3; acc: 0.56
Batch: 660; loss: 1.1; acc: 0.66
Batch: 680; loss: 1.36; acc: 0.59
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.48; acc: 0.52
Batch: 740; loss: 1.35; acc: 0.61
Batch: 760; loss: 1.05; acc: 0.61
Batch: 780; loss: 1.39; acc: 0.5
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.42
Batch: 20; loss: 1.4; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.06; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 1.53; acc: 0.5
Batch: 140; loss: 0.89; acc: 0.73
Val Epoch over. val_loss: 1.2233122017732851; val_accuracy: 0.5935509554140127 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.4; acc: 0.61
Batch: 20; loss: 1.54; acc: 0.45
Batch: 40; loss: 1.21; acc: 0.55
Batch: 60; loss: 1.35; acc: 0.53
Batch: 80; loss: 1.3; acc: 0.55
Batch: 100; loss: 1.28; acc: 0.52
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 1.34; acc: 0.56
Batch: 160; loss: 1.23; acc: 0.59
Batch: 180; loss: 0.94; acc: 0.66
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.4; acc: 0.56
Batch: 240; loss: 1.25; acc: 0.55
Batch: 260; loss: 1.16; acc: 0.72
Batch: 280; loss: 1.16; acc: 0.62
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.01; acc: 0.69
Batch: 340; loss: 1.16; acc: 0.64
Batch: 360; loss: 1.78; acc: 0.45
Batch: 380; loss: 1.37; acc: 0.47
Batch: 400; loss: 1.31; acc: 0.55
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.41; acc: 0.61
Batch: 460; loss: 1.4; acc: 0.52
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.4; acc: 0.52
Batch: 520; loss: 1.41; acc: 0.48
Batch: 540; loss: 1.38; acc: 0.58
Batch: 560; loss: 1.33; acc: 0.53
Batch: 580; loss: 1.39; acc: 0.62
Batch: 600; loss: 1.47; acc: 0.41
Batch: 620; loss: 1.4; acc: 0.52
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.0; acc: 0.69
Batch: 680; loss: 1.41; acc: 0.56
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.38; acc: 0.5
Batch: 740; loss: 1.18; acc: 0.62
Batch: 760; loss: 1.43; acc: 0.55
Batch: 780; loss: 1.32; acc: 0.5
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.42
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 0.89; acc: 0.72
Val Epoch over. val_loss: 1.223395309630473; val_accuracy: 0.5929538216560509 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.51; acc: 0.47
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 1.29; acc: 0.62
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.56
Batch: 100; loss: 1.51; acc: 0.52
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 1.69; acc: 0.5
Batch: 180; loss: 1.23; acc: 0.56
Batch: 200; loss: 1.44; acc: 0.52
Batch: 220; loss: 1.25; acc: 0.66
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.18; acc: 0.67
Batch: 280; loss: 1.28; acc: 0.62
Batch: 300; loss: 1.09; acc: 0.62
Batch: 320; loss: 1.43; acc: 0.55
Batch: 340; loss: 1.27; acc: 0.59
Batch: 360; loss: 1.15; acc: 0.56
Batch: 380; loss: 1.3; acc: 0.52
Batch: 400; loss: 1.29; acc: 0.56
Batch: 420; loss: 1.28; acc: 0.58
Batch: 440; loss: 1.32; acc: 0.59
Batch: 460; loss: 1.38; acc: 0.53
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 1.32; acc: 0.58
Batch: 520; loss: 1.46; acc: 0.47
Batch: 540; loss: 1.22; acc: 0.61
Batch: 560; loss: 1.46; acc: 0.55
Batch: 580; loss: 1.36; acc: 0.62
Batch: 600; loss: 1.17; acc: 0.58
Batch: 620; loss: 1.08; acc: 0.61
Batch: 640; loss: 1.07; acc: 0.75
Batch: 660; loss: 1.37; acc: 0.53
Batch: 680; loss: 1.47; acc: 0.52
Batch: 700; loss: 1.19; acc: 0.58
Batch: 720; loss: 1.52; acc: 0.58
Batch: 740; loss: 1.2; acc: 0.62
Batch: 760; loss: 1.08; acc: 0.61
Batch: 780; loss: 1.3; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.52; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.2245938219841879; val_accuracy: 0.5939490445859873 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.18; acc: 0.58
Batch: 20; loss: 1.55; acc: 0.48
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.24; acc: 0.58
Batch: 80; loss: 1.11; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.55
Batch: 120; loss: 1.26; acc: 0.52
Batch: 140; loss: 1.11; acc: 0.61
Batch: 160; loss: 1.32; acc: 0.58
Batch: 180; loss: 1.46; acc: 0.52
Batch: 200; loss: 1.29; acc: 0.58
Batch: 220; loss: 1.42; acc: 0.66
Batch: 240; loss: 1.38; acc: 0.52
Batch: 260; loss: 1.38; acc: 0.55
Batch: 280; loss: 1.37; acc: 0.58
Batch: 300; loss: 1.23; acc: 0.52
Batch: 320; loss: 1.22; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.17; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.62
Batch: 420; loss: 1.16; acc: 0.62
Batch: 440; loss: 0.97; acc: 0.66
Batch: 460; loss: 1.1; acc: 0.73
Batch: 480; loss: 1.33; acc: 0.56
Batch: 500; loss: 1.34; acc: 0.53
Batch: 520; loss: 1.21; acc: 0.56
Batch: 540; loss: 1.29; acc: 0.56
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.58
Batch: 600; loss: 1.6; acc: 0.5
Batch: 620; loss: 1.02; acc: 0.7
Batch: 640; loss: 1.27; acc: 0.62
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.36; acc: 0.55
Batch: 700; loss: 1.29; acc: 0.58
Batch: 720; loss: 1.1; acc: 0.56
Batch: 740; loss: 1.39; acc: 0.55
Batch: 760; loss: 1.3; acc: 0.58
Batch: 780; loss: 1.24; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.42
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.56; acc: 0.53
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.224276544182164; val_accuracy: 0.5928542993630573 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 1.27; acc: 0.61
Batch: 60; loss: 1.37; acc: 0.66
Batch: 80; loss: 1.06; acc: 0.62
Batch: 100; loss: 1.18; acc: 0.56
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.44; acc: 0.56
Batch: 160; loss: 1.44; acc: 0.55
Batch: 180; loss: 1.27; acc: 0.62
Batch: 200; loss: 1.15; acc: 0.66
Batch: 220; loss: 1.22; acc: 0.58
Batch: 240; loss: 1.47; acc: 0.56
Batch: 260; loss: 1.31; acc: 0.53
Batch: 280; loss: 1.29; acc: 0.5
Batch: 300; loss: 1.2; acc: 0.62
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.25; acc: 0.55
Batch: 360; loss: 1.16; acc: 0.66
Batch: 380; loss: 1.32; acc: 0.52
Batch: 400; loss: 1.42; acc: 0.56
Batch: 420; loss: 1.1; acc: 0.67
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.58; acc: 0.59
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 1.43; acc: 0.53
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 1.31; acc: 0.53
Batch: 600; loss: 1.44; acc: 0.61
Batch: 620; loss: 1.14; acc: 0.53
Batch: 640; loss: 1.08; acc: 0.64
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.26; acc: 0.58
Batch: 700; loss: 1.19; acc: 0.59
Batch: 720; loss: 1.26; acc: 0.59
Batch: 740; loss: 1.15; acc: 0.59
Batch: 760; loss: 1.31; acc: 0.59
Batch: 780; loss: 1.29; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.4; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.223536632622883; val_accuracy: 0.5930533439490446 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.4; acc: 0.56
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 0.99; acc: 0.67
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 1.1; acc: 0.55
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 1.46; acc: 0.58
Batch: 160; loss: 1.42; acc: 0.48
Batch: 180; loss: 0.93; acc: 0.72
Batch: 200; loss: 1.26; acc: 0.62
Batch: 220; loss: 1.04; acc: 0.62
Batch: 240; loss: 1.23; acc: 0.59
Batch: 260; loss: 1.41; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.27; acc: 0.59
Batch: 320; loss: 1.11; acc: 0.58
Batch: 340; loss: 1.1; acc: 0.59
Batch: 360; loss: 1.35; acc: 0.56
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.46; acc: 0.53
Batch: 420; loss: 1.25; acc: 0.62
Batch: 440; loss: 1.29; acc: 0.58
Batch: 460; loss: 1.41; acc: 0.52
Batch: 480; loss: 1.45; acc: 0.55
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 1.42; acc: 0.55
Batch: 540; loss: 1.55; acc: 0.47
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.22; acc: 0.59
Batch: 600; loss: 1.27; acc: 0.61
Batch: 620; loss: 1.58; acc: 0.53
Batch: 640; loss: 1.05; acc: 0.64
Batch: 660; loss: 1.21; acc: 0.55
Batch: 680; loss: 1.21; acc: 0.67
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.16; acc: 0.62
Batch: 740; loss: 1.26; acc: 0.59
Batch: 760; loss: 1.09; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 0.9; acc: 0.72
Val Epoch over. val_loss: 1.2240569356140818; val_accuracy: 0.5946457006369427 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.56
Batch: 40; loss: 1.17; acc: 0.67
Batch: 60; loss: 1.48; acc: 0.52
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.21; acc: 0.64
Batch: 160; loss: 1.31; acc: 0.69
Batch: 180; loss: 1.11; acc: 0.61
Batch: 200; loss: 0.98; acc: 0.67
Batch: 220; loss: 1.01; acc: 0.7
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 1.21; acc: 0.56
Batch: 280; loss: 1.71; acc: 0.52
Batch: 300; loss: 1.7; acc: 0.44
Batch: 320; loss: 1.38; acc: 0.53
Batch: 340; loss: 1.0; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.52
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.29; acc: 0.56
Batch: 420; loss: 1.33; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.47
Batch: 460; loss: 1.08; acc: 0.64
Batch: 480; loss: 1.04; acc: 0.64
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.52; acc: 0.5
Batch: 540; loss: 1.52; acc: 0.47
Batch: 560; loss: 1.32; acc: 0.53
Batch: 580; loss: 1.48; acc: 0.5
Batch: 600; loss: 1.36; acc: 0.59
Batch: 620; loss: 1.2; acc: 0.59
Batch: 640; loss: 1.27; acc: 0.52
Batch: 660; loss: 1.34; acc: 0.55
Batch: 680; loss: 1.19; acc: 0.62
Batch: 700; loss: 1.08; acc: 0.62
Batch: 720; loss: 1.22; acc: 0.62
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.51; acc: 0.55
Batch: 780; loss: 1.36; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.4; acc: 0.56
Batch: 40; loss: 0.89; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.2241557065848332; val_accuracy: 0.5938495222929936 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.66
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.09; acc: 0.58
Batch: 80; loss: 1.65; acc: 0.48
Batch: 100; loss: 1.33; acc: 0.59
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 1.06; acc: 0.62
Batch: 160; loss: 1.3; acc: 0.52
Batch: 180; loss: 1.18; acc: 0.59
Batch: 200; loss: 0.98; acc: 0.69
Batch: 220; loss: 1.34; acc: 0.59
Batch: 240; loss: 1.47; acc: 0.47
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.04; acc: 0.62
Batch: 300; loss: 1.51; acc: 0.47
Batch: 320; loss: 1.09; acc: 0.58
Batch: 340; loss: 1.56; acc: 0.44
Batch: 360; loss: 1.22; acc: 0.53
Batch: 380; loss: 1.19; acc: 0.64
Batch: 400; loss: 0.97; acc: 0.69
Batch: 420; loss: 1.49; acc: 0.5
Batch: 440; loss: 1.28; acc: 0.56
Batch: 460; loss: 1.22; acc: 0.53
Batch: 480; loss: 1.08; acc: 0.61
Batch: 500; loss: 1.19; acc: 0.53
Batch: 520; loss: 1.02; acc: 0.62
Batch: 540; loss: 1.33; acc: 0.53
Batch: 560; loss: 1.39; acc: 0.55
Batch: 580; loss: 1.07; acc: 0.62
Batch: 600; loss: 1.22; acc: 0.5
Batch: 620; loss: 1.69; acc: 0.47
Batch: 640; loss: 1.14; acc: 0.69
Batch: 660; loss: 1.25; acc: 0.69
Batch: 680; loss: 1.27; acc: 0.56
Batch: 700; loss: 1.25; acc: 0.56
Batch: 720; loss: 1.19; acc: 0.61
Batch: 740; loss: 1.11; acc: 0.66
Batch: 760; loss: 1.21; acc: 0.61
Batch: 780; loss: 1.3; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.42
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.223796049880374; val_accuracy: 0.5933519108280255 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 1.4; acc: 0.53
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.33; acc: 0.58
Batch: 120; loss: 1.53; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 1.14; acc: 0.61
Batch: 180; loss: 1.06; acc: 0.62
Batch: 200; loss: 1.42; acc: 0.59
Batch: 220; loss: 1.56; acc: 0.44
Batch: 240; loss: 1.25; acc: 0.61
Batch: 260; loss: 1.27; acc: 0.53
Batch: 280; loss: 1.39; acc: 0.53
Batch: 300; loss: 1.32; acc: 0.55
Batch: 320; loss: 1.4; acc: 0.53
Batch: 340; loss: 1.31; acc: 0.5
Batch: 360; loss: 1.4; acc: 0.53
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 1.43; acc: 0.44
Batch: 420; loss: 1.37; acc: 0.55
Batch: 440; loss: 1.42; acc: 0.58
Batch: 460; loss: 1.23; acc: 0.58
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 1.19; acc: 0.59
Batch: 520; loss: 1.29; acc: 0.53
Batch: 540; loss: 1.19; acc: 0.58
Batch: 560; loss: 1.33; acc: 0.5
Batch: 580; loss: 1.53; acc: 0.48
Batch: 600; loss: 1.07; acc: 0.69
Batch: 620; loss: 1.14; acc: 0.61
Batch: 640; loss: 1.29; acc: 0.53
Batch: 660; loss: 1.27; acc: 0.53
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.16; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.55
Batch: 740; loss: 1.29; acc: 0.55
Batch: 760; loss: 1.38; acc: 0.55
Batch: 780; loss: 1.44; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.41
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.225339923694635; val_accuracy: 0.59375 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.43; acc: 0.56
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 1.37; acc: 0.53
Batch: 60; loss: 1.29; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.44
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.67
Batch: 160; loss: 1.08; acc: 0.66
Batch: 180; loss: 1.3; acc: 0.5
Batch: 200; loss: 1.2; acc: 0.55
Batch: 220; loss: 1.4; acc: 0.55
Batch: 240; loss: 1.14; acc: 0.53
Batch: 260; loss: 1.26; acc: 0.58
Batch: 280; loss: 1.26; acc: 0.55
Batch: 300; loss: 1.06; acc: 0.61
Batch: 320; loss: 1.44; acc: 0.55
Batch: 340; loss: 1.27; acc: 0.55
Batch: 360; loss: 1.33; acc: 0.59
Batch: 380; loss: 1.16; acc: 0.62
Batch: 400; loss: 1.34; acc: 0.59
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.44; acc: 0.61
Batch: 460; loss: 1.36; acc: 0.61
Batch: 480; loss: 1.24; acc: 0.61
Batch: 500; loss: 1.13; acc: 0.64
Batch: 520; loss: 1.48; acc: 0.53
Batch: 540; loss: 1.31; acc: 0.5
Batch: 560; loss: 1.38; acc: 0.55
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.24; acc: 0.55
Batch: 620; loss: 1.26; acc: 0.58
Batch: 640; loss: 1.13; acc: 0.59
Batch: 660; loss: 1.2; acc: 0.59
Batch: 680; loss: 1.23; acc: 0.56
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.12; acc: 0.64
Batch: 740; loss: 1.32; acc: 0.59
Batch: 760; loss: 1.2; acc: 0.59
Batch: 780; loss: 1.22; acc: 0.64
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.58
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.223552581610953; val_accuracy: 0.59265525477707 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.42; acc: 0.56
Batch: 20; loss: 1.21; acc: 0.56
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 1.22; acc: 0.66
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.01; acc: 0.64
Batch: 140; loss: 1.02; acc: 0.69
Batch: 160; loss: 1.11; acc: 0.61
Batch: 180; loss: 1.01; acc: 0.66
Batch: 200; loss: 1.27; acc: 0.56
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 1.39; acc: 0.61
Batch: 260; loss: 1.17; acc: 0.66
Batch: 280; loss: 1.39; acc: 0.52
Batch: 300; loss: 1.34; acc: 0.47
Batch: 320; loss: 1.19; acc: 0.66
Batch: 340; loss: 1.33; acc: 0.53
Batch: 360; loss: 1.45; acc: 0.55
Batch: 380; loss: 1.35; acc: 0.53
Batch: 400; loss: 1.1; acc: 0.66
Batch: 420; loss: 1.34; acc: 0.53
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 1.26; acc: 0.56
Batch: 500; loss: 1.26; acc: 0.48
Batch: 520; loss: 1.12; acc: 0.59
Batch: 540; loss: 1.27; acc: 0.59
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.2; acc: 0.64
Batch: 600; loss: 1.08; acc: 0.66
Batch: 620; loss: 1.45; acc: 0.53
Batch: 640; loss: 1.38; acc: 0.55
Batch: 660; loss: 1.41; acc: 0.55
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 1.06; acc: 0.64
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.1; acc: 0.62
Batch: 760; loss: 1.16; acc: 0.69
Batch: 780; loss: 1.22; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.44
Batch: 20; loss: 1.4; acc: 0.58
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.05; acc: 0.69
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.2231302181626582; val_accuracy: 0.5929538216560509 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.3; acc: 0.58
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 1.14; acc: 0.58
Batch: 60; loss: 1.4; acc: 0.55
Batch: 80; loss: 1.08; acc: 0.61
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 1.24; acc: 0.56
Batch: 160; loss: 1.39; acc: 0.52
Batch: 180; loss: 1.1; acc: 0.56
Batch: 200; loss: 1.35; acc: 0.59
Batch: 220; loss: 1.5; acc: 0.58
Batch: 240; loss: 1.5; acc: 0.53
Batch: 260; loss: 1.32; acc: 0.55
Batch: 280; loss: 1.3; acc: 0.52
Batch: 300; loss: 1.05; acc: 0.64
Batch: 320; loss: 1.31; acc: 0.48
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 1.33; acc: 0.55
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 0.95; acc: 0.78
Batch: 420; loss: 1.32; acc: 0.58
Batch: 440; loss: 1.7; acc: 0.44
Batch: 460; loss: 1.28; acc: 0.53
Batch: 480; loss: 1.48; acc: 0.47
Batch: 500; loss: 1.14; acc: 0.66
Batch: 520; loss: 1.19; acc: 0.61
Batch: 540; loss: 1.34; acc: 0.55
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 1.13; acc: 0.66
Batch: 620; loss: 1.33; acc: 0.52
Batch: 640; loss: 1.3; acc: 0.55
Batch: 660; loss: 1.28; acc: 0.61
Batch: 680; loss: 1.31; acc: 0.61
Batch: 700; loss: 1.23; acc: 0.64
Batch: 720; loss: 1.11; acc: 0.58
Batch: 740; loss: 1.21; acc: 0.61
Batch: 760; loss: 1.2; acc: 0.61
Batch: 780; loss: 1.22; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.2237345458595617; val_accuracy: 0.5930533439490446 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.36; acc: 0.62
Batch: 20; loss: 1.61; acc: 0.48
Batch: 40; loss: 1.1; acc: 0.62
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.5; acc: 0.52
Batch: 100; loss: 1.42; acc: 0.52
Batch: 120; loss: 1.27; acc: 0.58
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.2; acc: 0.62
Batch: 180; loss: 1.1; acc: 0.61
Batch: 200; loss: 1.34; acc: 0.52
Batch: 220; loss: 1.03; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.62
Batch: 260; loss: 1.35; acc: 0.58
Batch: 280; loss: 1.17; acc: 0.59
Batch: 300; loss: 1.35; acc: 0.52
Batch: 320; loss: 1.2; acc: 0.62
Batch: 340; loss: 1.28; acc: 0.56
Batch: 360; loss: 1.29; acc: 0.5
Batch: 380; loss: 1.3; acc: 0.53
Batch: 400; loss: 1.39; acc: 0.55
Batch: 420; loss: 1.08; acc: 0.61
Batch: 440; loss: 1.07; acc: 0.64
Batch: 460; loss: 1.37; acc: 0.52
Batch: 480; loss: 1.13; acc: 0.67
Batch: 500; loss: 1.26; acc: 0.64
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.26; acc: 0.5
Batch: 560; loss: 1.39; acc: 0.59
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.5; acc: 0.55
Batch: 620; loss: 1.13; acc: 0.56
Batch: 640; loss: 1.37; acc: 0.62
Batch: 660; loss: 1.73; acc: 0.53
Batch: 680; loss: 1.35; acc: 0.52
Batch: 700; loss: 1.26; acc: 0.59
Batch: 720; loss: 1.31; acc: 0.58
Batch: 740; loss: 0.99; acc: 0.67
Batch: 760; loss: 1.11; acc: 0.62
Batch: 780; loss: 1.09; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.224049898469524; val_accuracy: 0.5941480891719745 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.35; acc: 0.5
Batch: 20; loss: 1.35; acc: 0.55
Batch: 40; loss: 1.14; acc: 0.64
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.31; acc: 0.62
Batch: 100; loss: 1.49; acc: 0.5
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 1.64; acc: 0.38
Batch: 160; loss: 1.58; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.48
Batch: 200; loss: 1.2; acc: 0.62
Batch: 220; loss: 1.14; acc: 0.64
Batch: 240; loss: 1.3; acc: 0.58
Batch: 260; loss: 1.33; acc: 0.58
Batch: 280; loss: 1.24; acc: 0.61
Batch: 300; loss: 1.3; acc: 0.56
Batch: 320; loss: 1.2; acc: 0.62
Batch: 340; loss: 1.28; acc: 0.61
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.08; acc: 0.59
Batch: 400; loss: 1.18; acc: 0.7
Batch: 420; loss: 1.49; acc: 0.5
Batch: 440; loss: 1.54; acc: 0.47
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.34; acc: 0.55
Batch: 500; loss: 1.58; acc: 0.5
Batch: 520; loss: 1.19; acc: 0.62
Batch: 540; loss: 1.02; acc: 0.61
Batch: 560; loss: 1.14; acc: 0.69
Batch: 580; loss: 1.25; acc: 0.55
Batch: 600; loss: 1.31; acc: 0.59
Batch: 620; loss: 1.34; acc: 0.62
Batch: 640; loss: 1.45; acc: 0.53
Batch: 660; loss: 1.18; acc: 0.67
Batch: 680; loss: 1.45; acc: 0.61
Batch: 700; loss: 1.44; acc: 0.58
Batch: 720; loss: 1.1; acc: 0.62
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.14; acc: 0.53
Batch: 780; loss: 1.4; acc: 0.48
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.93; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.56; acc: 0.52
Batch: 140; loss: 0.9; acc: 0.72
Val Epoch over. val_loss: 1.2243841074074908; val_accuracy: 0.5933519108280255 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.4; acc: 0.53
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 1.07; acc: 0.59
Batch: 60; loss: 1.42; acc: 0.56
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.17; acc: 0.58
Batch: 120; loss: 1.12; acc: 0.58
Batch: 140; loss: 1.09; acc: 0.64
Batch: 160; loss: 1.67; acc: 0.55
Batch: 180; loss: 1.27; acc: 0.64
Batch: 200; loss: 1.43; acc: 0.47
Batch: 220; loss: 1.29; acc: 0.53
Batch: 240; loss: 1.32; acc: 0.52
Batch: 260; loss: 1.08; acc: 0.62
Batch: 280; loss: 1.52; acc: 0.53
Batch: 300; loss: 1.18; acc: 0.61
Batch: 320; loss: 1.19; acc: 0.62
Batch: 340; loss: 1.3; acc: 0.5
Batch: 360; loss: 1.32; acc: 0.52
Batch: 380; loss: 1.01; acc: 0.69
Batch: 400; loss: 1.35; acc: 0.59
Batch: 420; loss: 1.53; acc: 0.5
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.23; acc: 0.53
Batch: 480; loss: 1.42; acc: 0.48
Batch: 500; loss: 1.46; acc: 0.53
Batch: 520; loss: 1.24; acc: 0.61
Batch: 540; loss: 1.3; acc: 0.66
Batch: 560; loss: 1.43; acc: 0.52
Batch: 580; loss: 1.76; acc: 0.47
Batch: 600; loss: 1.12; acc: 0.69
Batch: 620; loss: 1.26; acc: 0.58
Batch: 640; loss: 1.35; acc: 0.56
Batch: 660; loss: 1.41; acc: 0.5
Batch: 680; loss: 1.15; acc: 0.62
Batch: 700; loss: 1.05; acc: 0.59
Batch: 720; loss: 1.12; acc: 0.64
Batch: 740; loss: 1.2; acc: 0.66
Batch: 760; loss: 1.23; acc: 0.56
Batch: 780; loss: 1.55; acc: 0.47
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.2234909226939936; val_accuracy: 0.5936504777070064 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.58
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.29; acc: 0.56
Batch: 100; loss: 1.29; acc: 0.58
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 1.24; acc: 0.55
Batch: 160; loss: 1.54; acc: 0.52
Batch: 180; loss: 1.42; acc: 0.53
Batch: 200; loss: 1.24; acc: 0.59
Batch: 220; loss: 1.17; acc: 0.61
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 1.36; acc: 0.61
Batch: 280; loss: 1.24; acc: 0.56
Batch: 300; loss: 1.28; acc: 0.53
Batch: 320; loss: 1.27; acc: 0.59
Batch: 340; loss: 1.12; acc: 0.58
Batch: 360; loss: 1.28; acc: 0.61
Batch: 380; loss: 1.16; acc: 0.72
Batch: 400; loss: 1.44; acc: 0.53
Batch: 420; loss: 1.35; acc: 0.48
Batch: 440; loss: 1.38; acc: 0.52
Batch: 460; loss: 1.01; acc: 0.69
Batch: 480; loss: 1.24; acc: 0.61
Batch: 500; loss: 1.68; acc: 0.5
Batch: 520; loss: 1.52; acc: 0.59
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.75; acc: 0.53
Batch: 580; loss: 1.18; acc: 0.55
Batch: 600; loss: 1.18; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.53
Batch: 640; loss: 0.98; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.59
Batch: 680; loss: 1.45; acc: 0.52
Batch: 700; loss: 1.14; acc: 0.66
Batch: 720; loss: 1.38; acc: 0.58
Batch: 740; loss: 1.14; acc: 0.62
Batch: 760; loss: 1.54; acc: 0.45
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.42
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 0.89; acc: 0.72
Val Epoch over. val_loss: 1.2229775175167497; val_accuracy: 0.5929538216560509 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.28; acc: 0.53
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.56
Batch: 60; loss: 1.25; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.61
Batch: 100; loss: 1.22; acc: 0.58
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 1.34; acc: 0.5
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.17; acc: 0.55
Batch: 220; loss: 1.35; acc: 0.62
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 0.99; acc: 0.7
Batch: 280; loss: 1.43; acc: 0.45
Batch: 300; loss: 1.26; acc: 0.62
Batch: 320; loss: 1.1; acc: 0.59
Batch: 340; loss: 1.52; acc: 0.44
Batch: 360; loss: 1.25; acc: 0.59
Batch: 380; loss: 1.31; acc: 0.56
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 1.27; acc: 0.66
Batch: 440; loss: 1.47; acc: 0.5
Batch: 460; loss: 1.38; acc: 0.59
Batch: 480; loss: 1.35; acc: 0.53
Batch: 500; loss: 1.36; acc: 0.5
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 1.11; acc: 0.59
Batch: 560; loss: 1.38; acc: 0.58
Batch: 580; loss: 1.26; acc: 0.61
Batch: 600; loss: 1.35; acc: 0.52
Batch: 620; loss: 1.45; acc: 0.52
Batch: 640; loss: 1.21; acc: 0.67
Batch: 660; loss: 1.28; acc: 0.64
Batch: 680; loss: 1.36; acc: 0.56
Batch: 700; loss: 1.48; acc: 0.52
Batch: 720; loss: 1.48; acc: 0.45
Batch: 740; loss: 1.34; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.58
Batch: 780; loss: 0.89; acc: 0.7
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.2234106295427698; val_accuracy: 0.5935509554140127 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.42; acc: 0.55
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.56
Batch: 100; loss: 1.44; acc: 0.59
Batch: 120; loss: 1.03; acc: 0.64
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.21; acc: 0.58
Batch: 180; loss: 1.57; acc: 0.53
Batch: 200; loss: 1.39; acc: 0.58
Batch: 220; loss: 1.41; acc: 0.48
Batch: 240; loss: 1.54; acc: 0.5
Batch: 260; loss: 1.37; acc: 0.52
Batch: 280; loss: 1.33; acc: 0.56
Batch: 300; loss: 1.32; acc: 0.53
Batch: 320; loss: 1.62; acc: 0.45
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 1.11; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.66
Batch: 400; loss: 1.27; acc: 0.58
Batch: 420; loss: 1.15; acc: 0.64
Batch: 440; loss: 1.08; acc: 0.62
Batch: 460; loss: 1.42; acc: 0.53
Batch: 480; loss: 1.13; acc: 0.64
Batch: 500; loss: 1.6; acc: 0.45
Batch: 520; loss: 1.3; acc: 0.64
Batch: 540; loss: 1.36; acc: 0.53
Batch: 560; loss: 1.15; acc: 0.62
Batch: 580; loss: 1.34; acc: 0.56
Batch: 600; loss: 1.28; acc: 0.59
Batch: 620; loss: 1.23; acc: 0.69
Batch: 640; loss: 1.47; acc: 0.55
Batch: 660; loss: 1.32; acc: 0.5
Batch: 680; loss: 1.1; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.67
Batch: 720; loss: 1.4; acc: 0.55
Batch: 740; loss: 1.45; acc: 0.59
Batch: 760; loss: 1.31; acc: 0.59
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.51; acc: 0.42
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.223712669056692; val_accuracy: 0.59375 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 1.38; acc: 0.58
Batch: 40; loss: 1.48; acc: 0.53
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 1.46; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 1.35; acc: 0.61
Batch: 160; loss: 1.07; acc: 0.56
Batch: 180; loss: 1.26; acc: 0.53
Batch: 200; loss: 1.39; acc: 0.47
Batch: 220; loss: 1.0; acc: 0.69
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.47; acc: 0.52
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 1.42; acc: 0.53
Batch: 320; loss: 1.24; acc: 0.59
Batch: 340; loss: 1.25; acc: 0.55
Batch: 360; loss: 1.2; acc: 0.59
Batch: 380; loss: 1.24; acc: 0.66
Batch: 400; loss: 1.31; acc: 0.62
Batch: 420; loss: 1.4; acc: 0.59
Batch: 440; loss: 0.96; acc: 0.67
Batch: 460; loss: 1.39; acc: 0.59
Batch: 480; loss: 1.27; acc: 0.59
Batch: 500; loss: 1.56; acc: 0.45
Batch: 520; loss: 1.48; acc: 0.56
Batch: 540; loss: 1.25; acc: 0.55
Batch: 560; loss: 1.17; acc: 0.61
Batch: 580; loss: 1.44; acc: 0.53
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 1.5; acc: 0.52
Batch: 640; loss: 1.38; acc: 0.59
Batch: 660; loss: 1.31; acc: 0.52
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 1.24; acc: 0.61
Batch: 740; loss: 1.26; acc: 0.59
Batch: 760; loss: 1.32; acc: 0.56
Batch: 780; loss: 1.48; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.5; acc: 0.44
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 0.9; acc: 0.7
Val Epoch over. val_loss: 1.223348464555801; val_accuracy: 0.5930533439490446 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 15638
elements in E: 3331950
fraction nonzero: 0.0046933477393118145
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.29; acc: 0.17
Batch: 240; loss: 2.29; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.29; acc: 0.11
Batch: 300; loss: 2.26; acc: 0.16
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.28; acc: 0.19
Batch: 360; loss: 2.26; acc: 0.19
Batch: 380; loss: 2.27; acc: 0.22
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.27; acc: 0.19
Batch: 440; loss: 2.26; acc: 0.17
Batch: 460; loss: 2.26; acc: 0.23
Batch: 480; loss: 2.28; acc: 0.19
Batch: 500; loss: 2.28; acc: 0.11
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.27; acc: 0.16
Batch: 560; loss: 2.25; acc: 0.25
Batch: 580; loss: 2.28; acc: 0.09
Batch: 600; loss: 2.26; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.14
Batch: 640; loss: 2.26; acc: 0.23
Batch: 660; loss: 2.26; acc: 0.17
Batch: 680; loss: 2.24; acc: 0.19
Batch: 700; loss: 2.26; acc: 0.22
Batch: 720; loss: 2.25; acc: 0.16
Batch: 740; loss: 2.23; acc: 0.19
Batch: 760; loss: 2.21; acc: 0.27
Batch: 780; loss: 2.23; acc: 0.23
Train Epoch over. train_loss: 2.28; train_accuracy: 0.15 

Batch: 0; loss: 2.22; acc: 0.28
Batch: 20; loss: 2.21; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.28
Batch: 60; loss: 2.21; acc: 0.3
Batch: 80; loss: 2.19; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.16
Batch: 120; loss: 2.22; acc: 0.2
Batch: 140; loss: 2.22; acc: 0.23
Val Epoch over. val_loss: 2.219947205986946; val_accuracy: 0.23676353503184713 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.06
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.23; acc: 0.27
Batch: 60; loss: 2.24; acc: 0.2
Batch: 80; loss: 2.19; acc: 0.25
Batch: 100; loss: 2.17; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.23
Batch: 140; loss: 2.19; acc: 0.28
Batch: 160; loss: 2.12; acc: 0.42
Batch: 180; loss: 2.12; acc: 0.28
Batch: 200; loss: 2.12; acc: 0.31
Batch: 220; loss: 2.15; acc: 0.22
Batch: 240; loss: 2.17; acc: 0.23
Batch: 260; loss: 2.09; acc: 0.3
Batch: 280; loss: 2.03; acc: 0.36
Batch: 300; loss: 1.97; acc: 0.38
Batch: 320; loss: 1.94; acc: 0.42
Batch: 340; loss: 2.0; acc: 0.3
Batch: 360; loss: 1.96; acc: 0.31
Batch: 380; loss: 1.86; acc: 0.41
Batch: 400; loss: 1.72; acc: 0.48
Batch: 420; loss: 1.69; acc: 0.41
Batch: 440; loss: 1.71; acc: 0.38
Batch: 460; loss: 1.71; acc: 0.39
Batch: 480; loss: 1.52; acc: 0.52
Batch: 500; loss: 1.61; acc: 0.33
Batch: 520; loss: 1.51; acc: 0.45
Batch: 540; loss: 1.41; acc: 0.5
Batch: 560; loss: 1.36; acc: 0.55
Batch: 580; loss: 1.18; acc: 0.66
Batch: 600; loss: 1.23; acc: 0.56
Batch: 620; loss: 1.3; acc: 0.58
Batch: 640; loss: 1.37; acc: 0.55
Batch: 660; loss: 1.19; acc: 0.56
Batch: 680; loss: 1.31; acc: 0.55
Batch: 700; loss: 1.02; acc: 0.66
Batch: 720; loss: 1.28; acc: 0.61
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.16; acc: 0.59
Batch: 780; loss: 1.23; acc: 0.61
Train Epoch over. train_loss: 1.74; train_accuracy: 0.41 

Batch: 0; loss: 1.37; acc: 0.56
Batch: 20; loss: 1.26; acc: 0.56
Batch: 40; loss: 0.79; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.58
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 1.41; acc: 0.56
Batch: 140; loss: 1.32; acc: 0.59
Val Epoch over. val_loss: 1.1836435491112387; val_accuracy: 0.6086783439490446 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.59
Batch: 20; loss: 1.06; acc: 0.62
Batch: 40; loss: 1.28; acc: 0.59
Batch: 60; loss: 1.33; acc: 0.61
Batch: 80; loss: 1.35; acc: 0.55
Batch: 100; loss: 0.99; acc: 0.66
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.25; acc: 0.61
Batch: 160; loss: 1.11; acc: 0.58
Batch: 180; loss: 1.21; acc: 0.56
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.0; acc: 0.69
Batch: 240; loss: 1.13; acc: 0.67
Batch: 260; loss: 1.27; acc: 0.66
Batch: 280; loss: 1.15; acc: 0.61
Batch: 300; loss: 0.97; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.45; acc: 0.56
Batch: 360; loss: 1.18; acc: 0.61
Batch: 380; loss: 1.2; acc: 0.62
Batch: 400; loss: 1.08; acc: 0.61
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.06; acc: 0.62
Batch: 460; loss: 1.16; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.56
Batch: 500; loss: 1.03; acc: 0.66
Batch: 520; loss: 0.9; acc: 0.64
Batch: 540; loss: 0.85; acc: 0.69
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 0.94; acc: 0.75
Batch: 600; loss: 1.16; acc: 0.61
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 1.12; acc: 0.56
Batch: 660; loss: 1.22; acc: 0.56
Batch: 680; loss: 1.06; acc: 0.69
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.99; acc: 0.64
Batch: 740; loss: 1.24; acc: 0.59
Batch: 760; loss: 1.23; acc: 0.58
Batch: 780; loss: 1.07; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.64 

Batch: 0; loss: 1.18; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 1.38; acc: 0.53
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 0.95; acc: 0.66
Val Epoch over. val_loss: 1.1812668150397623; val_accuracy: 0.6077826433121019 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 0.85; acc: 0.75
Batch: 120; loss: 1.05; acc: 0.64
Batch: 140; loss: 0.91; acc: 0.75
Batch: 160; loss: 0.93; acc: 0.7
Batch: 180; loss: 0.89; acc: 0.72
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.04; acc: 0.67
Batch: 240; loss: 1.13; acc: 0.62
Batch: 260; loss: 0.95; acc: 0.72
Batch: 280; loss: 1.38; acc: 0.58
Batch: 300; loss: 1.07; acc: 0.64
Batch: 320; loss: 1.13; acc: 0.66
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 0.97; acc: 0.67
Batch: 380; loss: 1.33; acc: 0.55
Batch: 400; loss: 0.96; acc: 0.67
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.0; acc: 0.64
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.08; acc: 0.61
Batch: 500; loss: 1.13; acc: 0.62
Batch: 520; loss: 1.04; acc: 0.75
Batch: 540; loss: 1.28; acc: 0.66
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 0.93; acc: 0.73
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 1.11; acc: 0.62
Batch: 640; loss: 1.2; acc: 0.59
Batch: 660; loss: 1.31; acc: 0.62
Batch: 680; loss: 1.08; acc: 0.7
Batch: 700; loss: 1.01; acc: 0.61
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 1.1; acc: 0.67
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.72
Train Epoch over. train_loss: 1.06; train_accuracy: 0.66 

Batch: 0; loss: 1.17; acc: 0.61
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 0.75; acc: 0.73
Batch: 100; loss: 1.11; acc: 0.62
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.07; acc: 0.62
Val Epoch over. val_loss: 1.0815441555278316; val_accuracy: 0.63953025477707 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 0.87; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.53
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 0.94; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.72
Batch: 180; loss: 1.02; acc: 0.69
Batch: 200; loss: 0.87; acc: 0.67
Batch: 220; loss: 1.08; acc: 0.72
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.59
Batch: 300; loss: 0.99; acc: 0.62
Batch: 320; loss: 0.94; acc: 0.67
Batch: 340; loss: 0.82; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.61
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 1.07; acc: 0.69
Batch: 440; loss: 1.07; acc: 0.61
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.99; acc: 0.72
Batch: 500; loss: 1.2; acc: 0.58
Batch: 520; loss: 0.87; acc: 0.73
Batch: 540; loss: 1.28; acc: 0.64
Batch: 560; loss: 1.01; acc: 0.67
Batch: 580; loss: 1.15; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 0.72; acc: 0.77
Batch: 640; loss: 0.98; acc: 0.64
Batch: 660; loss: 1.34; acc: 0.52
Batch: 680; loss: 0.88; acc: 0.69
Batch: 700; loss: 0.96; acc: 0.69
Batch: 720; loss: 1.49; acc: 0.58
Batch: 740; loss: 0.87; acc: 0.77
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 1.1; acc: 0.61
Train Epoch over. train_loss: 1.02; train_accuracy: 0.67 

Batch: 0; loss: 1.19; acc: 0.53
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.63; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.77; acc: 0.69
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 1.33; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.69
Val Epoch over. val_loss: 1.0492465830152962; val_accuracy: 0.6472929936305732 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.93; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 0.83; acc: 0.73
Batch: 160; loss: 1.08; acc: 0.58
Batch: 180; loss: 1.28; acc: 0.69
Batch: 200; loss: 0.75; acc: 0.75
Batch: 220; loss: 0.89; acc: 0.72
Batch: 240; loss: 1.1; acc: 0.61
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 0.9; acc: 0.66
Batch: 300; loss: 1.12; acc: 0.61
Batch: 320; loss: 0.94; acc: 0.73
Batch: 340; loss: 1.08; acc: 0.61
Batch: 360; loss: 1.23; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.56
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.14; acc: 0.61
Batch: 440; loss: 1.04; acc: 0.59
Batch: 460; loss: 0.95; acc: 0.72
Batch: 480; loss: 1.13; acc: 0.64
Batch: 500; loss: 1.21; acc: 0.67
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 1.55; acc: 0.56
Batch: 560; loss: 1.16; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.64
Batch: 600; loss: 1.08; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.99; acc: 0.66
Batch: 680; loss: 1.15; acc: 0.62
Batch: 700; loss: 1.17; acc: 0.61
Batch: 720; loss: 1.04; acc: 0.64
Batch: 740; loss: 1.12; acc: 0.62
Batch: 760; loss: 1.07; acc: 0.66
Batch: 780; loss: 0.94; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.68 

Batch: 0; loss: 1.14; acc: 0.55
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.55; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 0.93; acc: 0.62
Val Epoch over. val_loss: 0.9825962170673783; val_accuracy: 0.6651074840764332 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 1.09; acc: 0.59
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 1.3; acc: 0.61
Batch: 100; loss: 0.77; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.85; acc: 0.73
Batch: 160; loss: 1.11; acc: 0.67
Batch: 180; loss: 1.34; acc: 0.61
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 1.02; acc: 0.66
Batch: 260; loss: 0.97; acc: 0.66
Batch: 280; loss: 0.99; acc: 0.67
Batch: 300; loss: 0.84; acc: 0.73
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 1.24; acc: 0.56
Batch: 380; loss: 1.02; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.67
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.83; acc: 0.7
Batch: 480; loss: 0.9; acc: 0.7
Batch: 500; loss: 1.04; acc: 0.67
Batch: 520; loss: 0.8; acc: 0.7
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.01; acc: 0.69
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.64
Batch: 640; loss: 1.04; acc: 0.67
Batch: 660; loss: 1.23; acc: 0.64
Batch: 680; loss: 0.87; acc: 0.7
Batch: 700; loss: 0.87; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.62
Batch: 740; loss: 1.13; acc: 0.64
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.01; train_accuracy: 0.67 

Batch: 0; loss: 1.08; acc: 0.58
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.95; acc: 0.66
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.79; acc: 0.66
Val Epoch over. val_loss: 0.917769137461474; val_accuracy: 0.6954617834394905 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.64
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.7
Batch: 160; loss: 0.89; acc: 0.72
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.94; acc: 0.67
Batch: 220; loss: 1.24; acc: 0.62
Batch: 240; loss: 0.85; acc: 0.69
Batch: 260; loss: 1.13; acc: 0.61
Batch: 280; loss: 0.84; acc: 0.66
Batch: 300; loss: 0.89; acc: 0.72
Batch: 320; loss: 1.04; acc: 0.61
Batch: 340; loss: 0.95; acc: 0.69
Batch: 360; loss: 1.0; acc: 0.62
Batch: 380; loss: 0.93; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 0.84; acc: 0.78
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 1.27; acc: 0.61
Batch: 480; loss: 0.9; acc: 0.67
Batch: 500; loss: 1.29; acc: 0.61
Batch: 520; loss: 1.23; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.69
Batch: 560; loss: 0.91; acc: 0.59
Batch: 580; loss: 1.02; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.67
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.93; acc: 0.64
Batch: 660; loss: 1.16; acc: 0.64
Batch: 680; loss: 0.96; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.64
Batch: 720; loss: 0.94; acc: 0.64
Batch: 740; loss: 1.02; acc: 0.66
Batch: 760; loss: 0.94; acc: 0.67
Batch: 780; loss: 0.84; acc: 0.78
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.05; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.65; acc: 0.69
Batch: 60; loss: 1.14; acc: 0.59
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.7
Batch: 120; loss: 1.49; acc: 0.59
Batch: 140; loss: 0.69; acc: 0.78
Val Epoch over. val_loss: 0.9739614785856502; val_accuracy: 0.6812300955414012 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 1.04; acc: 0.64
Batch: 60; loss: 0.98; acc: 0.66
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.05; acc: 0.61
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.96; acc: 0.61
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.81; acc: 0.73
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.87; acc: 0.66
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.94; acc: 0.61
Batch: 340; loss: 0.98; acc: 0.69
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 1.09; acc: 0.67
Batch: 420; loss: 0.92; acc: 0.78
Batch: 440; loss: 1.21; acc: 0.56
Batch: 460; loss: 0.95; acc: 0.72
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 0.98; acc: 0.62
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.61
Batch: 560; loss: 1.02; acc: 0.69
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 1.02; acc: 0.66
Batch: 620; loss: 0.94; acc: 0.73
Batch: 640; loss: 0.96; acc: 0.73
Batch: 660; loss: 0.8; acc: 0.7
Batch: 680; loss: 0.98; acc: 0.73
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 1.14; acc: 0.67
Batch: 740; loss: 0.87; acc: 0.73
Batch: 760; loss: 0.79; acc: 0.67
Batch: 780; loss: 1.13; acc: 0.64
Train Epoch over. train_loss: 0.98; train_accuracy: 0.69 

Batch: 0; loss: 1.02; acc: 0.67
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.78
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.9043662644875278; val_accuracy: 0.7062101910828026 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 1.08; acc: 0.66
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.58
Batch: 100; loss: 1.1; acc: 0.66
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 1.27; acc: 0.56
Batch: 160; loss: 1.04; acc: 0.67
Batch: 180; loss: 0.98; acc: 0.59
Batch: 200; loss: 0.85; acc: 0.7
Batch: 220; loss: 0.83; acc: 0.72
Batch: 240; loss: 1.23; acc: 0.62
Batch: 260; loss: 1.02; acc: 0.64
Batch: 280; loss: 0.91; acc: 0.67
Batch: 300; loss: 0.9; acc: 0.67
Batch: 320; loss: 1.29; acc: 0.59
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.21; acc: 0.64
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.81; acc: 0.69
Batch: 440; loss: 1.08; acc: 0.61
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 1.14; acc: 0.62
Batch: 500; loss: 0.75; acc: 0.75
Batch: 520; loss: 0.93; acc: 0.7
Batch: 540; loss: 0.92; acc: 0.72
Batch: 560; loss: 0.83; acc: 0.64
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 0.95; acc: 0.66
Batch: 620; loss: 1.07; acc: 0.61
Batch: 640; loss: 1.07; acc: 0.66
Batch: 660; loss: 0.82; acc: 0.73
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.01; acc: 0.66
Batch: 720; loss: 0.96; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.62
Batch: 760; loss: 1.1; acc: 0.69
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.97; train_accuracy: 0.69 

Batch: 0; loss: 1.19; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 1.05; acc: 0.66
Batch: 80; loss: 0.62; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.69
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.75; acc: 0.72
Val Epoch over. val_loss: 0.971741210123536; val_accuracy: 0.6845143312101911 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.36; acc: 0.56
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.62
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.66
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 1.02; acc: 0.61
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.84; acc: 0.73
Batch: 240; loss: 0.95; acc: 0.69
Batch: 260; loss: 1.17; acc: 0.62
Batch: 280; loss: 0.83; acc: 0.7
Batch: 300; loss: 1.16; acc: 0.66
Batch: 320; loss: 1.03; acc: 0.69
Batch: 340; loss: 1.05; acc: 0.61
Batch: 360; loss: 0.74; acc: 0.72
Batch: 380; loss: 0.93; acc: 0.69
Batch: 400; loss: 0.84; acc: 0.75
Batch: 420; loss: 1.35; acc: 0.52
Batch: 440; loss: 0.91; acc: 0.7
Batch: 460; loss: 0.91; acc: 0.72
Batch: 480; loss: 0.8; acc: 0.75
Batch: 500; loss: 0.84; acc: 0.75
Batch: 520; loss: 1.03; acc: 0.72
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 1.09; acc: 0.67
Batch: 580; loss: 1.03; acc: 0.66
Batch: 600; loss: 1.06; acc: 0.67
Batch: 620; loss: 0.96; acc: 0.66
Batch: 640; loss: 0.84; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.72
Batch: 680; loss: 1.09; acc: 0.67
Batch: 700; loss: 1.21; acc: 0.61
Batch: 720; loss: 1.02; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.61
Batch: 760; loss: 1.19; acc: 0.62
Batch: 780; loss: 1.0; acc: 0.61
Train Epoch over. train_loss: 0.95; train_accuracy: 0.69 

Batch: 0; loss: 1.1; acc: 0.56
Batch: 20; loss: 1.11; acc: 0.7
Batch: 40; loss: 0.55; acc: 0.77
Batch: 60; loss: 1.12; acc: 0.66
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.96; acc: 0.72
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 0.7; acc: 0.73
Val Epoch over. val_loss: 0.9284447215165302; val_accuracy: 0.6946656050955414 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.64
Batch: 60; loss: 0.95; acc: 0.66
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 1.11; acc: 0.58
Batch: 120; loss: 0.89; acc: 0.67
Batch: 140; loss: 0.83; acc: 0.66
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 0.94; acc: 0.67
Batch: 200; loss: 0.79; acc: 0.7
Batch: 220; loss: 1.13; acc: 0.64
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.94; acc: 0.62
Batch: 280; loss: 1.06; acc: 0.7
Batch: 300; loss: 0.87; acc: 0.81
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 1.05; acc: 0.69
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 0.77; acc: 0.73
Batch: 420; loss: 1.2; acc: 0.69
Batch: 440; loss: 0.81; acc: 0.72
Batch: 460; loss: 0.92; acc: 0.72
Batch: 480; loss: 0.97; acc: 0.7
Batch: 500; loss: 1.24; acc: 0.61
Batch: 520; loss: 0.88; acc: 0.7
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 1.28; acc: 0.59
Batch: 580; loss: 0.95; acc: 0.69
Batch: 600; loss: 0.87; acc: 0.7
Batch: 620; loss: 1.06; acc: 0.66
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.8; acc: 0.67
Batch: 680; loss: 0.82; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 1.1; acc: 0.69
Batch: 780; loss: 1.02; acc: 0.69
Train Epoch over. train_loss: 0.95; train_accuracy: 0.69 

Batch: 0; loss: 1.1; acc: 0.59
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 0.63; acc: 0.86
Val Epoch over. val_loss: 0.9046587163855315; val_accuracy: 0.706906847133758 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.83; acc: 0.67
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.58
Batch: 100; loss: 0.83; acc: 0.7
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.84; acc: 0.73
Batch: 180; loss: 1.1; acc: 0.7
Batch: 200; loss: 0.8; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.99; acc: 0.69
Batch: 260; loss: 0.71; acc: 0.77
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.87; acc: 0.77
Batch: 340; loss: 1.08; acc: 0.62
Batch: 360; loss: 0.95; acc: 0.7
Batch: 380; loss: 0.79; acc: 0.67
Batch: 400; loss: 1.01; acc: 0.7
Batch: 420; loss: 0.75; acc: 0.72
Batch: 440; loss: 0.96; acc: 0.66
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 1.38; acc: 0.64
Batch: 500; loss: 0.97; acc: 0.66
Batch: 520; loss: 0.98; acc: 0.66
Batch: 540; loss: 1.23; acc: 0.62
Batch: 560; loss: 1.01; acc: 0.66
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.77
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.67
Batch: 700; loss: 0.78; acc: 0.75
Batch: 720; loss: 1.03; acc: 0.69
Batch: 740; loss: 0.96; acc: 0.7
Batch: 760; loss: 1.22; acc: 0.58
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 0.95; train_accuracy: 0.7 

Batch: 0; loss: 1.05; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.98; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.72
Batch: 120; loss: 1.31; acc: 0.66
Batch: 140; loss: 0.65; acc: 0.81
Val Epoch over. val_loss: 0.8882330265014794; val_accuracy: 0.7113853503184714 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 1.16; acc: 0.62
Batch: 80; loss: 1.11; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 0.78; acc: 0.69
Batch: 140; loss: 1.01; acc: 0.64
Batch: 160; loss: 0.95; acc: 0.67
Batch: 180; loss: 1.05; acc: 0.67
Batch: 200; loss: 1.08; acc: 0.58
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.98; acc: 0.64
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.96; acc: 0.7
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.8; acc: 0.78
Batch: 340; loss: 0.85; acc: 0.69
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.92; acc: 0.67
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.97; acc: 0.64
Batch: 440; loss: 1.04; acc: 0.69
Batch: 460; loss: 0.93; acc: 0.67
Batch: 480; loss: 1.0; acc: 0.67
Batch: 500; loss: 0.94; acc: 0.67
Batch: 520; loss: 1.09; acc: 0.59
Batch: 540; loss: 0.86; acc: 0.69
Batch: 560; loss: 1.0; acc: 0.64
Batch: 580; loss: 0.92; acc: 0.67
Batch: 600; loss: 1.06; acc: 0.73
Batch: 620; loss: 0.88; acc: 0.69
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 1.03; acc: 0.67
Batch: 700; loss: 1.15; acc: 0.61
Batch: 720; loss: 0.98; acc: 0.67
Batch: 740; loss: 0.83; acc: 0.72
Batch: 760; loss: 0.96; acc: 0.67
Batch: 780; loss: 0.91; acc: 0.7
Train Epoch over. train_loss: 0.95; train_accuracy: 0.7 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.69
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.84; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.7; acc: 0.78
Val Epoch over. val_loss: 0.8910969148395927; val_accuracy: 0.7099920382165605 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.0; acc: 0.7
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.96; acc: 0.73
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.78
Batch: 160; loss: 1.13; acc: 0.62
Batch: 180; loss: 0.92; acc: 0.7
Batch: 200; loss: 0.88; acc: 0.7
Batch: 220; loss: 1.06; acc: 0.61
Batch: 240; loss: 0.97; acc: 0.7
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 1.05; acc: 0.61
Batch: 300; loss: 0.93; acc: 0.7
Batch: 320; loss: 0.91; acc: 0.7
Batch: 340; loss: 0.83; acc: 0.75
Batch: 360; loss: 0.93; acc: 0.67
Batch: 380; loss: 1.02; acc: 0.7
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 0.72; acc: 0.73
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.95; acc: 0.7
Batch: 500; loss: 1.01; acc: 0.72
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 0.8; acc: 0.72
Batch: 560; loss: 0.82; acc: 0.77
Batch: 580; loss: 0.98; acc: 0.73
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 0.94; acc: 0.78
Batch: 640; loss: 1.07; acc: 0.69
Batch: 660; loss: 0.78; acc: 0.67
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 1.02; acc: 0.66
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.81; acc: 0.75
Batch: 760; loss: 0.9; acc: 0.66
Batch: 780; loss: 0.88; acc: 0.69
Train Epoch over. train_loss: 0.95; train_accuracy: 0.69 

Batch: 0; loss: 1.1; acc: 0.59
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 1.36; acc: 0.64
Batch: 140; loss: 0.69; acc: 0.78
Val Epoch over. val_loss: 0.8823738716969824; val_accuracy: 0.7116839171974523 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.66
Batch: 80; loss: 0.96; acc: 0.73
Batch: 100; loss: 0.72; acc: 0.75
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.61
Batch: 160; loss: 0.76; acc: 0.75
Batch: 180; loss: 0.95; acc: 0.67
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.92; acc: 0.77
Batch: 240; loss: 1.0; acc: 0.67
Batch: 260; loss: 0.84; acc: 0.75
Batch: 280; loss: 0.95; acc: 0.66
Batch: 300; loss: 1.06; acc: 0.69
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 1.21; acc: 0.56
Batch: 360; loss: 0.7; acc: 0.77
Batch: 380; loss: 0.81; acc: 0.7
Batch: 400; loss: 0.95; acc: 0.64
Batch: 420; loss: 0.7; acc: 0.75
Batch: 440; loss: 0.89; acc: 0.69
Batch: 460; loss: 0.98; acc: 0.64
Batch: 480; loss: 1.07; acc: 0.62
Batch: 500; loss: 0.97; acc: 0.69
Batch: 520; loss: 0.92; acc: 0.67
Batch: 540; loss: 0.9; acc: 0.7
Batch: 560; loss: 0.98; acc: 0.66
Batch: 580; loss: 1.34; acc: 0.53
Batch: 600; loss: 1.07; acc: 0.75
Batch: 620; loss: 1.04; acc: 0.69
Batch: 640; loss: 1.25; acc: 0.59
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 0.97; acc: 0.72
Batch: 700; loss: 0.81; acc: 0.73
Batch: 720; loss: 1.35; acc: 0.58
Batch: 740; loss: 1.01; acc: 0.66
Batch: 760; loss: 0.99; acc: 0.69
Batch: 780; loss: 0.87; acc: 0.8
Train Epoch over. train_loss: 0.94; train_accuracy: 0.69 

Batch: 0; loss: 1.09; acc: 0.61
Batch: 20; loss: 1.1; acc: 0.69
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.04; acc: 0.67
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 0.72; acc: 0.73
Val Epoch over. val_loss: 0.8792613179061064; val_accuracy: 0.7118829617834395 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 1.0; acc: 0.67
Batch: 60; loss: 0.95; acc: 0.67
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.58
Batch: 160; loss: 1.15; acc: 0.61
Batch: 180; loss: 1.02; acc: 0.7
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.99; acc: 0.73
Batch: 240; loss: 1.05; acc: 0.69
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 1.0; acc: 0.66
Batch: 300; loss: 0.76; acc: 0.72
Batch: 320; loss: 0.93; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.62
Batch: 360; loss: 0.67; acc: 0.77
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 1.01; acc: 0.69
Batch: 420; loss: 1.02; acc: 0.7
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 0.9; acc: 0.69
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 0.9; acc: 0.72
Batch: 540; loss: 0.86; acc: 0.72
Batch: 560; loss: 0.73; acc: 0.73
Batch: 580; loss: 1.27; acc: 0.62
Batch: 600; loss: 0.99; acc: 0.67
Batch: 620; loss: 1.14; acc: 0.61
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.93; acc: 0.62
Batch: 680; loss: 1.13; acc: 0.66
Batch: 700; loss: 0.92; acc: 0.66
Batch: 720; loss: 0.8; acc: 0.75
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.97; acc: 0.69
Batch: 780; loss: 1.14; acc: 0.59
Train Epoch over. train_loss: 0.94; train_accuracy: 0.69 

Batch: 0; loss: 1.07; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 0.65; acc: 0.75
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 0.71; acc: 0.73
Val Epoch over. val_loss: 0.8864036211921911; val_accuracy: 0.7086982484076433 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.61
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.89; acc: 0.67
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 1.07; acc: 0.61
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 1.05; acc: 0.62
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 0.86; acc: 0.77
Batch: 200; loss: 0.84; acc: 0.69
Batch: 220; loss: 0.77; acc: 0.72
Batch: 240; loss: 0.86; acc: 0.73
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.83; acc: 0.67
Batch: 300; loss: 0.89; acc: 0.69
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 0.84; acc: 0.66
Batch: 360; loss: 0.77; acc: 0.72
Batch: 380; loss: 1.06; acc: 0.61
Batch: 400; loss: 0.79; acc: 0.69
Batch: 420; loss: 1.11; acc: 0.64
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 1.36; acc: 0.53
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 1.38; acc: 0.66
Batch: 520; loss: 0.83; acc: 0.7
Batch: 540; loss: 1.05; acc: 0.66
Batch: 560; loss: 1.18; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.58
Batch: 600; loss: 1.17; acc: 0.58
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.7
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.93; acc: 0.7
Batch: 700; loss: 0.85; acc: 0.72
Batch: 720; loss: 1.14; acc: 0.62
Batch: 740; loss: 1.56; acc: 0.64
Batch: 760; loss: 1.01; acc: 0.64
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 0.94; train_accuracy: 0.69 

Batch: 0; loss: 1.04; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.99; acc: 0.69
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.73; acc: 0.75
Val Epoch over. val_loss: 0.8819333555971741; val_accuracy: 0.7099920382165605 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.88; acc: 0.7
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.93; acc: 0.69
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.62
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 1.13; acc: 0.64
Batch: 160; loss: 0.78; acc: 0.75
Batch: 180; loss: 1.11; acc: 0.66
Batch: 200; loss: 1.0; acc: 0.73
Batch: 220; loss: 1.13; acc: 0.55
Batch: 240; loss: 1.0; acc: 0.64
Batch: 260; loss: 0.86; acc: 0.7
Batch: 280; loss: 0.82; acc: 0.75
Batch: 300; loss: 0.96; acc: 0.7
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 1.08; acc: 0.75
Batch: 360; loss: 0.88; acc: 0.66
Batch: 380; loss: 1.2; acc: 0.7
Batch: 400; loss: 0.89; acc: 0.73
Batch: 420; loss: 0.83; acc: 0.77
Batch: 440; loss: 1.05; acc: 0.59
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 1.04; acc: 0.61
Batch: 540; loss: 0.92; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 0.91; acc: 0.73
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 1.03; acc: 0.64
Batch: 640; loss: 1.02; acc: 0.69
Batch: 660; loss: 1.03; acc: 0.66
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.91; acc: 0.69
Batch: 720; loss: 0.8; acc: 0.7
Batch: 740; loss: 0.86; acc: 0.75
Batch: 760; loss: 1.21; acc: 0.62
Batch: 780; loss: 1.23; acc: 0.55
Train Epoch over. train_loss: 0.94; train_accuracy: 0.69 

Batch: 0; loss: 1.03; acc: 0.66
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 0.51; acc: 0.78
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 0.8779781500625002; val_accuracy: 0.7108877388535032 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 1.06; acc: 0.69
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 1.17; acc: 0.56
Batch: 100; loss: 1.02; acc: 0.64
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 1.15; acc: 0.62
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 0.91; acc: 0.69
Batch: 200; loss: 0.82; acc: 0.78
Batch: 220; loss: 1.21; acc: 0.58
Batch: 240; loss: 0.85; acc: 0.7
Batch: 260; loss: 1.12; acc: 0.59
Batch: 280; loss: 1.06; acc: 0.62
Batch: 300; loss: 1.12; acc: 0.62
Batch: 320; loss: 0.75; acc: 0.8
Batch: 340; loss: 0.82; acc: 0.72
Batch: 360; loss: 0.74; acc: 0.75
Batch: 380; loss: 0.86; acc: 0.7
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 0.89; acc: 0.69
Batch: 440; loss: 0.87; acc: 0.64
Batch: 460; loss: 1.06; acc: 0.66
Batch: 480; loss: 1.12; acc: 0.67
Batch: 500; loss: 1.36; acc: 0.59
Batch: 520; loss: 0.98; acc: 0.69
Batch: 540; loss: 0.99; acc: 0.72
Batch: 560; loss: 0.97; acc: 0.67
Batch: 580; loss: 0.95; acc: 0.7
Batch: 600; loss: 0.84; acc: 0.73
Batch: 620; loss: 0.95; acc: 0.66
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.91; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.72
Batch: 700; loss: 0.96; acc: 0.67
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.95; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 0.94; train_accuracy: 0.69 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.66
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.66
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.76; acc: 0.7
Val Epoch over. val_loss: 0.8789685526091582; val_accuracy: 0.7170581210191083 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.71; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.87; acc: 0.7
Batch: 60; loss: 1.03; acc: 0.61
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.86; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.59
Batch: 140; loss: 0.9; acc: 0.62
Batch: 160; loss: 0.83; acc: 0.64
Batch: 180; loss: 0.76; acc: 0.73
Batch: 200; loss: 0.88; acc: 0.69
Batch: 220; loss: 0.9; acc: 0.75
Batch: 240; loss: 1.12; acc: 0.64
Batch: 260; loss: 0.99; acc: 0.75
Batch: 280; loss: 1.0; acc: 0.66
Batch: 300; loss: 0.86; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.66
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 0.79; acc: 0.72
Batch: 380; loss: 0.87; acc: 0.7
Batch: 400; loss: 0.96; acc: 0.66
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.62
Batch: 480; loss: 1.13; acc: 0.66
Batch: 500; loss: 0.76; acc: 0.73
Batch: 520; loss: 0.81; acc: 0.72
Batch: 540; loss: 0.77; acc: 0.73
Batch: 560; loss: 0.8; acc: 0.75
Batch: 580; loss: 1.04; acc: 0.66
Batch: 600; loss: 0.93; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.59
Batch: 640; loss: 1.07; acc: 0.7
Batch: 660; loss: 0.69; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 0.86; acc: 0.69
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 1.15; acc: 0.58
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 1.31; acc: 0.58
Train Epoch over. train_loss: 0.93; train_accuracy: 0.69 

Batch: 0; loss: 1.02; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.75; acc: 0.72
Val Epoch over. val_loss: 0.8783470194810515; val_accuracy: 0.7137738853503185 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.97; acc: 0.62
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.61
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.69
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 1.47; acc: 0.58
Batch: 160; loss: 0.97; acc: 0.69
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.83; acc: 0.69
Batch: 220; loss: 1.09; acc: 0.67
Batch: 240; loss: 1.03; acc: 0.66
Batch: 260; loss: 0.83; acc: 0.72
Batch: 280; loss: 0.83; acc: 0.73
Batch: 300; loss: 0.99; acc: 0.69
Batch: 320; loss: 0.93; acc: 0.64
Batch: 340; loss: 1.02; acc: 0.7
Batch: 360; loss: 1.08; acc: 0.67
Batch: 380; loss: 0.81; acc: 0.66
Batch: 400; loss: 0.8; acc: 0.77
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.76; acc: 0.72
Batch: 460; loss: 1.27; acc: 0.64
Batch: 480; loss: 1.0; acc: 0.72
Batch: 500; loss: 0.87; acc: 0.67
Batch: 520; loss: 1.18; acc: 0.58
Batch: 540; loss: 1.02; acc: 0.75
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.73
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.69
Batch: 660; loss: 1.26; acc: 0.55
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.94; acc: 0.64
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 1.02; acc: 0.7
Batch: 760; loss: 0.9; acc: 0.69
Batch: 780; loss: 1.01; acc: 0.73
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.69
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.26; acc: 0.58
Batch: 140; loss: 0.76; acc: 0.7
Val Epoch over. val_loss: 0.8771512962071; val_accuracy: 0.714171974522293 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 0.83; acc: 0.7
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.97; acc: 0.62
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.7
Batch: 160; loss: 0.9; acc: 0.73
Batch: 180; loss: 0.91; acc: 0.73
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 0.73; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.73
Batch: 260; loss: 1.1; acc: 0.73
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 1.08; acc: 0.67
Batch: 320; loss: 0.91; acc: 0.73
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 0.85; acc: 0.67
Batch: 400; loss: 1.02; acc: 0.61
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.69
Batch: 460; loss: 1.01; acc: 0.75
Batch: 480; loss: 0.87; acc: 0.78
Batch: 500; loss: 0.98; acc: 0.69
Batch: 520; loss: 0.72; acc: 0.81
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.88; acc: 0.73
Batch: 600; loss: 0.65; acc: 0.77
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.89; acc: 0.7
Batch: 660; loss: 1.0; acc: 0.64
Batch: 680; loss: 0.75; acc: 0.72
Batch: 700; loss: 0.87; acc: 0.69
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 0.94; acc: 0.62
Batch: 760; loss: 1.05; acc: 0.67
Batch: 780; loss: 0.83; acc: 0.72
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.67
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 0.76; acc: 0.72
Val Epoch over. val_loss: 0.8758439886721836; val_accuracy: 0.7152667197452229 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 0.95; acc: 0.67
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.9; acc: 0.67
Batch: 160; loss: 0.95; acc: 0.64
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 0.98; acc: 0.58
Batch: 220; loss: 0.9; acc: 0.72
Batch: 240; loss: 1.04; acc: 0.59
Batch: 260; loss: 0.86; acc: 0.69
Batch: 280; loss: 1.15; acc: 0.61
Batch: 300; loss: 0.83; acc: 0.7
Batch: 320; loss: 0.96; acc: 0.69
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 0.94; acc: 0.73
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.74; acc: 0.7
Batch: 480; loss: 0.85; acc: 0.72
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 1.01; acc: 0.62
Batch: 540; loss: 1.05; acc: 0.62
Batch: 560; loss: 0.94; acc: 0.73
Batch: 580; loss: 1.0; acc: 0.73
Batch: 600; loss: 1.05; acc: 0.64
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 0.68; acc: 0.73
Batch: 660; loss: 0.58; acc: 0.77
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.8; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.7
Batch: 740; loss: 1.01; acc: 0.62
Batch: 760; loss: 0.78; acc: 0.73
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.74; acc: 0.69
Val Epoch over. val_loss: 0.8817991530819304; val_accuracy: 0.7133757961783439 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 0.86; acc: 0.73
Batch: 120; loss: 0.75; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.69
Batch: 160; loss: 1.13; acc: 0.61
Batch: 180; loss: 0.76; acc: 0.81
Batch: 200; loss: 1.01; acc: 0.69
Batch: 220; loss: 0.94; acc: 0.67
Batch: 240; loss: 1.45; acc: 0.62
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.96; acc: 0.69
Batch: 300; loss: 0.79; acc: 0.7
Batch: 320; loss: 0.98; acc: 0.66
Batch: 340; loss: 0.99; acc: 0.62
Batch: 360; loss: 0.87; acc: 0.73
Batch: 380; loss: 1.16; acc: 0.59
Batch: 400; loss: 1.13; acc: 0.66
Batch: 420; loss: 1.33; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.59
Batch: 460; loss: 0.92; acc: 0.7
Batch: 480; loss: 0.74; acc: 0.72
Batch: 500; loss: 1.1; acc: 0.62
Batch: 520; loss: 0.86; acc: 0.77
Batch: 540; loss: 0.88; acc: 0.62
Batch: 560; loss: 1.11; acc: 0.64
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 1.02; acc: 0.67
Batch: 620; loss: 0.93; acc: 0.62
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.89; acc: 0.73
Batch: 700; loss: 0.77; acc: 0.72
Batch: 720; loss: 0.97; acc: 0.69
Batch: 740; loss: 1.06; acc: 0.69
Batch: 760; loss: 1.2; acc: 0.62
Batch: 780; loss: 0.77; acc: 0.72
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.61
Batch: 140; loss: 0.76; acc: 0.72
Val Epoch over. val_loss: 0.8834868787200587; val_accuracy: 0.7166600318471338 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.13; acc: 0.59
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.93; acc: 0.66
Batch: 80; loss: 0.88; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.69
Batch: 120; loss: 1.42; acc: 0.48
Batch: 140; loss: 0.87; acc: 0.69
Batch: 160; loss: 0.82; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.77
Batch: 200; loss: 1.08; acc: 0.64
Batch: 220; loss: 0.8; acc: 0.72
Batch: 240; loss: 0.93; acc: 0.7
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 1.0; acc: 0.66
Batch: 300; loss: 1.04; acc: 0.73
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 0.81; acc: 0.75
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.94; acc: 0.69
Batch: 420; loss: 1.01; acc: 0.72
Batch: 440; loss: 1.01; acc: 0.62
Batch: 460; loss: 0.88; acc: 0.67
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.96; acc: 0.66
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.86; acc: 0.72
Batch: 560; loss: 0.87; acc: 0.69
Batch: 580; loss: 0.71; acc: 0.73
Batch: 600; loss: 1.06; acc: 0.67
Batch: 620; loss: 0.74; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.7
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.74; acc: 0.77
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 1.06; acc: 0.75
Batch: 740; loss: 1.17; acc: 0.59
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 0.96; acc: 0.67
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 1.0; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 0.77; acc: 0.7
Val Epoch over. val_loss: 0.8829628705598747; val_accuracy: 0.714968152866242 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.72
Batch: 20; loss: 0.87; acc: 0.62
Batch: 40; loss: 0.87; acc: 0.67
Batch: 60; loss: 1.07; acc: 0.69
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.67
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.86; acc: 0.69
Batch: 160; loss: 1.05; acc: 0.66
Batch: 180; loss: 0.94; acc: 0.61
Batch: 200; loss: 1.07; acc: 0.58
Batch: 220; loss: 0.95; acc: 0.64
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.84; acc: 0.69
Batch: 280; loss: 0.98; acc: 0.7
Batch: 300; loss: 0.96; acc: 0.69
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 0.86; acc: 0.72
Batch: 360; loss: 1.06; acc: 0.64
Batch: 380; loss: 0.98; acc: 0.72
Batch: 400; loss: 1.03; acc: 0.69
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.01; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.64
Batch: 480; loss: 0.68; acc: 0.77
Batch: 500; loss: 0.9; acc: 0.7
Batch: 520; loss: 1.17; acc: 0.69
Batch: 540; loss: 0.75; acc: 0.73
Batch: 560; loss: 0.71; acc: 0.72
Batch: 580; loss: 0.94; acc: 0.64
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.96; acc: 0.67
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.87; acc: 0.67
Batch: 700; loss: 0.72; acc: 0.73
Batch: 720; loss: 1.06; acc: 0.64
Batch: 740; loss: 0.78; acc: 0.84
Batch: 760; loss: 1.14; acc: 0.64
Batch: 780; loss: 0.88; acc: 0.77
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 0.76; acc: 0.72
Val Epoch over. val_loss: 0.8769823903111136; val_accuracy: 0.7131767515923567 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 0.96; acc: 0.7
Batch: 40; loss: 1.01; acc: 0.69
Batch: 60; loss: 1.06; acc: 0.64
Batch: 80; loss: 0.97; acc: 0.66
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 1.03; acc: 0.59
Batch: 160; loss: 0.85; acc: 0.75
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 1.12; acc: 0.64
Batch: 220; loss: 0.75; acc: 0.72
Batch: 240; loss: 0.6; acc: 0.75
Batch: 260; loss: 0.96; acc: 0.72
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 1.06; acc: 0.61
Batch: 320; loss: 1.28; acc: 0.58
Batch: 340; loss: 0.88; acc: 0.67
Batch: 360; loss: 0.77; acc: 0.75
Batch: 380; loss: 0.93; acc: 0.73
Batch: 400; loss: 0.94; acc: 0.67
Batch: 420; loss: 1.07; acc: 0.66
Batch: 440; loss: 1.03; acc: 0.73
Batch: 460; loss: 0.92; acc: 0.72
Batch: 480; loss: 1.08; acc: 0.61
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.83; acc: 0.72
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 1.15; acc: 0.59
Batch: 580; loss: 0.94; acc: 0.7
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.7
Batch: 640; loss: 0.94; acc: 0.73
Batch: 660; loss: 0.86; acc: 0.72
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 1.0; acc: 0.7
Batch: 720; loss: 0.83; acc: 0.73
Batch: 740; loss: 1.04; acc: 0.67
Batch: 760; loss: 0.97; acc: 0.75
Batch: 780; loss: 0.9; acc: 0.62
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.66
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.99; acc: 0.67
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 0.75; acc: 0.72
Val Epoch over. val_loss: 0.878202646211454; val_accuracy: 0.7121815286624203 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 0.98; acc: 0.66
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.67
Batch: 100; loss: 0.87; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 1.19; acc: 0.66
Batch: 160; loss: 0.93; acc: 0.67
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.96; acc: 0.64
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.67
Batch: 300; loss: 0.73; acc: 0.72
Batch: 320; loss: 1.06; acc: 0.67
Batch: 340; loss: 0.99; acc: 0.73
Batch: 360; loss: 0.95; acc: 0.77
Batch: 380; loss: 0.92; acc: 0.67
Batch: 400; loss: 0.71; acc: 0.72
Batch: 420; loss: 0.9; acc: 0.72
Batch: 440; loss: 0.94; acc: 0.67
Batch: 460; loss: 0.76; acc: 0.69
Batch: 480; loss: 0.81; acc: 0.72
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 0.92; acc: 0.7
Batch: 540; loss: 0.66; acc: 0.83
Batch: 560; loss: 0.95; acc: 0.73
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 1.1; acc: 0.58
Batch: 620; loss: 1.03; acc: 0.67
Batch: 640; loss: 0.92; acc: 0.67
Batch: 660; loss: 0.84; acc: 0.67
Batch: 680; loss: 0.94; acc: 0.66
Batch: 700; loss: 1.23; acc: 0.62
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 0.92; acc: 0.69
Batch: 760; loss: 0.99; acc: 0.72
Batch: 780; loss: 1.34; acc: 0.62
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 0.74; acc: 0.7
Val Epoch over. val_loss: 0.8769510654127521; val_accuracy: 0.7162619426751592 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 1.01; acc: 0.62
Batch: 60; loss: 0.96; acc: 0.72
Batch: 80; loss: 0.79; acc: 0.67
Batch: 100; loss: 0.98; acc: 0.67
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.83; acc: 0.7
Batch: 160; loss: 0.8; acc: 0.7
Batch: 180; loss: 0.85; acc: 0.66
Batch: 200; loss: 0.84; acc: 0.73
Batch: 220; loss: 0.97; acc: 0.73
Batch: 240; loss: 0.89; acc: 0.67
Batch: 260; loss: 0.92; acc: 0.66
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 0.93; acc: 0.67
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 0.85; acc: 0.72
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 1.03; acc: 0.62
Batch: 400; loss: 1.0; acc: 0.75
Batch: 420; loss: 1.04; acc: 0.64
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 0.95; acc: 0.73
Batch: 500; loss: 0.78; acc: 0.72
Batch: 520; loss: 0.75; acc: 0.75
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.93; acc: 0.64
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.7
Batch: 620; loss: 0.87; acc: 0.72
Batch: 640; loss: 1.43; acc: 0.64
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.78; acc: 0.72
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 1.06; acc: 0.62
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.98; acc: 0.69
Batch: 780; loss: 0.89; acc: 0.73
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 0.75; acc: 0.7
Val Epoch over. val_loss: 0.8809604443562259; val_accuracy: 0.7116839171974523 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.77; acc: 0.73
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.09; acc: 0.62
Batch: 140; loss: 1.06; acc: 0.66
Batch: 160; loss: 1.11; acc: 0.64
Batch: 180; loss: 0.81; acc: 0.73
Batch: 200; loss: 0.8; acc: 0.72
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 1.02; acc: 0.66
Batch: 280; loss: 1.08; acc: 0.61
Batch: 300; loss: 0.89; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.66
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 1.02; acc: 0.7
Batch: 380; loss: 1.06; acc: 0.62
Batch: 400; loss: 1.02; acc: 0.7
Batch: 420; loss: 0.93; acc: 0.73
Batch: 440; loss: 1.09; acc: 0.67
Batch: 460; loss: 0.99; acc: 0.69
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.96; acc: 0.7
Batch: 520; loss: 0.93; acc: 0.72
Batch: 540; loss: 0.94; acc: 0.66
Batch: 560; loss: 0.75; acc: 0.73
Batch: 580; loss: 1.04; acc: 0.64
Batch: 600; loss: 1.28; acc: 0.58
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 1.15; acc: 0.59
Batch: 660; loss: 0.95; acc: 0.62
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.81; acc: 0.69
Batch: 740; loss: 0.86; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.67
Batch: 780; loss: 1.18; acc: 0.61
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 0.74; acc: 0.7
Val Epoch over. val_loss: 0.8753630699245794; val_accuracy: 0.7172571656050956 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 0.72; acc: 0.75
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.61
Batch: 160; loss: 0.95; acc: 0.7
Batch: 180; loss: 0.92; acc: 0.72
Batch: 200; loss: 0.76; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.7
Batch: 240; loss: 0.94; acc: 0.62
Batch: 260; loss: 1.02; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.64
Batch: 300; loss: 0.95; acc: 0.72
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 1.02; acc: 0.64
Batch: 360; loss: 0.98; acc: 0.69
Batch: 380; loss: 1.1; acc: 0.67
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.62
Batch: 480; loss: 1.06; acc: 0.64
Batch: 500; loss: 0.74; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 1.11; acc: 0.58
Batch: 580; loss: 0.89; acc: 0.72
Batch: 600; loss: 0.96; acc: 0.69
Batch: 620; loss: 0.91; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.66
Batch: 660; loss: 1.11; acc: 0.62
Batch: 680; loss: 0.99; acc: 0.7
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.72
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.64
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.74; acc: 0.7
Val Epoch over. val_loss: 0.8762088269944404; val_accuracy: 0.716859076433121 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 1.22; acc: 0.73
Batch: 100; loss: 0.88; acc: 0.75
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 1.1; acc: 0.64
Batch: 160; loss: 0.92; acc: 0.67
Batch: 180; loss: 0.93; acc: 0.72
Batch: 200; loss: 1.23; acc: 0.66
Batch: 220; loss: 0.74; acc: 0.78
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 0.83; acc: 0.69
Batch: 280; loss: 0.94; acc: 0.72
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 1.02; acc: 0.66
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 1.01; acc: 0.62
Batch: 380; loss: 1.0; acc: 0.66
Batch: 400; loss: 1.14; acc: 0.59
Batch: 420; loss: 0.87; acc: 0.64
Batch: 440; loss: 1.07; acc: 0.62
Batch: 460; loss: 0.84; acc: 0.7
Batch: 480; loss: 0.8; acc: 0.72
Batch: 500; loss: 0.91; acc: 0.67
Batch: 520; loss: 1.03; acc: 0.72
Batch: 540; loss: 1.0; acc: 0.7
Batch: 560; loss: 1.1; acc: 0.69
Batch: 580; loss: 0.85; acc: 0.75
Batch: 600; loss: 0.88; acc: 0.67
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 0.98; acc: 0.7
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 0.86; acc: 0.7
Batch: 740; loss: 0.77; acc: 0.7
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.81; acc: 0.73
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.61
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.67
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 0.74; acc: 0.69
Val Epoch over. val_loss: 0.8754047504655874; val_accuracy: 0.714968152866242 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.97; acc: 0.62
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.9; acc: 0.69
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 1.18; acc: 0.56
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 1.46; acc: 0.41
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.9; acc: 0.67
Batch: 220; loss: 0.85; acc: 0.69
Batch: 240; loss: 1.11; acc: 0.69
Batch: 260; loss: 0.89; acc: 0.72
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.0; acc: 0.69
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.76; acc: 0.7
Batch: 360; loss: 0.88; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.64
Batch: 400; loss: 0.64; acc: 0.81
Batch: 420; loss: 0.88; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.67
Batch: 460; loss: 1.04; acc: 0.66
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 1.15; acc: 0.62
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.88; acc: 0.72
Batch: 580; loss: 0.99; acc: 0.77
Batch: 600; loss: 0.72; acc: 0.73
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.92; acc: 0.67
Batch: 660; loss: 1.13; acc: 0.59
Batch: 680; loss: 1.0; acc: 0.64
Batch: 700; loss: 0.79; acc: 0.7
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 0.93; acc: 0.64
Batch: 760; loss: 0.88; acc: 0.69
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 0.74; acc: 0.7
Val Epoch over. val_loss: 0.8760237891203279; val_accuracy: 0.7169585987261147 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 1.35; acc: 0.52
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.72
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.0; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 1.03; acc: 0.67
Batch: 200; loss: 1.11; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 0.77; acc: 0.73
Batch: 260; loss: 0.94; acc: 0.69
Batch: 280; loss: 0.79; acc: 0.77
Batch: 300; loss: 1.28; acc: 0.61
Batch: 320; loss: 0.97; acc: 0.69
Batch: 340; loss: 1.15; acc: 0.61
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.86; acc: 0.73
Batch: 400; loss: 1.05; acc: 0.62
Batch: 420; loss: 0.75; acc: 0.7
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 1.08; acc: 0.69
Batch: 500; loss: 0.92; acc: 0.72
Batch: 520; loss: 1.17; acc: 0.59
Batch: 540; loss: 1.08; acc: 0.66
Batch: 560; loss: 0.82; acc: 0.69
Batch: 580; loss: 0.87; acc: 0.66
Batch: 600; loss: 0.98; acc: 0.7
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 1.13; acc: 0.66
Batch: 680; loss: 1.11; acc: 0.64
Batch: 700; loss: 0.89; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.64
Batch: 740; loss: 1.15; acc: 0.67
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 0.75; acc: 0.67
Val Epoch over. val_loss: 0.875778424701873; val_accuracy: 0.7173566878980892 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.82; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 1.04; acc: 0.72
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.94; acc: 0.62
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.61
Batch: 160; loss: 1.07; acc: 0.64
Batch: 180; loss: 0.93; acc: 0.72
Batch: 200; loss: 0.73; acc: 0.73
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 1.16; acc: 0.66
Batch: 260; loss: 1.01; acc: 0.59
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.13; acc: 0.66
Batch: 320; loss: 0.82; acc: 0.78
Batch: 340; loss: 0.93; acc: 0.67
Batch: 360; loss: 0.88; acc: 0.67
Batch: 380; loss: 0.8; acc: 0.7
Batch: 400; loss: 0.95; acc: 0.73
Batch: 420; loss: 1.02; acc: 0.64
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.62
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 0.76; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.69
Batch: 540; loss: 0.99; acc: 0.67
Batch: 560; loss: 0.9; acc: 0.67
Batch: 580; loss: 1.08; acc: 0.59
Batch: 600; loss: 1.09; acc: 0.7
Batch: 620; loss: 0.9; acc: 0.75
Batch: 640; loss: 0.77; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.69
Batch: 680; loss: 0.83; acc: 0.73
Batch: 700; loss: 1.24; acc: 0.64
Batch: 720; loss: 0.75; acc: 0.77
Batch: 740; loss: 0.85; acc: 0.67
Batch: 760; loss: 0.84; acc: 0.67
Batch: 780; loss: 0.95; acc: 0.64
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.73
Val Epoch over. val_loss: 0.8753050461316564; val_accuracy: 0.7180533439490446 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.67
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.71; acc: 0.72
Batch: 60; loss: 0.95; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 1.02; acc: 0.62
Batch: 140; loss: 0.93; acc: 0.69
Batch: 160; loss: 0.97; acc: 0.66
Batch: 180; loss: 1.05; acc: 0.59
Batch: 200; loss: 0.98; acc: 0.7
Batch: 220; loss: 1.01; acc: 0.73
Batch: 240; loss: 0.84; acc: 0.7
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 1.16; acc: 0.56
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.94; acc: 0.67
Batch: 380; loss: 0.86; acc: 0.62
Batch: 400; loss: 1.13; acc: 0.62
Batch: 420; loss: 0.98; acc: 0.64
Batch: 440; loss: 0.92; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.55
Batch: 480; loss: 0.93; acc: 0.69
Batch: 500; loss: 0.79; acc: 0.72
Batch: 520; loss: 1.08; acc: 0.62
Batch: 540; loss: 0.89; acc: 0.69
Batch: 560; loss: 1.01; acc: 0.61
Batch: 580; loss: 0.91; acc: 0.69
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 1.02; acc: 0.66
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 0.88; acc: 0.69
Batch: 700; loss: 1.1; acc: 0.69
Batch: 720; loss: 0.91; acc: 0.72
Batch: 740; loss: 0.9; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.7
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 0.74; acc: 0.69
Val Epoch over. val_loss: 0.8753303598826099; val_accuracy: 0.7161624203821656 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.67
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.27; acc: 0.52
Batch: 80; loss: 0.76; acc: 0.7
Batch: 100; loss: 0.86; acc: 0.7
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.99; acc: 0.64
Batch: 160; loss: 1.05; acc: 0.67
Batch: 180; loss: 0.92; acc: 0.64
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.67
Batch: 240; loss: 0.89; acc: 0.75
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 1.18; acc: 0.69
Batch: 300; loss: 1.0; acc: 0.64
Batch: 320; loss: 0.98; acc: 0.62
Batch: 340; loss: 0.96; acc: 0.7
Batch: 360; loss: 0.79; acc: 0.81
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 1.0; acc: 0.7
Batch: 420; loss: 0.93; acc: 0.72
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.73
Batch: 480; loss: 1.01; acc: 0.61
Batch: 500; loss: 1.08; acc: 0.62
Batch: 520; loss: 0.91; acc: 0.75
Batch: 540; loss: 1.26; acc: 0.59
Batch: 560; loss: 0.97; acc: 0.64
Batch: 580; loss: 0.92; acc: 0.75
Batch: 600; loss: 0.93; acc: 0.64
Batch: 620; loss: 0.77; acc: 0.75
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.89; acc: 0.7
Batch: 700; loss: 0.89; acc: 0.72
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.93; acc: 0.69
Batch: 760; loss: 0.88; acc: 0.7
Batch: 780; loss: 0.89; acc: 0.7
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.86; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.67
Val Epoch over. val_loss: 0.877502008228545; val_accuracy: 0.7147691082802548 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.94; acc: 0.73
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.93; acc: 0.7
Batch: 100; loss: 1.02; acc: 0.62
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.67
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 0.73; acc: 0.75
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.24; acc: 0.56
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.88; acc: 0.69
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 1.1; acc: 0.62
Batch: 360; loss: 0.85; acc: 0.7
Batch: 380; loss: 0.83; acc: 0.7
Batch: 400; loss: 0.7; acc: 0.77
Batch: 420; loss: 0.96; acc: 0.7
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.86; acc: 0.78
Batch: 480; loss: 0.94; acc: 0.73
Batch: 500; loss: 0.87; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.62
Batch: 540; loss: 1.01; acc: 0.75
Batch: 560; loss: 1.36; acc: 0.62
Batch: 580; loss: 0.89; acc: 0.69
Batch: 600; loss: 1.07; acc: 0.58
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 0.68; acc: 0.72
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.98; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.73
Batch: 720; loss: 0.97; acc: 0.67
Batch: 740; loss: 0.96; acc: 0.69
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.96; acc: 0.67
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.73
Val Epoch over. val_loss: 0.8764773871108984; val_accuracy: 0.7161624203821656 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 0.78; acc: 0.72
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.72
Batch: 180; loss: 0.83; acc: 0.72
Batch: 200; loss: 0.82; acc: 0.72
Batch: 220; loss: 0.98; acc: 0.62
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 1.05; acc: 0.64
Batch: 280; loss: 1.04; acc: 0.69
Batch: 300; loss: 1.03; acc: 0.59
Batch: 320; loss: 0.83; acc: 0.72
Batch: 340; loss: 0.81; acc: 0.67
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 1.31; acc: 0.52
Batch: 440; loss: 1.26; acc: 0.62
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.83; acc: 0.69
Batch: 500; loss: 0.95; acc: 0.59
Batch: 520; loss: 0.83; acc: 0.72
Batch: 540; loss: 0.87; acc: 0.7
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 0.91; acc: 0.72
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.72
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 0.87; acc: 0.75
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.86; acc: 0.73
Batch: 740; loss: 0.8; acc: 0.75
Batch: 760; loss: 1.18; acc: 0.67
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.7
Val Epoch over. val_loss: 0.8751526025070506; val_accuracy: 0.7171576433121019 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.22; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.61
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.9; acc: 0.67
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.97; acc: 0.67
Batch: 200; loss: 0.85; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.64
Batch: 240; loss: 0.85; acc: 0.7
Batch: 260; loss: 0.94; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.7
Batch: 300; loss: 0.85; acc: 0.69
Batch: 320; loss: 0.89; acc: 0.73
Batch: 340; loss: 0.98; acc: 0.69
Batch: 360; loss: 1.0; acc: 0.66
Batch: 380; loss: 0.85; acc: 0.7
Batch: 400; loss: 1.15; acc: 0.62
Batch: 420; loss: 0.89; acc: 0.73
Batch: 440; loss: 0.9; acc: 0.69
Batch: 460; loss: 1.09; acc: 0.69
Batch: 480; loss: 0.85; acc: 0.69
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 1.2; acc: 0.61
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 1.06; acc: 0.72
Batch: 580; loss: 0.98; acc: 0.73
Batch: 600; loss: 1.0; acc: 0.73
Batch: 620; loss: 1.02; acc: 0.67
Batch: 640; loss: 1.02; acc: 0.72
Batch: 660; loss: 1.0; acc: 0.64
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 0.95; acc: 0.69
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 1.1; acc: 0.73
Batch: 760; loss: 1.14; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.69
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.7
Val Epoch over. val_loss: 0.875624855992141; val_accuracy: 0.7152667197452229 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.84; acc: 0.67
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.75
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.86; acc: 0.72
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 1.0; acc: 0.61
Batch: 240; loss: 1.0; acc: 0.7
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.86; acc: 0.66
Batch: 300; loss: 1.17; acc: 0.64
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 0.9; acc: 0.7
Batch: 360; loss: 0.96; acc: 0.72
Batch: 380; loss: 0.98; acc: 0.64
Batch: 400; loss: 0.97; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.64
Batch: 440; loss: 0.86; acc: 0.7
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 1.14; acc: 0.62
Batch: 500; loss: 0.82; acc: 0.72
Batch: 520; loss: 0.73; acc: 0.72
Batch: 540; loss: 0.76; acc: 0.69
Batch: 560; loss: 1.05; acc: 0.59
Batch: 580; loss: 0.91; acc: 0.7
Batch: 600; loss: 0.89; acc: 0.75
Batch: 620; loss: 0.9; acc: 0.72
Batch: 640; loss: 1.05; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 0.88; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 0.78; acc: 0.73
Batch: 760; loss: 1.02; acc: 0.61
Batch: 780; loss: 1.07; acc: 0.58
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.7
Val Epoch over. val_loss: 0.8756091296672821; val_accuracy: 0.716859076433121 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.97; acc: 0.61
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.8; acc: 0.7
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.96; acc: 0.7
Batch: 160; loss: 1.09; acc: 0.69
Batch: 180; loss: 1.13; acc: 0.64
Batch: 200; loss: 1.32; acc: 0.53
Batch: 220; loss: 1.16; acc: 0.69
Batch: 240; loss: 0.8; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.64
Batch: 280; loss: 0.82; acc: 0.67
Batch: 300; loss: 0.78; acc: 0.73
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 1.04; acc: 0.62
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 0.76; acc: 0.72
Batch: 440; loss: 1.09; acc: 0.64
Batch: 460; loss: 0.95; acc: 0.75
Batch: 480; loss: 0.95; acc: 0.72
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 0.95; acc: 0.73
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 1.02; acc: 0.67
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.94; acc: 0.66
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 1.15; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.67
Batch: 720; loss: 0.87; acc: 0.72
Batch: 740; loss: 0.93; acc: 0.66
Batch: 760; loss: 0.84; acc: 0.77
Batch: 780; loss: 0.82; acc: 0.73
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.69
Val Epoch over. val_loss: 0.8751594416654793; val_accuracy: 0.7164609872611465 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.72
Batch: 80; loss: 0.79; acc: 0.69
Batch: 100; loss: 1.04; acc: 0.62
Batch: 120; loss: 0.86; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.77
Batch: 160; loss: 0.77; acc: 0.7
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.99; acc: 0.7
Batch: 260; loss: 1.02; acc: 0.73
Batch: 280; loss: 0.97; acc: 0.69
Batch: 300; loss: 0.83; acc: 0.75
Batch: 320; loss: 1.08; acc: 0.66
Batch: 340; loss: 0.91; acc: 0.72
Batch: 360; loss: 1.03; acc: 0.7
Batch: 380; loss: 0.94; acc: 0.67
Batch: 400; loss: 1.03; acc: 0.72
Batch: 420; loss: 0.84; acc: 0.67
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 0.86; acc: 0.7
Batch: 480; loss: 0.93; acc: 0.73
Batch: 500; loss: 1.14; acc: 0.67
Batch: 520; loss: 0.71; acc: 0.73
Batch: 540; loss: 0.9; acc: 0.75
Batch: 560; loss: 0.96; acc: 0.7
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 1.15; acc: 0.61
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 1.07; acc: 0.7
Batch: 660; loss: 1.08; acc: 0.64
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.88; acc: 0.66
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.83; acc: 0.72
Batch: 760; loss: 1.08; acc: 0.62
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.73
Val Epoch over. val_loss: 0.8758732270283304; val_accuracy: 0.7173566878980892 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.9; acc: 0.69
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.86; acc: 0.67
Batch: 100; loss: 1.12; acc: 0.66
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.58
Batch: 160; loss: 0.88; acc: 0.72
Batch: 180; loss: 0.85; acc: 0.75
Batch: 200; loss: 0.79; acc: 0.72
Batch: 220; loss: 0.94; acc: 0.66
Batch: 240; loss: 0.89; acc: 0.72
Batch: 260; loss: 0.78; acc: 0.78
Batch: 280; loss: 0.84; acc: 0.7
Batch: 300; loss: 0.93; acc: 0.69
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.77
Batch: 400; loss: 0.88; acc: 0.72
Batch: 420; loss: 1.04; acc: 0.77
Batch: 440; loss: 0.97; acc: 0.67
Batch: 460; loss: 1.11; acc: 0.72
Batch: 480; loss: 0.94; acc: 0.7
Batch: 500; loss: 1.0; acc: 0.56
Batch: 520; loss: 1.07; acc: 0.64
Batch: 540; loss: 0.91; acc: 0.77
Batch: 560; loss: 0.75; acc: 0.73
Batch: 580; loss: 0.82; acc: 0.67
Batch: 600; loss: 0.77; acc: 0.75
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.95; acc: 0.69
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.71; acc: 0.73
Batch: 700; loss: 1.4; acc: 0.59
Batch: 720; loss: 0.98; acc: 0.64
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 1.09; acc: 0.64
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.98; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.73
Val Epoch over. val_loss: 0.875343684178249; val_accuracy: 0.7177547770700637 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.2; acc: 0.58
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 0.93; acc: 0.66
Batch: 80; loss: 0.93; acc: 0.72
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.9; acc: 0.73
Batch: 160; loss: 1.29; acc: 0.61
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 1.02; acc: 0.7
Batch: 220; loss: 0.94; acc: 0.73
Batch: 240; loss: 0.81; acc: 0.73
Batch: 260; loss: 1.08; acc: 0.7
Batch: 280; loss: 0.97; acc: 0.61
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 0.82; acc: 0.8
Batch: 340; loss: 1.0; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.58
Batch: 380; loss: 0.88; acc: 0.66
Batch: 400; loss: 0.88; acc: 0.69
Batch: 420; loss: 0.97; acc: 0.66
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 1.11; acc: 0.69
Batch: 480; loss: 1.17; acc: 0.61
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 1.11; acc: 0.61
Batch: 580; loss: 1.04; acc: 0.67
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.9; acc: 0.64
Batch: 640; loss: 0.91; acc: 0.69
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.06; acc: 0.7
Batch: 700; loss: 1.0; acc: 0.64
Batch: 720; loss: 0.94; acc: 0.75
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 1.09; acc: 0.67
Batch: 780; loss: 1.07; acc: 0.7
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.72
Val Epoch over. val_loss: 0.8751380595431966; val_accuracy: 0.7156648089171974 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.94; acc: 0.72
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 1.04; acc: 0.73
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.75
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.88; acc: 0.7
Batch: 160; loss: 1.18; acc: 0.72
Batch: 180; loss: 0.86; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.75
Batch: 220; loss: 0.98; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.7
Batch: 260; loss: 0.95; acc: 0.67
Batch: 280; loss: 0.95; acc: 0.66
Batch: 300; loss: 0.98; acc: 0.66
Batch: 320; loss: 0.92; acc: 0.67
Batch: 340; loss: 0.83; acc: 0.69
Batch: 360; loss: 0.86; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.52
Batch: 420; loss: 0.85; acc: 0.72
Batch: 440; loss: 0.95; acc: 0.72
Batch: 460; loss: 0.82; acc: 0.7
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 1.45; acc: 0.5
Batch: 520; loss: 1.08; acc: 0.59
Batch: 540; loss: 1.27; acc: 0.61
Batch: 560; loss: 1.38; acc: 0.56
Batch: 580; loss: 0.72; acc: 0.7
Batch: 600; loss: 0.9; acc: 0.7
Batch: 620; loss: 0.95; acc: 0.64
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 0.98; acc: 0.61
Batch: 680; loss: 0.98; acc: 0.67
Batch: 700; loss: 0.78; acc: 0.72
Batch: 720; loss: 1.27; acc: 0.58
Batch: 740; loss: 0.91; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.7
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.74; acc: 0.72
Val Epoch over. val_loss: 0.8748025294322117; val_accuracy: 0.7167595541401274 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.78; acc: 0.75
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 1.36; acc: 0.61
Batch: 200; loss: 0.89; acc: 0.78
Batch: 220; loss: 1.08; acc: 0.66
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 0.76; acc: 0.7
Batch: 280; loss: 0.88; acc: 0.78
Batch: 300; loss: 0.99; acc: 0.67
Batch: 320; loss: 0.95; acc: 0.67
Batch: 340; loss: 1.0; acc: 0.66
Batch: 360; loss: 0.62; acc: 0.75
Batch: 380; loss: 0.7; acc: 0.77
Batch: 400; loss: 0.96; acc: 0.69
Batch: 420; loss: 1.1; acc: 0.59
Batch: 440; loss: 0.88; acc: 0.7
Batch: 460; loss: 0.9; acc: 0.66
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.91; acc: 0.7
Batch: 540; loss: 0.97; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.66
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 1.03; acc: 0.62
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 0.84; acc: 0.67
Batch: 660; loss: 1.13; acc: 0.59
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 1.1; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.64
Batch: 740; loss: 1.02; acc: 0.64
Batch: 760; loss: 0.94; acc: 0.69
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.74; acc: 0.7
Val Epoch over. val_loss: 0.8746403167202215; val_accuracy: 0.7154657643312102 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.7
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 1.07; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 0.8; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.59
Batch: 160; loss: 0.82; acc: 0.75
Batch: 180; loss: 1.03; acc: 0.7
Batch: 200; loss: 0.83; acc: 0.67
Batch: 220; loss: 1.07; acc: 0.64
Batch: 240; loss: 0.89; acc: 0.66
Batch: 260; loss: 0.88; acc: 0.66
Batch: 280; loss: 0.92; acc: 0.72
Batch: 300; loss: 1.12; acc: 0.64
Batch: 320; loss: 0.91; acc: 0.7
Batch: 340; loss: 1.03; acc: 0.69
Batch: 360; loss: 0.86; acc: 0.69
Batch: 380; loss: 1.07; acc: 0.67
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.96; acc: 0.73
Batch: 460; loss: 1.05; acc: 0.66
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.64
Batch: 520; loss: 0.85; acc: 0.7
Batch: 540; loss: 0.82; acc: 0.77
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 1.04; acc: 0.69
Batch: 600; loss: 1.25; acc: 0.61
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 0.96; acc: 0.67
Batch: 660; loss: 0.89; acc: 0.62
Batch: 680; loss: 1.01; acc: 0.67
Batch: 700; loss: 1.05; acc: 0.67
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.69
Val Epoch over. val_loss: 0.874864299396041; val_accuracy: 0.7161624203821656 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.38; acc: 0.56
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 0.96; acc: 0.64
Batch: 80; loss: 1.26; acc: 0.56
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 1.21; acc: 0.72
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.72
Batch: 200; loss: 0.97; acc: 0.69
Batch: 220; loss: 0.81; acc: 0.67
Batch: 240; loss: 0.88; acc: 0.72
Batch: 260; loss: 1.02; acc: 0.62
Batch: 280; loss: 0.81; acc: 0.72
Batch: 300; loss: 1.25; acc: 0.66
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 0.97; acc: 0.69
Batch: 380; loss: 0.68; acc: 0.78
Batch: 400; loss: 0.88; acc: 0.67
Batch: 420; loss: 1.3; acc: 0.62
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.72; acc: 0.75
Batch: 480; loss: 0.9; acc: 0.73
Batch: 500; loss: 1.08; acc: 0.69
Batch: 520; loss: 0.69; acc: 0.73
Batch: 540; loss: 0.83; acc: 0.78
Batch: 560; loss: 0.78; acc: 0.73
Batch: 580; loss: 0.76; acc: 0.7
Batch: 600; loss: 0.88; acc: 0.8
Batch: 620; loss: 1.18; acc: 0.59
Batch: 640; loss: 0.9; acc: 0.7
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 0.93; acc: 0.73
Batch: 740; loss: 0.7; acc: 0.72
Batch: 760; loss: 1.02; acc: 0.67
Batch: 780; loss: 1.23; acc: 0.58
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.99; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 0.74; acc: 0.69
Val Epoch over. val_loss: 0.8747594373620999; val_accuracy: 0.7157643312101911 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 20812
elements in E: 4442600
fraction nonzero: 0.004684644127312835
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.29; acc: 0.17
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.27; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.28; acc: 0.19
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.28; acc: 0.11
Batch: 400; loss: 2.28; acc: 0.06
Batch: 420; loss: 2.27; acc: 0.19
Batch: 440; loss: 2.27; acc: 0.22
Batch: 460; loss: 2.27; acc: 0.27
Batch: 480; loss: 2.27; acc: 0.14
Batch: 500; loss: 2.27; acc: 0.22
Batch: 520; loss: 2.27; acc: 0.08
Batch: 540; loss: 2.26; acc: 0.22
Batch: 560; loss: 2.26; acc: 0.2
Batch: 580; loss: 2.26; acc: 0.23
Batch: 600; loss: 2.26; acc: 0.2
Batch: 620; loss: 2.24; acc: 0.28
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.23; acc: 0.28
Batch: 680; loss: 2.22; acc: 0.27
Batch: 700; loss: 2.24; acc: 0.06
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.23; acc: 0.14
Batch: 760; loss: 2.2; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.12
Train Epoch over. train_loss: 2.27; train_accuracy: 0.16 

Batch: 0; loss: 2.2; acc: 0.19
Batch: 20; loss: 2.15; acc: 0.33
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.19; acc: 0.3
Batch: 80; loss: 2.18; acc: 0.23
Batch: 100; loss: 2.18; acc: 0.19
Batch: 120; loss: 2.19; acc: 0.27
Batch: 140; loss: 2.16; acc: 0.23
Val Epoch over. val_loss: 2.1890706104837405; val_accuracy: 0.2154657643312102 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.19
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.21; acc: 0.12
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.16; acc: 0.27
Batch: 100; loss: 2.09; acc: 0.34
Batch: 120; loss: 2.09; acc: 0.25
Batch: 140; loss: 2.11; acc: 0.25
Batch: 160; loss: 2.02; acc: 0.33
Batch: 180; loss: 1.94; acc: 0.41
Batch: 200; loss: 1.9; acc: 0.41
Batch: 220; loss: 1.82; acc: 0.5
Batch: 240; loss: 1.77; acc: 0.48
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.48
Batch: 320; loss: 1.49; acc: 0.52
Batch: 340; loss: 1.49; acc: 0.55
Batch: 360; loss: 1.51; acc: 0.47
Batch: 380; loss: 1.42; acc: 0.48
Batch: 400; loss: 1.17; acc: 0.62
Batch: 420; loss: 1.08; acc: 0.62
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.62
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 1.13; acc: 0.58
Batch: 540; loss: 1.01; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.62
Batch: 580; loss: 1.02; acc: 0.7
Batch: 600; loss: 1.14; acc: 0.62
Batch: 620; loss: 1.1; acc: 0.72
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 1.14; acc: 0.61
Batch: 680; loss: 1.17; acc: 0.64
Batch: 700; loss: 0.93; acc: 0.64
Batch: 720; loss: 1.07; acc: 0.66
Batch: 740; loss: 0.92; acc: 0.66
Batch: 760; loss: 1.0; acc: 0.7
Batch: 780; loss: 0.88; acc: 0.83
Train Epoch over. train_loss: 1.46; train_accuracy: 0.53 

Batch: 0; loss: 1.14; acc: 0.61
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.7; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.67
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.8
Val Epoch over. val_loss: 0.952404047653174; val_accuracy: 0.6970541401273885 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.66
Batch: 20; loss: 1.02; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.72
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.97; acc: 0.67
Batch: 160; loss: 1.11; acc: 0.69
Batch: 180; loss: 0.87; acc: 0.7
Batch: 200; loss: 0.9; acc: 0.72
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 0.92; acc: 0.72
Batch: 260; loss: 1.01; acc: 0.67
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.93; acc: 0.64
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 0.92; acc: 0.7
Batch: 380; loss: 1.08; acc: 0.62
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 0.93; acc: 0.67
Batch: 440; loss: 1.07; acc: 0.62
Batch: 460; loss: 1.07; acc: 0.59
Batch: 480; loss: 0.85; acc: 0.72
Batch: 500; loss: 1.08; acc: 0.7
Batch: 520; loss: 0.82; acc: 0.75
Batch: 540; loss: 0.89; acc: 0.8
Batch: 560; loss: 0.93; acc: 0.67
Batch: 580; loss: 0.67; acc: 0.77
Batch: 600; loss: 0.93; acc: 0.78
Batch: 620; loss: 0.83; acc: 0.72
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 1.37; acc: 0.55
Batch: 680; loss: 0.95; acc: 0.7
Batch: 700; loss: 0.74; acc: 0.77
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 1.04; acc: 0.75
Batch: 760; loss: 0.79; acc: 0.73
Batch: 780; loss: 1.0; acc: 0.77
Train Epoch over. train_loss: 0.95; train_accuracy: 0.7 

Batch: 0; loss: 1.02; acc: 0.62
Batch: 20; loss: 0.89; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.7
Batch: 80; loss: 0.8; acc: 0.69
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 1.01; acc: 0.7
Batch: 140; loss: 0.58; acc: 0.84
Val Epoch over. val_loss: 0.8889417627434821; val_accuracy: 0.7126791401273885 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 0.88; acc: 0.75
Batch: 60; loss: 0.96; acc: 0.62
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 0.84; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.75
Batch: 160; loss: 0.94; acc: 0.64
Batch: 180; loss: 0.9; acc: 0.77
Batch: 200; loss: 1.07; acc: 0.7
Batch: 220; loss: 0.89; acc: 0.7
Batch: 240; loss: 0.84; acc: 0.7
Batch: 260; loss: 0.75; acc: 0.72
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 0.77; acc: 0.7
Batch: 320; loss: 0.86; acc: 0.69
Batch: 340; loss: 0.97; acc: 0.67
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 0.91; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.62
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 1.11; acc: 0.59
Batch: 480; loss: 1.0; acc: 0.69
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.72
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 0.96; acc: 0.72
Batch: 580; loss: 0.75; acc: 0.75
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.98; acc: 0.72
Batch: 660; loss: 1.05; acc: 0.62
Batch: 680; loss: 0.93; acc: 0.72
Batch: 700; loss: 0.92; acc: 0.66
Batch: 720; loss: 0.84; acc: 0.73
Batch: 740; loss: 0.97; acc: 0.66
Batch: 760; loss: 0.9; acc: 0.75
Batch: 780; loss: 0.68; acc: 0.78
Train Epoch over. train_loss: 0.89; train_accuracy: 0.72 

Batch: 0; loss: 1.04; acc: 0.58
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.51; acc: 0.84
Val Epoch over. val_loss: 0.8738354448300258; val_accuracy: 0.7236265923566879 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.96; acc: 0.72
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.75; acc: 0.73
Batch: 100; loss: 1.4; acc: 0.52
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.83; acc: 0.69
Batch: 160; loss: 0.78; acc: 0.75
Batch: 180; loss: 0.67; acc: 0.73
Batch: 200; loss: 0.87; acc: 0.69
Batch: 220; loss: 0.99; acc: 0.72
Batch: 240; loss: 0.96; acc: 0.7
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.95; acc: 0.67
Batch: 300; loss: 0.89; acc: 0.72
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 1.07; acc: 0.61
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.93; acc: 0.7
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 0.92; acc: 0.64
Batch: 560; loss: 0.87; acc: 0.77
Batch: 580; loss: 0.74; acc: 0.73
Batch: 600; loss: 0.9; acc: 0.72
Batch: 620; loss: 0.78; acc: 0.72
Batch: 640; loss: 1.08; acc: 0.59
Batch: 660; loss: 1.19; acc: 0.69
Batch: 680; loss: 0.95; acc: 0.67
Batch: 700; loss: 0.87; acc: 0.7
Batch: 720; loss: 1.08; acc: 0.64
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.75
Batch: 780; loss: 0.87; acc: 0.72
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 1.01; acc: 0.59
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.8178929874471798; val_accuracy: 0.7411425159235668 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.89; acc: 0.69
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 0.52; acc: 0.78
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.94; acc: 0.77
Batch: 200; loss: 0.76; acc: 0.75
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 0.9; acc: 0.7
Batch: 260; loss: 0.92; acc: 0.72
Batch: 280; loss: 0.7; acc: 0.7
Batch: 300; loss: 1.17; acc: 0.59
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.87; acc: 0.78
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 1.06; acc: 0.7
Batch: 440; loss: 0.88; acc: 0.72
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.98; acc: 0.7
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 1.03; acc: 0.64
Batch: 560; loss: 0.9; acc: 0.69
Batch: 580; loss: 0.79; acc: 0.75
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.92; acc: 0.67
Batch: 700; loss: 0.99; acc: 0.64
Batch: 720; loss: 0.92; acc: 0.7
Batch: 740; loss: 1.05; acc: 0.69
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.77
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 1.05; acc: 0.58
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.95; acc: 0.67
Batch: 80; loss: 0.62; acc: 0.77
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.56; acc: 0.8
Val Epoch over. val_loss: 0.8490529350794045; val_accuracy: 0.7189490445859873 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.92; acc: 0.7
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.97; acc: 0.64
Batch: 100; loss: 1.01; acc: 0.67
Batch: 120; loss: 1.04; acc: 0.67
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.89; acc: 0.69
Batch: 180; loss: 0.91; acc: 0.69
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.84; acc: 0.73
Batch: 260; loss: 0.87; acc: 0.59
Batch: 280; loss: 0.73; acc: 0.75
Batch: 300; loss: 0.8; acc: 0.69
Batch: 320; loss: 0.93; acc: 0.69
Batch: 340; loss: 0.96; acc: 0.7
Batch: 360; loss: 0.83; acc: 0.72
Batch: 380; loss: 0.76; acc: 0.77
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 0.84; acc: 0.72
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.72; acc: 0.73
Batch: 480; loss: 0.8; acc: 0.75
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 1.08; acc: 0.73
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 1.24; acc: 0.66
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 0.73; acc: 0.86
Batch: 720; loss: 0.85; acc: 0.69
Batch: 740; loss: 0.92; acc: 0.69
Batch: 760; loss: 0.77; acc: 0.73
Batch: 780; loss: 0.92; acc: 0.67
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.43; acc: 0.91
Val Epoch over. val_loss: 0.7664276594948617; val_accuracy: 0.7592555732484076 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.86; acc: 0.77
Batch: 60; loss: 0.91; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.73
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.99; acc: 0.73
Batch: 200; loss: 0.94; acc: 0.67
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.84; acc: 0.69
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.8; acc: 0.8
Batch: 300; loss: 0.95; acc: 0.73
Batch: 320; loss: 0.8; acc: 0.67
Batch: 340; loss: 0.81; acc: 0.72
Batch: 360; loss: 0.69; acc: 0.81
Batch: 380; loss: 0.91; acc: 0.75
Batch: 400; loss: 0.77; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.97; acc: 0.7
Batch: 460; loss: 1.22; acc: 0.67
Batch: 480; loss: 0.73; acc: 0.75
Batch: 500; loss: 0.79; acc: 0.75
Batch: 520; loss: 1.11; acc: 0.64
Batch: 540; loss: 0.74; acc: 0.7
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.72; acc: 0.77
Batch: 600; loss: 0.81; acc: 0.78
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.92; acc: 0.67
Batch: 660; loss: 0.83; acc: 0.7
Batch: 680; loss: 0.72; acc: 0.75
Batch: 700; loss: 0.91; acc: 0.73
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.85; acc: 0.72
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

slurmstepd: error: _is_a_lwp: open() /proc/1362/status failed: No such file or directory
Batch: 0; loss: 0.92; acc: 0.64
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.44; acc: 0.86
Val Epoch over. val_loss: 0.7971669393739883; val_accuracy: 0.7463176751592356 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.88; acc: 0.69
Batch: 40; loss: 0.96; acc: 0.72
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.67
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.81; acc: 0.67
Batch: 220; loss: 0.67; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.71; acc: 0.78
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.85; acc: 0.7
Batch: 340; loss: 1.01; acc: 0.64
Batch: 360; loss: 0.93; acc: 0.73
Batch: 380; loss: 0.91; acc: 0.7
Batch: 400; loss: 1.02; acc: 0.64
Batch: 420; loss: 0.92; acc: 0.72
Batch: 440; loss: 0.89; acc: 0.66
Batch: 460; loss: 0.96; acc: 0.7
Batch: 480; loss: 0.9; acc: 0.73
Batch: 500; loss: 0.85; acc: 0.78
Batch: 520; loss: 0.91; acc: 0.69
Batch: 540; loss: 1.14; acc: 0.58
Batch: 560; loss: 0.86; acc: 0.78
Batch: 580; loss: 0.83; acc: 0.72
Batch: 600; loss: 0.67; acc: 0.77
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.84; acc: 0.75
Batch: 660; loss: 0.67; acc: 0.73
Batch: 680; loss: 0.89; acc: 0.72
Batch: 700; loss: 0.81; acc: 0.67
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.7
Batch: 760; loss: 0.68; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.64
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 1.0; acc: 0.67
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.84
Val Epoch over. val_loss: 0.8122838435658983; val_accuracy: 0.740843949044586 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.98; acc: 0.66
Batch: 40; loss: 0.76; acc: 0.72
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.91; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.72
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 1.02; acc: 0.67
Batch: 200; loss: 0.91; acc: 0.69
Batch: 220; loss: 1.0; acc: 0.73
Batch: 240; loss: 1.16; acc: 0.7
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 1.07; acc: 0.69
Batch: 340; loss: 0.92; acc: 0.72
Batch: 360; loss: 0.91; acc: 0.73
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.95; acc: 0.72
Batch: 440; loss: 0.8; acc: 0.75
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 1.16; acc: 0.64
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.87; acc: 0.73
Batch: 540; loss: 0.9; acc: 0.73
Batch: 560; loss: 0.87; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 0.98; acc: 0.75
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 0.93; acc: 0.75
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 1.17; acc: 0.64
Batch: 700; loss: 0.88; acc: 0.69
Batch: 720; loss: 0.91; acc: 0.69
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.73
Batch: 780; loss: 0.67; acc: 0.77
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.89; acc: 0.67
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.68; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.67
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.45; acc: 0.84
Val Epoch over. val_loss: 0.8251353084661399; val_accuracy: 0.7368630573248408 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 0.84; acc: 0.72
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.83; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.67
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 1.01; acc: 0.64
Batch: 160; loss: 0.82; acc: 0.69
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.77
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.64
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.93; acc: 0.72
Batch: 300; loss: 1.1; acc: 0.66
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.75; acc: 0.73
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 0.76; acc: 0.7
Batch: 400; loss: 0.82; acc: 0.69
Batch: 420; loss: 1.39; acc: 0.56
Batch: 440; loss: 0.79; acc: 0.75
Batch: 460; loss: 0.75; acc: 0.75
Batch: 480; loss: 0.87; acc: 0.73
Batch: 500; loss: 0.85; acc: 0.73
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.69
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.75
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 1.02; acc: 0.67
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.82; acc: 0.81
Batch: 720; loss: 0.98; acc: 0.67
Batch: 740; loss: 0.81; acc: 0.7
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 1.05; acc: 0.7
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.7674139580529207; val_accuracy: 0.7592555732484076 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.81
Batch: 180; loss: 0.63; acc: 0.77
Batch: 200; loss: 0.91; acc: 0.8
Batch: 220; loss: 0.86; acc: 0.77
Batch: 240; loss: 0.87; acc: 0.72
Batch: 260; loss: 0.95; acc: 0.61
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.83; acc: 0.77
Batch: 340; loss: 0.77; acc: 0.67
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 1.06; acc: 0.67
Batch: 400; loss: 0.79; acc: 0.8
Batch: 420; loss: 0.91; acc: 0.73
Batch: 440; loss: 0.89; acc: 0.77
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 1.13; acc: 0.62
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 0.98; acc: 0.72
Batch: 600; loss: 1.01; acc: 0.72
Batch: 620; loss: 0.69; acc: 0.77
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.72
Batch: 700; loss: 0.79; acc: 0.75
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.89; acc: 0.7
Batch: 760; loss: 0.81; acc: 0.77
Batch: 780; loss: 0.97; acc: 0.7
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.58; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.69
Batch: 140; loss: 0.41; acc: 0.89
Val Epoch over. val_loss: 0.7643909539766373; val_accuracy: 0.7560708598726115 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.7; acc: 0.73
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.92; acc: 0.7
Batch: 200; loss: 0.7; acc: 0.7
Batch: 220; loss: 0.83; acc: 0.69
Batch: 240; loss: 0.63; acc: 0.78
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 0.8; acc: 0.73
Batch: 320; loss: 0.7; acc: 0.72
Batch: 340; loss: 0.82; acc: 0.73
Batch: 360; loss: 0.87; acc: 0.75
Batch: 380; loss: 0.69; acc: 0.77
Batch: 400; loss: 0.79; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.75
Batch: 440; loss: 0.94; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 0.96; acc: 0.67
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 0.92; acc: 0.73
Batch: 540; loss: 0.91; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 0.85; acc: 0.77
Batch: 660; loss: 1.02; acc: 0.69
Batch: 680; loss: 0.92; acc: 0.69
Batch: 700; loss: 0.7; acc: 0.75
Batch: 720; loss: 1.01; acc: 0.73
Batch: 740; loss: 0.83; acc: 0.78
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.96; acc: 0.7
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.59; acc: 0.78
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.41; acc: 0.89
Val Epoch over. val_loss: 0.7607680991007264; val_accuracy: 0.7593550955414012 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.72
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.92; acc: 0.78
Batch: 80; loss: 1.14; acc: 0.7
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.72
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.82; acc: 0.73
Batch: 220; loss: 0.76; acc: 0.72
Batch: 240; loss: 1.0; acc: 0.72
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.78; acc: 0.78
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 1.01; acc: 0.73
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 1.08; acc: 0.67
Batch: 440; loss: 0.89; acc: 0.67
Batch: 460; loss: 1.0; acc: 0.77
Batch: 480; loss: 0.81; acc: 0.77
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.84; acc: 0.73
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.73; acc: 0.78
Batch: 620; loss: 0.92; acc: 0.73
Batch: 640; loss: 0.76; acc: 0.8
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 1.19; acc: 0.62
Batch: 700; loss: 0.83; acc: 0.75
Batch: 720; loss: 0.8; acc: 0.69
Batch: 740; loss: 0.81; acc: 0.7
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.99; acc: 0.66
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.88
Val Epoch over. val_loss: 0.7628397632176709; val_accuracy: 0.7609474522292994 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.72
Batch: 40; loss: 0.74; acc: 0.7
Batch: 60; loss: 0.66; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.78; acc: 0.73
Batch: 180; loss: 0.76; acc: 0.72
Batch: 200; loss: 0.92; acc: 0.73
Batch: 220; loss: 0.86; acc: 0.7
Batch: 240; loss: 0.86; acc: 0.67
Batch: 260; loss: 0.96; acc: 0.75
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.89; acc: 0.7
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.73
Batch: 400; loss: 0.81; acc: 0.69
Batch: 420; loss: 0.94; acc: 0.77
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.99; acc: 0.67
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.87; acc: 0.73
Batch: 600; loss: 0.73; acc: 0.73
Batch: 620; loss: 0.61; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.75
Batch: 660; loss: 0.64; acc: 0.77
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.81; acc: 0.73
Batch: 760; loss: 0.84; acc: 0.67
Batch: 780; loss: 1.09; acc: 0.67
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.7
Batch: 20; loss: 0.78; acc: 0.7
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.54; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.7571198112645726; val_accuracy: 0.7604498407643312 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.92; acc: 0.69
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 1.03; acc: 0.67
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.75; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 1.04; acc: 0.7
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.75; acc: 0.72
Batch: 280; loss: 0.97; acc: 0.75
Batch: 300; loss: 0.78; acc: 0.77
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.8; acc: 0.8
Batch: 360; loss: 0.93; acc: 0.69
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.81
Batch: 420; loss: 0.61; acc: 0.77
Batch: 440; loss: 0.9; acc: 0.7
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 0.72; acc: 0.75
Batch: 500; loss: 0.93; acc: 0.7
Batch: 520; loss: 1.03; acc: 0.72
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.89; acc: 0.73
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 0.88; acc: 0.67
Batch: 620; loss: 0.89; acc: 0.7
Batch: 640; loss: 0.94; acc: 0.77
Batch: 660; loss: 0.88; acc: 0.72
Batch: 680; loss: 0.73; acc: 0.72
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 1.0; acc: 0.69
Batch: 740; loss: 0.88; acc: 0.69
Batch: 760; loss: 0.79; acc: 0.72
Batch: 780; loss: 0.63; acc: 0.75
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.72
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.72
Batch: 80; loss: 0.6; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.7601541390844212; val_accuracy: 0.7605493630573248 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.72
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 1.09; acc: 0.72
Batch: 160; loss: 0.92; acc: 0.69
Batch: 180; loss: 0.8; acc: 0.72
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.83; acc: 0.75
Batch: 240; loss: 0.78; acc: 0.73
Batch: 260; loss: 0.92; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.73
Batch: 300; loss: 0.58; acc: 0.78
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 1.06; acc: 0.64
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.95; acc: 0.64
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.97; acc: 0.67
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 0.99; acc: 0.72
Batch: 520; loss: 0.92; acc: 0.7
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 0.78; acc: 0.75
Batch: 580; loss: 0.91; acc: 0.73
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.71; acc: 0.8
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.71; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.67
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.93; acc: 0.72
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.81; acc: 0.73
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.72
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7563979932267195; val_accuracy: 0.7664211783439491 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.7
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 1.06; acc: 0.64
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 0.71; acc: 0.73
Batch: 200; loss: 0.89; acc: 0.75
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 1.09; acc: 0.69
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.83; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 1.01; acc: 0.67
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 1.26; acc: 0.62
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.75
Batch: 580; loss: 1.01; acc: 0.67
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 0.8; acc: 0.75
Batch: 640; loss: 0.7; acc: 0.73
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 1.2; acc: 0.69
Batch: 760; loss: 0.84; acc: 0.72
Batch: 780; loss: 0.85; acc: 0.72
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.99; acc: 0.64
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.45; acc: 0.84
Val Epoch over. val_loss: 0.7722582074866933; val_accuracy: 0.7577627388535032 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 0.75; acc: 0.72
Batch: 80; loss: 0.83; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.94; acc: 0.67
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 0.85; acc: 0.69
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.85; acc: 0.7
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.78
Batch: 380; loss: 1.09; acc: 0.67
Batch: 400; loss: 0.89; acc: 0.67
Batch: 420; loss: 0.83; acc: 0.72
Batch: 440; loss: 0.91; acc: 0.69
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.91; acc: 0.69
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 1.0; acc: 0.69
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 1.09; acc: 0.69
Batch: 600; loss: 0.71; acc: 0.69
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 1.03; acc: 0.72
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.69
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.84; acc: 0.67
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.94; acc: 0.69
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.7
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7578637077929867; val_accuracy: 0.7643312101910829 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.73
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 0.83; acc: 0.72
Batch: 180; loss: 0.7; acc: 0.77
Batch: 200; loss: 0.76; acc: 0.77
Batch: 220; loss: 0.88; acc: 0.73
Batch: 240; loss: 0.92; acc: 0.67
Batch: 260; loss: 0.96; acc: 0.75
Batch: 280; loss: 0.9; acc: 0.67
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 0.89; acc: 0.69
Batch: 340; loss: 0.73; acc: 0.7
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.92; acc: 0.7
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 0.94; acc: 0.66
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.91; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.66
Batch: 560; loss: 0.74; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.73; acc: 0.77
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.67
Batch: 660; loss: 0.93; acc: 0.72
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.8; acc: 0.78
Batch: 720; loss: 0.8; acc: 0.72
Batch: 740; loss: 0.83; acc: 0.7
Batch: 760; loss: 0.95; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.66
Train Epoch over. train_loss: 0.83; train_accuracy: 0.74 

Batch: 0; loss: 1.02; acc: 0.66
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.41; acc: 0.88
Val Epoch over. val_loss: 0.7679048972145007; val_accuracy: 0.7584593949044586 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 0.8; acc: 0.72
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 0.81; acc: 0.73
Batch: 180; loss: 0.98; acc: 0.7
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.7
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.83; acc: 0.73
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.86; acc: 0.7
Batch: 400; loss: 0.87; acc: 0.73
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.79; acc: 0.75
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.75
Batch: 500; loss: 0.79; acc: 0.67
Batch: 520; loss: 0.89; acc: 0.78
Batch: 540; loss: 0.88; acc: 0.69
Batch: 560; loss: 0.82; acc: 0.75
Batch: 580; loss: 0.82; acc: 0.7
Batch: 600; loss: 0.88; acc: 0.69
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.95; acc: 0.75
Batch: 700; loss: 0.49; acc: 0.92
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 1.09; acc: 0.61
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.81; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.7509923803198869; val_accuracy: 0.7658240445859873 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 0.74; acc: 0.73
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.7
Batch: 260; loss: 0.76; acc: 0.73
Batch: 280; loss: 0.82; acc: 0.7
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 1.08; acc: 0.66
Batch: 340; loss: 0.96; acc: 0.64
Batch: 360; loss: 0.92; acc: 0.73
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.77
Batch: 460; loss: 0.92; acc: 0.69
Batch: 480; loss: 0.81; acc: 0.72
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 0.9; acc: 0.7
Batch: 540; loss: 1.14; acc: 0.66
Batch: 560; loss: 0.82; acc: 0.7
Batch: 580; loss: 1.03; acc: 0.64
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 0.97; acc: 0.69
Batch: 680; loss: 0.72; acc: 0.75
Batch: 700; loss: 0.78; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 1.01; acc: 0.72
Batch: 760; loss: 1.01; acc: 0.64
Batch: 780; loss: 0.86; acc: 0.7
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.7496740874970794; val_accuracy: 0.7665207006369427 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.79; acc: 0.73
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 1.09; acc: 0.56
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 0.94; acc: 0.66
Batch: 120; loss: 1.02; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.66
Batch: 160; loss: 0.84; acc: 0.75
Batch: 180; loss: 1.0; acc: 0.67
Batch: 200; loss: 0.97; acc: 0.72
Batch: 220; loss: 0.93; acc: 0.69
Batch: 240; loss: 0.61; acc: 0.78
Batch: 260; loss: 0.69; acc: 0.8
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.99; acc: 0.7
Batch: 320; loss: 0.81; acc: 0.75
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.69
Batch: 380; loss: 0.92; acc: 0.73
Batch: 400; loss: 0.9; acc: 0.72
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.94; acc: 0.72
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.87; acc: 0.8
Batch: 520; loss: 0.97; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.73
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.8; acc: 0.78
Batch: 660; loss: 0.8; acc: 0.78
Batch: 680; loss: 0.9; acc: 0.7
Batch: 700; loss: 0.77; acc: 0.77
Batch: 720; loss: 0.9; acc: 0.73
Batch: 740; loss: 0.76; acc: 0.72
Batch: 760; loss: 0.89; acc: 0.58
Batch: 780; loss: 0.93; acc: 0.7
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.69
Batch: 140; loss: 0.41; acc: 0.88
Val Epoch over. val_loss: 0.7510004446954485; val_accuracy: 0.7669187898089171 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.85; acc: 0.7
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.88; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.72
Batch: 160; loss: 0.82; acc: 0.69
Batch: 180; loss: 1.13; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.72
Batch: 220; loss: 1.04; acc: 0.7
Batch: 240; loss: 1.01; acc: 0.69
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.89; acc: 0.67
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.7
Batch: 340; loss: 0.86; acc: 0.7
Batch: 360; loss: 1.01; acc: 0.66
Batch: 380; loss: 0.85; acc: 0.7
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 0.75; acc: 0.73
Batch: 580; loss: 0.83; acc: 0.78
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.94; acc: 0.73
Batch: 640; loss: 0.6; acc: 0.77
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.82; acc: 0.7
Batch: 720; loss: 0.91; acc: 0.72
Batch: 740; loss: 0.93; acc: 0.7
Batch: 760; loss: 0.65; acc: 0.77
Batch: 780; loss: 0.71; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.42; acc: 0.88
Val Epoch over. val_loss: 0.7522739711081147; val_accuracy: 0.7669187898089171 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 1.11; acc: 0.66
Batch: 100; loss: 0.83; acc: 0.72
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 1.08; acc: 0.66
Batch: 160; loss: 0.75; acc: 0.81
Batch: 180; loss: 0.96; acc: 0.72
Batch: 200; loss: 1.0; acc: 0.72
Batch: 220; loss: 0.92; acc: 0.7
Batch: 240; loss: 1.12; acc: 0.64
Batch: 260; loss: 0.93; acc: 0.67
Batch: 280; loss: 0.92; acc: 0.67
Batch: 300; loss: 0.86; acc: 0.7
Batch: 320; loss: 0.87; acc: 0.62
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.72
Batch: 380; loss: 1.32; acc: 0.66
Batch: 400; loss: 0.86; acc: 0.69
Batch: 420; loss: 0.91; acc: 0.66
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.69
Batch: 480; loss: 0.75; acc: 0.67
Batch: 500; loss: 0.98; acc: 0.72
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 0.88; acc: 0.78
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.69
Batch: 640; loss: 0.91; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.73
Batch: 680; loss: 1.02; acc: 0.67
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.79; acc: 0.78
Batch: 740; loss: 0.89; acc: 0.72
Batch: 760; loss: 0.83; acc: 0.72
Batch: 780; loss: 0.68; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.78; acc: 0.72
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7463450904484767; val_accuracy: 0.7685111464968153 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.02; acc: 0.61
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.72; acc: 0.72
Batch: 60; loss: 0.65; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.75
Batch: 100; loss: 0.77; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 1.01; acc: 0.66
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.81; acc: 0.73
Batch: 220; loss: 0.82; acc: 0.73
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.9; acc: 0.67
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.94; acc: 0.72
Batch: 320; loss: 0.79; acc: 0.7
Batch: 340; loss: 0.85; acc: 0.8
Batch: 360; loss: 0.94; acc: 0.7
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 0.83; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.83; acc: 0.69
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.91; acc: 0.64
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.99; acc: 0.7
Batch: 640; loss: 0.77; acc: 0.77
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.83; acc: 0.72
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 0.82; acc: 0.77
Batch: 780; loss: 0.77; acc: 0.7
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.7478595434860059; val_accuracy: 0.7690087579617835 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 0.72; acc: 0.72
Batch: 260; loss: 0.65; acc: 0.8
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 1.0; acc: 0.66
Batch: 340; loss: 0.71; acc: 0.77
Batch: 360; loss: 0.82; acc: 0.64
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.96; acc: 0.66
Batch: 420; loss: 0.86; acc: 0.72
Batch: 440; loss: 0.85; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.56
Batch: 480; loss: 0.91; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.73
Batch: 520; loss: 1.06; acc: 0.62
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.87; acc: 0.78
Batch: 580; loss: 0.77; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.73
Batch: 620; loss: 1.05; acc: 0.72
Batch: 640; loss: 1.18; acc: 0.67
Batch: 660; loss: 0.88; acc: 0.72
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 0.65; acc: 0.78
Batch: 760; loss: 0.83; acc: 0.69
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 0.79; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7470555692721325; val_accuracy: 0.7671178343949044 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.0; acc: 0.66
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 0.8; acc: 0.7
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.69
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.91; acc: 0.73
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 1.01; acc: 0.73
Batch: 280; loss: 0.61; acc: 0.75
Batch: 300; loss: 0.86; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.62
Batch: 340; loss: 0.63; acc: 0.73
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.92; acc: 0.73
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.79; acc: 0.72
Batch: 460; loss: 0.82; acc: 0.7
Batch: 480; loss: 1.23; acc: 0.67
Batch: 500; loss: 0.81; acc: 0.73
Batch: 520; loss: 0.76; acc: 0.78
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 1.01; acc: 0.66
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.92; acc: 0.7
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.77; acc: 0.7
Batch: 760; loss: 0.92; acc: 0.73
Batch: 780; loss: 0.81; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.79; acc: 0.72
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.7480566651578162; val_accuracy: 0.7666202229299363 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.9; acc: 0.69
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 1.12; acc: 0.69
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 0.86; acc: 0.77
Batch: 220; loss: 0.75; acc: 0.77
Batch: 240; loss: 0.66; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 0.94; acc: 0.7
Batch: 300; loss: 0.7; acc: 0.81
Batch: 320; loss: 1.0; acc: 0.72
Batch: 340; loss: 0.83; acc: 0.72
Batch: 360; loss: 0.8; acc: 0.77
Batch: 380; loss: 0.82; acc: 0.77
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 1.03; acc: 0.67
Batch: 500; loss: 0.8; acc: 0.75
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.63; acc: 0.77
Batch: 560; loss: 0.96; acc: 0.66
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 1.08; acc: 0.69
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.7; acc: 0.77
Batch: 660; loss: 0.81; acc: 0.69
Batch: 680; loss: 0.75; acc: 0.7
Batch: 700; loss: 0.81; acc: 0.69
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 0.76; acc: 0.72
Batch: 760; loss: 1.05; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.64
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.72
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.77; acc: 0.72
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7486144334647307; val_accuracy: 0.7646297770700637 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.25; acc: 0.59
Batch: 20; loss: 0.79; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.66
Batch: 80; loss: 0.89; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.73
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 0.87; acc: 0.72
Batch: 180; loss: 0.7; acc: 0.78
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.77
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.72
Batch: 280; loss: 0.89; acc: 0.78
Batch: 300; loss: 0.94; acc: 0.72
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 1.07; acc: 0.7
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.74; acc: 0.75
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.84; acc: 0.7
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 1.03; acc: 0.7
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.81; acc: 0.7
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 1.04; acc: 0.69
Batch: 660; loss: 0.79; acc: 0.66
Batch: 680; loss: 0.8; acc: 0.7
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 0.67; acc: 0.75
Batch: 760; loss: 0.95; acc: 0.66
Batch: 780; loss: 0.93; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7505344529250625; val_accuracy: 0.7654259554140127 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.97; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.75; acc: 0.73
Batch: 60; loss: 0.97; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.66
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 0.74; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.71; acc: 0.73
Batch: 260; loss: 1.02; acc: 0.7
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 1.09; acc: 0.7
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.7
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.99; acc: 0.66
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 1.04; acc: 0.66
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.85; acc: 0.75
Batch: 520; loss: 0.92; acc: 0.67
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.75
Batch: 580; loss: 1.0; acc: 0.77
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.95; acc: 0.69
Batch: 660; loss: 0.98; acc: 0.66
Batch: 680; loss: 1.1; acc: 0.62
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.81; acc: 0.73
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.746630390738226; val_accuracy: 0.767515923566879 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.76; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.78; acc: 0.75
Batch: 160; loss: 0.84; acc: 0.69
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.88; acc: 0.81
Batch: 340; loss: 0.95; acc: 0.7
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.7
Batch: 400; loss: 1.02; acc: 0.67
Batch: 420; loss: 1.13; acc: 0.64
Batch: 440; loss: 0.84; acc: 0.69
Batch: 460; loss: 0.89; acc: 0.72
Batch: 480; loss: 0.97; acc: 0.77
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.73; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.72
Batch: 560; loss: 0.7; acc: 0.72
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.82; acc: 0.7
Batch: 620; loss: 0.86; acc: 0.77
Batch: 640; loss: 0.85; acc: 0.72
Batch: 660; loss: 0.72; acc: 0.72
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.89; acc: 0.72
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.92; acc: 0.69
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7470227596676273; val_accuracy: 0.7663216560509554 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.99; acc: 0.75
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 1.01; acc: 0.66
Batch: 140; loss: 0.89; acc: 0.8
Batch: 160; loss: 1.08; acc: 0.7
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 1.02; acc: 0.7
Batch: 280; loss: 0.8; acc: 0.73
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.83; acc: 0.77
Batch: 360; loss: 0.93; acc: 0.67
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 1.03; acc: 0.67
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 1.12; acc: 0.67
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.77
Batch: 500; loss: 0.91; acc: 0.7
Batch: 520; loss: 0.99; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.92; acc: 0.73
Batch: 580; loss: 0.93; acc: 0.8
Batch: 600; loss: 0.85; acc: 0.77
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 0.62; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.73
Batch: 680; loss: 0.97; acc: 0.7
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.86; acc: 0.69
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 0.75; acc: 0.75
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.747296475965506; val_accuracy: 0.7668192675159236 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.94; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 1.22; acc: 0.61
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.85; acc: 0.77
Batch: 260; loss: 0.86; acc: 0.75
Batch: 280; loss: 0.94; acc: 0.72
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.72
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.97; acc: 0.77
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.72
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.66; acc: 0.78
Batch: 500; loss: 0.84; acc: 0.77
Batch: 520; loss: 0.96; acc: 0.66
Batch: 540; loss: 0.63; acc: 0.77
Batch: 560; loss: 0.8; acc: 0.7
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.77; acc: 0.69
Batch: 660; loss: 1.02; acc: 0.75
Batch: 680; loss: 0.99; acc: 0.66
Batch: 700; loss: 0.79; acc: 0.75
Batch: 720; loss: 1.04; acc: 0.69
Batch: 740; loss: 0.95; acc: 0.67
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.84; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.7
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.7487168252278286; val_accuracy: 0.7673168789808917 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.71; acc: 0.75
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.84; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 0.97; acc: 0.64
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.69; acc: 0.73
Batch: 180; loss: 0.85; acc: 0.7
Batch: 200; loss: 0.73; acc: 0.72
Batch: 220; loss: 0.86; acc: 0.73
Batch: 240; loss: 0.89; acc: 0.72
Batch: 260; loss: 0.81; acc: 0.69
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.91; acc: 0.64
Batch: 320; loss: 0.93; acc: 0.67
Batch: 340; loss: 0.98; acc: 0.69
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.7
Batch: 400; loss: 0.88; acc: 0.7
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.84; acc: 0.78
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 1.04; acc: 0.69
Batch: 560; loss: 0.87; acc: 0.7
Batch: 580; loss: 0.93; acc: 0.72
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.69; acc: 0.78
Batch: 640; loss: 0.81; acc: 0.77
Batch: 660; loss: 1.05; acc: 0.66
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.79; acc: 0.75
Batch: 720; loss: 1.02; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.67
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.68; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7457484686450594; val_accuracy: 0.7670183121019108 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.99; acc: 0.64
Batch: 60; loss: 0.72; acc: 0.72
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 1.19; acc: 0.61
Batch: 200; loss: 0.85; acc: 0.75
Batch: 220; loss: 0.94; acc: 0.69
Batch: 240; loss: 0.95; acc: 0.69
Batch: 260; loss: 0.85; acc: 0.7
Batch: 280; loss: 0.83; acc: 0.67
Batch: 300; loss: 0.81; acc: 0.77
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.93; acc: 0.77
Batch: 360; loss: 0.94; acc: 0.73
Batch: 380; loss: 0.66; acc: 0.77
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.7
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.0; acc: 0.69
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.87; acc: 0.72
Batch: 540; loss: 0.85; acc: 0.64
Batch: 560; loss: 0.97; acc: 0.75
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.69; acc: 0.75
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 1.11; acc: 0.62
Batch: 760; loss: 0.63; acc: 0.78
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.7446111453946229; val_accuracy: 0.76671974522293 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 0.85; acc: 0.72
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.69
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.95; acc: 0.77
Batch: 220; loss: 0.9; acc: 0.67
Batch: 240; loss: 1.0; acc: 0.69
Batch: 260; loss: 0.85; acc: 0.69
Batch: 280; loss: 0.89; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.7
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.77; acc: 0.77
Batch: 360; loss: 0.79; acc: 0.73
Batch: 380; loss: 0.61; acc: 0.77
Batch: 400; loss: 0.73; acc: 0.75
Batch: 420; loss: 0.91; acc: 0.67
Batch: 440; loss: 0.88; acc: 0.61
Batch: 460; loss: 0.8; acc: 0.73
Batch: 480; loss: 0.89; acc: 0.7
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.92; acc: 0.66
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.87; acc: 0.69
Batch: 600; loss: 0.6; acc: 0.8
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 0.82; acc: 0.7
Batch: 700; loss: 0.81; acc: 0.69
Batch: 720; loss: 0.74; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.71; acc: 0.73
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.78; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7463439885218432; val_accuracy: 0.7674164012738853 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.7
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.72; acc: 0.77
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.82; acc: 0.72
Batch: 120; loss: 0.78; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.73; acc: 0.73
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.91; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.64
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 0.87; acc: 0.7
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.75
Batch: 440; loss: 0.77; acc: 0.72
Batch: 460; loss: 0.67; acc: 0.75
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.92; acc: 0.73
Batch: 520; loss: 0.88; acc: 0.67
Batch: 540; loss: 0.98; acc: 0.67
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 1.04; acc: 0.66
Batch: 780; loss: 0.76; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.7470929608413368; val_accuracy: 0.7672173566878981 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.67
Batch: 40; loss: 0.93; acc: 0.73
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 0.83; acc: 0.7
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.86; acc: 0.72
Batch: 180; loss: 0.95; acc: 0.75
Batch: 200; loss: 0.8; acc: 0.78
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.91; acc: 0.75
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 1.0; acc: 0.69
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.9; acc: 0.75
Batch: 440; loss: 0.9; acc: 0.77
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.92; acc: 0.7
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.8; acc: 0.73
Batch: 620; loss: 0.8; acc: 0.73
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.78; acc: 0.73
Batch: 680; loss: 0.87; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.73
Batch: 720; loss: 0.75; acc: 0.75
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.88
Val Epoch over. val_loss: 0.7476747147027095; val_accuracy: 0.7650278662420382 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.67
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.77
Batch: 180; loss: 1.01; acc: 0.7
Batch: 200; loss: 0.81; acc: 0.75
Batch: 220; loss: 0.95; acc: 0.66
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.91; acc: 0.69
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 0.92; acc: 0.72
Batch: 320; loss: 0.95; acc: 0.69
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.92; acc: 0.69
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 1.0; acc: 0.64
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.82; acc: 0.73
Batch: 500; loss: 0.84; acc: 0.7
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.98; acc: 0.72
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.82; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.77
Batch: 720; loss: 0.7; acc: 0.72
Batch: 740; loss: 0.89; acc: 0.69
Batch: 760; loss: 1.02; acc: 0.69
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7457045522654892; val_accuracy: 0.7677149681528662 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.73
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 1.02; acc: 0.73
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.8; acc: 0.75
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.95; acc: 0.75
Batch: 220; loss: 1.13; acc: 0.73
Batch: 240; loss: 0.84; acc: 0.69
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.73
Batch: 300; loss: 0.75; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.73
Batch: 360; loss: 0.79; acc: 0.7
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.88; acc: 0.75
Batch: 440; loss: 0.9; acc: 0.72
Batch: 460; loss: 0.97; acc: 0.73
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 0.93; acc: 0.7
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 0.9; acc: 0.77
Batch: 580; loss: 0.95; acc: 0.73
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 0.94; acc: 0.72
Batch: 640; loss: 0.96; acc: 0.72
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.81; acc: 0.77
Batch: 760; loss: 1.01; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.62
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7453043389662056; val_accuracy: 0.7668192675159236 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 1.06; acc: 0.7
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.93; acc: 0.7
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.69; acc: 0.77
Batch: 300; loss: 0.92; acc: 0.7
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.96; acc: 0.7
Batch: 360; loss: 0.91; acc: 0.69
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.75
Batch: 420; loss: 0.76; acc: 0.7
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 0.84; acc: 0.66
Batch: 500; loss: 0.77; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.95; acc: 0.73
Batch: 580; loss: 0.75; acc: 0.77
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.86; acc: 0.75
Batch: 640; loss: 0.78; acc: 0.77
Batch: 660; loss: 0.95; acc: 0.7
Batch: 680; loss: 0.81; acc: 0.72
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.79; acc: 0.75
Batch: 740; loss: 0.8; acc: 0.7
Batch: 760; loss: 0.63; acc: 0.75
Batch: 780; loss: 0.88; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.745430108468244; val_accuracy: 0.7664211783439491 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.73
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 1.02; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.72
Batch: 160; loss: 0.9; acc: 0.73
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.91; acc: 0.72
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.9; acc: 0.67
Batch: 280; loss: 0.83; acc: 0.72
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.93; acc: 0.67
Batch: 340; loss: 0.83; acc: 0.7
Batch: 360; loss: 1.1; acc: 0.66
Batch: 380; loss: 0.73; acc: 0.75
Batch: 400; loss: 0.59; acc: 0.77
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.71; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.78; acc: 0.75
Batch: 560; loss: 1.24; acc: 0.69
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.78; acc: 0.78
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7447572851636607; val_accuracy: 0.7678144904458599 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.88; acc: 0.7
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 1.11; acc: 0.67
Batch: 60; loss: 0.95; acc: 0.66
Batch: 80; loss: 0.7; acc: 0.73
Batch: 100; loss: 0.9; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 0.89; acc: 0.72
Batch: 160; loss: 0.64; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 1.09; acc: 0.75
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.98; acc: 0.69
Batch: 260; loss: 0.87; acc: 0.69
Batch: 280; loss: 0.77; acc: 0.73
Batch: 300; loss: 0.75; acc: 0.75
Batch: 320; loss: 0.9; acc: 0.69
Batch: 340; loss: 0.9; acc: 0.7
Batch: 360; loss: 0.76; acc: 0.77
Batch: 380; loss: 0.93; acc: 0.7
Batch: 400; loss: 0.89; acc: 0.64
Batch: 420; loss: 0.69; acc: 0.75
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.73
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.93; acc: 0.72
Batch: 560; loss: 0.82; acc: 0.69
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.62
Batch: 660; loss: 1.24; acc: 0.7
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.7
Batch: 720; loss: 0.91; acc: 0.73
Batch: 740; loss: 0.9; acc: 0.67
Batch: 760; loss: 0.92; acc: 0.75
Batch: 780; loss: 0.73; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7437928947293835; val_accuracy: 0.7684116242038217 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.82; acc: 0.67
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.95; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.67
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 1.07; acc: 0.67
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.79; acc: 0.7
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.84; acc: 0.72
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.88; acc: 0.72
Batch: 320; loss: 0.78; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.73
Batch: 360; loss: 0.63; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.72
Batch: 400; loss: 0.8; acc: 0.73
Batch: 420; loss: 0.84; acc: 0.75
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.64
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.8; acc: 0.7
Batch: 520; loss: 0.81; acc: 0.75
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.89; acc: 0.69
Batch: 620; loss: 0.7; acc: 0.7
Batch: 640; loss: 1.14; acc: 0.62
Batch: 660; loss: 0.91; acc: 0.62
Batch: 680; loss: 0.87; acc: 0.64
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.75
Batch: 760; loss: 0.79; acc: 0.72
Batch: 780; loss: 0.69; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7442771999319647; val_accuracy: 0.7682125796178344 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.79; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.69
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.76; acc: 0.75
Batch: 220; loss: 0.97; acc: 0.77
Batch: 240; loss: 0.8; acc: 0.75
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 1.02; acc: 0.58
Batch: 300; loss: 0.95; acc: 0.75
Batch: 320; loss: 0.91; acc: 0.7
Batch: 340; loss: 0.86; acc: 0.67
Batch: 360; loss: 0.98; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.8; acc: 0.69
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.69
Batch: 480; loss: 0.95; acc: 0.69
Batch: 500; loss: 0.86; acc: 0.72
Batch: 520; loss: 0.83; acc: 0.72
Batch: 540; loss: 0.74; acc: 0.77
Batch: 560; loss: 1.2; acc: 0.64
Batch: 580; loss: 0.98; acc: 0.7
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 0.97; acc: 0.67
Batch: 660; loss: 0.84; acc: 0.72
Batch: 680; loss: 0.96; acc: 0.67
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 1.0; acc: 0.67
Batch: 740; loss: 0.71; acc: 0.77
Batch: 760; loss: 1.0; acc: 0.67
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7446517118602801; val_accuracy: 0.7679140127388535 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.72
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 1.01; acc: 0.66
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.82; acc: 0.73
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.84; acc: 0.67
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 0.81; acc: 0.75
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.72
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.73
Batch: 380; loss: 0.82; acc: 0.66
Batch: 400; loss: 0.9; acc: 0.62
Batch: 420; loss: 0.91; acc: 0.72
Batch: 440; loss: 0.74; acc: 0.7
Batch: 460; loss: 0.91; acc: 0.64
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.97; acc: 0.73
Batch: 520; loss: 0.77; acc: 0.86
Batch: 540; loss: 1.05; acc: 0.67
Batch: 560; loss: 1.01; acc: 0.67
Batch: 580; loss: 0.79; acc: 0.73
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.71; acc: 0.72
Batch: 660; loss: 0.6; acc: 0.78
Batch: 680; loss: 1.0; acc: 0.67
Batch: 700; loss: 0.8; acc: 0.83
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.9; acc: 0.69
Batch: 760; loss: 1.05; acc: 0.64
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7442471571029372; val_accuracy: 0.768312101910828 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.73
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.98; acc: 0.67
Batch: 160; loss: 0.94; acc: 0.72
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 0.8; acc: 0.73
Batch: 220; loss: 0.96; acc: 0.66
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.77
Batch: 300; loss: 0.89; acc: 0.73
Batch: 320; loss: 0.83; acc: 0.69
Batch: 340; loss: 0.95; acc: 0.75
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.99; acc: 0.72
Batch: 440; loss: 0.9; acc: 0.7
Batch: 460; loss: 0.78; acc: 0.69
Batch: 480; loss: 0.83; acc: 0.78
Batch: 500; loss: 0.75; acc: 0.7
Batch: 520; loss: 0.82; acc: 0.81
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.82; acc: 0.72
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.69; acc: 0.75
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 0.78; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.67
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 1.05; acc: 0.67
Batch: 720; loss: 0.83; acc: 0.7
Batch: 740; loss: 0.96; acc: 0.66
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.73; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7440420892208245; val_accuracy: 0.7695063694267515 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.71; acc: 0.73
Batch: 220; loss: 0.94; acc: 0.7
Batch: 240; loss: 0.86; acc: 0.7
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 1.07; acc: 0.7
Batch: 320; loss: 0.94; acc: 0.67
Batch: 340; loss: 0.93; acc: 0.73
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.86; acc: 0.78
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.78
Batch: 520; loss: 1.03; acc: 0.66
Batch: 540; loss: 0.97; acc: 0.75
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 1.09; acc: 0.72
Batch: 600; loss: 0.94; acc: 0.69
Batch: 620; loss: 0.68; acc: 0.77
Batch: 640; loss: 0.87; acc: 0.73
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.95; acc: 0.72
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.73
Batch: 740; loss: 1.28; acc: 0.64
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.92; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.7
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7447598648678725; val_accuracy: 0.7669187898089171 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.72; acc: 0.73
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.86; acc: 0.77
Batch: 200; loss: 0.83; acc: 0.7
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.67
Batch: 260; loss: 0.82; acc: 0.7
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.9; acc: 0.72
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.99; acc: 0.7
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.77
Batch: 400; loss: 0.7; acc: 0.8
Batch: 420; loss: 1.04; acc: 0.62
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.85; acc: 0.77
Batch: 480; loss: 0.85; acc: 0.75
Batch: 500; loss: 1.06; acc: 0.72
Batch: 520; loss: 0.82; acc: 0.8
Batch: 540; loss: 0.85; acc: 0.69
Batch: 560; loss: 0.96; acc: 0.69
Batch: 580; loss: 0.76; acc: 0.75
Batch: 600; loss: 0.8; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.7
Batch: 640; loss: 0.86; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.67
Batch: 720; loss: 0.91; acc: 0.77
Batch: 740; loss: 0.52; acc: 0.81
Batch: 760; loss: 0.93; acc: 0.75
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.7446988037057743; val_accuracy: 0.7678144904458599 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 26201
elements in E: 5553250
fraction nonzero: 0.0047181380272813215
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.05
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.28; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.09
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.25; acc: 0.2
Batch: 320; loss: 2.28; acc: 0.09
Batch: 340; loss: 2.26; acc: 0.19
Batch: 360; loss: 2.25; acc: 0.33
Batch: 380; loss: 2.24; acc: 0.33
Batch: 400; loss: 2.25; acc: 0.17
Batch: 420; loss: 2.24; acc: 0.34
Batch: 440; loss: 2.22; acc: 0.34
Batch: 460; loss: 2.18; acc: 0.41
Batch: 480; loss: 2.19; acc: 0.38
Batch: 500; loss: 2.19; acc: 0.27
Batch: 520; loss: 2.16; acc: 0.41
Batch: 540; loss: 2.07; acc: 0.47
Batch: 560; loss: 2.04; acc: 0.52
Batch: 580; loss: 2.02; acc: 0.38
Batch: 600; loss: 1.93; acc: 0.41
Batch: 620; loss: 1.88; acc: 0.3
Batch: 640; loss: 1.81; acc: 0.42
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.56; acc: 0.55
Batch: 700; loss: 1.52; acc: 0.44
Batch: 720; loss: 1.07; acc: 0.69
Batch: 740; loss: 1.26; acc: 0.58
Batch: 760; loss: 1.06; acc: 0.61
Batch: 780; loss: 1.25; acc: 0.53
Train Epoch over. train_loss: 2.05; train_accuracy: 0.28 

Batch: 0; loss: 1.43; acc: 0.52
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 0.99; acc: 0.69
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.93; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.58
Batch: 140; loss: 0.93; acc: 0.69
Val Epoch over. val_loss: 1.1074911059847303; val_accuracy: 0.6117635350318471 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.53
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 1.26; acc: 0.61
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 1.04; acc: 0.62
Batch: 100; loss: 1.03; acc: 0.64
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 1.06; acc: 0.56
Batch: 160; loss: 1.0; acc: 0.62
Batch: 180; loss: 1.08; acc: 0.59
Batch: 200; loss: 0.83; acc: 0.69
Batch: 220; loss: 0.96; acc: 0.7
Batch: 240; loss: 1.0; acc: 0.64
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.88; acc: 0.7
Batch: 300; loss: 0.75; acc: 0.78
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.84; acc: 0.7
Batch: 360; loss: 1.1; acc: 0.62
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.75
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 0.89; acc: 0.72
Batch: 480; loss: 0.89; acc: 0.7
Batch: 500; loss: 0.95; acc: 0.72
Batch: 520; loss: 0.72; acc: 0.7
Batch: 540; loss: 0.65; acc: 0.75
Batch: 560; loss: 0.92; acc: 0.7
Batch: 580; loss: 1.03; acc: 0.7
Batch: 600; loss: 1.08; acc: 0.7
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.74; acc: 0.8
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 0.9; acc: 0.72
Batch: 700; loss: 0.88; acc: 0.69
Batch: 720; loss: 0.98; acc: 0.69
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.96; acc: 0.69
Batch: 780; loss: 0.75; acc: 0.73
Train Epoch over. train_loss: 0.88; train_accuracy: 0.71 

Batch: 0; loss: 1.06; acc: 0.66
Batch: 20; loss: 0.84; acc: 0.66
Batch: 40; loss: 0.67; acc: 0.75
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.59
Batch: 140; loss: 0.51; acc: 0.8
Val Epoch over. val_loss: 0.8221713304519653; val_accuracy: 0.7209394904458599 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 0.77; acc: 0.78
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.59; acc: 0.75
Batch: 120; loss: 0.83; acc: 0.72
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.72
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 1.11; acc: 0.7
Batch: 400; loss: 0.79; acc: 0.75
Batch: 420; loss: 0.65; acc: 0.73
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.7
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.9; acc: 0.62
Batch: 660; loss: 1.01; acc: 0.7
Batch: 680; loss: 0.74; acc: 0.7
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 0.57; acc: 0.75
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.78
Batch: 60; loss: 0.62; acc: 0.73
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.7293482198836697; val_accuracy: 0.7661226114649682 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.72
Batch: 260; loss: 0.71; acc: 0.78
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.75
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.99; acc: 0.69
Batch: 440; loss: 0.69; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.67; acc: 0.77
Batch: 520; loss: 0.58; acc: 0.8
Batch: 540; loss: 0.71; acc: 0.72
Batch: 560; loss: 0.92; acc: 0.61
Batch: 580; loss: 0.59; acc: 0.75
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.94; acc: 0.72
Batch: 640; loss: 0.92; acc: 0.73
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.66; acc: 0.77
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.59; acc: 0.75
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.73
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.45; acc: 0.89
Val Epoch over. val_loss: 0.88958100832192; val_accuracy: 0.716859076433121 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.66
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.99; acc: 0.66
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.67
Batch: 120; loss: 0.6; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.75
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.5; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.77
Batch: 360; loss: 0.89; acc: 0.72
Batch: 380; loss: 0.77; acc: 0.72
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.99; acc: 0.69
Batch: 500; loss: 0.82; acc: 0.75
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.55; acc: 0.8
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.65; acc: 0.75
Batch: 720; loss: 0.96; acc: 0.61
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.67; train_accuracy: 0.78 

Batch: 0; loss: 0.64; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.84
Val Epoch over. val_loss: 0.6730018802888834; val_accuracy: 0.7830414012738853 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.85; acc: 0.7
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.77; acc: 0.77
Batch: 420; loss: 0.94; acc: 0.8
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 1.02; acc: 0.64
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.75
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.75
Batch: 20; loss: 0.72; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.73
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.83
Val Epoch over. val_loss: 0.6818700711818257; val_accuracy: 0.7773686305732485 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.77
Batch: 20; loss: 0.67; acc: 0.75
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.7; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.67
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.72
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.84; acc: 0.77
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.63; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.77
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.83; acc: 0.73
Batch: 640; loss: 0.57; acc: 0.77
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.64; acc: 0.77
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.73
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.6023059388634505; val_accuracy: 0.806031050955414 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.77
Batch: 20; loss: 0.84; acc: 0.66
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.77
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.91; acc: 0.73
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.7
Batch: 300; loss: 0.7; acc: 0.72
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.91; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.95; acc: 0.62
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.72
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.58; acc: 0.78
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.53; acc: 0.78
Batch: 680; loss: 0.66; acc: 0.73
Batch: 700; loss: 0.66; acc: 0.75
Batch: 720; loss: 0.58; acc: 0.78
Batch: 740; loss: 0.74; acc: 0.72
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.7; acc: 0.72
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.77
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.6705757947104751; val_accuracy: 0.783140923566879 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.7
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.77
Batch: 240; loss: 0.64; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 0.94; acc: 0.72
Batch: 440; loss: 0.66; acc: 0.77
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.73; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.77
Batch: 580; loss: 0.72; acc: 0.73
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.77
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.73
Batch: 740; loss: 0.69; acc: 0.78
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.6313050976794237; val_accuracy: 0.7975716560509554 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.74; acc: 0.7
Batch: 60; loss: 0.89; acc: 0.73
Batch: 80; loss: 0.77; acc: 0.73
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.7
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.92; acc: 0.67
Batch: 260; loss: 0.77; acc: 0.73
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 0.67; acc: 0.88
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.59; acc: 0.78
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.47; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 1.01; acc: 0.77
Batch: 540; loss: 0.64; acc: 0.78
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.78
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.79; acc: 0.7
Batch: 700; loss: 0.58; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.75
Batch: 740; loss: 0.8; acc: 0.73
Batch: 760; loss: 0.73; acc: 0.73
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 1.01; acc: 0.67
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.86
Val Epoch over. val_loss: 0.6575947600375315; val_accuracy: 0.7920979299363057 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.83; acc: 0.72
Batch: 340; loss: 0.67; acc: 0.7
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 1.12; acc: 0.64
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.56; acc: 0.78
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.8
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.92; acc: 0.73
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.78
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5815343684071947; val_accuracy: 0.8149880573248408 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.81; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.72; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.73
Batch: 260; loss: 0.72; acc: 0.72
Batch: 280; loss: 0.61; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.78
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.59; acc: 0.8
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.41; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.73
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.72
Batch: 480; loss: 0.5; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.75
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.73
Batch: 580; loss: 1.0; acc: 0.7
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.78
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.49; acc: 0.78
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.5748898912766937; val_accuracy: 0.8198646496815286 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.92; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.67; acc: 0.75
Batch: 360; loss: 0.9; acc: 0.78
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.78; acc: 0.73
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.5728544638415051; val_accuracy: 0.8269307324840764 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.71; acc: 0.77
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.82; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.78
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.75
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.72; acc: 0.77
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.56; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.5960746549876632; val_accuracy: 0.8116042993630573 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.73
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.75; acc: 0.77
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.85; acc: 0.69
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.46; acc: 0.81
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.56; acc: 0.78
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.77
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.78
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.73
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.88
Val Epoch over. val_loss: 0.5698657996335607; val_accuracy: 0.8204617834394905 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.75
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.88; acc: 0.75
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.77
Batch: 460; loss: 0.86; acc: 0.7
Batch: 480; loss: 0.54; acc: 0.78
Batch: 500; loss: 0.75; acc: 0.73
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.75
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.7
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 0.82; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5656759314666129; val_accuracy: 0.8227507961783439 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.78
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.77
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.73
Batch: 320; loss: 0.29; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.75
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.88; acc: 0.8
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.73; acc: 0.78
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.44; acc: 0.8
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.86; acc: 0.8
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.8
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.75
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.5710438915119049; val_accuracy: 0.8253383757961783 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.75
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.85; acc: 0.7
Batch: 160; loss: 0.48; acc: 0.78
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.61; acc: 0.77
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.78
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.83; acc: 0.72
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.78
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.77; acc: 0.75
Batch: 440; loss: 0.49; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.69
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.79; acc: 0.72
Batch: 600; loss: 0.67; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.78; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.5662669275596643; val_accuracy: 0.8265326433121019 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.75
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.75
Batch: 240; loss: 0.88; acc: 0.72
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.82; acc: 0.73
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.75
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.68; acc: 0.78
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.8
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.7
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.5680787890769874; val_accuracy: 0.8261345541401274 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.73
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.77
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.85; acc: 0.73
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.72
Batch: 460; loss: 1.04; acc: 0.62
Batch: 480; loss: 0.61; acc: 0.78
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.54; acc: 0.77
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.5904324437212792; val_accuracy: 0.814390923566879 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.95; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.78
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.77
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.67; acc: 0.8
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.78
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.81
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.73
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.97; acc: 0.7
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.7
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.5516043356649435; val_accuracy: 0.8289211783439491 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.7; acc: 0.73
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 1.07; acc: 0.67
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.8
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.77
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.6; acc: 0.77
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5464084095256344; val_accuracy: 0.8305135350318471 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.6; acc: 0.75
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.78; acc: 0.7
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.78
Batch: 460; loss: 0.6; acc: 0.77
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.77
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.5598643965022579; val_accuracy: 0.8297173566878981 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.77
Batch: 180; loss: 0.93; acc: 0.77
Batch: 200; loss: 0.76; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.77; acc: 0.73
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.8
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.83
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.8
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.61; acc: 0.75
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.87; acc: 0.72
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.77
Batch: 780; loss: 0.63; acc: 0.77
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.7
Batch: 40; loss: 0.28; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.5459891553897007; val_accuracy: 0.8324044585987261 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.83
Batch: 240; loss: 1.03; acc: 0.72
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.78
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.54; acc: 0.77
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.72
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.5437257107655713; val_accuracy: 0.8334992038216561 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.75
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.95; acc: 0.78
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.69; acc: 0.78
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.89; acc: 0.8
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.85; acc: 0.8
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.78
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.5485661007036828; val_accuracy: 0.8325039808917197 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.78
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.93; acc: 0.77
Batch: 420; loss: 0.64; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.94; acc: 0.8
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.91; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.72
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.88
Val Epoch over. val_loss: 0.5456406792068178; val_accuracy: 0.8298168789808917 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.98; acc: 0.7
Batch: 580; loss: 0.47; acc: 0.81
Batch: 600; loss: 0.62; acc: 0.78
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.7
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.88
Val Epoch over. val_loss: 0.5566726505376731; val_accuracy: 0.8249402866242038 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.75
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.8
Batch: 160; loss: 0.67; acc: 0.73
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.5; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.69; acc: 0.75
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.5; acc: 0.8
Batch: 400; loss: 0.41; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.78
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.67; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.81; acc: 0.75
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.49; acc: 0.81
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.75
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.5514502835691355; val_accuracy: 0.8289211783439491 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.78
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.87; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.9; acc: 0.77
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.81; acc: 0.77
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.62; acc: 0.7
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.78
Batch: 640; loss: 0.81; acc: 0.78
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.85; acc: 0.75
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.7
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5493827546667901; val_accuracy: 0.82703025477707 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.77
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.77
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.75
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.77
Batch: 320; loss: 0.53; acc: 0.78
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.77
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.75
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.86; acc: 0.78
Batch: 660; loss: 0.65; acc: 0.72
Batch: 680; loss: 0.64; acc: 0.78
Batch: 700; loss: 0.75; acc: 0.72
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.31; acc: 0.84
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5394632863770624; val_accuracy: 0.8333001592356688 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.77
Batch: 160; loss: 0.76; acc: 0.72
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.78
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.51; acc: 0.78
Batch: 560; loss: 0.73; acc: 0.73
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.72
Batch: 640; loss: 0.62; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.81
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.78
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.72
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5393092176716798; val_accuracy: 0.8349920382165605 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.86; acc: 0.72
Batch: 220; loss: 0.59; acc: 0.75
Batch: 240; loss: 0.6; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.8
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.75
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.78
Batch: 620; loss: 0.88; acc: 0.78
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.77
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5398962259482426; val_accuracy: 0.8323049363057324 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.88; acc: 0.7
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 1.07; acc: 0.67
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 1.0; acc: 0.67
Batch: 180; loss: 0.66; acc: 0.73
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.66; acc: 0.73
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.77
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.75
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.73
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.67
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.540740693071086; val_accuracy: 0.8343949044585988 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.78
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.73
Batch: 340; loss: 0.63; acc: 0.78
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.72
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.84; acc: 0.77
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.88; acc: 0.73
Batch: 740; loss: 0.82; acc: 0.77
Batch: 760; loss: 0.49; acc: 0.81
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5384150766263343; val_accuracy: 0.8333996815286624 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.6; acc: 0.78
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.95; acc: 0.75
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.69; acc: 0.75
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.98; acc: 0.78
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.73
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 1.02; acc: 0.73
Batch: 480; loss: 0.48; acc: 0.8
Batch: 500; loss: 0.41; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.73
Batch: 580; loss: 0.68; acc: 0.75
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.5390047923584652; val_accuracy: 0.8358877388535032 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.83; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.8; acc: 0.77
Batch: 300; loss: 0.94; acc: 0.7
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.8; acc: 0.69
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.75
Batch: 540; loss: 0.6; acc: 0.75
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.75
Batch: 640; loss: 0.41; acc: 0.81
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.75; acc: 0.73
Batch: 780; loss: 0.54; acc: 0.77
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.72
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5408329003176112; val_accuracy: 0.8334992038216561 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.75
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.78
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.67; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.72; acc: 0.73
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.75
Batch: 520; loss: 0.72; acc: 0.73
Batch: 540; loss: 0.59; acc: 0.75
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.8
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.77
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.78
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.78
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5368099240170923; val_accuracy: 0.837281050955414 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.81; acc: 0.72
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.43; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.78
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.78
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.538095277585801; val_accuracy: 0.8365843949044586 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.78
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.75
Batch: 200; loss: 0.54; acc: 0.78
Batch: 220; loss: 0.48; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 0.86; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.73
Batch: 320; loss: 0.67; acc: 0.78
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.81
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.74; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5368243953224959; val_accuracy: 0.8355891719745223 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.75
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.8
Batch: 300; loss: 0.62; acc: 0.77
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.75
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.78
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.81
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.84; acc: 0.7
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.535459610022557; val_accuracy: 0.8373805732484076 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.85; acc: 0.75
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.77
Batch: 240; loss: 0.92; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.71; acc: 0.72
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.78
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.75
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.8
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5356756198178431; val_accuracy: 0.8378781847133758 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.8
Batch: 160; loss: 0.86; acc: 0.69
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 1.1; acc: 0.69
Batch: 220; loss: 0.69; acc: 0.75
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.73
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.78
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.77
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5358546892548822; val_accuracy: 0.8367834394904459 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.91; acc: 0.78
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.93; acc: 0.77
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.95; acc: 0.75
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.76; acc: 0.77
Batch: 680; loss: 0.61; acc: 0.73
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.75
Batch: 780; loss: 0.59; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5367389181806783; val_accuracy: 0.8374800955414012 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.78
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.75
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.73; acc: 0.7
Batch: 460; loss: 0.87; acc: 0.81
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.93; acc: 0.73
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.74; acc: 0.77
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.75; acc: 0.72
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 0.44; acc: 0.81
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.72
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5352526891763043; val_accuracy: 0.837281050955414 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.77
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.77
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.84; acc: 0.73
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.74; acc: 0.75
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.85; acc: 0.7
Batch: 580; loss: 0.88; acc: 0.78
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.8
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.86; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.84; acc: 0.78
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5351139724634255; val_accuracy: 0.8377786624203821 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.85; acc: 0.73
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.77
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.81
Batch: 500; loss: 0.7; acc: 0.77
Batch: 520; loss: 0.59; acc: 0.77
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.5350948571209695; val_accuracy: 0.8371815286624203 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.75
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.84; acc: 0.75
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.83; acc: 0.78
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.44; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.92; acc: 0.73
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.71; acc: 0.77
Batch: 560; loss: 0.69; acc: 0.75
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.76; acc: 0.73
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5351004255045752; val_accuracy: 0.8377786624203821 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.76; acc: 0.73
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.88; acc: 0.72
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.83
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.78; acc: 0.75
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.93; acc: 0.77
Batch: 760; loss: 0.61; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.534468290817206; val_accuracy: 0.8373805732484076 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.71; acc: 0.73
Batch: 260; loss: 0.88; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.77
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.78
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.72
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.81; acc: 0.7
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5345880232609002; val_accuracy: 0.8384753184713376 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_125_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 31709
elements in E: 6663900
fraction nonzero: 0.004758324704752472
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.16
Batch: 160; loss: 2.28; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.29; acc: 0.09
Batch: 220; loss: 2.28; acc: 0.19
Batch: 240; loss: 2.26; acc: 0.23
Batch: 260; loss: 2.26; acc: 0.2
Batch: 280; loss: 2.27; acc: 0.3
Batch: 300; loss: 2.24; acc: 0.41
Batch: 320; loss: 2.26; acc: 0.27
Batch: 340; loss: 2.25; acc: 0.27
Batch: 360; loss: 2.23; acc: 0.44
Batch: 380; loss: 2.22; acc: 0.44
Batch: 400; loss: 2.23; acc: 0.33
Batch: 420; loss: 2.19; acc: 0.38
Batch: 440; loss: 2.18; acc: 0.36
Batch: 460; loss: 2.14; acc: 0.48
Batch: 480; loss: 2.15; acc: 0.38
Batch: 500; loss: 2.09; acc: 0.44
Batch: 520; loss: 2.01; acc: 0.42
Batch: 540; loss: 1.93; acc: 0.48
Batch: 560; loss: 1.79; acc: 0.56
Batch: 580; loss: 1.63; acc: 0.59
Batch: 600; loss: 1.47; acc: 0.64
Batch: 620; loss: 1.13; acc: 0.73
Batch: 640; loss: 1.11; acc: 0.61
Batch: 660; loss: 1.15; acc: 0.58
Batch: 680; loss: 1.21; acc: 0.56
Batch: 700; loss: 1.29; acc: 0.56
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 1.14; acc: 0.66
Batch: 760; loss: 0.86; acc: 0.7
Batch: 780; loss: 1.05; acc: 0.67
Train Epoch over. train_loss: 1.93; train_accuracy: 0.37 

Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 1.03; acc: 0.69
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.7
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 1.05; acc: 0.61
Batch: 140; loss: 0.57; acc: 0.84
Val Epoch over. val_loss: 0.8679419941962905; val_accuracy: 0.721437101910828 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 1.19; acc: 0.62
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 1.11; acc: 0.59
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 1.21; acc: 0.64
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.84; acc: 0.72
Batch: 200; loss: 0.94; acc: 0.69
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 1.11; acc: 0.69
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.75
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.7
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 1.04; acc: 0.7
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.86; acc: 0.7
Batch: 500; loss: 1.02; acc: 0.66
Batch: 520; loss: 0.65; acc: 0.73
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.85; acc: 0.67
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.8
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.98; acc: 0.69
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 0.82; acc: 0.69
Batch: 760; loss: 0.9; acc: 0.7
Batch: 780; loss: 0.72; acc: 0.75
Train Epoch over. train_loss: 0.8; train_accuracy: 0.74 

Batch: 0; loss: 0.9; acc: 0.61
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.72
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.7; acc: 0.73
Val Epoch over. val_loss: 0.9720644821786577; val_accuracy: 0.6804339171974523 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.64
Batch: 20; loss: 0.75; acc: 0.72
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.85; acc: 0.75
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.57; acc: 0.75
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.67
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.78
Batch: 460; loss: 0.59; acc: 0.78
Batch: 480; loss: 0.91; acc: 0.72
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 1.0; acc: 0.67
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.91; acc: 0.8
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.64; acc: 0.75
Batch: 20; loss: 0.6; acc: 0.73
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.5950170759182827; val_accuracy: 0.8129976114649682 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.71; acc: 0.77
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 0.74; acc: 0.73
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.9; acc: 0.78
Batch: 280; loss: 0.89; acc: 0.78
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.77
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.93; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.75
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.78
Batch: 640; loss: 0.78; acc: 0.84
Batch: 660; loss: 0.81; acc: 0.73
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.52; acc: 0.8
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.65; acc: 0.53
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.56
Batch: 80; loss: 1.27; acc: 0.58
Batch: 100; loss: 1.72; acc: 0.47
Batch: 120; loss: 1.6; acc: 0.5
Batch: 140; loss: 1.03; acc: 0.61
Val Epoch over. val_loss: 1.5969788154978661; val_accuracy: 0.5289609872611465 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.13; acc: 0.44
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.78
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.77
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.75; acc: 0.73
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.69
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 1.19; acc: 0.56
Batch: 140; loss: 0.46; acc: 0.86
Val Epoch over. val_loss: 0.7515010454092815; val_accuracy: 0.7525875796178344 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.7
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.73
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.83; acc: 0.75
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.78
Batch: 240; loss: 0.56; acc: 0.8
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.8
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.78
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.73
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.61
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6089157099556771; val_accuracy: 0.8054339171974523 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.75
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.78
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.85; acc: 0.67
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.9; acc: 0.66
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.54; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.49777991813459216; val_accuracy: 0.851015127388535 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.74; acc: 0.73
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.77
Batch: 240; loss: 0.57; acc: 0.78
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 0.59; acc: 0.77
Batch: 320; loss: 0.69; acc: 0.78
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.88; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.49; acc: 0.78
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.81
Batch: 540; loss: 0.6; acc: 0.77
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.78
Batch: 700; loss: 0.53; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.89
Val Epoch over. val_loss: 0.48244805491653975; val_accuracy: 0.8548964968152867 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.73
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.49; acc: 0.77
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.76; acc: 0.73
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.75
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.85; acc: 0.72
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.7
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5406017754298107; val_accuracy: 0.835390127388535 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 1.01; acc: 0.7
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.99; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.78
Batch: 480; loss: 0.63; acc: 0.75
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.77
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.72
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.5556997957692784; val_accuracy: 0.82703025477707 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.88; acc: 0.72
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.72; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4283757234454914; val_accuracy: 0.8698248407643312 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.75
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.77
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.75
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4431835966315239; val_accuracy: 0.8661425159235668 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.77
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.42; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.44698055942726744; val_accuracy: 0.8654458598726115 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.78
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.8
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.77
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.46956867435175903; val_accuracy: 0.8538017515923567 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.81
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.4; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.8
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.44078080327647506; val_accuracy: 0.8626592356687898 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.81
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.75
Batch: 600; loss: 0.64; acc: 0.75
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4580516934774484; val_accuracy: 0.8565883757961783 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.78
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.73
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.63; acc: 0.75
Batch: 700; loss: 0.39; acc: 0.83
Batch: 720; loss: 0.37; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.44626288272582804; val_accuracy: 0.8625597133757962 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.73
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.71; acc: 0.73
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.41; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.44953160405538645; val_accuracy: 0.8645501592356688 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.72
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.53; acc: 0.81
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.75
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.42441914325496954; val_accuracy: 0.8695262738853503 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.75
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.81
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.77
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.78
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.81; acc: 0.77
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.78
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 1.04; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.4946279820932704; val_accuracy: 0.8471337579617835 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.41243485404047997; val_accuracy: 0.8744028662420382 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.63; acc: 0.78
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.44; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.8
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.417546409520374; val_accuracy: 0.8757961783439491 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4146372894192957; val_accuracy: 0.8743033439490446 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.73
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.42693061414797595; val_accuracy: 0.870421974522293 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.93; acc: 0.72
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.92; acc: 0.72
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4217863640967448; val_accuracy: 0.8697253184713376 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.78
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.8
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.83
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.41270628324739494; val_accuracy: 0.8764928343949044 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.81
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.8
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.75
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.41121283368130396; val_accuracy: 0.875 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.75; acc: 0.78
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4431346221144792; val_accuracy: 0.8619625796178344 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.83; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.38; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.4157750220245616; val_accuracy: 0.8728105095541401 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.71; acc: 0.7
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.41837530530941713; val_accuracy: 0.8752985668789809 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.8
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.78
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.83
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.8
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.406796906736626; val_accuracy: 0.8773885350318471 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.8
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.83
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.32; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.4098116803890581; val_accuracy: 0.876890923566879 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.8
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.8
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40731989075044156; val_accuracy: 0.8777866242038217 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.97; acc: 0.77
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.84; acc: 0.78
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.7; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40867237547400653; val_accuracy: 0.8783837579617835 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.63; acc: 0.75
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.78
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40641088911872003; val_accuracy: 0.8757961783439491 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.94; acc: 0.78
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.76; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.40578839125906585; val_accuracy: 0.8774880573248408 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.55; acc: 0.8
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.77
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.61; acc: 0.8
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.49; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4086651659695206; val_accuracy: 0.878781847133758 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.77
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.40557001796877307; val_accuracy: 0.8784832802547771 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.8
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.4051751382411665; val_accuracy: 0.8771894904458599 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.64; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.6; acc: 0.8
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4037269307359768; val_accuracy: 0.8755971337579618 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.77
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.78
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.68; acc: 0.75
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4028688974346325; val_accuracy: 0.8780851910828026 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.66; acc: 0.78
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4033302068235768; val_accuracy: 0.8777866242038217 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4034711468466528; val_accuracy: 0.8779856687898089 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.78
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.73
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.81
Batch: 200; loss: 0.84; acc: 0.81
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.78
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.73; acc: 0.75
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.78
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4037319729282598; val_accuracy: 0.8783837579617835 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.8
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.8
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40405040808544035; val_accuracy: 0.8765923566878981 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.8
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.73
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40328963513776755; val_accuracy: 0.8778861464968153 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.83
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4029126712091409; val_accuracy: 0.8784832802547771 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.73
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.40363380725786185; val_accuracy: 0.8764928343949044 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.8
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40235171372153955; val_accuracy: 0.8781847133757962 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.40402337301308944; val_accuracy: 0.8772890127388535 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_150_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 36796
elements in E: 7774550
fraction nonzero: 0.004732878430262845
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.09
Batch: 160; loss: 2.28; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.06
Batch: 220; loss: 2.28; acc: 0.2
Batch: 240; loss: 2.27; acc: 0.19
Batch: 260; loss: 2.27; acc: 0.16
Batch: 280; loss: 2.28; acc: 0.16
Batch: 300; loss: 2.24; acc: 0.34
Batch: 320; loss: 2.27; acc: 0.08
Batch: 340; loss: 2.26; acc: 0.16
Batch: 360; loss: 2.23; acc: 0.34
Batch: 380; loss: 2.24; acc: 0.3
Batch: 400; loss: 2.24; acc: 0.16
Batch: 420; loss: 2.22; acc: 0.31
Batch: 440; loss: 2.19; acc: 0.34
Batch: 460; loss: 2.17; acc: 0.41
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.13; acc: 0.25
Batch: 520; loss: 2.1; acc: 0.27
Batch: 540; loss: 1.97; acc: 0.45
Batch: 560; loss: 1.88; acc: 0.45
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.61; acc: 0.55
Batch: 620; loss: 1.4; acc: 0.55
Batch: 640; loss: 1.42; acc: 0.56
Batch: 660; loss: 1.1; acc: 0.64
Batch: 680; loss: 1.37; acc: 0.52
Batch: 700; loss: 1.27; acc: 0.62
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 1.05; acc: 0.69
Batch: 760; loss: 0.85; acc: 0.75
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 1.94; train_accuracy: 0.32 

Batch: 0; loss: 0.9; acc: 0.69
Batch: 20; loss: 0.87; acc: 0.64
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.69
Batch: 140; loss: 0.43; acc: 0.88
Val Epoch over. val_loss: 0.7453751588702962; val_accuracy: 0.7647292993630573 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.7
Batch: 20; loss: 0.85; acc: 0.73
Batch: 40; loss: 1.48; acc: 0.53
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.72
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.9; acc: 0.7
Batch: 160; loss: 0.81; acc: 0.7
Batch: 180; loss: 0.95; acc: 0.72
Batch: 200; loss: 0.67; acc: 0.72
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 0.82; acc: 0.73
Batch: 260; loss: 0.75; acc: 0.77
Batch: 280; loss: 0.62; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.75
Batch: 360; loss: 1.07; acc: 0.7
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.66; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.92; acc: 0.7
Batch: 520; loss: 0.56; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.77
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.78
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 0.77; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.89; acc: 0.66
Batch: 20; loss: 0.88; acc: 0.67
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.73
Batch: 120; loss: 0.81; acc: 0.7
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.683181557211147; val_accuracy: 0.7707006369426752 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.72
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.72
Batch: 180; loss: 0.69; acc: 0.73
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.57; acc: 0.8
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.44; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.9; acc: 0.72
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.73
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.81
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.8
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.86; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.8
Val Epoch over. val_loss: 0.7052502875115462; val_accuracy: 0.772093949044586 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.84; acc: 0.72
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.78
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.83; acc: 0.77
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.77; acc: 0.75
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.8
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.73
Batch: 720; loss: 0.62; acc: 0.78
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.74; acc: 0.72
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.84
Val Epoch over. val_loss: 0.6552902357593463; val_accuracy: 0.7855294585987261 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.7
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.7
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.77
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.69; acc: 0.77
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.55
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 0.54; acc: 0.8
Val Epoch over. val_loss: 1.0881929262808174; val_accuracy: 0.6902866242038217 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.69; acc: 0.73
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.8
Val Epoch over. val_loss: 0.5584998939447342; val_accuracy: 0.8192675159235668 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.67; acc: 0.73
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.73
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.69; acc: 0.73
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.8
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.389896868235746; val_accuracy: 0.8807722929936306 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.78
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.85 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.72
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 0.53; acc: 0.83
Val Epoch over. val_loss: 0.8099165346212448; val_accuracy: 0.7592555732484076 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.82; acc: 0.72
Batch: 560; loss: 0.51; acc: 0.8
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.410982237025431; val_accuracy: 0.8740047770700637 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.67; acc: 0.8
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.8
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.72
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.73
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.88
Val Epoch over. val_loss: 0.6370702815853106; val_accuracy: 0.7946855095541401 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.51; acc: 0.8
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.88; acc: 0.73
Batch: 440; loss: 0.57; acc: 0.8
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.77
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.39808980570097635; val_accuracy: 0.8751990445859873 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.75
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.4095269293542121; val_accuracy: 0.8734076433121019 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.8
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.78
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.83
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.39764400044823905; val_accuracy: 0.8783837579617835 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.39; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.83
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.78
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.81
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.42737155744604244; val_accuracy: 0.8641520700636943 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.78
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.43; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.37040860068266557; val_accuracy: 0.8869426751592356 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.83
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.81; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3831332708904698; val_accuracy: 0.8808718152866242 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.3711255499796503; val_accuracy: 0.8888335987261147 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.78
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.40543519558420604; val_accuracy: 0.8753980891719745 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.83
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.3646886296997404; val_accuracy: 0.8865445859872612 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.78; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.40162749574252754; val_accuracy: 0.8764928343949044 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.3483028366782103; val_accuracy: 0.8929140127388535 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.8
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3519855596267494; val_accuracy: 0.8923168789808917 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.8
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.8
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.36515654680455567; val_accuracy: 0.88953025477707 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.34748282506587397; val_accuracy: 0.8942078025477707 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.78
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.38417027800515957; val_accuracy: 0.8815684713375797 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.3459427872565901; val_accuracy: 0.8946058917197452 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.81
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.8
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.34560542605864775; val_accuracy: 0.8949044585987261 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.81
Batch: 600; loss: 0.41; acc: 0.78
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3834080777730152; val_accuracy: 0.8830613057324841 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.8
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3554142933742256; val_accuracy: 0.8926154458598726 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3504293238756004; val_accuracy: 0.8912221337579618 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.31; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.26; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.84
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3426585868950103; val_accuracy: 0.8957006369426752 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.81
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3477935728373801; val_accuracy: 0.8943073248407644 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.83
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.34461528860080015; val_accuracy: 0.8953025477707006 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.31; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.34; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.34299481755039496; val_accuracy: 0.8950039808917197 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.81
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.34481100638391105; val_accuracy: 0.8942078025477707 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.78
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.72
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3450438136792487; val_accuracy: 0.8948049363057324 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3504912326954732; val_accuracy: 0.8937101910828026 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.81
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3478035522019787; val_accuracy: 0.892515923566879 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.77
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.81
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.3422628008541028; val_accuracy: 0.8943073248407644 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.81
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.78
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.34413686162157425; val_accuracy: 0.8956011146496815 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.34224082819025986; val_accuracy: 0.8956011146496815 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3428756963865013; val_accuracy: 0.8950039808917197 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.8
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.34251272279745454; val_accuracy: 0.8960987261146497 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.43; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3424463118812081; val_accuracy: 0.8957006369426752 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.71; acc: 0.8
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3437279906527252; val_accuracy: 0.8946058917197452 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.77
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.34141878959289784; val_accuracy: 0.896297770700637 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.8
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.78
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3415540022075556; val_accuracy: 0.8961982484076433 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.81
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3415468779813712; val_accuracy: 0.8957006369426752 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3417260583703685; val_accuracy: 0.8959992038216561 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.8
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.12; acc: 1.0
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3421482526382823; val_accuracy: 0.8960987261146497 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_175_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 42117
elements in E: 8885200
fraction nonzero: 0.004740129653806329
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.28; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.12
Batch: 200; loss: 2.27; acc: 0.06
Batch: 220; loss: 2.25; acc: 0.19
Batch: 240; loss: 2.25; acc: 0.19
Batch: 260; loss: 2.24; acc: 0.27
Batch: 280; loss: 2.21; acc: 0.39
Batch: 300; loss: 2.2; acc: 0.39
Batch: 320; loss: 2.18; acc: 0.41
Batch: 340; loss: 2.16; acc: 0.45
Batch: 360; loss: 2.12; acc: 0.41
Batch: 380; loss: 2.11; acc: 0.38
Batch: 400; loss: 2.02; acc: 0.39
Batch: 420; loss: 1.77; acc: 0.55
Batch: 440; loss: 1.48; acc: 0.66
Batch: 460; loss: 1.33; acc: 0.58
Batch: 480; loss: 1.15; acc: 0.62
Batch: 500; loss: 1.08; acc: 0.64
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 1.06; acc: 0.69
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.64; acc: 0.78
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.6; acc: 0.78
Batch: 680; loss: 1.11; acc: 0.72
Batch: 700; loss: 0.91; acc: 0.69
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 1.12; acc: 0.69
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.77
Train Epoch over. train_loss: 1.62; train_accuracy: 0.46 

Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 1.01; acc: 0.61
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.5760110678376665; val_accuracy: 0.8245421974522293 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 1.05; acc: 0.66
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.44; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 0.99; acc: 0.73
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.84; acc: 0.73
Batch: 500; loss: 0.84; acc: 0.75
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.77
Batch: 740; loss: 0.39; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.51; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.5968942913659818; val_accuracy: 0.8081210191082803 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.7; acc: 0.73
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.85 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.542600795816464; val_accuracy: 0.8266321656050956 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.78
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.84
Val Epoch over. val_loss: 0.6370425826994477; val_accuracy: 0.7943869426751592 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.72
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.72; acc: 0.75
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.81
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.8
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.65; acc: 0.7
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.75
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4920565552392583; val_accuracy: 0.8408638535031847 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.8
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.84
Val Epoch over. val_loss: 0.46589433268946445; val_accuracy: 0.8546974522292994 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.81
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.35293101068514926; val_accuracy: 0.8854498407643312 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.75
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.8
Batch: 540; loss: 0.35; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.5333263234822614; val_accuracy: 0.8331011146496815 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.75
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.78
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3697225539262887; val_accuracy: 0.8821656050955414 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.8
Batch: 160; loss: 0.38; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.79; acc: 0.8
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.81
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.73
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.6217881615754146; val_accuracy: 0.8024482484076433 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.73
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.81
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.33677894735981706; val_accuracy: 0.8888335987261147 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.37569191971212434; val_accuracy: 0.881468949044586 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.8
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.81
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.3452966880456657; val_accuracy: 0.8883359872611465 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.6; acc: 0.8
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.34517093434645113; val_accuracy: 0.8865445859872612 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.32609624002769494; val_accuracy: 0.8966958598726115 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.41; acc: 0.83
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.3293126616982897; val_accuracy: 0.8953025477707006 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.32811249242087076; val_accuracy: 0.8963972929936306 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.71; acc: 0.86
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.30832518248041724; val_accuracy: 0.9008757961783439 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.32769760399297543; val_accuracy: 0.8965963375796179 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.32583161320086496; val_accuracy: 0.8960987261146497 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3040030143063539; val_accuracy: 0.9021695859872612 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.64; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.83
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.3111415341686291; val_accuracy: 0.8996815286624203 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3043816026988303; val_accuracy: 0.9014729299363057 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.30214761743310153; val_accuracy: 0.9021695859872612 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.81; acc: 0.78
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.92
Val Epoch over. val_loss: 0.3165067276757234; val_accuracy: 0.8984872611464968 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.299764916727877; val_accuracy: 0.9034633757961783 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.309705219830677; val_accuracy: 0.9010748407643312 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.83
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.31797939894875143; val_accuracy: 0.8976910828025477 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.61; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.81
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3050119549890233; val_accuracy: 0.9008757961783439 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3079355460633138; val_accuracy: 0.900577229299363 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2969806204271165; val_accuracy: 0.9037619426751592 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.30000146891281104; val_accuracy: 0.9027667197452229 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2971056337664082; val_accuracy: 0.9025676751592356 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.8
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.298910955050189; val_accuracy: 0.9026671974522293 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.29861967887278573; val_accuracy: 0.9029657643312102 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.63; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.2; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.2991587588931345; val_accuracy: 0.9056528662420382 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.81
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.77
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2998378773688511; val_accuracy: 0.9044585987261147 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.29845164564384774; val_accuracy: 0.9020700636942676 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.84
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.84
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.29841658197770454; val_accuracy: 0.9039609872611465 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.3003259462061202; val_accuracy: 0.9034633757961783 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.29757574091481553; val_accuracy: 0.9022691082802548 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2967355341003959; val_accuracy: 0.9028662420382165 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2980347595586898; val_accuracy: 0.9029657643312102 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.83
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.83
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.1; acc: 1.0
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.23; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.29766781262721226; val_accuracy: 0.9031648089171974 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3002989937545388; val_accuracy: 0.90515525477707 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.29759379101406996; val_accuracy: 0.9029657643312102 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.78
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.29628986447669897; val_accuracy: 0.9036624203821656 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.8
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2968231578636321; val_accuracy: 0.9037619426751592 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.84
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29663893015711174; val_accuracy: 0.9025676751592356 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.81
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.29685573574084384; val_accuracy: 0.9029657643312102 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 44148
elements in E: 9329460
fraction nonzero: 0.004732106681415644
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.06
Batch: 160; loss: 2.26; acc: 0.2
Batch: 180; loss: 2.28; acc: 0.11
Batch: 200; loss: 2.26; acc: 0.14
Batch: 220; loss: 2.23; acc: 0.2
Batch: 240; loss: 2.2; acc: 0.3
Batch: 260; loss: 2.22; acc: 0.2
Batch: 280; loss: 2.17; acc: 0.34
Batch: 300; loss: 2.14; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.28
Batch: 340; loss: 2.05; acc: 0.27
Batch: 360; loss: 1.94; acc: 0.52
Batch: 380; loss: 1.84; acc: 0.52
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.35; acc: 0.62
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 0.96; acc: 0.69
Batch: 500; loss: 0.9; acc: 0.69
Batch: 520; loss: 0.9; acc: 0.64
Batch: 540; loss: 0.87; acc: 0.69
Batch: 560; loss: 0.84; acc: 0.75
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.58; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.66
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.75; acc: 0.73
Batch: 680; loss: 0.92; acc: 0.7
Batch: 700; loss: 1.27; acc: 0.62
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.97; acc: 0.72
Batch: 760; loss: 0.55; acc: 0.8
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 1.55; train_accuracy: 0.46 

Batch: 0; loss: 0.79; acc: 0.62
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.72
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.02; acc: 0.61
Batch: 140; loss: 0.53; acc: 0.8
Val Epoch over. val_loss: 0.6531788792200149; val_accuracy: 0.785031847133758 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.73
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.74; acc: 0.72
Batch: 100; loss: 0.8; acc: 0.75
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.69; acc: 0.78
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.94; acc: 0.72
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.9; acc: 0.69
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.46; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.75
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.72
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.72
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.8; acc: 0.73
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.65; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.77
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.6; train_accuracy: 0.8 

Batch: 0; loss: 0.67; acc: 0.75
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.73
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.67
Batch: 140; loss: 0.36; acc: 0.88
Val Epoch over. val_loss: 0.6325840539992995; val_accuracy: 0.7871218152866242 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.75
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.83; acc: 0.78
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.77
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.97; acc: 0.64
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.5569089524875022; val_accuracy: 0.8218550955414012 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.72
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.8
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.73; acc: 0.73
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.83
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 1.01; acc: 0.72
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.96; acc: 0.73
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 0.49; acc: 0.81
Val Epoch over. val_loss: 0.786738690297315; val_accuracy: 0.7722929936305732 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.7
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.8
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.78
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.8
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.8
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.6047191078875475; val_accuracy: 0.8146894904458599 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.83; acc: 0.78
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.8
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.77
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.66
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.5077888274648387; val_accuracy: 0.8339968152866242 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.8
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.64; acc: 0.75
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.3734274782762406; val_accuracy: 0.8825636942675159 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.81
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.82; acc: 0.73
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 1.05; acc: 0.61
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 0.76; acc: 0.75
Val Epoch over. val_loss: 0.8259010388972653; val_accuracy: 0.7513933121019108 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.72
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.83
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.8
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.29; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.88
Val Epoch over. val_loss: 0.4102807613030361; val_accuracy: 0.8649482484076433 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.34180569036561215; val_accuracy: 0.8913216560509554 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.84
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.77; acc: 0.77
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.3478288514314184; val_accuracy: 0.8915207006369427 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.8
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.358173936937645; val_accuracy: 0.8846536624203821 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.8
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.45695245351381364; val_accuracy: 0.8557921974522293 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.83
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.77
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.3198309738165254; val_accuracy: 0.8973925159235668 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.76; acc: 0.73
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.8
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.3263906446659261; val_accuracy: 0.8967953821656051 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.84
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.3108832827609056; val_accuracy: 0.9012738853503185 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.92
Val Epoch over. val_loss: 0.31537489524217927; val_accuracy: 0.9018710191082803 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.83
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.84
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.83
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.31898222963331613; val_accuracy: 0.900577229299363 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.8
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3025680835934202; val_accuracy: 0.9054538216560509 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.8
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3343534002163608; val_accuracy: 0.8958996815286624 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.78
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.29918786086094606; val_accuracy: 0.9040605095541401 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.82; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.300025668968061; val_accuracy: 0.903562898089172 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.3024803929885102; val_accuracy: 0.9069466560509554 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.21; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.30115747798210496; val_accuracy: 0.9076433121019108 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.30042337725876245; val_accuracy: 0.9059514331210191 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.29252168192130745; val_accuracy: 0.9110270700636943 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.8
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.8
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.8
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.2944537051923715; val_accuracy: 0.9083399681528662 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.8
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.30568343704673134; val_accuracy: 0.9040605095541401 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.2987558170678509; val_accuracy: 0.9068471337579618 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.30570347880007354; val_accuracy: 0.9018710191082803 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.78
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.2909778433193447; val_accuracy: 0.9096337579617835 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.2942881485221872; val_accuracy: 0.9080414012738853 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.29250134335486755; val_accuracy: 0.9098328025477707 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.28; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.29203058966690565; val_accuracy: 0.9080414012738853 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.2912403451883869; val_accuracy: 0.9103304140127388 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.29069988882750464; val_accuracy: 0.9112261146496815 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.29555547239769037; val_accuracy: 0.9085390127388535 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.75
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.29315739376529765; val_accuracy: 0.9104299363057324 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.73; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2882120553760012; val_accuracy: 0.9098328025477707 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.2910586428964973; val_accuracy: 0.9097332802547771 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.08; acc: 1.0
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.2901149191389418; val_accuracy: 0.9105294585987261 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.28913192749972555; val_accuracy: 0.9106289808917197 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.8
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.2912112947102565; val_accuracy: 0.910828025477707 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.28916093189815045; val_accuracy: 0.9101313694267515 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.91
Batch: 460; loss: 0.67; acc: 0.84
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.78
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.28914039840648886; val_accuracy: 0.910828025477707 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.2908010790539775; val_accuracy: 0.9102308917197452 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.82; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2888139715287716; val_accuracy: 0.9112261146496815 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.83
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.49; acc: 0.81
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.81
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.28896078310764517; val_accuracy: 0.9113256369426752 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.83
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.78
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.83
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.28836670778929047; val_accuracy: 0.9110270700636943 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.83
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.81
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.78
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2894436312708885; val_accuracy: 0.910031847133758 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_210_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 46434
elements in E: 9773720
fraction nonzero: 0.0047509034431107095
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.09
Batch: 200; loss: 2.28; acc: 0.03
Batch: 220; loss: 2.26; acc: 0.14
Batch: 240; loss: 2.25; acc: 0.16
Batch: 260; loss: 2.24; acc: 0.22
Batch: 280; loss: 2.22; acc: 0.31
Batch: 300; loss: 2.19; acc: 0.38
Batch: 320; loss: 2.22; acc: 0.3
Batch: 340; loss: 2.16; acc: 0.47
Batch: 360; loss: 2.14; acc: 0.48
Batch: 380; loss: 2.08; acc: 0.55
Batch: 400; loss: 2.01; acc: 0.38
Batch: 420; loss: 1.9; acc: 0.47
Batch: 440; loss: 1.72; acc: 0.53
Batch: 460; loss: 1.7; acc: 0.41
Batch: 480; loss: 1.31; acc: 0.55
Batch: 500; loss: 1.05; acc: 0.62
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.85; acc: 0.66
Batch: 560; loss: 0.76; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.69
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.88; acc: 0.64
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.9; acc: 0.72
Batch: 780; loss: 0.72; acc: 0.73
Train Epoch over. train_loss: 1.62; train_accuracy: 0.44 

Batch: 0; loss: 0.64; acc: 0.73
Batch: 20; loss: 0.92; acc: 0.7
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.81
Val Epoch over. val_loss: 0.585572651996734; val_accuracy: 0.8127985668789809 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 1.18; acc: 0.7
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.83; acc: 0.73
Batch: 100; loss: 0.57; acc: 0.73
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.77; acc: 0.72
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.72
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.73
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.78
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.82 

Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.62
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.6287436225232045; val_accuracy: 0.7974721337579618 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.78
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.8
Val Epoch over. val_loss: 0.6764470424242081; val_accuracy: 0.7867237261146497 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.78
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.78
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.7
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.78
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.7; acc: 0.73
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.83
Val Epoch over. val_loss: 0.73212578389675; val_accuracy: 0.7772691082802548 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.69; acc: 0.8
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.81
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.75; acc: 0.72
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.43265091357337443; val_accuracy: 0.86046974522293 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.36; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.81
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.83
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.5751228083850471; val_accuracy: 0.8123009554140127 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.72
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.8
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.53; acc: 0.8
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3479015761215216; val_accuracy: 0.8922173566878981 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.83
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.5051890576531173; val_accuracy: 0.8470342356687898 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.3156193535134291; val_accuracy: 0.9002786624203821 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.78
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.63; acc: 0.84
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.87; acc: 0.81
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.8
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.36174605753581235; val_accuracy: 0.8851512738853503 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2912855705922576; val_accuracy: 0.9101313694267515 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2858742773058308; val_accuracy: 0.9125199044585988 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2887940961796387; val_accuracy: 0.9130175159235668 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2929729412125934; val_accuracy: 0.9064490445859873 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2909372179371536; val_accuracy: 0.9113256369426752 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.78
Batch: 320; loss: 0.33; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2805414167084512; val_accuracy: 0.9144108280254777 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.28400466159270826; val_accuracy: 0.9158041401273885 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.81
Batch: 480; loss: 0.35; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3138615819773856; val_accuracy: 0.9021695859872612 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2845211046374148; val_accuracy: 0.9122213375796179 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.83
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.8
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2908423029028686; val_accuracy: 0.9090366242038217 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.09; acc: 1.0
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2680984323096883; val_accuracy: 0.9187898089171974 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2692707534286247; val_accuracy: 0.9195859872611465 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.09; acc: 1.0
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2755178118919491; val_accuracy: 0.9163017515923567 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.27299999170432426; val_accuracy: 0.9150079617834395 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.1; acc: 1.0
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2736593425084072; val_accuracy: 0.9160031847133758 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2694502339526347; val_accuracy: 0.9166003184713376 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2695237835216674; val_accuracy: 0.918093152866242 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.83
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.27586638310532663; val_accuracy: 0.9173964968152867 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.83
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2716001135765747; val_accuracy: 0.9161027070063694 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2657678475378046; val_accuracy: 0.9194864649681529 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.83
Batch: 680; loss: 0.3; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26750587297093337; val_accuracy: 0.9166003184713376 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26529973780938015; val_accuracy: 0.9182921974522293 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26542945656996625; val_accuracy: 0.9184912420382165 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.8
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.265449540440444; val_accuracy: 0.9189888535031847 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.86
Batch: 420; loss: 0.09; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26575917558400497; val_accuracy: 0.9171974522292994 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.09; acc: 1.0
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26612315118123014; val_accuracy: 0.918093152866242 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26433262703525034; val_accuracy: 0.919984076433121 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2656502390078678; val_accuracy: 0.9184912420382165 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.81
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2660427046049932; val_accuracy: 0.9185907643312102 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26632457724802056; val_accuracy: 0.9165007961783439 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2648211766485196; val_accuracy: 0.9178941082802548 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2638887626350306; val_accuracy: 0.9178941082802548 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2642428325904403; val_accuracy: 0.9187898089171974 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.81
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26493761299332236; val_accuracy: 0.9187898089171974 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.81
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26427152529833425; val_accuracy: 0.9176950636942676 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2641593163749974; val_accuracy: 0.9182921974522293 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26458652122954657; val_accuracy: 0.9186902866242038 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.81
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2639732926039939; val_accuracy: 0.9183917197452229 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26427764222500433; val_accuracy: 0.9185907643312102 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.86
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26444357130557866; val_accuracy: 0.9189888535031847 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_220_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 48657
elements in E: 10217980
fraction nonzero: 0.004761900101585636
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.12
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.06
Batch: 220; loss: 2.27; acc: 0.12
Batch: 240; loss: 2.25; acc: 0.22
Batch: 260; loss: 2.26; acc: 0.14
Batch: 280; loss: 2.25; acc: 0.16
Batch: 300; loss: 2.22; acc: 0.31
Batch: 320; loss: 2.26; acc: 0.16
Batch: 340; loss: 2.22; acc: 0.19
Batch: 360; loss: 2.18; acc: 0.34
Batch: 380; loss: 2.18; acc: 0.28
Batch: 400; loss: 2.12; acc: 0.38
Batch: 420; loss: 2.08; acc: 0.42
Batch: 440; loss: 1.99; acc: 0.34
Batch: 460; loss: 1.91; acc: 0.44
Batch: 480; loss: 1.75; acc: 0.42
Batch: 500; loss: 1.61; acc: 0.47
Batch: 520; loss: 1.3; acc: 0.62
Batch: 540; loss: 1.13; acc: 0.64
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 0.97; acc: 0.7
Batch: 600; loss: 0.86; acc: 0.78
Batch: 620; loss: 0.77; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 1.03; acc: 0.64
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 1.73; train_accuracy: 0.39 

Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.86
Val Epoch over. val_loss: 0.6000980032477409; val_accuracy: 0.8096138535031847 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 1.01; acc: 0.67
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.77
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.77
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.86; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.78
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.77
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.75
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.77
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.41663245358474693; val_accuracy: 0.8784832802547771 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.8
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.73
Batch: 400; loss: 0.4; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.81
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.72
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.4907015435824728; val_accuracy: 0.8400676751592356 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.78
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.76; acc: 0.8
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.78
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.66; acc: 0.73
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6160282269594776; val_accuracy: 0.8146894904458599 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.78
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.8
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.73
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.5336274391716453; val_accuracy: 0.8316082802547771 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.11; acc: 1.0
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.81
Batch: 500; loss: 0.22; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.75
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.68; acc: 0.75
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.75
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.88
Val Epoch over. val_loss: 0.45958954788696993; val_accuracy: 0.8591759554140127 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.83
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3958522405973665; val_accuracy: 0.8758957006369427 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.75
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.4856305248133696; val_accuracy: 0.8522093949044586 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.75
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.3825178584380514; val_accuracy: 0.8809713375796179 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.88; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.83
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.24; acc: 0.89
Val Epoch over. val_loss: 0.5074742269364132; val_accuracy: 0.8431528662420382 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.3; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.78
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.92
Val Epoch over. val_loss: 0.3780209022532603; val_accuracy: 0.8862460191082803 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.37184132982021684; val_accuracy: 0.8845541401273885 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.37606594901365836; val_accuracy: 0.8838574840764332 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.35128933513999744; val_accuracy: 0.8901273885350318 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.341002801184062; val_accuracy: 0.8936106687898089 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3486434416311562; val_accuracy: 0.8931130573248408 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.8
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.86
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.77
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.33663201517170405; val_accuracy: 0.8980891719745223 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.76; acc: 0.83
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.77
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.3398456162517997; val_accuracy: 0.8974920382165605 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3381083993490335; val_accuracy: 0.8965963375796179 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3594608072452484; val_accuracy: 0.8893312101910829 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.84
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.32060145458598044; val_accuracy: 0.9027667197452229 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.78
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.57; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3271131514079252; val_accuracy: 0.9003781847133758 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.81
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.8
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.32377670368381367; val_accuracy: 0.9006767515923567 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3215174048569552; val_accuracy: 0.9031648089171974 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3425352269202281; val_accuracy: 0.8961982484076433 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.32160777256936784; val_accuracy: 0.9023686305732485 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.32607902553240964; val_accuracy: 0.9013734076433121 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.84
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.83
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3367181361006324; val_accuracy: 0.897890127388535 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3310773989102643; val_accuracy: 0.8990843949044586 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3333290178494848; val_accuracy: 0.8976910828025477 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3193176606563246; val_accuracy: 0.9031648089171974 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31962843571498895; val_accuracy: 0.9028662420382165 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.86
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.319868766435772; val_accuracy: 0.9026671974522293 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.77
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.32339665032685944; val_accuracy: 0.9021695859872612 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3196911209612895; val_accuracy: 0.9013734076433121 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.35; acc: 0.84
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.31845843175034616; val_accuracy: 0.9029657643312102 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.16; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.32537726341349305; val_accuracy: 0.9011743630573248 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.321082185645392; val_accuracy: 0.9021695859872612 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3186289562256473; val_accuracy: 0.9036624203821656 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31996583506749693; val_accuracy: 0.9023686305732485 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3176669220256198; val_accuracy: 0.9036624203821656 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.83
Batch: 320; loss: 0.32; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31847688119122936; val_accuracy: 0.9028662420382165 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31913784307659054; val_accuracy: 0.9020700636942676 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.8
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31803382691114573; val_accuracy: 0.9034633757961783 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.8
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31860789058694416; val_accuracy: 0.9014729299363057 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.77; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31760067393066016; val_accuracy: 0.902468152866242 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31759710230265453; val_accuracy: 0.9036624203821656 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.86
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31825894397345317; val_accuracy: 0.9014729299363057 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.77; acc: 0.75
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31749933455020757; val_accuracy: 0.9019705414012739 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.81
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.78
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.11; acc: 1.0
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3187855092013717; val_accuracy: 0.9029657643312102 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_230_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 50766
elements in E: 10662240
fraction nonzero: 0.0047612884347004005
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.26; acc: 0.23
Batch: 180; loss: 2.28; acc: 0.17
Batch: 200; loss: 2.26; acc: 0.22
Batch: 220; loss: 2.24; acc: 0.2
Batch: 240; loss: 2.21; acc: 0.34
Batch: 260; loss: 2.21; acc: 0.28
Batch: 280; loss: 2.14; acc: 0.36
Batch: 300; loss: 2.14; acc: 0.33
Batch: 320; loss: 2.09; acc: 0.38
Batch: 340; loss: 1.96; acc: 0.42
Batch: 360; loss: 1.79; acc: 0.52
Batch: 380; loss: 1.62; acc: 0.53
Batch: 400; loss: 1.37; acc: 0.55
Batch: 420; loss: 1.1; acc: 0.62
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 1.13; acc: 0.59
Batch: 480; loss: 0.77; acc: 0.75
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.97; acc: 0.73
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.83; acc: 0.69
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.88; acc: 0.67
Batch: 700; loss: 1.09; acc: 0.59
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.73
Train Epoch over. train_loss: 1.48; train_accuracy: 0.49 

Batch: 0; loss: 0.73; acc: 0.75
Batch: 20; loss: 0.68; acc: 0.72
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5721899222606307; val_accuracy: 0.8250398089171974 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.73
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.76; acc: 0.72
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.9; acc: 0.77
Batch: 200; loss: 0.74; acc: 0.69
Batch: 220; loss: 0.5; acc: 0.78
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 0.92; acc: 0.81
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.8
Batch: 320; loss: 0.57; acc: 0.81
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.77; acc: 0.7
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.84
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.51; acc: 0.81
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.416872008068926; val_accuracy: 0.8674363057324841 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.77
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.5367401291610329; val_accuracy: 0.8274283439490446 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.77
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.72
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.96; acc: 0.75
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.78
Val Epoch over. val_loss: 0.6005495386138843; val_accuracy: 0.8100119426751592 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.72
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.67
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.6322139157041623; val_accuracy: 0.7938893312101911 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.81
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.39; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.82; acc: 0.75
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.81
Val Epoch over. val_loss: 0.5115368125165344; val_accuracy: 0.8390724522292994 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.78
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.81
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.8
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.86
Val Epoch over. val_loss: 0.41339048605625794; val_accuracy: 0.8727109872611465 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.8
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.8
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.81
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.49805507282162925; val_accuracy: 0.837281050955414 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.57; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.77
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.34; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.41811752694237764; val_accuracy: 0.8664410828025477 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.59; acc: 0.75
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.77
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.77
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.75
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.5257917513513262; val_accuracy: 0.8259355095541401 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 0.38; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.86
Batch: 700; loss: 0.65; acc: 0.81
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3200738913836373; val_accuracy: 0.9048566878980892 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.81
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3194463656871182; val_accuracy: 0.9027667197452229 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.83
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.4; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3527326799312215; val_accuracy: 0.8916202229299363 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.32538869872594334; val_accuracy: 0.9012738853503185 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.92
Val Epoch over. val_loss: 0.3194906771135558; val_accuracy: 0.8998805732484076 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.18; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.3483002731564698; val_accuracy: 0.893312101910828 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.31315140286163917; val_accuracy: 0.9040605095541401 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.77
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.83
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.74; acc: 0.81
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.22; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.3088155454795831; val_accuracy: 0.9060509554140127 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.27; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3209608790648591; val_accuracy: 0.9025676751592356 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.83
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3128581409384111; val_accuracy: 0.9054538216560509 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.29397671176179957; val_accuracy: 0.9113256369426752 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.84
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2980621115654517; val_accuracy: 0.9092356687898089 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.81
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29871634026147; val_accuracy: 0.9112261146496815 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.2937678195014121; val_accuracy: 0.9121218152866242 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.56; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.290158291246481; val_accuracy: 0.912718949044586 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.285654085006114; val_accuracy: 0.9145103503184714 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.8
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2920832570855785; val_accuracy: 0.9125199044585988 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.86
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.29613428268652814; val_accuracy: 0.910828025477707 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.62; acc: 0.83
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.84
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.2967216063314562; val_accuracy: 0.9093351910828026 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3042537132693324; val_accuracy: 0.9069466560509554 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2844081116947019; val_accuracy: 0.9151074840764332 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.66; acc: 0.75
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.28667409177039077; val_accuracy: 0.9140127388535032 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.28413440137627016; val_accuracy: 0.9138136942675159 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.81
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.34; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2866861353966461; val_accuracy: 0.912718949044586 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.28389204196204804; val_accuracy: 0.9144108280254777 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.26; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2842997310647539; val_accuracy: 0.9155055732484076 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.288436734491283; val_accuracy: 0.9126194267515924 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2872419841112984; val_accuracy: 0.9138136942675159 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.28317615076614794; val_accuracy: 0.9152070063694268 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 1.0
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2849572379687789; val_accuracy: 0.9149084394904459 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2820240058669239; val_accuracy: 0.9155055732484076 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2831375140720492; val_accuracy: 0.9150079617834395 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.8
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.28282787767090617; val_accuracy: 0.9150079617834395 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.34; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.59; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2826513030868807; val_accuracy: 0.9157046178343949 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.86
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.68; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2820561429260263; val_accuracy: 0.9161027070063694 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.81
Batch: 180; loss: 0.3; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.28181332273847737; val_accuracy: 0.9157046178343949 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2823341962211071; val_accuracy: 0.9139132165605095 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.98
Batch: 520; loss: 0.35; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2816094372207951; val_accuracy: 0.9150079617834395 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2832854337230989; val_accuracy: 0.9132165605095541 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2828123104894996; val_accuracy: 0.9144108280254777 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_240_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 52582
elements in E: 11106500
fraction nonzero: 0.004734344753072525
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.26; acc: 0.28
Batch: 180; loss: 2.28; acc: 0.23
Batch: 200; loss: 2.27; acc: 0.25
Batch: 220; loss: 2.25; acc: 0.36
Batch: 240; loss: 2.24; acc: 0.41
Batch: 260; loss: 2.24; acc: 0.36
Batch: 280; loss: 2.21; acc: 0.41
Batch: 300; loss: 2.2; acc: 0.36
Batch: 320; loss: 2.2; acc: 0.34
Batch: 340; loss: 2.17; acc: 0.41
Batch: 360; loss: 2.12; acc: 0.55
Batch: 380; loss: 2.13; acc: 0.48
Batch: 400; loss: 2.04; acc: 0.52
Batch: 420; loss: 1.92; acc: 0.41
Batch: 440; loss: 1.63; acc: 0.53
Batch: 460; loss: 1.42; acc: 0.62
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 1.04; acc: 0.64
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.75; acc: 0.75
Batch: 600; loss: 0.64; acc: 0.8
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.68; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.75
Batch: 680; loss: 1.05; acc: 0.64
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.93; acc: 0.73
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 1.6; train_accuracy: 0.49 

Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.7
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.44; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 1.03; acc: 0.73
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.5173949114266475; val_accuracy: 0.836484872611465 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.78; acc: 0.73
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.75
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.7
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.77
Batch: 740; loss: 0.58; acc: 0.75
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.38269338851711554; val_accuracy: 0.8830613057324841 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 1.22; acc: 0.75
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3794958158663124; val_accuracy: 0.8846536624203821 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.83; acc: 0.78
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.31; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.32; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.45735770136497583; val_accuracy: 0.8556926751592356 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.64; acc: 0.78
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.8; acc: 0.72
Batch: 20; loss: 1.1; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 0.92; acc: 0.78
Batch: 80; loss: 1.05; acc: 0.8
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.7497828046607363; val_accuracy: 0.7735867834394905 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.3; acc: 0.81
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.76; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.28331444830082025; val_accuracy: 0.9171974522292994 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.28112941942397196; val_accuracy: 0.9179936305732485 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.59; acc: 0.78
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.32; acc: 0.83
Val Epoch over. val_loss: 0.5548156450508507; val_accuracy: 0.8259355095541401 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.32620101920358696; val_accuracy: 0.9003781847133758 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3053238805217348; val_accuracy: 0.9080414012738853 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2797120206390217; val_accuracy: 0.9164012738853503 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.28796623039777114; val_accuracy: 0.9134156050955414 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.09; acc: 1.0
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26849881455207325; val_accuracy: 0.9200835987261147 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.83
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.28010760399566337; val_accuracy: 0.9159036624203821 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.88
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.83
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25063855547434205; val_accuracy: 0.9236664012738853 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2615773910834531; val_accuracy: 0.919187898089172 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.42; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23080014204903013; val_accuracy: 0.9299363057324841 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2719931242762098; val_accuracy: 0.9189888535031847 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23222643962711287; val_accuracy: 0.9292396496815286 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.24689488036996998; val_accuracy: 0.9252587579617835 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.21869796130118097; val_accuracy: 0.9343152866242038 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22270446009696668; val_accuracy: 0.9300358280254777 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21875718345118175; val_accuracy: 0.9327229299363057 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2197253884878128; val_accuracy: 0.931827229299363 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.22141592451341593; val_accuracy: 0.9332205414012739 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.22575405097691117; val_accuracy: 0.9317277070063694 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22133717254088942; val_accuracy: 0.9323248407643312 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.8
Batch: 600; loss: 0.2; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2316973882780713; val_accuracy: 0.9295382165605095 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.08; acc: 1.0
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21961031991774868; val_accuracy: 0.9331210191082803 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.217961984360294; val_accuracy: 0.9339171974522293 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21660709694312635; val_accuracy: 0.9339171974522293 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.215904845933246; val_accuracy: 0.9352109872611465 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2155540832763265; val_accuracy: 0.9343152866242038 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21730558654874754; val_accuracy: 0.9331210191082803 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.56; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.88
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21741334913642543; val_accuracy: 0.9346138535031847 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21705078570895894; val_accuracy: 0.9335191082802548 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21682349326694086; val_accuracy: 0.9341162420382165 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21766791003904526; val_accuracy: 0.9340167197452229 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21612100962810454; val_accuracy: 0.9344148089171974 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.5; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2181306378382027; val_accuracy: 0.9333200636942676 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21587757215758038; val_accuracy: 0.9355095541401274 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2164781743743617; val_accuracy: 0.9349124203821656 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21511334902162005; val_accuracy: 0.9351114649681529 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21506684752786234; val_accuracy: 0.9360071656050956 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21556831431236995; val_accuracy: 0.9353105095541401 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21486074890300727; val_accuracy: 0.9350119426751592 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2154832944084125; val_accuracy: 0.9344148089171974 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21565771069686124; val_accuracy: 0.935609076433121 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21615183842220123; val_accuracy: 0.9349124203821656 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21625110193802294; val_accuracy: 0.934812898089172 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_250_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 54739
elements in E: 11550760
fraction nonzero: 0.004738995529298505
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.12
Batch: 160; loss: 2.26; acc: 0.3
Batch: 180; loss: 2.28; acc: 0.17
Batch: 200; loss: 2.25; acc: 0.25
Batch: 220; loss: 2.23; acc: 0.31
Batch: 240; loss: 2.2; acc: 0.33
Batch: 260; loss: 2.18; acc: 0.39
Batch: 280; loss: 2.12; acc: 0.44
Batch: 300; loss: 2.07; acc: 0.42
Batch: 320; loss: 2.06; acc: 0.27
Batch: 340; loss: 1.89; acc: 0.45
Batch: 360; loss: 1.58; acc: 0.69
Batch: 380; loss: 1.32; acc: 0.67
Batch: 400; loss: 1.05; acc: 0.7
Batch: 420; loss: 1.0; acc: 0.67
Batch: 440; loss: 0.61; acc: 0.77
Batch: 460; loss: 0.94; acc: 0.69
Batch: 480; loss: 0.65; acc: 0.8
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.62; acc: 0.83
Batch: 580; loss: 0.95; acc: 0.78
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.8
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.8
Batch: 680; loss: 0.85; acc: 0.7
Batch: 700; loss: 0.79; acc: 0.73
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.74; acc: 0.75
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.5434493107400882; val_accuracy: 0.8267316878980892 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 1.3; acc: 0.61
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.72
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.84; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.81
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.8
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.43964788175312575; val_accuracy: 0.8670382165605095 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.77
Batch: 660; loss: 0.79; acc: 0.72
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.52; acc: 0.81
Val Epoch over. val_loss: 0.8647483592959726; val_accuracy: 0.7250199044585988 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.8
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.73
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.5566806501833497; val_accuracy: 0.8341958598726115 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.75
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.72
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4217976562821182; val_accuracy: 0.8683320063694268 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.8
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 1.0; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3622677209460811; val_accuracy: 0.8885350318471338 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.83
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3198873386925952; val_accuracy: 0.8994824840764332 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 1.14; acc: 0.73
Batch: 140; loss: 0.36; acc: 0.84
Val Epoch over. val_loss: 0.6454327202336804; val_accuracy: 0.8134952229299363 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.78
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.35; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.30494770768341745; val_accuracy: 0.9056528662420382 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.3209183273991202; val_accuracy: 0.9007762738853503 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.83
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3036606749816305; val_accuracy: 0.9074442675159236 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3367654730559914; val_accuracy: 0.9015724522292994 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.84
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2905529088750007; val_accuracy: 0.9128184713375797 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2826024394980661; val_accuracy: 0.9126194267515924 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.83
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2689813851455974; val_accuracy: 0.9179936305732485 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.86
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2831706151271322; val_accuracy: 0.9121218152866242 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2851026735393105; val_accuracy: 0.9118232484076433 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.27715106204057194; val_accuracy: 0.9174960191082803 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.08; acc: 1.0
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2601985972683141; val_accuracy: 0.9219745222929936 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.84
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3068869271475798; val_accuracy: 0.9099323248407644 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.86
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26659579764885505; val_accuracy: 0.9215764331210191 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2645188169019997; val_accuracy: 0.9193869426751592 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.84
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.25848781445603464; val_accuracy: 0.9230692675159236 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2568810549890919; val_accuracy: 0.9239649681528662 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.81
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.7; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26633542353750034; val_accuracy: 0.9192874203821656 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2600944881226606; val_accuracy: 0.92296974522293 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.78
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2659712202704636; val_accuracy: 0.9209792993630573 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.89
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2733361855813652; val_accuracy: 0.9193869426751592 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.84
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.26072738390819283; val_accuracy: 0.9221735668789809 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2595862584413996; val_accuracy: 0.9213773885350318 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.89
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.25410718038962904; val_accuracy: 0.9230692675159236 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25314505986726965; val_accuracy: 0.9233678343949044 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25311352473915005; val_accuracy: 0.9224721337579618 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.09; acc: 1.0
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.98
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2518123137723109; val_accuracy: 0.923765923566879 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.8
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.25062953381781367; val_accuracy: 0.9244625796178344 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.46; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25060907822505685; val_accuracy: 0.9251592356687898 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.253409163112853; val_accuracy: 0.9224721337579618 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2504548246788371; val_accuracy: 0.9234673566878981 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.79; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.25110722399631125; val_accuracy: 0.9249601910828026 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25253218882212974; val_accuracy: 0.9244625796178344 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24991761921507538; val_accuracy: 0.92296974522293 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2500939677190629; val_accuracy: 0.923765923566879 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.65; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25202292441182833; val_accuracy: 0.923765923566879 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2517551921640232; val_accuracy: 0.9239649681528662 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.84
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.250304864755102; val_accuracy: 0.9236664012738853 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2502271069842539; val_accuracy: 0.9239649681528662 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2489449500942686; val_accuracy: 0.9248606687898089 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24884083615556643; val_accuracy: 0.9253582802547771 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24932396207835264; val_accuracy: 0.9240644904458599 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.86
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24986791719866405; val_accuracy: 0.924562101910828 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_260_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 56962
elements in E: 11995020
fraction nonzero: 0.00474880408702945
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.29; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.09
Batch: 160; loss: 2.26; acc: 0.2
Batch: 180; loss: 2.27; acc: 0.11
Batch: 200; loss: 2.25; acc: 0.2
Batch: 220; loss: 2.24; acc: 0.17
Batch: 240; loss: 2.22; acc: 0.34
Batch: 260; loss: 2.22; acc: 0.23
Batch: 280; loss: 2.18; acc: 0.33
Batch: 300; loss: 2.17; acc: 0.36
Batch: 320; loss: 2.14; acc: 0.31
Batch: 340; loss: 2.11; acc: 0.3
Batch: 360; loss: 2.0; acc: 0.5
Batch: 380; loss: 1.9; acc: 0.55
Batch: 400; loss: 1.72; acc: 0.45
Batch: 420; loss: 1.46; acc: 0.52
Batch: 440; loss: 1.09; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.59
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.75
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.64; acc: 0.78
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.73
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.97; acc: 0.72
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 1.49; train_accuracy: 0.49 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.73
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.42624616537504134; val_accuracy: 0.8711186305732485 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.8
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.4326117567860397; val_accuracy: 0.8655453821656051 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.75
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.77
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.4116618167727616; val_accuracy: 0.8740047770700637 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.8
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.78
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.41488993025509413; val_accuracy: 0.8665406050955414 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.84
Batch: 220; loss: 0.31; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.86
Val Epoch over. val_loss: 0.833067195240859; val_accuracy: 0.7709992038216561 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2923682195840368; val_accuracy: 0.9071457006369427 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.28; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26190380714121897; val_accuracy: 0.9190883757961783 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.83
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.48470140898683267; val_accuracy: 0.8534036624203821 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25934689833669905; val_accuracy: 0.9217754777070064 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.3928539968410115; val_accuracy: 0.8741042993630573 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.08; acc: 1.0
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.237261264306155; val_accuracy: 0.9285429936305732 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2445449427624417; val_accuracy: 0.9254578025477707 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.83
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24697289150801433; val_accuracy: 0.9259554140127388 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.83
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.28691743841027; val_accuracy: 0.9131170382165605 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.24305075905315435; val_accuracy: 0.9274482484076433 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.24187738498210148; val_accuracy: 0.9255573248407644 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.23988744248725047; val_accuracy: 0.9272492038216561 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2343580724707075; val_accuracy: 0.929140127388535 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2342178563878035; val_accuracy: 0.9285429936305732 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.23626268174570458; val_accuracy: 0.9289410828025477 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.22498067393661683; val_accuracy: 0.932921974522293 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22609734254040917; val_accuracy: 0.9315286624203821 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2260996616166678; val_accuracy: 0.9320262738853503 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.24; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.23185609990529193; val_accuracy: 0.9313296178343949 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.81
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.59; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22596132732737975; val_accuracy: 0.9313296178343949 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.98
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22985896443248174; val_accuracy: 0.9298367834394905 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22782295497407198; val_accuracy: 0.9308320063694268 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.25024269962576545; val_accuracy: 0.9232683121019108 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.89
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.21; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23436450831307348; val_accuracy: 0.9281449044585988 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2344600366677638; val_accuracy: 0.9269506369426752 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22324149649898717; val_accuracy: 0.9320262738853503 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.19; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22561321226037612; val_accuracy: 0.9321257961783439 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.83
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22439374521991629; val_accuracy: 0.9317277070063694 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.81
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22673308012923996; val_accuracy: 0.9312300955414012 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.86
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2263605930978895; val_accuracy: 0.9326234076433121 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22490023408725762; val_accuracy: 0.9315286624203821 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2254556519495454; val_accuracy: 0.9330214968152867 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22520187949156686; val_accuracy: 0.9326234076433121 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22481187937223607; val_accuracy: 0.9313296178343949 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22494019955918668; val_accuracy: 0.9323248407643312 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.24; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22284190357917813; val_accuracy: 0.932921974522293 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22406373136220084; val_accuracy: 0.9331210191082803 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22385997376197084; val_accuracy: 0.9332205414012739 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.08; acc: 1.0
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.22437488560558885; val_accuracy: 0.9327229299363057 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.57; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22337635088071323; val_accuracy: 0.9315286624203821 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.222509625539848; val_accuracy: 0.9331210191082803 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.27; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.08; acc: 1.0
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.22289743834430245; val_accuracy: 0.9324243630573248 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.84
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.2221995679317576; val_accuracy: 0.9322253184713376 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.2235865116024473; val_accuracy: 0.9328224522292994 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.86
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.22304313922905997; val_accuracy: 0.932921974522293 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_270_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 58843
elements in E: 12439280
fraction nonzero: 0.004730418480812394
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.26; acc: 0.16
Batch: 180; loss: 2.27; acc: 0.16
Batch: 200; loss: 2.26; acc: 0.22
Batch: 220; loss: 2.24; acc: 0.23
Batch: 240; loss: 2.22; acc: 0.31
Batch: 260; loss: 2.2; acc: 0.28
Batch: 280; loss: 2.13; acc: 0.41
Batch: 300; loss: 2.1; acc: 0.48
Batch: 320; loss: 2.07; acc: 0.34
Batch: 340; loss: 1.94; acc: 0.36
Batch: 360; loss: 1.82; acc: 0.53
Batch: 380; loss: 1.51; acc: 0.61
Batch: 400; loss: 1.23; acc: 0.59
Batch: 420; loss: 1.06; acc: 0.62
Batch: 440; loss: 0.92; acc: 0.73
Batch: 460; loss: 1.15; acc: 0.55
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.73
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 1.01; acc: 0.62
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.78
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.77
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 1.19; acc: 0.59
Batch: 700; loss: 0.76; acc: 0.72
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 1.46; train_accuracy: 0.5 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.34; acc: 0.86
Val Epoch over. val_loss: 0.5644966582204126; val_accuracy: 0.8238455414012739 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 1.01; acc: 0.72
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.75
Batch: 100; loss: 0.49; acc: 0.8
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.78
Batch: 360; loss: 0.56; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.8
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.8
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.85 

Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.88
Val Epoch over. val_loss: 0.390329890237872; val_accuracy: 0.8742038216560509 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.36; acc: 0.84
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.37491919128758133; val_accuracy: 0.8825636942675159 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.83
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.78; acc: 0.72
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.5416668934427249; val_accuracy: 0.8206608280254777 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.81; acc: 0.67
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6266770000290719; val_accuracy: 0.7928941082802548 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.84
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.30421904427040913; val_accuracy: 0.9079418789808917 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.24; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.83
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.07; acc: 1.0
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.27509491454074336; val_accuracy: 0.9182921974522293 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.77
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3456936643524155; val_accuracy: 0.8929140127388535 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.8
Batch: 460; loss: 0.2; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3444535336012294; val_accuracy: 0.8916202229299363 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26418171818279157; val_accuracy: 0.9201831210191083 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.53; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2473064130943292; val_accuracy: 0.9274482484076433 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2567857940485523; val_accuracy: 0.9259554140127388 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.25883055651548564; val_accuracy: 0.9202826433121019 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2616252611587002; val_accuracy: 0.92078025477707 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.26160965845653206; val_accuracy: 0.9254578025477707 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.23949760775182657; val_accuracy: 0.9282444267515924 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.89
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.25451397297868306; val_accuracy: 0.9216759554140127 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.25234903459241437; val_accuracy: 0.9266520700636943 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23862076123618775; val_accuracy: 0.9271496815286624 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.24986736981826982; val_accuracy: 0.9266520700636943 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2388295324246405; val_accuracy: 0.9279458598726115 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.61; acc: 0.89
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.24237537330645284; val_accuracy: 0.9258558917197452 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.08; acc: 1.0
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.81
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23493550421231113; val_accuracy: 0.9311305732484076 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23236488028886212; val_accuracy: 0.9299363057324841 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23407477109580282; val_accuracy: 0.9294386942675159 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.23234805244074505; val_accuracy: 0.9314291401273885 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2304327871864009; val_accuracy: 0.9316281847133758 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.61; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2457039311385838; val_accuracy: 0.9260549363057324 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.24016895168905805; val_accuracy: 0.9287420382165605 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23600944551597736; val_accuracy: 0.9286425159235668 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23030263996997458; val_accuracy: 0.9298367834394905 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2294288971313056; val_accuracy: 0.9308320063694268 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2287450079112106; val_accuracy: 0.9308320063694268 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.77
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22884000995593845; val_accuracy: 0.9321257961783439 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.23022103743853084; val_accuracy: 0.9297372611464968 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.07; acc: 1.0
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.8
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22845555418378607; val_accuracy: 0.9301353503184714 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22982598573064347; val_accuracy: 0.9314291401273885 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2284386255985992; val_accuracy: 0.931031050955414 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22785947136106385; val_accuracy: 0.9311305732484076 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.81
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2306646295010474; val_accuracy: 0.9296377388535032 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.88
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.1; acc: 1.0
Batch: 200; loss: 0.22; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2279254705022285; val_accuracy: 0.9313296178343949 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.07; acc: 1.0
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2276023861827554; val_accuracy: 0.9322253184713376 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.86
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22843388356505687; val_accuracy: 0.9312300955414012 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22762144995248243; val_accuracy: 0.9315286624203821 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.81
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22788508522330195; val_accuracy: 0.9315286624203821 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22696283344582768; val_accuracy: 0.9319267515923567 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22718265181656477; val_accuracy: 0.9320262738853503 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22708403969266613; val_accuracy: 0.9321257961783439 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.08; acc: 1.0
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.13; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.81
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2273050821060018; val_accuracy: 0.9319267515923567 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.22779419829558795; val_accuracy: 0.931827229299363 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_280_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 60880
elements in E: 12883540
fraction nonzero: 0.004725409320730172
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.26; acc: 0.23
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.24; acc: 0.25
Batch: 220; loss: 2.23; acc: 0.27
Batch: 240; loss: 2.19; acc: 0.38
Batch: 260; loss: 2.17; acc: 0.19
Batch: 280; loss: 2.09; acc: 0.39
Batch: 300; loss: 2.05; acc: 0.34
Batch: 320; loss: 1.96; acc: 0.38
Batch: 340; loss: 1.68; acc: 0.47
Batch: 360; loss: 1.42; acc: 0.62
Batch: 380; loss: 1.14; acc: 0.66
Batch: 400; loss: 0.86; acc: 0.72
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 1.0; acc: 0.66
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.8; acc: 0.7
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.75
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.9; acc: 0.66
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.95; acc: 0.73
Train Epoch over. train_loss: 1.39; train_accuracy: 0.52 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.44409394482518455; val_accuracy: 0.8573845541401274 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.77
Batch: 20; loss: 0.66; acc: 0.75
Batch: 40; loss: 0.76; acc: 0.73
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.75
Batch: 200; loss: 0.94; acc: 0.64
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.77
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.8
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.77
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.48694896849857017; val_accuracy: 0.8439490445859873 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.84
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3267402413069822; val_accuracy: 0.9017714968152867 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.8
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.86
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.44; acc: 0.8
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.84
Val Epoch over. val_loss: 0.6138782965339673; val_accuracy: 0.8097133757961783 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.4; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.97; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.45037752057716346; val_accuracy: 0.8586783439490446 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.58; acc: 0.77
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.77; acc: 0.77
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.3397551470311584; val_accuracy: 0.8940087579617835 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.78
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.29807739111648246; val_accuracy: 0.9118232484076433 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.75
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.78
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 1.39; acc: 0.67
Batch: 40; loss: 0.27; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.8
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 1.31; acc: 0.7
Batch: 140; loss: 0.74; acc: 0.8
Val Epoch over. val_loss: 0.813380518441747; val_accuracy: 0.7749800955414012 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.98
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2791669629039658; val_accuracy: 0.9166998407643312 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.81
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2957298772729886; val_accuracy: 0.9129179936305732 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2597102669488852; val_accuracy: 0.9217754777070064 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2950757873618299; val_accuracy: 0.9130175159235668 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.94
Val Epoch over. val_loss: 0.25102481234130586; val_accuracy: 0.9233678343949044 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.38499426666148906; val_accuracy: 0.880672770700637 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2610284012926232; val_accuracy: 0.9240644904458599 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2575768614128517; val_accuracy: 0.9240644904458599 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.12; acc: 1.0
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26503820871566514; val_accuracy: 0.9174960191082803 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.2570944130895244; val_accuracy: 0.9233678343949044 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.24366740785112048; val_accuracy: 0.9271496815286624 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2863476261923647; val_accuracy: 0.9147093949044586 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.22963386297131042; val_accuracy: 0.9327229299363057 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.1; acc: 1.0
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2381910102999514; val_accuracy: 0.9280453821656051 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.23599495767218293; val_accuracy: 0.9319267515923567 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.84
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.23578258461443483; val_accuracy: 0.9295382165605095 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.8
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.2347596856012086; val_accuracy: 0.9330214968152867 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.2341894081491194; val_accuracy: 0.9300358280254777 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.23639296301326174; val_accuracy: 0.9296377388535032 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.81
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2525929269517303; val_accuracy: 0.9267515923566879 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.23951824737866972; val_accuracy: 0.9279458598726115 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.24468824616188456; val_accuracy: 0.9301353503184714 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.23033435804069421; val_accuracy: 0.9327229299363057 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.88
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2291429988375515; val_accuracy: 0.9326234076433121 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22985784621660116; val_accuracy: 0.9324243630573248 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.23092232286265701; val_accuracy: 0.9313296178343949 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.23136487731318564; val_accuracy: 0.9317277070063694 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.84
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22995582251412094; val_accuracy: 0.9333200636942676 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.23132688374181462; val_accuracy: 0.9327229299363057 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.23250559525220257; val_accuracy: 0.931827229299363 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.227674497777869; val_accuracy: 0.934812898089172 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23192021862906256; val_accuracy: 0.9332205414012739 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2293670918246743; val_accuracy: 0.9328224522292994 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22973710564291402; val_accuracy: 0.9328224522292994 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.84
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.13; acc: 1.0
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22875456708916433; val_accuracy: 0.9336186305732485 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23092885313045447; val_accuracy: 0.9315286624203821 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22797925020479093; val_accuracy: 0.9336186305732485 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22772494357102996; val_accuracy: 0.9344148089171974 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22821907158110552; val_accuracy: 0.9334195859872612 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2284452382973425; val_accuracy: 0.9327229299363057 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2287557023535868; val_accuracy: 0.9332205414012739 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22989413668965078; val_accuracy: 0.932921974522293 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_290_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 63196
elements in E: 13327800
fraction nonzero: 0.004741667792133735
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.27; acc: 0.17
Batch: 160; loss: 2.25; acc: 0.28
Batch: 180; loss: 2.27; acc: 0.25
Batch: 200; loss: 2.23; acc: 0.38
Batch: 220; loss: 2.2; acc: 0.36
Batch: 240; loss: 2.14; acc: 0.5
Batch: 260; loss: 2.13; acc: 0.36
Batch: 280; loss: 1.96; acc: 0.41
Batch: 300; loss: 1.85; acc: 0.48
Batch: 320; loss: 1.69; acc: 0.44
Batch: 340; loss: 1.34; acc: 0.64
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.69
Batch: 400; loss: 0.83; acc: 0.69
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.76; acc: 0.69
Batch: 560; loss: 0.57; acc: 0.8
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.88; acc: 0.7
Batch: 700; loss: 0.73; acc: 0.73
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.75
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.73
Train Epoch over. train_loss: 1.32; train_accuracy: 0.56 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.67
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.4632061544307478; val_accuracy: 0.8513136942675159 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.3796641545690549; val_accuracy: 0.8789808917197452 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.8
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.34782406673499733; val_accuracy: 0.88953025477707 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.77
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.64; acc: 0.77
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.4377919236660763; val_accuracy: 0.8714171974522293 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2700985068823122; val_accuracy: 0.9182921974522293 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.76; acc: 0.77
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.86
Val Epoch over. val_loss: 0.570815306560249; val_accuracy: 0.8177746815286624 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2742124151224923; val_accuracy: 0.919187898089172 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.79; acc: 0.73
Batch: 20; loss: 1.45; acc: 0.69
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 1.11; acc: 0.75
Batch: 140; loss: 0.64; acc: 0.81
Val Epoch over. val_loss: 0.7514282662397737; val_accuracy: 0.7963773885350318 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.25466071510580696; val_accuracy: 0.92296974522293 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.94
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.83
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.43; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.4305613988144383; val_accuracy: 0.8640525477707006 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.23964572088069216; val_accuracy: 0.9278463375796179 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.81
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2570713885413234; val_accuracy: 0.923765923566879 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.81
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.27462694224468465; val_accuracy: 0.9186902866242038 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2643195905598106; val_accuracy: 0.9210788216560509 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.1; acc: 1.0
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.81
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2448234970972037; val_accuracy: 0.9270501592356688 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.24224099894143214; val_accuracy: 0.9273487261146497 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26162388458088703; val_accuracy: 0.9202826433121019 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.21; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.24079342853206737; val_accuracy: 0.9298367834394905 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24713968039508077; val_accuracy: 0.9270501592356688 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.75
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3218276813436466; val_accuracy: 0.9001791401273885 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21846120633706925; val_accuracy: 0.9369028662420382 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2341385702750865; val_accuracy: 0.929140127388535 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21661667549496244; val_accuracy: 0.93859474522293 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22200986396545058; val_accuracy: 0.9345143312101911 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22428375169350084; val_accuracy: 0.9334195859872612 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22004185576632523; val_accuracy: 0.9363057324840764 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22575089833728826; val_accuracy: 0.9357085987261147 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2476315337476457; val_accuracy: 0.9279458598726115 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22263981295144483; val_accuracy: 0.9352109872611465 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.88
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.22305488043055413; val_accuracy: 0.9351114649681529 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.25; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21645259679218007; val_accuracy: 0.9375995222929936 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2146259777152994; val_accuracy: 0.9381966560509554 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.1; acc: 1.0
Batch: 100; loss: 0.27; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21872298042200933; val_accuracy: 0.9375 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.1; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21652109073890244; val_accuracy: 0.9381966560509554 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21686252581465776; val_accuracy: 0.9366042993630573 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.8
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22079977004011725; val_accuracy: 0.9367038216560509 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.1; acc: 1.0
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21806999488146442; val_accuracy: 0.9373009554140127 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21764782619229547; val_accuracy: 0.9378980891719745 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.215144586672259; val_accuracy: 0.9379976114649682 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.86
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21691249329952678; val_accuracy: 0.9384952229299363 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21577738858996684; val_accuracy: 0.93859474522293 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.08; acc: 1.0
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.86
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21461287598795953; val_accuracy: 0.9388933121019108 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2146992101600975; val_accuracy: 0.9389928343949044 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.45; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.1; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21635907181319158; val_accuracy: 0.9391918789808917 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.27; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21418703269142253; val_accuracy: 0.9390923566878981 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21366902303164173; val_accuracy: 0.9397890127388535 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21539275440725553; val_accuracy: 0.9380971337579618 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2145107428359378; val_accuracy: 0.9386942675159236 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.84
Batch: 240; loss: 0.15; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2147670483370875; val_accuracy: 0.9381966560509554 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21525799869826645; val_accuracy: 0.9388933121019108 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 73714
elements in E: 15549100
fraction nonzero: 0.004740724543542713
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.27; acc: 0.2
Batch: 160; loss: 2.24; acc: 0.38
Batch: 180; loss: 2.26; acc: 0.23
Batch: 200; loss: 2.22; acc: 0.3
Batch: 220; loss: 2.2; acc: 0.34
Batch: 240; loss: 2.11; acc: 0.44
Batch: 260; loss: 2.1; acc: 0.36
Batch: 280; loss: 1.9; acc: 0.53
Batch: 300; loss: 1.73; acc: 0.47
Batch: 320; loss: 1.49; acc: 0.5
Batch: 340; loss: 1.23; acc: 0.64
Batch: 360; loss: 0.9; acc: 0.72
Batch: 380; loss: 1.0; acc: 0.62
Batch: 400; loss: 0.68; acc: 0.73
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 1.26; acc: 0.58
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.8
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.78
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.75; acc: 0.77
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.73
Batch: 660; loss: 0.43; acc: 0.83
Batch: 680; loss: 0.98; acc: 0.62
Batch: 700; loss: 0.77; acc: 0.83
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 1.3; train_accuracy: 0.57 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.4022352886712475; val_accuracy: 0.8745023885350318 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 1.05; acc: 0.69
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.71; acc: 0.72
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.8
Batch: 180; loss: 0.72; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.9; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.8
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.78
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.8; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.75; acc: 0.73
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.6700565553015205; val_accuracy: 0.779359076433121 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.73
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.84
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.4009817054696903; val_accuracy: 0.872312898089172 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.42; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.89
Val Epoch over. val_loss: 0.6021938042086401; val_accuracy: 0.8196656050955414 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.38; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.5122594974792687; val_accuracy: 0.8377786624203821 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.32245474066703944; val_accuracy: 0.896297770700637 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24920465946672068; val_accuracy: 0.9253582802547771 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.42321794679400265; val_accuracy: 0.8663415605095541 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.26044670494783456; val_accuracy: 0.9198845541401274 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.97
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2935493325779013; val_accuracy: 0.9091361464968153 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22740410365591382; val_accuracy: 0.9278463375796179 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.89
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.23078212497910117; val_accuracy: 0.9292396496815286 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22820211432067453; val_accuracy: 0.9298367834394905 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23022538065245957; val_accuracy: 0.9277468152866242 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21396767915149403; val_accuracy: 0.9351114649681529 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20664691697260376; val_accuracy: 0.9362062101910829 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.91
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2406325983773371; val_accuracy: 0.9254578025477707 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2007165170114511; val_accuracy: 0.9375995222929936 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2002127762575438; val_accuracy: 0.9381966560509554 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20302477623721596; val_accuracy: 0.9361066878980892 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1847357679229633; val_accuracy: 0.9440684713375797 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1864863808035471; val_accuracy: 0.9433718152866242 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.83
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18808725706426202; val_accuracy: 0.9408837579617835 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18429434861821734; val_accuracy: 0.9442675159235668 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19418826276899143; val_accuracy: 0.9405851910828026 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18597689781124424; val_accuracy: 0.9419785031847133 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1890985949831024; val_accuracy: 0.9411823248407644 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.88
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2161531978921526; val_accuracy: 0.933718152866242 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19805744076800194; val_accuracy: 0.9386942675159236 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19029761746430854; val_accuracy: 0.942078025477707 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1862846512086452; val_accuracy: 0.9416799363057324 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18358500031339134; val_accuracy: 0.9428742038216561 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.21; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1840770358018055; val_accuracy: 0.9428742038216561 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18246856640288783; val_accuracy: 0.9429737261146497 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18430422671186697; val_accuracy: 0.9432722929936306 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18498930667236352; val_accuracy: 0.9427746815286624 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18466200907329086; val_accuracy: 0.9434713375796179 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.88
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18168880543701207; val_accuracy: 0.943968949044586 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1829997546448829; val_accuracy: 0.943968949044586 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18514515703935533; val_accuracy: 0.9428742038216561 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18189140985820704; val_accuracy: 0.9437699044585988 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18323094649299695; val_accuracy: 0.9440684713375797 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18189843933863245; val_accuracy: 0.943968949044586 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18247892921137962; val_accuracy: 0.9440684713375797 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18232399507597752; val_accuracy: 0.9437699044585988 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18240775462168796; val_accuracy: 0.944765127388535 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18178808648779893; val_accuracy: 0.9443670382165605 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18183760511077893; val_accuracy: 0.9440684713375797 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18200936154195457; val_accuracy: 0.9444665605095541 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1827857877798141; val_accuracy: 0.9451632165605095 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_350_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 84511
elements in E: 17770400
fraction nonzero: 0.004755717372709674
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.08
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.21; acc: 0.34
Batch: 180; loss: 2.2; acc: 0.2
Batch: 200; loss: 2.15; acc: 0.2
Batch: 220; loss: 2.09; acc: 0.23
Batch: 240; loss: 1.84; acc: 0.55
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 0.97; acc: 0.67
Batch: 320; loss: 0.86; acc: 0.7
Batch: 340; loss: 1.1; acc: 0.69
Batch: 360; loss: 0.71; acc: 0.73
Batch: 380; loss: 0.73; acc: 0.72
Batch: 400; loss: 0.77; acc: 0.7
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.75
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.84; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 1.17; train_accuracy: 0.6 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3276230019463855; val_accuracy: 0.900577229299363 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3574886485744434; val_accuracy: 0.8942078025477707 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.81
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2145178459442345; val_accuracy: 0.93640525477707 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.30290941917782377; val_accuracy: 0.9060509554140127 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 1.02; acc: 0.7
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.59; acc: 0.78
Batch: 60; loss: 1.23; acc: 0.78
Batch: 80; loss: 1.3; acc: 0.73
Batch: 100; loss: 1.24; acc: 0.7
Batch: 120; loss: 1.18; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.88
Val Epoch over. val_loss: 1.0476622004417857; val_accuracy: 0.7361664012738853 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.78
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26842376369105025; val_accuracy: 0.9118232484076433 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19303301100138645; val_accuracy: 0.9448646496815286 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.86
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.89
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.48; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.31377685074783435; val_accuracy: 0.9067476114649682 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1944508004672588; val_accuracy: 0.9454617834394905 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.98
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2336141198731152; val_accuracy: 0.9321257961783439 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.8
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.19306137975139223; val_accuracy: 0.9449641719745223 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18752175873252236; val_accuracy: 0.945859872611465 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.89
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.13; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19528012672901912; val_accuracy: 0.9417794585987261 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.94
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19688203240371055; val_accuracy: 0.9430732484076433 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.08; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.19916451421038361; val_accuracy: 0.9423765923566879 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.19859413503651407; val_accuracy: 0.9404856687898089 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18968026392209303; val_accuracy: 0.9437699044585988 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19220720229160254; val_accuracy: 0.9448646496815286 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17297689768539112; val_accuracy: 0.9510350318471338 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.20708581183556538; val_accuracy: 0.9407842356687898 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16762772677051033; val_accuracy: 0.9509355095541401 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.51; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17585119348802383; val_accuracy: 0.9478503184713376 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16919812197043638; val_accuracy: 0.9495421974522293 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16562301015398304; val_accuracy: 0.9526273885350318 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.88
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16883469496373157; val_accuracy: 0.9517316878980892 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16676165329612744; val_accuracy: 0.9511345541401274 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17088354027764813; val_accuracy: 0.9510350318471338 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.174638984902839; val_accuracy: 0.9502388535031847 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16852602374496733; val_accuracy: 0.9506369426751592 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.86
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.08; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1695404099241184; val_accuracy: 0.9510350318471338 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16664403916639128; val_accuracy: 0.9511345541401274 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.16509285225135506; val_accuracy: 0.9518312101910829 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16561323867007427; val_accuracy: 0.9529259554140127 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.16617043034001522; val_accuracy: 0.9516321656050956 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.88
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16491109882570376; val_accuracy: 0.9521297770700637 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16486947093704704; val_accuracy: 0.9524283439490446 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1659880768578903; val_accuracy: 0.9525278662420382 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16489539131712003; val_accuracy: 0.95203025477707 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16343238218954415; val_accuracy: 0.9529259554140127 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.94
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16463808179091496; val_accuracy: 0.9529259554140127 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16370212028075934; val_accuracy: 0.9532245222929936 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16353305407865032; val_accuracy: 0.9528264331210191 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16360184526557375; val_accuracy: 0.9526273885350318 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1640163185964724; val_accuracy: 0.9523288216560509 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.16337718833593806; val_accuracy: 0.9525278662420382 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.84
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1631132348138056; val_accuracy: 0.9529259554140127 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.16400343896287262; val_accuracy: 0.9526273885350318 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1631056085750935; val_accuracy: 0.9526273885350318 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1640293835929245; val_accuracy: 0.9521297770700637 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.16399950159203475; val_accuracy: 0.9529259554140127 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 94868
elements in E: 19991700
fraction nonzero: 0.004745369328271232
Epoch 1 start
The current lr is: 1.0
/home/llang/thesis-intrinsic-dimension/logging_helper.py:44: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax1 = plt.subplots()
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.27; acc: 0.23
Batch: 140; loss: 2.26; acc: 0.2
Batch: 160; loss: 2.19; acc: 0.41
Batch: 180; loss: 2.19; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.34
Batch: 220; loss: 1.97; acc: 0.41
Batch: 240; loss: 1.58; acc: 0.72
Batch: 260; loss: 1.47; acc: 0.44
Batch: 280; loss: 0.97; acc: 0.67
Batch: 300; loss: 1.13; acc: 0.64
Batch: 320; loss: 0.92; acc: 0.72
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.87; acc: 0.66
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 1.17; acc: 0.64
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.81
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.66; acc: 0.75
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.8
Batch: 700; loss: 0.82; acc: 0.72
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.77
Train Epoch over. train_loss: 1.17; train_accuracy: 0.61 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.4032271107196049; val_accuracy: 0.8778861464968153 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.95; acc: 0.78
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.3796989690441235; val_accuracy: 0.8764928343949044 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.75
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.78
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.5739880741401843; val_accuracy: 0.8389729299363057 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.5; acc: 0.73
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.83
Val Epoch over. val_loss: 0.736872132037096; val_accuracy: 0.772890127388535 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.47748495011952274; val_accuracy: 0.8603702229299363 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24895515826762102; val_accuracy: 0.9233678343949044 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.19757158833609265; val_accuracy: 0.9414808917197452 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.1; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.6274417859923308; val_accuracy: 0.8246417197452229 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.19245379588976028; val_accuracy: 0.9419785031847133 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.18077920220649926; val_accuracy: 0.9454617834394905 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.17299512137842785; val_accuracy: 0.9475517515923567 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18092278300956555; val_accuracy: 0.9470541401273885 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1997136520399789; val_accuracy: 0.939390923566879 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17454246740052654; val_accuracy: 0.9481488853503185 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.89
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.17329743141486387; val_accuracy: 0.9497412420382165 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.09; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16847691848684268; val_accuracy: 0.951234076433121 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.18021015981864777; val_accuracy: 0.9462579617834395 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.20657729827294682; val_accuracy: 0.9369028662420382 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18589751386813297; val_accuracy: 0.9449641719745223 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18086738642423775; val_accuracy: 0.943968949044586 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16030020311854448; val_accuracy: 0.9538216560509554 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15883006010750297; val_accuracy: 0.9528264331210191 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15692459291238695; val_accuracy: 0.9545183121019108 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15522569410835102; val_accuracy: 0.9554140127388535 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15674733259021098; val_accuracy: 0.953125 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.88
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15592727980985763; val_accuracy: 0.9524283439490446 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1572828243017956; val_accuracy: 0.9541202229299363 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16915892479810746; val_accuracy: 0.9492436305732485 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16303845741756404; val_accuracy: 0.9534235668789809 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.12; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1588699119580779; val_accuracy: 0.9529259554140127 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1529057021163831; val_accuracy: 0.9539211783439491 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15319782599901696; val_accuracy: 0.9537221337579618 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.07; acc: 1.0
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1515383535557112; val_accuracy: 0.9556130573248408 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.154163929117713; val_accuracy: 0.9536226114649682 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1579101935598501; val_accuracy: 0.9535230891719745 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15603704985444714; val_accuracy: 0.9538216560509554 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15361799667500387; val_accuracy: 0.9556130573248408 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15234817251278338; val_accuracy: 0.9555135350318471 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15161430959109287; val_accuracy: 0.9555135350318471 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15291210294813867; val_accuracy: 0.955812101910828 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15154168809390373; val_accuracy: 0.9556130573248408 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15230405949957812; val_accuracy: 0.9547173566878981 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15266308639269727; val_accuracy: 0.9549164012738853 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1516717495574693; val_accuracy: 0.9560111464968153 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.92
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15212887675044642; val_accuracy: 0.955812101910828 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.88
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1513548223361088; val_accuracy: 0.9562101910828026 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.83
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15131553841434467; val_accuracy: 0.9549164012738853 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.88
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15180042547405145; val_accuracy: 0.9560111464968153 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1505848275865339; val_accuracy: 0.955015923566879 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15366820886636237; val_accuracy: 0.9548168789808917 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_450_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 105097
elements in E: 22213000
fraction nonzero: 0.004731328501328051
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.19
Batch: 120; loss: 2.26; acc: 0.27
Batch: 140; loss: 2.22; acc: 0.23
Batch: 160; loss: 2.15; acc: 0.44
Batch: 180; loss: 2.08; acc: 0.41
Batch: 200; loss: 1.88; acc: 0.48
Batch: 220; loss: 1.48; acc: 0.59
Batch: 240; loss: 1.05; acc: 0.69
Batch: 260; loss: 1.22; acc: 0.61
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.76; acc: 0.72
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.67; acc: 0.73
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.65; acc: 0.78
Batch: 660; loss: 0.29; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 1.02; train_accuracy: 0.66 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.27658001278900796; val_accuracy: 0.9163017515923567 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.6603433849515429; val_accuracy: 0.7960788216560509 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.72
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.3199368767013216; val_accuracy: 0.8999800955414012 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25697242330973313; val_accuracy: 0.9214769108280255 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26272499188780785; val_accuracy: 0.9165007961783439 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.64; acc: 0.75
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21453296274516234; val_accuracy: 0.9336186305732485 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.19160367349150834; val_accuracy: 0.93859474522293 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17807024741058897; val_accuracy: 0.9461584394904459 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.16337794136659356; val_accuracy: 0.9510350318471338 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.89
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 1.07; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.41539190434346535; val_accuracy: 0.8737062101910829 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14475202323145167; val_accuracy: 0.9574044585987261 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.08; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1591750283482348; val_accuracy: 0.9524283439490446 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15235821940716665; val_accuracy: 0.9567078025477707 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.18497435895690492; val_accuracy: 0.9455613057324841 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15212091435767283; val_accuracy: 0.9557125796178344 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.15661024126657255; val_accuracy: 0.9537221337579618 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.89
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16827344550353707; val_accuracy: 0.9492436305732485 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16824200324666727; val_accuracy: 0.9516321656050956 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1513013501716837; val_accuracy: 0.9568073248407644 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1658622413446569; val_accuracy: 0.9517316878980892 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14217099307733735; val_accuracy: 0.9590963375796179 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14712896096004044; val_accuracy: 0.9582006369426752 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1440698049582873; val_accuracy: 0.9592953821656051 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.07; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1411767648473667; val_accuracy: 0.959593949044586 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1459553490280156; val_accuracy: 0.9582006369426752 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1463242217922097; val_accuracy: 0.9594944267515924 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.89
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1442587591328059; val_accuracy: 0.9583996815286624 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15186587737718965; val_accuracy: 0.9556130573248408 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14710008029108215; val_accuracy: 0.9575039808917197 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1456694287858951; val_accuracy: 0.9579020700636943 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14227294276474386; val_accuracy: 0.9585987261146497 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14060622644699683; val_accuracy: 0.9592953821656051 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14124706459890127; val_accuracy: 0.9590963375796179 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14149158972368878; val_accuracy: 0.9593949044585988 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1413741933098834; val_accuracy: 0.9612858280254777 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.91
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14137809855305844; val_accuracy: 0.9581011146496815 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14078065318761357; val_accuracy: 0.9601910828025477 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14099245965718085; val_accuracy: 0.9602906050955414 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14005086088114105; val_accuracy: 0.9585987261146497 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.29; acc: 0.86
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13996603070598118; val_accuracy: 0.9601910828025477 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13962910795335176; val_accuracy: 0.9591958598726115 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13930133759833066; val_accuracy: 0.9597929936305732 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.88
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.92
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1392196270216043; val_accuracy: 0.958797770700637 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13948497679203178; val_accuracy: 0.9598925159235668 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13883100550882754; val_accuracy: 0.959593949044586 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13848427937004218; val_accuracy: 0.9601910828025477 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13863147606563037; val_accuracy: 0.9596934713375797 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.13; acc: 0.92
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13888258845278412; val_accuracy: 0.9599920382165605 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13918925480098482; val_accuracy: 0.959593949044586 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.91
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13979806462718045; val_accuracy: 0.9602906050955414 

plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/lenet/2020-01-19 22:21:20/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4386321/slurm_script: line 25: --print_freq=20: command not found
