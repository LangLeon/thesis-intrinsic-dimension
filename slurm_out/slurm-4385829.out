Namespace(batch_size=64, chunked=False, ddim_vs_acc=True, dense=False, device=device(type='cuda'), lr=1.0, model='lenet', n_epochs=50, non_wrapped=False, optimizer='SGD', parameter_correction=False, print_freq=20, print_prec=2, schedule=True, schedule_freq=10, schedule_gamma=0.4, seed=1, subspace_training=True, timestamp='2020-01-19 17:34:33')
nonzero elements in E: 10556
elements in E: 2221300
fraction nonzero: 0.0047521721514428485
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.14
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.19
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.12
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.2
Batch: 680; loss: 2.29; acc: 0.12
Batch: 700; loss: 2.3; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.17
Batch: 740; loss: 2.3; acc: 0.08
Batch: 760; loss: 2.27; acc: 0.09
Batch: 780; loss: 2.28; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.28; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.19
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.29; acc: 0.09
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.27; acc: 0.16
Val Epoch over. val_loss: 2.2819386454904156; val_accuracy: 0.14470541401273884 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.19
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.27; acc: 0.2
Batch: 200; loss: 2.26; acc: 0.2
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.26; acc: 0.2
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.17
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.28; acc: 0.17
Batch: 360; loss: 2.28; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.3
Batch: 400; loss: 2.27; acc: 0.19
Batch: 420; loss: 2.26; acc: 0.22
Batch: 440; loss: 2.25; acc: 0.2
Batch: 460; loss: 2.24; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.23
Batch: 500; loss: 2.24; acc: 0.23
Batch: 520; loss: 2.22; acc: 0.31
Batch: 540; loss: 2.26; acc: 0.27
Batch: 560; loss: 2.24; acc: 0.28
Batch: 580; loss: 2.23; acc: 0.3
Batch: 600; loss: 2.24; acc: 0.27
Batch: 620; loss: 2.24; acc: 0.33
Batch: 640; loss: 2.25; acc: 0.16
Batch: 660; loss: 2.24; acc: 0.19
Batch: 680; loss: 2.26; acc: 0.16
Batch: 700; loss: 2.21; acc: 0.25
Batch: 720; loss: 2.21; acc: 0.2
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.18; acc: 0.27
Batch: 780; loss: 2.19; acc: 0.14
Train Epoch over. train_loss: 2.26; train_accuracy: 0.19 

Batch: 0; loss: 2.22; acc: 0.2
Batch: 20; loss: 2.14; acc: 0.38
Batch: 40; loss: 2.19; acc: 0.23
Batch: 60; loss: 2.16; acc: 0.31
Batch: 80; loss: 2.18; acc: 0.23
Batch: 100; loss: 2.18; acc: 0.34
Batch: 120; loss: 2.19; acc: 0.31
Batch: 140; loss: 2.18; acc: 0.25
Val Epoch over. val_loss: 2.191037612356198; val_accuracy: 0.2300955414012739 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.19; acc: 0.22
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.17; acc: 0.12
Batch: 60; loss: 2.15; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.27
Batch: 100; loss: 2.1; acc: 0.39
Batch: 120; loss: 2.1; acc: 0.3
Batch: 140; loss: 2.11; acc: 0.25
Batch: 160; loss: 2.04; acc: 0.23
Batch: 180; loss: 2.01; acc: 0.34
Batch: 200; loss: 1.89; acc: 0.39
Batch: 220; loss: 1.92; acc: 0.38
Batch: 240; loss: 1.83; acc: 0.41
Batch: 260; loss: 1.95; acc: 0.27
Batch: 280; loss: 1.84; acc: 0.41
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.83; acc: 0.34
Batch: 340; loss: 1.93; acc: 0.31
Batch: 360; loss: 1.88; acc: 0.34
Batch: 380; loss: 1.84; acc: 0.34
Batch: 400; loss: 1.7; acc: 0.47
Batch: 420; loss: 1.65; acc: 0.47
Batch: 440; loss: 1.59; acc: 0.41
Batch: 460; loss: 1.89; acc: 0.36
Batch: 480; loss: 1.75; acc: 0.41
Batch: 500; loss: 1.66; acc: 0.39
Batch: 520; loss: 1.7; acc: 0.39
Batch: 540; loss: 1.72; acc: 0.41
Batch: 560; loss: 1.7; acc: 0.38
Batch: 580; loss: 1.62; acc: 0.5
Batch: 600; loss: 1.88; acc: 0.27
Batch: 620; loss: 1.72; acc: 0.33
Batch: 640; loss: 1.76; acc: 0.36
Batch: 660; loss: 1.65; acc: 0.52
Batch: 680; loss: 1.44; acc: 0.48
Batch: 700; loss: 1.41; acc: 0.56
Batch: 720; loss: 1.54; acc: 0.47
Batch: 740; loss: 1.65; acc: 0.42
Batch: 760; loss: 1.43; acc: 0.47
Batch: 780; loss: 1.34; acc: 0.59
Train Epoch over. train_loss: 1.82; train_accuracy: 0.38 

Batch: 0; loss: 1.75; acc: 0.47
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.23; acc: 0.66
Batch: 60; loss: 1.26; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 1.64; acc: 0.36
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.26; acc: 0.55
Val Epoch over. val_loss: 1.535153891250586; val_accuracy: 0.4912420382165605 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.52
Batch: 20; loss: 1.55; acc: 0.5
Batch: 40; loss: 1.64; acc: 0.42
Batch: 60; loss: 1.52; acc: 0.45
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.62; acc: 0.44
Batch: 120; loss: 1.58; acc: 0.5
Batch: 140; loss: 1.52; acc: 0.48
Batch: 160; loss: 1.49; acc: 0.53
Batch: 180; loss: 1.42; acc: 0.58
Batch: 200; loss: 1.64; acc: 0.44
Batch: 220; loss: 1.69; acc: 0.44
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.48; acc: 0.47
Batch: 280; loss: 1.53; acc: 0.5
Batch: 300; loss: 1.31; acc: 0.55
Batch: 320; loss: 1.48; acc: 0.5
Batch: 340; loss: 1.8; acc: 0.47
Batch: 360; loss: 1.36; acc: 0.55
Batch: 380; loss: 1.54; acc: 0.48
Batch: 400; loss: 1.68; acc: 0.48
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.32; acc: 0.55
Batch: 460; loss: 1.76; acc: 0.39
Batch: 480; loss: 1.3; acc: 0.61
Batch: 500; loss: 1.37; acc: 0.55
Batch: 520; loss: 1.76; acc: 0.52
Batch: 540; loss: 1.34; acc: 0.56
Batch: 560; loss: 1.71; acc: 0.47
Batch: 580; loss: 1.43; acc: 0.55
Batch: 600; loss: 1.58; acc: 0.52
Batch: 620; loss: 1.3; acc: 0.55
Batch: 640; loss: 1.77; acc: 0.56
Batch: 660; loss: 1.58; acc: 0.48
Batch: 680; loss: 1.54; acc: 0.53
Batch: 700; loss: 1.36; acc: 0.55
Batch: 720; loss: 1.31; acc: 0.59
Batch: 740; loss: 1.53; acc: 0.5
Batch: 760; loss: 1.38; acc: 0.52
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.47; train_accuracy: 0.52 

Batch: 0; loss: 1.47; acc: 0.53
Batch: 20; loss: 1.38; acc: 0.56
Batch: 40; loss: 1.32; acc: 0.53
Batch: 60; loss: 1.27; acc: 0.62
Batch: 80; loss: 1.13; acc: 0.7
Batch: 100; loss: 1.36; acc: 0.58
Batch: 120; loss: 1.42; acc: 0.52
Batch: 140; loss: 1.43; acc: 0.56
Val Epoch over. val_loss: 1.4101910253239285; val_accuracy: 0.5436902866242038 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.39; acc: 0.59
Batch: 40; loss: 1.57; acc: 0.53
Batch: 60; loss: 1.25; acc: 0.52
Batch: 80; loss: 1.38; acc: 0.58
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.55
Batch: 160; loss: 1.22; acc: 0.59
Batch: 180; loss: 1.3; acc: 0.53
Batch: 200; loss: 1.67; acc: 0.45
Batch: 220; loss: 1.73; acc: 0.48
Batch: 240; loss: 1.33; acc: 0.61
Batch: 260; loss: 1.58; acc: 0.47
Batch: 280; loss: 1.65; acc: 0.48
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.18; acc: 0.61
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.43; acc: 0.58
Batch: 380; loss: 1.55; acc: 0.52
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 1.4; acc: 0.56
Batch: 440; loss: 1.48; acc: 0.48
Batch: 460; loss: 1.41; acc: 0.55
Batch: 480; loss: 1.28; acc: 0.61
Batch: 500; loss: 1.46; acc: 0.5
Batch: 520; loss: 1.37; acc: 0.56
Batch: 540; loss: 1.32; acc: 0.61
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.3; acc: 0.53
Batch: 600; loss: 1.34; acc: 0.61
Batch: 620; loss: 1.26; acc: 0.58
Batch: 640; loss: 1.47; acc: 0.56
Batch: 660; loss: 1.42; acc: 0.52
Batch: 680; loss: 1.33; acc: 0.61
Batch: 700; loss: 1.39; acc: 0.48
Batch: 720; loss: 1.58; acc: 0.48
Batch: 740; loss: 1.46; acc: 0.48
Batch: 760; loss: 1.31; acc: 0.59
Batch: 780; loss: 1.28; acc: 0.62
Train Epoch over. train_loss: 1.39; train_accuracy: 0.55 

Batch: 0; loss: 1.37; acc: 0.52
Batch: 20; loss: 1.56; acc: 0.56
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.16; acc: 0.62
Batch: 80; loss: 1.14; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.54; acc: 0.45
Batch: 140; loss: 1.35; acc: 0.62
Val Epoch over. val_loss: 1.3714955988203643; val_accuracy: 0.5499601910828026 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.62
Batch: 40; loss: 1.48; acc: 0.52
Batch: 60; loss: 1.36; acc: 0.56
Batch: 80; loss: 1.48; acc: 0.52
Batch: 100; loss: 1.32; acc: 0.53
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.24; acc: 0.59
Batch: 160; loss: 1.14; acc: 0.64
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.28; acc: 0.61
Batch: 220; loss: 1.4; acc: 0.58
Batch: 240; loss: 1.31; acc: 0.53
Batch: 260; loss: 1.37; acc: 0.59
Batch: 280; loss: 1.39; acc: 0.53
Batch: 300; loss: 1.54; acc: 0.44
Batch: 320; loss: 1.43; acc: 0.48
Batch: 340; loss: 1.27; acc: 0.58
Batch: 360; loss: 1.38; acc: 0.58
Batch: 380; loss: 1.48; acc: 0.55
Batch: 400; loss: 1.33; acc: 0.56
Batch: 420; loss: 1.37; acc: 0.55
Batch: 440; loss: 1.12; acc: 0.67
Batch: 460; loss: 1.07; acc: 0.72
Batch: 480; loss: 1.39; acc: 0.53
Batch: 500; loss: 1.39; acc: 0.58
Batch: 520; loss: 1.38; acc: 0.58
Batch: 540; loss: 1.96; acc: 0.42
Batch: 560; loss: 1.64; acc: 0.41
Batch: 580; loss: 1.27; acc: 0.48
Batch: 600; loss: 1.37; acc: 0.61
Batch: 620; loss: 1.22; acc: 0.64
Batch: 640; loss: 1.09; acc: 0.64
Batch: 660; loss: 1.08; acc: 0.64
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.24; acc: 0.56
Batch: 720; loss: 1.39; acc: 0.52
Batch: 740; loss: 1.49; acc: 0.52
Batch: 760; loss: 1.34; acc: 0.56
Batch: 780; loss: 1.5; acc: 0.5
Train Epoch over. train_loss: 1.36; train_accuracy: 0.56 

Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.42; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.61
Batch: 60; loss: 1.39; acc: 0.58
Batch: 80; loss: 1.22; acc: 0.56
Batch: 100; loss: 1.33; acc: 0.62
Batch: 120; loss: 1.49; acc: 0.45
Batch: 140; loss: 1.41; acc: 0.56
Val Epoch over. val_loss: 1.3880480789834526; val_accuracy: 0.5443869426751592 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.42
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 1.34; acc: 0.53
Batch: 60; loss: 1.23; acc: 0.58
Batch: 80; loss: 1.62; acc: 0.5
Batch: 100; loss: 1.24; acc: 0.61
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.33; acc: 0.53
Batch: 160; loss: 1.25; acc: 0.55
Batch: 180; loss: 1.36; acc: 0.58
Batch: 200; loss: 1.21; acc: 0.5
Batch: 220; loss: 1.4; acc: 0.53
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.39; acc: 0.47
Batch: 280; loss: 1.37; acc: 0.55
Batch: 300; loss: 1.22; acc: 0.58
Batch: 320; loss: 1.62; acc: 0.5
Batch: 340; loss: 1.51; acc: 0.53
Batch: 360; loss: 1.37; acc: 0.5
Batch: 380; loss: 1.27; acc: 0.62
Batch: 400; loss: 1.34; acc: 0.58
Batch: 420; loss: 1.4; acc: 0.48
Batch: 440; loss: 0.99; acc: 0.7
Batch: 460; loss: 1.51; acc: 0.5
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.39; acc: 0.59
Batch: 520; loss: 1.28; acc: 0.59
Batch: 540; loss: 1.36; acc: 0.52
Batch: 560; loss: 1.22; acc: 0.58
Batch: 580; loss: 1.19; acc: 0.56
Batch: 600; loss: 1.49; acc: 0.53
Batch: 620; loss: 1.42; acc: 0.45
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.41; acc: 0.5
Batch: 680; loss: 1.43; acc: 0.5
Batch: 700; loss: 1.23; acc: 0.62
Batch: 720; loss: 1.24; acc: 0.64
Batch: 740; loss: 1.3; acc: 0.53
Batch: 760; loss: 1.03; acc: 0.66
Batch: 780; loss: 1.31; acc: 0.53
Train Epoch over. train_loss: 1.32; train_accuracy: 0.57 

Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.01; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.56
Batch: 80; loss: 1.07; acc: 0.61
Batch: 100; loss: 1.24; acc: 0.58
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.19; acc: 0.53
Val Epoch over. val_loss: 1.2486038682567087; val_accuracy: 0.5887738853503185 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.37; acc: 0.48
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.24; acc: 0.56
Batch: 80; loss: 1.36; acc: 0.5
Batch: 100; loss: 1.21; acc: 0.58
Batch: 120; loss: 1.2; acc: 0.58
Batch: 140; loss: 1.17; acc: 0.58
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 1.39; acc: 0.52
Batch: 200; loss: 1.01; acc: 0.64
Batch: 220; loss: 1.3; acc: 0.61
Batch: 240; loss: 1.37; acc: 0.52
Batch: 260; loss: 1.57; acc: 0.52
Batch: 280; loss: 1.17; acc: 0.59
Batch: 300; loss: 1.12; acc: 0.55
Batch: 320; loss: 1.41; acc: 0.61
Batch: 340; loss: 1.3; acc: 0.66
Batch: 360; loss: 1.17; acc: 0.59
Batch: 380; loss: 1.48; acc: 0.52
Batch: 400; loss: 1.34; acc: 0.62
Batch: 420; loss: 1.14; acc: 0.66
Batch: 440; loss: 1.23; acc: 0.5
Batch: 460; loss: 1.28; acc: 0.67
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 1.44; acc: 0.5
Batch: 520; loss: 1.34; acc: 0.58
Batch: 540; loss: 1.19; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.59
Batch: 580; loss: 1.49; acc: 0.58
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.09; acc: 0.56
Batch: 640; loss: 1.26; acc: 0.61
Batch: 660; loss: 1.31; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.55
Batch: 700; loss: 1.32; acc: 0.55
Batch: 720; loss: 1.11; acc: 0.66
Batch: 740; loss: 1.35; acc: 0.52
Batch: 760; loss: 1.06; acc: 0.61
Batch: 780; loss: 1.07; acc: 0.64
Train Epoch over. train_loss: 1.29; train_accuracy: 0.57 

Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.27; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.7
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.66
Batch: 100; loss: 1.35; acc: 0.56
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.62
Val Epoch over. val_loss: 1.2654587254402743; val_accuracy: 0.5766321656050956 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.48; acc: 0.5
Batch: 40; loss: 1.37; acc: 0.55
Batch: 60; loss: 1.48; acc: 0.42
Batch: 80; loss: 1.47; acc: 0.52
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.44; acc: 0.5
Batch: 140; loss: 1.17; acc: 0.67
Batch: 160; loss: 1.27; acc: 0.61
Batch: 180; loss: 1.46; acc: 0.53
Batch: 200; loss: 1.26; acc: 0.58
Batch: 220; loss: 1.32; acc: 0.53
Batch: 240; loss: 1.36; acc: 0.5
Batch: 260; loss: 1.4; acc: 0.58
Batch: 280; loss: 1.26; acc: 0.59
Batch: 300; loss: 1.33; acc: 0.52
Batch: 320; loss: 1.19; acc: 0.62
Batch: 340; loss: 1.29; acc: 0.59
Batch: 360; loss: 1.52; acc: 0.55
Batch: 380; loss: 1.39; acc: 0.52
Batch: 400; loss: 1.31; acc: 0.58
Batch: 420; loss: 1.26; acc: 0.58
Batch: 440; loss: 1.07; acc: 0.64
Batch: 460; loss: 1.06; acc: 0.59
Batch: 480; loss: 1.24; acc: 0.55
Batch: 500; loss: 1.35; acc: 0.5
Batch: 520; loss: 1.19; acc: 0.55
Batch: 540; loss: 1.56; acc: 0.48
Batch: 560; loss: 1.43; acc: 0.58
Batch: 580; loss: 1.29; acc: 0.52
Batch: 600; loss: 1.37; acc: 0.58
Batch: 620; loss: 1.18; acc: 0.61
Batch: 640; loss: 1.3; acc: 0.55
Batch: 660; loss: 1.2; acc: 0.66
Batch: 680; loss: 1.22; acc: 0.58
Batch: 700; loss: 1.23; acc: 0.5
Batch: 720; loss: 1.24; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.53
Batch: 760; loss: 1.14; acc: 0.61
Batch: 780; loss: 1.3; acc: 0.55
Train Epoch over. train_loss: 1.29; train_accuracy: 0.57 

Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 1.33; acc: 0.55
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.4; acc: 0.45
Batch: 120; loss: 1.41; acc: 0.48
Batch: 140; loss: 1.18; acc: 0.56
Val Epoch over. val_loss: 1.2633921097797953; val_accuracy: 0.5675756369426752 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.52
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 1.37; acc: 0.52
Batch: 60; loss: 1.39; acc: 0.55
Batch: 80; loss: 1.41; acc: 0.52
Batch: 100; loss: 1.12; acc: 0.61
Batch: 120; loss: 1.36; acc: 0.53
Batch: 140; loss: 1.67; acc: 0.5
Batch: 160; loss: 1.08; acc: 0.61
Batch: 180; loss: 1.38; acc: 0.56
Batch: 200; loss: 1.33; acc: 0.52
Batch: 220; loss: 1.41; acc: 0.52
Batch: 240; loss: 1.35; acc: 0.59
Batch: 260; loss: 1.34; acc: 0.48
Batch: 280; loss: 0.92; acc: 0.72
Batch: 300; loss: 1.08; acc: 0.64
Batch: 320; loss: 1.65; acc: 0.45
Batch: 340; loss: 1.55; acc: 0.47
Batch: 360; loss: 1.55; acc: 0.53
Batch: 380; loss: 1.28; acc: 0.64
Batch: 400; loss: 1.04; acc: 0.67
Batch: 420; loss: 1.47; acc: 0.53
Batch: 440; loss: 1.28; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.58
Batch: 480; loss: 1.43; acc: 0.56
Batch: 500; loss: 1.06; acc: 0.66
Batch: 520; loss: 1.34; acc: 0.59
Batch: 540; loss: 1.41; acc: 0.61
Batch: 560; loss: 1.2; acc: 0.55
Batch: 580; loss: 1.41; acc: 0.61
Batch: 600; loss: 1.11; acc: 0.64
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 1.12; acc: 0.67
Batch: 660; loss: 1.37; acc: 0.53
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.38; acc: 0.52
Batch: 720; loss: 1.44; acc: 0.47
Batch: 740; loss: 1.45; acc: 0.53
Batch: 760; loss: 1.2; acc: 0.55
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.29; train_accuracy: 0.57 

Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.5
Batch: 40; loss: 1.05; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 1.41; acc: 0.47
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 1.3; acc: 0.59
Val Epoch over. val_loss: 1.3023074394578387; val_accuracy: 0.5613057324840764 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.9; acc: 0.48
Batch: 20; loss: 1.29; acc: 0.53
Batch: 40; loss: 1.36; acc: 0.55
Batch: 60; loss: 1.03; acc: 0.72
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.47
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.3; acc: 0.56
Batch: 180; loss: 1.42; acc: 0.5
Batch: 200; loss: 1.18; acc: 0.64
Batch: 220; loss: 1.1; acc: 0.72
Batch: 240; loss: 1.26; acc: 0.59
Batch: 260; loss: 1.17; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.58
Batch: 300; loss: 1.45; acc: 0.53
Batch: 320; loss: 1.48; acc: 0.47
Batch: 340; loss: 1.55; acc: 0.52
Batch: 360; loss: 1.24; acc: 0.58
Batch: 380; loss: 1.33; acc: 0.53
Batch: 400; loss: 1.41; acc: 0.48
Batch: 420; loss: 1.54; acc: 0.52
Batch: 440; loss: 1.23; acc: 0.55
Batch: 460; loss: 1.35; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.59
Batch: 500; loss: 1.15; acc: 0.62
Batch: 520; loss: 1.26; acc: 0.62
Batch: 540; loss: 1.21; acc: 0.56
Batch: 560; loss: 1.35; acc: 0.56
Batch: 580; loss: 1.21; acc: 0.55
Batch: 600; loss: 1.17; acc: 0.59
Batch: 620; loss: 1.36; acc: 0.52
Batch: 640; loss: 1.17; acc: 0.58
Batch: 660; loss: 1.21; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.53
Batch: 700; loss: 1.36; acc: 0.53
Batch: 720; loss: 1.25; acc: 0.61
Batch: 740; loss: 1.54; acc: 0.55
Batch: 760; loss: 1.42; acc: 0.55
Batch: 780; loss: 1.21; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.23; acc: 0.56
Batch: 40; loss: 1.01; acc: 0.69
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 0.94; acc: 0.72
Batch: 100; loss: 1.33; acc: 0.42
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 1.23; acc: 0.55
Val Epoch over. val_loss: 1.249552218018064; val_accuracy: 0.5769307324840764 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.24; acc: 0.53
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 1.52; acc: 0.58
Batch: 60; loss: 1.2; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.47; acc: 0.53
Batch: 120; loss: 1.28; acc: 0.53
Batch: 140; loss: 1.13; acc: 0.62
Batch: 160; loss: 1.19; acc: 0.61
Batch: 180; loss: 1.24; acc: 0.61
Batch: 200; loss: 1.12; acc: 0.62
Batch: 220; loss: 1.31; acc: 0.59
Batch: 240; loss: 1.16; acc: 0.66
Batch: 260; loss: 1.42; acc: 0.52
Batch: 280; loss: 1.13; acc: 0.61
Batch: 300; loss: 1.23; acc: 0.53
Batch: 320; loss: 1.36; acc: 0.5
Batch: 340; loss: 1.18; acc: 0.56
Batch: 360; loss: 1.56; acc: 0.5
Batch: 380; loss: 1.34; acc: 0.52
Batch: 400; loss: 1.35; acc: 0.48
Batch: 420; loss: 1.4; acc: 0.55
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.28; acc: 0.55
Batch: 480; loss: 1.43; acc: 0.53
Batch: 500; loss: 1.36; acc: 0.53
Batch: 520; loss: 1.39; acc: 0.61
Batch: 540; loss: 1.13; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.5
Batch: 580; loss: 1.72; acc: 0.45
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.26; acc: 0.61
Batch: 640; loss: 1.39; acc: 0.48
Batch: 660; loss: 1.03; acc: 0.64
Batch: 680; loss: 1.56; acc: 0.38
Batch: 700; loss: 1.2; acc: 0.59
Batch: 720; loss: 1.11; acc: 0.62
Batch: 740; loss: 1.44; acc: 0.62
Batch: 760; loss: 1.38; acc: 0.48
Batch: 780; loss: 1.32; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.21; acc: 0.56
Batch: 40; loss: 0.93; acc: 0.66
Batch: 60; loss: 1.21; acc: 0.58
Batch: 80; loss: 0.93; acc: 0.75
Batch: 100; loss: 1.36; acc: 0.42
Batch: 120; loss: 1.36; acc: 0.55
Batch: 140; loss: 1.13; acc: 0.62
Val Epoch over. val_loss: 1.2271265429296312; val_accuracy: 0.5890724522292994 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.46; acc: 0.52
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 1.23; acc: 0.56
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.11; acc: 0.59
Batch: 100; loss: 1.16; acc: 0.61
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 1.14; acc: 0.69
Batch: 160; loss: 1.0; acc: 0.66
Batch: 180; loss: 1.38; acc: 0.52
Batch: 200; loss: 1.27; acc: 0.66
Batch: 220; loss: 1.2; acc: 0.58
Batch: 240; loss: 1.25; acc: 0.62
Batch: 260; loss: 1.18; acc: 0.62
Batch: 280; loss: 1.57; acc: 0.44
Batch: 300; loss: 1.33; acc: 0.56
Batch: 320; loss: 1.12; acc: 0.64
Batch: 340; loss: 1.24; acc: 0.59
Batch: 360; loss: 1.39; acc: 0.59
Batch: 380; loss: 1.15; acc: 0.62
Batch: 400; loss: 1.4; acc: 0.61
Batch: 420; loss: 1.26; acc: 0.61
Batch: 440; loss: 1.48; acc: 0.5
Batch: 460; loss: 1.17; acc: 0.58
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.37; acc: 0.55
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.46; acc: 0.53
Batch: 560; loss: 1.31; acc: 0.52
Batch: 580; loss: 1.01; acc: 0.73
Batch: 600; loss: 1.48; acc: 0.52
Batch: 620; loss: 1.01; acc: 0.64
Batch: 640; loss: 0.9; acc: 0.69
Batch: 660; loss: 1.5; acc: 0.52
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.37; acc: 0.55
Batch: 720; loss: 1.37; acc: 0.56
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 1.32; acc: 0.59
Batch: 780; loss: 1.34; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.61
Batch: 80; loss: 0.94; acc: 0.75
Batch: 100; loss: 1.31; acc: 0.42
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 1.14; acc: 0.59
Val Epoch over. val_loss: 1.2221298992254173; val_accuracy: 0.59265525477707 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.07; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 1.0; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.55
Batch: 80; loss: 1.11; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.48; acc: 0.48
Batch: 160; loss: 1.29; acc: 0.5
Batch: 180; loss: 1.32; acc: 0.52
Batch: 200; loss: 1.29; acc: 0.53
Batch: 220; loss: 1.26; acc: 0.55
Batch: 240; loss: 1.11; acc: 0.64
Batch: 260; loss: 1.27; acc: 0.58
Batch: 280; loss: 1.54; acc: 0.48
Batch: 300; loss: 1.15; acc: 0.62
Batch: 320; loss: 1.31; acc: 0.58
Batch: 340; loss: 1.37; acc: 0.56
Batch: 360; loss: 1.25; acc: 0.56
Batch: 380; loss: 1.17; acc: 0.62
Batch: 400; loss: 1.13; acc: 0.64
Batch: 420; loss: 1.37; acc: 0.64
Batch: 440; loss: 1.55; acc: 0.53
Batch: 460; loss: 1.51; acc: 0.44
Batch: 480; loss: 1.33; acc: 0.55
Batch: 500; loss: 1.16; acc: 0.61
Batch: 520; loss: 1.46; acc: 0.47
Batch: 540; loss: 1.19; acc: 0.59
Batch: 560; loss: 1.42; acc: 0.53
Batch: 580; loss: 1.11; acc: 0.64
Batch: 600; loss: 1.29; acc: 0.59
Batch: 620; loss: 1.14; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.62
Batch: 660; loss: 1.11; acc: 0.62
Batch: 680; loss: 1.5; acc: 0.53
Batch: 700; loss: 1.25; acc: 0.62
Batch: 720; loss: 1.26; acc: 0.55
Batch: 740; loss: 1.35; acc: 0.52
Batch: 760; loss: 1.26; acc: 0.64
Batch: 780; loss: 1.37; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.97; acc: 0.66
Batch: 60; loss: 1.18; acc: 0.61
Batch: 80; loss: 0.95; acc: 0.73
Batch: 100; loss: 1.35; acc: 0.47
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 1.16; acc: 0.61
Val Epoch over. val_loss: 1.2244274764303948; val_accuracy: 0.5931528662420382 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.5
Batch: 40; loss: 1.01; acc: 0.62
Batch: 60; loss: 1.24; acc: 0.53
Batch: 80; loss: 1.08; acc: 0.62
Batch: 100; loss: 1.15; acc: 0.61
Batch: 120; loss: 1.26; acc: 0.56
Batch: 140; loss: 1.32; acc: 0.47
Batch: 160; loss: 1.39; acc: 0.56
Batch: 180; loss: 1.4; acc: 0.56
Batch: 200; loss: 1.26; acc: 0.56
Batch: 220; loss: 1.29; acc: 0.56
Batch: 240; loss: 1.3; acc: 0.52
Batch: 260; loss: 1.25; acc: 0.64
Batch: 280; loss: 1.38; acc: 0.55
Batch: 300; loss: 1.26; acc: 0.53
Batch: 320; loss: 1.29; acc: 0.66
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.4; acc: 0.59
Batch: 380; loss: 1.17; acc: 0.59
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.31; acc: 0.52
Batch: 440; loss: 1.54; acc: 0.45
Batch: 460; loss: 1.41; acc: 0.47
Batch: 480; loss: 1.35; acc: 0.59
Batch: 500; loss: 1.09; acc: 0.62
Batch: 520; loss: 1.25; acc: 0.58
Batch: 540; loss: 1.11; acc: 0.55
Batch: 560; loss: 0.99; acc: 0.64
Batch: 580; loss: 1.33; acc: 0.55
Batch: 600; loss: 1.23; acc: 0.58
Batch: 620; loss: 1.27; acc: 0.66
Batch: 640; loss: 1.23; acc: 0.59
Batch: 660; loss: 1.18; acc: 0.61
Batch: 680; loss: 1.16; acc: 0.58
Batch: 700; loss: 1.31; acc: 0.58
Batch: 720; loss: 1.26; acc: 0.55
Batch: 740; loss: 1.16; acc: 0.62
Batch: 760; loss: 1.04; acc: 0.67
Batch: 780; loss: 1.52; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.53
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.36; acc: 0.45
Batch: 120; loss: 1.31; acc: 0.56
Batch: 140; loss: 1.16; acc: 0.59
Val Epoch over. val_loss: 1.2254538798028496; val_accuracy: 0.5843949044585988 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 1.02; acc: 0.67
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.59
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 1.2; acc: 0.59
Batch: 140; loss: 1.33; acc: 0.56
Batch: 160; loss: 1.16; acc: 0.52
Batch: 180; loss: 1.44; acc: 0.58
Batch: 200; loss: 1.18; acc: 0.61
Batch: 220; loss: 1.29; acc: 0.64
Batch: 240; loss: 1.06; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.47; acc: 0.5
Batch: 300; loss: 1.41; acc: 0.52
Batch: 320; loss: 1.33; acc: 0.61
Batch: 340; loss: 1.37; acc: 0.53
Batch: 360; loss: 1.34; acc: 0.56
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 1.25; acc: 0.59
Batch: 420; loss: 1.05; acc: 0.69
Batch: 440; loss: 1.47; acc: 0.56
Batch: 460; loss: 1.8; acc: 0.47
Batch: 480; loss: 1.28; acc: 0.61
Batch: 500; loss: 1.26; acc: 0.58
Batch: 520; loss: 1.36; acc: 0.56
Batch: 540; loss: 1.21; acc: 0.61
Batch: 560; loss: 1.31; acc: 0.59
Batch: 580; loss: 1.63; acc: 0.52
Batch: 600; loss: 1.56; acc: 0.5
Batch: 620; loss: 1.35; acc: 0.56
Batch: 640; loss: 1.31; acc: 0.62
Batch: 660; loss: 1.37; acc: 0.47
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 1.53; acc: 0.45
Batch: 720; loss: 1.45; acc: 0.58
Batch: 740; loss: 1.34; acc: 0.47
Batch: 760; loss: 1.21; acc: 0.61
Batch: 780; loss: 1.13; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.97; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 0.93; acc: 0.7
Batch: 100; loss: 1.35; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.14; acc: 0.61
Val Epoch over. val_loss: 1.2169738586541194; val_accuracy: 0.5894705414012739 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.58
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 1.42; acc: 0.5
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 1.47; acc: 0.52
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.55
Batch: 160; loss: 1.42; acc: 0.58
Batch: 180; loss: 1.6; acc: 0.39
Batch: 200; loss: 1.38; acc: 0.55
Batch: 220; loss: 1.26; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.67
Batch: 260; loss: 1.39; acc: 0.56
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.2; acc: 0.61
Batch: 320; loss: 0.98; acc: 0.73
Batch: 340; loss: 1.4; acc: 0.58
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.21; acc: 0.58
Batch: 400; loss: 1.37; acc: 0.55
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 1.56; acc: 0.47
Batch: 480; loss: 1.09; acc: 0.56
Batch: 500; loss: 1.41; acc: 0.55
Batch: 520; loss: 1.42; acc: 0.45
Batch: 540; loss: 1.22; acc: 0.56
Batch: 560; loss: 1.37; acc: 0.61
Batch: 580; loss: 1.08; acc: 0.66
Batch: 600; loss: 1.05; acc: 0.59
Batch: 620; loss: 1.47; acc: 0.52
Batch: 640; loss: 1.23; acc: 0.59
Batch: 660; loss: 1.25; acc: 0.59
Batch: 680; loss: 1.27; acc: 0.59
Batch: 700; loss: 1.0; acc: 0.67
Batch: 720; loss: 1.15; acc: 0.66
Batch: 740; loss: 1.33; acc: 0.58
Batch: 760; loss: 1.22; acc: 0.62
Batch: 780; loss: 1.31; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.19; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.22; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.36; acc: 0.42
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 1.15; acc: 0.61
Val Epoch over. val_loss: 1.2209292319929523; val_accuracy: 0.5890724522292994 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.32; acc: 0.56
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 1.5; acc: 0.47
Batch: 60; loss: 1.39; acc: 0.5
Batch: 80; loss: 1.32; acc: 0.55
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 1.53; acc: 0.45
Batch: 160; loss: 1.37; acc: 0.55
Batch: 180; loss: 1.15; acc: 0.66
Batch: 200; loss: 1.19; acc: 0.64
Batch: 220; loss: 1.19; acc: 0.58
Batch: 240; loss: 1.12; acc: 0.64
Batch: 260; loss: 1.45; acc: 0.53
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 1.25; acc: 0.56
Batch: 320; loss: 1.17; acc: 0.61
Batch: 340; loss: 1.15; acc: 0.62
Batch: 360; loss: 0.96; acc: 0.7
Batch: 380; loss: 1.34; acc: 0.58
Batch: 400; loss: 1.16; acc: 0.58
Batch: 420; loss: 1.58; acc: 0.47
Batch: 440; loss: 1.15; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 0.91; acc: 0.7
Batch: 540; loss: 1.11; acc: 0.56
Batch: 560; loss: 1.14; acc: 0.59
Batch: 580; loss: 1.3; acc: 0.53
Batch: 600; loss: 1.17; acc: 0.58
Batch: 620; loss: 0.98; acc: 0.62
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.09; acc: 0.64
Batch: 700; loss: 1.12; acc: 0.72
Batch: 720; loss: 1.22; acc: 0.58
Batch: 740; loss: 1.53; acc: 0.48
Batch: 760; loss: 0.94; acc: 0.64
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.32; acc: 0.45
Batch: 120; loss: 1.28; acc: 0.56
Batch: 140; loss: 1.13; acc: 0.59
Val Epoch over. val_loss: 1.2177687382242481; val_accuracy: 0.5943471337579618 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.24; acc: 0.59
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 1.46; acc: 0.47
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.24; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.45
Batch: 120; loss: 1.24; acc: 0.52
Batch: 140; loss: 1.3; acc: 0.53
Batch: 160; loss: 1.16; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.48; acc: 0.5
Batch: 240; loss: 1.42; acc: 0.52
Batch: 260; loss: 1.2; acc: 0.58
Batch: 280; loss: 1.36; acc: 0.61
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 1.29; acc: 0.59
Batch: 360; loss: 1.25; acc: 0.56
Batch: 380; loss: 1.6; acc: 0.5
Batch: 400; loss: 1.42; acc: 0.59
Batch: 420; loss: 1.24; acc: 0.55
Batch: 440; loss: 1.57; acc: 0.44
Batch: 460; loss: 1.13; acc: 0.61
Batch: 480; loss: 1.31; acc: 0.52
Batch: 500; loss: 1.0; acc: 0.69
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.07; acc: 0.59
Batch: 560; loss: 0.92; acc: 0.7
Batch: 580; loss: 1.36; acc: 0.52
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 1.34; acc: 0.52
Batch: 640; loss: 1.38; acc: 0.53
Batch: 660; loss: 1.19; acc: 0.59
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.56
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 1.14; acc: 0.59
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.42; acc: 0.45
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.61
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.33; acc: 0.44
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.14; acc: 0.59
Val Epoch over. val_loss: 1.220463942190644; val_accuracy: 0.5915605095541401 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.21; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 1.27; acc: 0.59
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 1.13; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.53
Batch: 120; loss: 1.36; acc: 0.53
Batch: 140; loss: 1.43; acc: 0.48
Batch: 160; loss: 1.39; acc: 0.58
Batch: 180; loss: 1.22; acc: 0.52
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.24; acc: 0.58
Batch: 240; loss: 1.24; acc: 0.61
Batch: 260; loss: 1.48; acc: 0.58
Batch: 280; loss: 1.16; acc: 0.61
Batch: 300; loss: 1.37; acc: 0.56
Batch: 320; loss: 1.32; acc: 0.61
Batch: 340; loss: 1.38; acc: 0.53
Batch: 360; loss: 1.23; acc: 0.58
Batch: 380; loss: 1.19; acc: 0.67
Batch: 400; loss: 1.21; acc: 0.67
Batch: 420; loss: 1.16; acc: 0.66
Batch: 440; loss: 1.2; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.53
Batch: 480; loss: 1.25; acc: 0.61
Batch: 500; loss: 1.45; acc: 0.5
Batch: 520; loss: 1.33; acc: 0.59
Batch: 540; loss: 1.35; acc: 0.59
Batch: 560; loss: 1.35; acc: 0.53
Batch: 580; loss: 1.2; acc: 0.56
Batch: 600; loss: 1.31; acc: 0.55
Batch: 620; loss: 1.07; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.62
Batch: 660; loss: 1.21; acc: 0.61
Batch: 680; loss: 1.03; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.56
Batch: 720; loss: 1.44; acc: 0.53
Batch: 740; loss: 1.16; acc: 0.69
Batch: 760; loss: 1.23; acc: 0.58
Batch: 780; loss: 1.21; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.64
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 0.94; acc: 0.73
Batch: 100; loss: 1.35; acc: 0.47
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.61
Val Epoch over. val_loss: 1.2264837349296376; val_accuracy: 0.5915605095541401 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.43; acc: 0.59
Batch: 20; loss: 1.21; acc: 0.59
Batch: 40; loss: 1.22; acc: 0.59
Batch: 60; loss: 1.26; acc: 0.67
Batch: 80; loss: 1.22; acc: 0.55
Batch: 100; loss: 1.32; acc: 0.55
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 1.31; acc: 0.59
Batch: 160; loss: 1.13; acc: 0.59
Batch: 180; loss: 1.31; acc: 0.58
Batch: 200; loss: 1.47; acc: 0.45
Batch: 220; loss: 1.45; acc: 0.55
Batch: 240; loss: 1.34; acc: 0.61
Batch: 260; loss: 1.06; acc: 0.61
Batch: 280; loss: 1.3; acc: 0.58
Batch: 300; loss: 1.24; acc: 0.59
Batch: 320; loss: 1.4; acc: 0.58
Batch: 340; loss: 1.37; acc: 0.59
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.32; acc: 0.52
Batch: 400; loss: 1.35; acc: 0.48
Batch: 420; loss: 1.29; acc: 0.61
Batch: 440; loss: 1.11; acc: 0.64
Batch: 460; loss: 1.48; acc: 0.5
Batch: 480; loss: 1.25; acc: 0.55
Batch: 500; loss: 1.28; acc: 0.56
Batch: 520; loss: 1.23; acc: 0.61
Batch: 540; loss: 1.07; acc: 0.67
Batch: 560; loss: 1.3; acc: 0.59
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.46; acc: 0.52
Batch: 620; loss: 1.14; acc: 0.61
Batch: 640; loss: 1.17; acc: 0.61
Batch: 660; loss: 1.04; acc: 0.62
Batch: 680; loss: 1.37; acc: 0.56
Batch: 700; loss: 1.15; acc: 0.59
Batch: 720; loss: 1.15; acc: 0.59
Batch: 740; loss: 1.24; acc: 0.58
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.31; acc: 0.52
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.21; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 1.36; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.12; acc: 0.62
Val Epoch over. val_loss: 1.2142541810965082; val_accuracy: 0.5917595541401274 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.34; acc: 0.61
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 1.21; acc: 0.64
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.45; acc: 0.56
Batch: 100; loss: 1.12; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.59
Batch: 140; loss: 1.77; acc: 0.44
Batch: 160; loss: 1.01; acc: 0.67
Batch: 180; loss: 1.42; acc: 0.5
Batch: 200; loss: 1.32; acc: 0.52
Batch: 220; loss: 1.16; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.67
Batch: 260; loss: 1.37; acc: 0.59
Batch: 280; loss: 1.36; acc: 0.48
Batch: 300; loss: 1.34; acc: 0.58
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.34; acc: 0.55
Batch: 360; loss: 1.27; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.5
Batch: 400; loss: 1.02; acc: 0.67
Batch: 420; loss: 1.03; acc: 0.59
Batch: 440; loss: 1.25; acc: 0.64
Batch: 460; loss: 1.44; acc: 0.55
Batch: 480; loss: 1.42; acc: 0.48
Batch: 500; loss: 1.52; acc: 0.55
Batch: 520; loss: 1.15; acc: 0.58
Batch: 540; loss: 1.29; acc: 0.62
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.67
Batch: 620; loss: 1.39; acc: 0.58
Batch: 640; loss: 1.34; acc: 0.53
Batch: 660; loss: 1.29; acc: 0.55
Batch: 680; loss: 1.23; acc: 0.5
Batch: 700; loss: 1.19; acc: 0.66
Batch: 720; loss: 1.33; acc: 0.59
Batch: 740; loss: 1.31; acc: 0.56
Batch: 760; loss: 1.49; acc: 0.48
Batch: 780; loss: 1.27; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.35; acc: 0.44
Batch: 120; loss: 1.32; acc: 0.58
Batch: 140; loss: 1.13; acc: 0.61
Val Epoch over. val_loss: 1.2143722100622336; val_accuracy: 0.5955414012738853 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 1.58; acc: 0.47
Batch: 60; loss: 1.36; acc: 0.53
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.27; acc: 0.56
Batch: 140; loss: 1.33; acc: 0.59
Batch: 160; loss: 1.04; acc: 0.59
Batch: 180; loss: 1.42; acc: 0.5
Batch: 200; loss: 1.13; acc: 0.67
Batch: 220; loss: 1.57; acc: 0.48
Batch: 240; loss: 1.35; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.48
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 1.23; acc: 0.55
Batch: 320; loss: 1.15; acc: 0.61
Batch: 340; loss: 1.42; acc: 0.53
Batch: 360; loss: 1.59; acc: 0.52
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 0.96; acc: 0.64
Batch: 420; loss: 1.21; acc: 0.58
Batch: 440; loss: 1.36; acc: 0.59
Batch: 460; loss: 1.29; acc: 0.52
Batch: 480; loss: 1.12; acc: 0.64
Batch: 500; loss: 1.27; acc: 0.55
Batch: 520; loss: 1.25; acc: 0.56
Batch: 540; loss: 1.25; acc: 0.56
Batch: 560; loss: 1.05; acc: 0.61
Batch: 580; loss: 1.32; acc: 0.64
Batch: 600; loss: 1.02; acc: 0.64
Batch: 620; loss: 1.25; acc: 0.55
Batch: 640; loss: 1.4; acc: 0.56
Batch: 660; loss: 1.4; acc: 0.55
Batch: 680; loss: 1.15; acc: 0.64
Batch: 700; loss: 1.08; acc: 0.61
Batch: 720; loss: 1.52; acc: 0.55
Batch: 740; loss: 1.33; acc: 0.64
Batch: 760; loss: 1.27; acc: 0.55
Batch: 780; loss: 1.45; acc: 0.5
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.69
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.41
Batch: 120; loss: 1.31; acc: 0.56
Batch: 140; loss: 1.13; acc: 0.59
Val Epoch over. val_loss: 1.2152071378792926; val_accuracy: 0.5917595541401274 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.31; acc: 0.56
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 1.26; acc: 0.56
Batch: 60; loss: 1.47; acc: 0.48
Batch: 80; loss: 1.58; acc: 0.5
Batch: 100; loss: 1.22; acc: 0.59
Batch: 120; loss: 1.8; acc: 0.44
Batch: 140; loss: 1.02; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.58
Batch: 180; loss: 1.53; acc: 0.42
Batch: 200; loss: 1.44; acc: 0.56
Batch: 220; loss: 1.18; acc: 0.58
Batch: 240; loss: 1.27; acc: 0.66
Batch: 260; loss: 1.32; acc: 0.62
Batch: 280; loss: 1.51; acc: 0.56
Batch: 300; loss: 1.23; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.56
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.28; acc: 0.64
Batch: 380; loss: 1.22; acc: 0.62
Batch: 400; loss: 1.38; acc: 0.55
Batch: 420; loss: 1.42; acc: 0.56
Batch: 440; loss: 1.07; acc: 0.62
Batch: 460; loss: 0.94; acc: 0.67
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.35; acc: 0.52
Batch: 520; loss: 1.27; acc: 0.64
Batch: 540; loss: 1.33; acc: 0.58
Batch: 560; loss: 1.24; acc: 0.59
Batch: 580; loss: 1.29; acc: 0.56
Batch: 600; loss: 1.26; acc: 0.61
Batch: 620; loss: 1.39; acc: 0.52
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.34; acc: 0.59
Batch: 700; loss: 1.19; acc: 0.61
Batch: 720; loss: 1.36; acc: 0.52
Batch: 740; loss: 1.23; acc: 0.66
Batch: 760; loss: 1.45; acc: 0.5
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.64
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.69
Batch: 60; loss: 1.19; acc: 0.62
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.59
Val Epoch over. val_loss: 1.2119286102094469; val_accuracy: 0.5957404458598726 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.38; acc: 0.56
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.35; acc: 0.53
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.41; acc: 0.55
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 1.28; acc: 0.53
Batch: 160; loss: 1.57; acc: 0.5
Batch: 180; loss: 1.06; acc: 0.69
Batch: 200; loss: 1.58; acc: 0.47
Batch: 220; loss: 1.45; acc: 0.55
Batch: 240; loss: 1.3; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.52
Batch: 280; loss: 1.32; acc: 0.58
Batch: 300; loss: 1.17; acc: 0.7
Batch: 320; loss: 1.52; acc: 0.5
Batch: 340; loss: 1.24; acc: 0.61
Batch: 360; loss: 1.11; acc: 0.61
Batch: 380; loss: 1.65; acc: 0.5
Batch: 400; loss: 1.2; acc: 0.61
Batch: 420; loss: 1.43; acc: 0.48
Batch: 440; loss: 1.17; acc: 0.56
Batch: 460; loss: 1.26; acc: 0.58
Batch: 480; loss: 1.35; acc: 0.55
Batch: 500; loss: 1.33; acc: 0.56
Batch: 520; loss: 1.41; acc: 0.52
Batch: 540; loss: 1.28; acc: 0.56
Batch: 560; loss: 1.45; acc: 0.55
Batch: 580; loss: 1.14; acc: 0.59
Batch: 600; loss: 1.2; acc: 0.53
Batch: 620; loss: 1.12; acc: 0.58
Batch: 640; loss: 1.27; acc: 0.56
Batch: 660; loss: 1.06; acc: 0.67
Batch: 680; loss: 1.47; acc: 0.53
Batch: 700; loss: 1.19; acc: 0.58
Batch: 720; loss: 1.28; acc: 0.56
Batch: 740; loss: 1.32; acc: 0.52
Batch: 760; loss: 1.3; acc: 0.53
Batch: 780; loss: 1.31; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.56
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.21; acc: 0.61
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.56
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2147379673210679; val_accuracy: 0.5927547770700637 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 1.3; acc: 0.53
Batch: 40; loss: 1.29; acc: 0.56
Batch: 60; loss: 1.21; acc: 0.62
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 1.26; acc: 0.59
Batch: 120; loss: 1.31; acc: 0.45
Batch: 140; loss: 1.26; acc: 0.58
Batch: 160; loss: 1.23; acc: 0.59
Batch: 180; loss: 1.05; acc: 0.69
Batch: 200; loss: 1.35; acc: 0.55
Batch: 220; loss: 1.23; acc: 0.58
Batch: 240; loss: 1.31; acc: 0.58
Batch: 260; loss: 1.56; acc: 0.52
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.21; acc: 0.62
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.2; acc: 0.56
Batch: 360; loss: 1.85; acc: 0.47
Batch: 380; loss: 1.31; acc: 0.52
Batch: 400; loss: 1.06; acc: 0.66
Batch: 420; loss: 1.32; acc: 0.64
Batch: 440; loss: 1.21; acc: 0.59
Batch: 460; loss: 1.26; acc: 0.52
Batch: 480; loss: 1.21; acc: 0.56
Batch: 500; loss: 1.44; acc: 0.53
Batch: 520; loss: 1.09; acc: 0.67
Batch: 540; loss: 1.03; acc: 0.67
Batch: 560; loss: 1.3; acc: 0.53
Batch: 580; loss: 1.18; acc: 0.64
Batch: 600; loss: 0.88; acc: 0.69
Batch: 620; loss: 1.29; acc: 0.67
Batch: 640; loss: 1.3; acc: 0.61
Batch: 660; loss: 1.15; acc: 0.58
Batch: 680; loss: 0.84; acc: 0.73
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.18; acc: 0.55
Batch: 740; loss: 1.52; acc: 0.47
Batch: 760; loss: 1.29; acc: 0.55
Batch: 780; loss: 1.31; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.97; acc: 0.69
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 1.34; acc: 0.41
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.13; acc: 0.59
Val Epoch over. val_loss: 1.2195208843346614; val_accuracy: 0.5905652866242038 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.2; acc: 0.5
Batch: 20; loss: 1.22; acc: 0.56
Batch: 40; loss: 1.16; acc: 0.59
Batch: 60; loss: 1.51; acc: 0.52
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.25; acc: 0.5
Batch: 120; loss: 1.3; acc: 0.5
Batch: 140; loss: 1.24; acc: 0.59
Batch: 160; loss: 1.2; acc: 0.58
Batch: 180; loss: 1.04; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.58
Batch: 220; loss: 1.48; acc: 0.44
Batch: 240; loss: 1.28; acc: 0.59
Batch: 260; loss: 1.02; acc: 0.7
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.27; acc: 0.52
Batch: 320; loss: 1.42; acc: 0.52
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.24; acc: 0.56
Batch: 380; loss: 1.07; acc: 0.69
Batch: 400; loss: 1.45; acc: 0.59
Batch: 420; loss: 1.13; acc: 0.58
Batch: 440; loss: 1.26; acc: 0.53
Batch: 460; loss: 1.53; acc: 0.52
Batch: 480; loss: 1.18; acc: 0.59
Batch: 500; loss: 1.02; acc: 0.66
Batch: 520; loss: 1.38; acc: 0.52
Batch: 540; loss: 1.06; acc: 0.53
Batch: 560; loss: 1.06; acc: 0.69
Batch: 580; loss: 1.42; acc: 0.58
Batch: 600; loss: 1.22; acc: 0.59
Batch: 620; loss: 1.35; acc: 0.56
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.11; acc: 0.61
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.58; acc: 0.5
Batch: 740; loss: 1.17; acc: 0.58
Batch: 760; loss: 1.37; acc: 0.53
Batch: 780; loss: 0.99; acc: 0.67
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.56
Batch: 40; loss: 0.95; acc: 0.66
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 1.35; acc: 0.42
Batch: 120; loss: 1.31; acc: 0.56
Batch: 140; loss: 1.13; acc: 0.61
Val Epoch over. val_loss: 1.2127476197898768; val_accuracy: 0.59265525477707 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.26; acc: 0.55
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 1.19; acc: 0.64
Batch: 60; loss: 1.42; acc: 0.47
Batch: 80; loss: 1.04; acc: 0.61
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.06; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.56
Batch: 160; loss: 1.25; acc: 0.64
Batch: 180; loss: 1.13; acc: 0.59
Batch: 200; loss: 1.34; acc: 0.52
Batch: 220; loss: 1.05; acc: 0.56
Batch: 240; loss: 1.07; acc: 0.62
Batch: 260; loss: 1.27; acc: 0.55
Batch: 280; loss: 1.34; acc: 0.53
Batch: 300; loss: 1.53; acc: 0.5
Batch: 320; loss: 1.55; acc: 0.44
Batch: 340; loss: 1.22; acc: 0.52
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 1.41; acc: 0.52
Batch: 400; loss: 1.02; acc: 0.59
Batch: 420; loss: 1.31; acc: 0.55
Batch: 440; loss: 1.45; acc: 0.55
Batch: 460; loss: 1.27; acc: 0.5
Batch: 480; loss: 1.34; acc: 0.5
Batch: 500; loss: 1.16; acc: 0.62
Batch: 520; loss: 1.33; acc: 0.61
Batch: 540; loss: 1.06; acc: 0.59
Batch: 560; loss: 1.43; acc: 0.5
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.36; acc: 0.58
Batch: 620; loss: 1.25; acc: 0.56
Batch: 640; loss: 1.11; acc: 0.67
Batch: 660; loss: 1.28; acc: 0.56
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 1.38; acc: 0.58
Batch: 720; loss: 1.29; acc: 0.5
Batch: 740; loss: 1.39; acc: 0.62
Batch: 760; loss: 1.03; acc: 0.69
Batch: 780; loss: 1.23; acc: 0.48
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.18; acc: 0.59
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.33; acc: 0.42
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2115487012134236; val_accuracy: 0.5951433121019108 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.51; acc: 0.56
Batch: 20; loss: 1.17; acc: 0.61
Batch: 40; loss: 1.46; acc: 0.52
Batch: 60; loss: 1.36; acc: 0.55
Batch: 80; loss: 1.39; acc: 0.53
Batch: 100; loss: 1.11; acc: 0.58
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 1.1; acc: 0.62
Batch: 160; loss: 1.57; acc: 0.45
Batch: 180; loss: 1.05; acc: 0.66
Batch: 200; loss: 1.2; acc: 0.58
Batch: 220; loss: 1.34; acc: 0.5
Batch: 240; loss: 1.28; acc: 0.61
Batch: 260; loss: 1.25; acc: 0.62
Batch: 280; loss: 1.33; acc: 0.56
Batch: 300; loss: 0.95; acc: 0.73
Batch: 320; loss: 1.22; acc: 0.59
Batch: 340; loss: 1.3; acc: 0.55
Batch: 360; loss: 1.09; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.58
Batch: 400; loss: 1.11; acc: 0.59
Batch: 420; loss: 1.17; acc: 0.56
Batch: 440; loss: 1.32; acc: 0.53
Batch: 460; loss: 1.29; acc: 0.48
Batch: 480; loss: 1.22; acc: 0.56
Batch: 500; loss: 1.39; acc: 0.56
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.2; acc: 0.64
Batch: 560; loss: 1.54; acc: 0.53
Batch: 580; loss: 1.15; acc: 0.61
Batch: 600; loss: 1.58; acc: 0.44
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 1.35; acc: 0.55
Batch: 660; loss: 1.12; acc: 0.61
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.37; acc: 0.53
Batch: 720; loss: 1.25; acc: 0.55
Batch: 740; loss: 1.07; acc: 0.62
Batch: 760; loss: 1.4; acc: 0.56
Batch: 780; loss: 1.4; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.66
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 1.37; acc: 0.42
Batch: 120; loss: 1.32; acc: 0.56
Batch: 140; loss: 1.13; acc: 0.61
Val Epoch over. val_loss: 1.219709034178667; val_accuracy: 0.5868829617834395 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.31; acc: 0.58
Batch: 20; loss: 1.23; acc: 0.52
Batch: 40; loss: 1.23; acc: 0.55
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 1.24; acc: 0.55
Batch: 100; loss: 1.09; acc: 0.64
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.33; acc: 0.5
Batch: 160; loss: 1.2; acc: 0.59
Batch: 180; loss: 1.33; acc: 0.52
Batch: 200; loss: 1.09; acc: 0.64
Batch: 220; loss: 1.33; acc: 0.59
Batch: 240; loss: 1.52; acc: 0.44
Batch: 260; loss: 1.03; acc: 0.61
Batch: 280; loss: 1.56; acc: 0.55
Batch: 300; loss: 1.41; acc: 0.56
Batch: 320; loss: 1.54; acc: 0.48
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.19; acc: 0.62
Batch: 380; loss: 1.36; acc: 0.56
Batch: 400; loss: 1.31; acc: 0.61
Batch: 420; loss: 1.27; acc: 0.55
Batch: 440; loss: 1.49; acc: 0.53
Batch: 460; loss: 1.01; acc: 0.73
Batch: 480; loss: 1.34; acc: 0.59
Batch: 500; loss: 1.29; acc: 0.53
Batch: 520; loss: 1.17; acc: 0.56
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 1.29; acc: 0.55
Batch: 580; loss: 1.14; acc: 0.67
Batch: 600; loss: 1.08; acc: 0.67
Batch: 620; loss: 1.49; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.53
Batch: 660; loss: 1.2; acc: 0.64
Batch: 680; loss: 1.18; acc: 0.56
Batch: 700; loss: 1.16; acc: 0.66
Batch: 720; loss: 1.3; acc: 0.52
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 1.43; acc: 0.48
Batch: 780; loss: 1.31; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.56
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.35; acc: 0.44
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 1.11; acc: 0.61
Val Epoch over. val_loss: 1.2150918936273853; val_accuracy: 0.5901671974522293 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 1.44; acc: 0.53
Batch: 60; loss: 1.42; acc: 0.47
Batch: 80; loss: 1.16; acc: 0.58
Batch: 100; loss: 1.24; acc: 0.56
Batch: 120; loss: 1.42; acc: 0.47
Batch: 140; loss: 1.2; acc: 0.62
Batch: 160; loss: 1.31; acc: 0.61
Batch: 180; loss: 1.42; acc: 0.59
Batch: 200; loss: 1.16; acc: 0.62
Batch: 220; loss: 1.44; acc: 0.59
Batch: 240; loss: 1.33; acc: 0.62
Batch: 260; loss: 1.52; acc: 0.5
Batch: 280; loss: 1.15; acc: 0.58
Batch: 300; loss: 1.22; acc: 0.69
Batch: 320; loss: 1.33; acc: 0.52
Batch: 340; loss: 1.16; acc: 0.61
Batch: 360; loss: 1.29; acc: 0.55
Batch: 380; loss: 1.41; acc: 0.53
Batch: 400; loss: 1.15; acc: 0.61
Batch: 420; loss: 1.29; acc: 0.47
Batch: 440; loss: 1.36; acc: 0.59
Batch: 460; loss: 1.09; acc: 0.62
Batch: 480; loss: 1.33; acc: 0.56
Batch: 500; loss: 1.27; acc: 0.62
Batch: 520; loss: 1.47; acc: 0.5
Batch: 540; loss: 1.26; acc: 0.53
Batch: 560; loss: 0.99; acc: 0.7
Batch: 580; loss: 1.37; acc: 0.56
Batch: 600; loss: 1.43; acc: 0.48
Batch: 620; loss: 1.15; acc: 0.58
Batch: 640; loss: 1.44; acc: 0.47
Batch: 660; loss: 1.41; acc: 0.48
Batch: 680; loss: 1.35; acc: 0.59
Batch: 700; loss: 1.52; acc: 0.52
Batch: 720; loss: 1.4; acc: 0.56
Batch: 740; loss: 1.0; acc: 0.69
Batch: 760; loss: 1.45; acc: 0.53
Batch: 780; loss: 1.29; acc: 0.56
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.64
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.59
Val Epoch over. val_loss: 1.2106306537701066; val_accuracy: 0.59484474522293 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 1.21; acc: 0.59
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.5
Batch: 100; loss: 1.22; acc: 0.55
Batch: 120; loss: 1.06; acc: 0.64
Batch: 140; loss: 1.6; acc: 0.48
Batch: 160; loss: 1.41; acc: 0.55
Batch: 180; loss: 1.43; acc: 0.52
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 0.85; acc: 0.66
Batch: 240; loss: 1.22; acc: 0.62
Batch: 260; loss: 1.31; acc: 0.53
Batch: 280; loss: 1.34; acc: 0.59
Batch: 300; loss: 1.25; acc: 0.61
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.38; acc: 0.52
Batch: 360; loss: 1.09; acc: 0.62
Batch: 380; loss: 1.67; acc: 0.53
Batch: 400; loss: 1.25; acc: 0.62
Batch: 420; loss: 1.3; acc: 0.59
Batch: 440; loss: 1.25; acc: 0.61
Batch: 460; loss: 1.27; acc: 0.62
Batch: 480; loss: 1.23; acc: 0.56
Batch: 500; loss: 1.34; acc: 0.55
Batch: 520; loss: 1.07; acc: 0.64
Batch: 540; loss: 1.23; acc: 0.59
Batch: 560; loss: 1.24; acc: 0.58
Batch: 580; loss: 1.16; acc: 0.52
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 1.33; acc: 0.53
Batch: 640; loss: 1.26; acc: 0.56
Batch: 660; loss: 1.22; acc: 0.58
Batch: 680; loss: 1.37; acc: 0.58
Batch: 700; loss: 1.48; acc: 0.52
Batch: 720; loss: 1.28; acc: 0.59
Batch: 740; loss: 1.0; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.59
Batch: 780; loss: 1.37; acc: 0.61
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.56
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.210892304873011; val_accuracy: 0.5953423566878981 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 1.38; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.58
Batch: 60; loss: 1.36; acc: 0.56
Batch: 80; loss: 1.23; acc: 0.62
Batch: 100; loss: 1.56; acc: 0.47
Batch: 120; loss: 1.09; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.58
Batch: 160; loss: 1.15; acc: 0.61
Batch: 180; loss: 1.02; acc: 0.66
Batch: 200; loss: 1.61; acc: 0.47
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.55
Batch: 280; loss: 1.43; acc: 0.55
Batch: 300; loss: 1.25; acc: 0.5
Batch: 320; loss: 0.97; acc: 0.72
Batch: 340; loss: 1.32; acc: 0.53
Batch: 360; loss: 1.34; acc: 0.48
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 1.58; acc: 0.53
Batch: 420; loss: 1.34; acc: 0.53
Batch: 440; loss: 1.37; acc: 0.53
Batch: 460; loss: 1.16; acc: 0.61
Batch: 480; loss: 1.09; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.58
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.2; acc: 0.62
Batch: 560; loss: 1.29; acc: 0.58
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.5
Batch: 620; loss: 1.29; acc: 0.62
Batch: 640; loss: 1.06; acc: 0.61
Batch: 660; loss: 1.04; acc: 0.67
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.06; acc: 0.64
Batch: 720; loss: 1.43; acc: 0.53
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.2; acc: 0.48
Batch: 780; loss: 1.41; acc: 0.48
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.12; acc: 0.66
Val Epoch over. val_loss: 1.2121918839254198; val_accuracy: 0.5940485668789809 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.36; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 1.33; acc: 0.61
Batch: 60; loss: 1.41; acc: 0.53
Batch: 80; loss: 1.65; acc: 0.34
Batch: 100; loss: 1.46; acc: 0.52
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 1.32; acc: 0.59
Batch: 160; loss: 1.63; acc: 0.45
Batch: 180; loss: 1.19; acc: 0.58
Batch: 200; loss: 1.27; acc: 0.53
Batch: 220; loss: 1.22; acc: 0.55
Batch: 240; loss: 1.18; acc: 0.66
Batch: 260; loss: 1.27; acc: 0.59
Batch: 280; loss: 1.5; acc: 0.5
Batch: 300; loss: 1.17; acc: 0.62
Batch: 320; loss: 1.26; acc: 0.61
Batch: 340; loss: 1.28; acc: 0.64
Batch: 360; loss: 1.08; acc: 0.7
Batch: 380; loss: 1.29; acc: 0.59
Batch: 400; loss: 0.92; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.59; acc: 0.52
Batch: 460; loss: 1.29; acc: 0.56
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 1.23; acc: 0.58
Batch: 520; loss: 1.5; acc: 0.55
Batch: 540; loss: 1.16; acc: 0.69
Batch: 560; loss: 1.33; acc: 0.56
Batch: 580; loss: 1.21; acc: 0.64
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.62
Batch: 660; loss: 1.34; acc: 0.64
Batch: 680; loss: 1.24; acc: 0.5
Batch: 700; loss: 1.45; acc: 0.48
Batch: 720; loss: 1.4; acc: 0.52
Batch: 740; loss: 1.43; acc: 0.52
Batch: 760; loss: 1.34; acc: 0.55
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.45
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.61
Val Epoch over. val_loss: 1.2120827709793285; val_accuracy: 0.5958399681528662 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.45; acc: 0.53
Batch: 140; loss: 1.06; acc: 0.67
Batch: 160; loss: 1.05; acc: 0.62
Batch: 180; loss: 1.43; acc: 0.61
Batch: 200; loss: 1.16; acc: 0.64
Batch: 220; loss: 1.35; acc: 0.47
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.32; acc: 0.62
Batch: 280; loss: 1.04; acc: 0.61
Batch: 300; loss: 1.43; acc: 0.56
Batch: 320; loss: 1.34; acc: 0.59
Batch: 340; loss: 1.35; acc: 0.58
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 1.12; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.59
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 1.47; acc: 0.52
Batch: 460; loss: 1.22; acc: 0.66
Batch: 480; loss: 1.27; acc: 0.58
Batch: 500; loss: 1.21; acc: 0.59
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.48; acc: 0.55
Batch: 560; loss: 1.3; acc: 0.59
Batch: 580; loss: 1.22; acc: 0.62
Batch: 600; loss: 1.33; acc: 0.53
Batch: 620; loss: 0.95; acc: 0.73
Batch: 640; loss: 1.26; acc: 0.59
Batch: 660; loss: 1.49; acc: 0.55
Batch: 680; loss: 1.36; acc: 0.56
Batch: 700; loss: 1.37; acc: 0.58
Batch: 720; loss: 1.4; acc: 0.5
Batch: 740; loss: 1.43; acc: 0.47
Batch: 760; loss: 1.06; acc: 0.58
Batch: 780; loss: 1.06; acc: 0.67
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.56
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 1.35; acc: 0.42
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.12; acc: 0.62
Val Epoch over. val_loss: 1.2121608340816132; val_accuracy: 0.5932523885350318 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.07; acc: 0.66
Batch: 20; loss: 1.47; acc: 0.52
Batch: 40; loss: 1.39; acc: 0.5
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.2; acc: 0.66
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.42; acc: 0.52
Batch: 160; loss: 1.21; acc: 0.64
Batch: 180; loss: 1.42; acc: 0.52
Batch: 200; loss: 1.16; acc: 0.61
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.41; acc: 0.52
Batch: 260; loss: 1.43; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.52
Batch: 300; loss: 1.44; acc: 0.59
Batch: 320; loss: 1.01; acc: 0.64
Batch: 340; loss: 1.3; acc: 0.59
Batch: 360; loss: 1.13; acc: 0.52
Batch: 380; loss: 1.19; acc: 0.62
Batch: 400; loss: 1.22; acc: 0.53
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.54; acc: 0.58
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.17; acc: 0.64
Batch: 500; loss: 1.27; acc: 0.55
Batch: 520; loss: 1.39; acc: 0.55
Batch: 540; loss: 1.44; acc: 0.55
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.34; acc: 0.48
Batch: 600; loss: 1.32; acc: 0.53
Batch: 620; loss: 1.51; acc: 0.5
Batch: 640; loss: 1.11; acc: 0.66
Batch: 660; loss: 1.04; acc: 0.66
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 1.41; acc: 0.53
Batch: 720; loss: 1.13; acc: 0.66
Batch: 740; loss: 1.36; acc: 0.56
Batch: 760; loss: 0.99; acc: 0.66
Batch: 780; loss: 1.36; acc: 0.52
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2106126114061684; val_accuracy: 0.5962380573248408 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.48; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.93; acc: 0.72
Batch: 60; loss: 1.08; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.59
Batch: 100; loss: 1.08; acc: 0.61
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 1.36; acc: 0.56
Batch: 160; loss: 1.25; acc: 0.59
Batch: 180; loss: 1.25; acc: 0.66
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.19; acc: 0.58
Batch: 240; loss: 1.12; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.69
Batch: 280; loss: 1.54; acc: 0.5
Batch: 300; loss: 1.32; acc: 0.61
Batch: 320; loss: 1.0; acc: 0.61
Batch: 340; loss: 1.11; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.56
Batch: 380; loss: 1.26; acc: 0.59
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 1.24; acc: 0.52
Batch: 440; loss: 1.24; acc: 0.61
Batch: 460; loss: 1.48; acc: 0.61
Batch: 480; loss: 1.58; acc: 0.48
Batch: 500; loss: 1.05; acc: 0.62
Batch: 520; loss: 1.45; acc: 0.5
Batch: 540; loss: 1.26; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 1.03; acc: 0.67
Batch: 600; loss: 1.36; acc: 0.55
Batch: 620; loss: 1.53; acc: 0.56
Batch: 640; loss: 0.98; acc: 0.69
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.19; acc: 0.59
Batch: 700; loss: 1.48; acc: 0.52
Batch: 720; loss: 1.1; acc: 0.61
Batch: 740; loss: 1.28; acc: 0.59
Batch: 760; loss: 1.48; acc: 0.48
Batch: 780; loss: 1.21; acc: 0.58
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.59
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.33; acc: 0.42
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.64
Val Epoch over. val_loss: 1.2123133061797755; val_accuracy: 0.5936504777070064 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.33; acc: 0.53
Batch: 20; loss: 1.23; acc: 0.55
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 1.27; acc: 0.56
Batch: 80; loss: 1.14; acc: 0.64
Batch: 100; loss: 1.28; acc: 0.52
Batch: 120; loss: 1.29; acc: 0.55
Batch: 140; loss: 1.48; acc: 0.47
Batch: 160; loss: 1.24; acc: 0.58
Batch: 180; loss: 1.39; acc: 0.52
Batch: 200; loss: 1.25; acc: 0.58
Batch: 220; loss: 1.38; acc: 0.56
Batch: 240; loss: 1.26; acc: 0.56
Batch: 260; loss: 1.46; acc: 0.58
Batch: 280; loss: 1.32; acc: 0.56
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.29; acc: 0.62
Batch: 340; loss: 1.16; acc: 0.59
Batch: 360; loss: 1.21; acc: 0.64
Batch: 380; loss: 1.14; acc: 0.61
Batch: 400; loss: 1.34; acc: 0.48
Batch: 420; loss: 1.35; acc: 0.55
Batch: 440; loss: 1.29; acc: 0.59
Batch: 460; loss: 1.08; acc: 0.62
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.34; acc: 0.53
Batch: 520; loss: 1.4; acc: 0.56
Batch: 540; loss: 1.16; acc: 0.58
Batch: 560; loss: 1.05; acc: 0.69
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.38; acc: 0.55
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.52; acc: 0.52
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.13; acc: 0.59
Batch: 700; loss: 1.2; acc: 0.55
Batch: 720; loss: 1.27; acc: 0.5
Batch: 740; loss: 1.21; acc: 0.61
Batch: 760; loss: 1.21; acc: 0.61
Batch: 780; loss: 1.16; acc: 0.61
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 1.35; acc: 0.42
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.212706205571533; val_accuracy: 0.5935509554140127 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 1.22; acc: 0.64
Batch: 60; loss: 1.18; acc: 0.62
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.64; acc: 0.44
Batch: 120; loss: 1.32; acc: 0.66
Batch: 140; loss: 1.0; acc: 0.66
Batch: 160; loss: 1.2; acc: 0.61
Batch: 180; loss: 1.41; acc: 0.53
Batch: 200; loss: 1.02; acc: 0.62
Batch: 220; loss: 1.1; acc: 0.62
Batch: 240; loss: 1.36; acc: 0.59
Batch: 260; loss: 1.27; acc: 0.62
Batch: 280; loss: 1.21; acc: 0.58
Batch: 300; loss: 1.26; acc: 0.66
Batch: 320; loss: 1.12; acc: 0.64
Batch: 340; loss: 1.28; acc: 0.52
Batch: 360; loss: 1.1; acc: 0.64
Batch: 380; loss: 1.31; acc: 0.53
Batch: 400; loss: 1.14; acc: 0.64
Batch: 420; loss: 1.4; acc: 0.53
Batch: 440; loss: 1.3; acc: 0.52
Batch: 460; loss: 1.26; acc: 0.53
Batch: 480; loss: 1.27; acc: 0.61
Batch: 500; loss: 1.33; acc: 0.53
Batch: 520; loss: 1.22; acc: 0.61
Batch: 540; loss: 0.98; acc: 0.62
Batch: 560; loss: 1.63; acc: 0.53
Batch: 580; loss: 1.15; acc: 0.58
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 1.38; acc: 0.58
Batch: 640; loss: 1.03; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.52
Batch: 680; loss: 1.37; acc: 0.53
Batch: 700; loss: 1.07; acc: 0.64
Batch: 720; loss: 1.07; acc: 0.61
Batch: 740; loss: 1.38; acc: 0.59
Batch: 760; loss: 1.12; acc: 0.62
Batch: 780; loss: 1.41; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.56
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.35; acc: 0.44
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2113647366025646; val_accuracy: 0.5958399681528662 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.31; acc: 0.56
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 1.18; acc: 0.61
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.72; acc: 0.45
Batch: 100; loss: 1.25; acc: 0.56
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 1.39; acc: 0.55
Batch: 180; loss: 1.37; acc: 0.56
Batch: 200; loss: 1.29; acc: 0.58
Batch: 220; loss: 1.24; acc: 0.53
Batch: 240; loss: 1.42; acc: 0.58
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.21; acc: 0.56
Batch: 300; loss: 1.38; acc: 0.59
Batch: 320; loss: 1.51; acc: 0.45
Batch: 340; loss: 1.36; acc: 0.55
Batch: 360; loss: 1.2; acc: 0.61
Batch: 380; loss: 1.13; acc: 0.59
Batch: 400; loss: 1.41; acc: 0.52
Batch: 420; loss: 1.18; acc: 0.58
Batch: 440; loss: 1.32; acc: 0.53
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 1.39; acc: 0.42
Batch: 500; loss: 1.28; acc: 0.53
Batch: 520; loss: 1.08; acc: 0.62
Batch: 540; loss: 1.27; acc: 0.64
Batch: 560; loss: 1.28; acc: 0.5
Batch: 580; loss: 1.43; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.55
Batch: 620; loss: 1.15; acc: 0.62
Batch: 640; loss: 1.04; acc: 0.61
Batch: 660; loss: 1.34; acc: 0.55
Batch: 680; loss: 1.42; acc: 0.58
Batch: 700; loss: 1.26; acc: 0.58
Batch: 720; loss: 1.14; acc: 0.61
Batch: 740; loss: 1.39; acc: 0.55
Batch: 760; loss: 1.26; acc: 0.55
Batch: 780; loss: 1.35; acc: 0.56
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 0.96; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.29; acc: 0.56
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2118852081572173; val_accuracy: 0.5939490445859873 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.52
Batch: 40; loss: 1.4; acc: 0.52
Batch: 60; loss: 1.46; acc: 0.52
Batch: 80; loss: 1.32; acc: 0.44
Batch: 100; loss: 1.05; acc: 0.67
Batch: 120; loss: 1.08; acc: 0.66
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.32; acc: 0.55
Batch: 180; loss: 1.07; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.58
Batch: 220; loss: 1.63; acc: 0.45
Batch: 240; loss: 1.29; acc: 0.64
Batch: 260; loss: 1.21; acc: 0.58
Batch: 280; loss: 1.29; acc: 0.55
Batch: 300; loss: 1.34; acc: 0.53
Batch: 320; loss: 1.08; acc: 0.66
Batch: 340; loss: 1.45; acc: 0.5
Batch: 360; loss: 1.38; acc: 0.55
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 1.09; acc: 0.59
Batch: 420; loss: 1.04; acc: 0.66
Batch: 440; loss: 1.29; acc: 0.53
Batch: 460; loss: 1.19; acc: 0.55
Batch: 480; loss: 1.16; acc: 0.58
Batch: 500; loss: 0.96; acc: 0.69
Batch: 520; loss: 1.38; acc: 0.52
Batch: 540; loss: 1.04; acc: 0.61
Batch: 560; loss: 1.31; acc: 0.58
Batch: 580; loss: 1.31; acc: 0.52
Batch: 600; loss: 1.42; acc: 0.53
Batch: 620; loss: 1.26; acc: 0.62
Batch: 640; loss: 1.28; acc: 0.52
Batch: 660; loss: 1.38; acc: 0.5
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 1.37; acc: 0.61
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 1.25; acc: 0.52
Batch: 760; loss: 1.44; acc: 0.56
Batch: 780; loss: 1.28; acc: 0.58
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.35; acc: 0.44
Batch: 120; loss: 1.31; acc: 0.56
Batch: 140; loss: 1.11; acc: 0.61
Val Epoch over. val_loss: 1.2114862256748662; val_accuracy: 0.5944466560509554 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.33; acc: 0.52
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.21; acc: 0.56
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.33; acc: 0.62
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 1.1; acc: 0.64
Batch: 180; loss: 1.32; acc: 0.61
Batch: 200; loss: 1.09; acc: 0.66
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.39; acc: 0.5
Batch: 260; loss: 0.94; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.66
Batch: 300; loss: 1.08; acc: 0.62
Batch: 320; loss: 1.15; acc: 0.55
Batch: 340; loss: 1.7; acc: 0.42
Batch: 360; loss: 1.34; acc: 0.61
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.28; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.56
Batch: 440; loss: 1.09; acc: 0.58
Batch: 460; loss: 1.31; acc: 0.52
Batch: 480; loss: 1.16; acc: 0.62
Batch: 500; loss: 1.38; acc: 0.56
Batch: 520; loss: 1.18; acc: 0.66
Batch: 540; loss: 1.1; acc: 0.61
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.47; acc: 0.56
Batch: 600; loss: 1.12; acc: 0.66
Batch: 620; loss: 0.99; acc: 0.66
Batch: 640; loss: 1.29; acc: 0.64
Batch: 660; loss: 1.46; acc: 0.53
Batch: 680; loss: 1.23; acc: 0.59
Batch: 700; loss: 1.15; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.69
Batch: 740; loss: 1.4; acc: 0.48
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2108585515599342; val_accuracy: 0.5947452229299363 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.33; acc: 0.58
Batch: 20; loss: 1.47; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.53
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 0.98; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.21; acc: 0.64
Batch: 160; loss: 1.32; acc: 0.58
Batch: 180; loss: 1.09; acc: 0.55
Batch: 200; loss: 1.57; acc: 0.45
Batch: 220; loss: 1.32; acc: 0.53
Batch: 240; loss: 1.05; acc: 0.67
Batch: 260; loss: 1.27; acc: 0.59
Batch: 280; loss: 1.28; acc: 0.61
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 1.25; acc: 0.5
Batch: 340; loss: 1.3; acc: 0.58
Batch: 360; loss: 1.34; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.64
Batch: 400; loss: 1.16; acc: 0.56
Batch: 420; loss: 1.25; acc: 0.53
Batch: 440; loss: 1.25; acc: 0.53
Batch: 460; loss: 1.37; acc: 0.55
Batch: 480; loss: 1.3; acc: 0.61
Batch: 500; loss: 1.19; acc: 0.58
Batch: 520; loss: 1.23; acc: 0.55
Batch: 540; loss: 1.28; acc: 0.56
Batch: 560; loss: 1.23; acc: 0.64
Batch: 580; loss: 1.29; acc: 0.59
Batch: 600; loss: 1.33; acc: 0.55
Batch: 620; loss: 1.35; acc: 0.56
Batch: 640; loss: 1.25; acc: 0.58
Batch: 660; loss: 0.92; acc: 0.7
Batch: 680; loss: 1.42; acc: 0.59
Batch: 700; loss: 1.52; acc: 0.52
Batch: 720; loss: 1.15; acc: 0.62
Batch: 740; loss: 1.11; acc: 0.64
Batch: 760; loss: 1.52; acc: 0.5
Batch: 780; loss: 1.08; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.62
Val Epoch over. val_loss: 1.2110137556009233; val_accuracy: 0.5950437898089171 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.59
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 1.28; acc: 0.52
Batch: 60; loss: 1.32; acc: 0.52
Batch: 80; loss: 1.25; acc: 0.59
Batch: 100; loss: 1.32; acc: 0.59
Batch: 120; loss: 1.27; acc: 0.59
Batch: 140; loss: 1.28; acc: 0.61
Batch: 160; loss: 1.16; acc: 0.62
Batch: 180; loss: 1.23; acc: 0.61
Batch: 200; loss: 1.3; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 1.36; acc: 0.58
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.2; acc: 0.59
Batch: 300; loss: 1.33; acc: 0.55
Batch: 320; loss: 1.38; acc: 0.52
Batch: 340; loss: 1.51; acc: 0.55
Batch: 360; loss: 1.11; acc: 0.66
Batch: 380; loss: 1.39; acc: 0.52
Batch: 400; loss: 1.54; acc: 0.47
Batch: 420; loss: 1.11; acc: 0.66
Batch: 440; loss: 1.19; acc: 0.61
Batch: 460; loss: 1.18; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.59
Batch: 500; loss: 1.34; acc: 0.58
Batch: 520; loss: 1.24; acc: 0.59
Batch: 540; loss: 1.4; acc: 0.55
Batch: 560; loss: 1.45; acc: 0.56
Batch: 580; loss: 1.29; acc: 0.53
Batch: 600; loss: 1.41; acc: 0.47
Batch: 620; loss: 1.08; acc: 0.59
Batch: 640; loss: 1.32; acc: 0.64
Batch: 660; loss: 1.5; acc: 0.48
Batch: 680; loss: 1.21; acc: 0.59
Batch: 700; loss: 1.14; acc: 0.58
Batch: 720; loss: 1.35; acc: 0.53
Batch: 740; loss: 1.06; acc: 0.58
Batch: 760; loss: 1.31; acc: 0.58
Batch: 780; loss: 1.42; acc: 0.53
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.2116580013256923; val_accuracy: 0.5955414012738853 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.35; acc: 0.62
Batch: 20; loss: 0.97; acc: 0.62
Batch: 40; loss: 1.18; acc: 0.58
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.32; acc: 0.55
Batch: 100; loss: 1.23; acc: 0.61
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.39; acc: 0.56
Batch: 160; loss: 1.11; acc: 0.58
Batch: 180; loss: 1.26; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.52
Batch: 220; loss: 1.19; acc: 0.61
Batch: 240; loss: 1.21; acc: 0.62
Batch: 260; loss: 1.32; acc: 0.58
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 1.44; acc: 0.5
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.64
Batch: 360; loss: 1.18; acc: 0.59
Batch: 380; loss: 1.12; acc: 0.56
Batch: 400; loss: 1.21; acc: 0.58
Batch: 420; loss: 1.1; acc: 0.64
Batch: 440; loss: 1.59; acc: 0.48
Batch: 460; loss: 1.43; acc: 0.56
Batch: 480; loss: 1.31; acc: 0.52
Batch: 500; loss: 1.68; acc: 0.39
Batch: 520; loss: 1.51; acc: 0.58
Batch: 540; loss: 1.1; acc: 0.62
Batch: 560; loss: 1.28; acc: 0.5
Batch: 580; loss: 1.21; acc: 0.59
Batch: 600; loss: 1.58; acc: 0.48
Batch: 620; loss: 1.02; acc: 0.67
Batch: 640; loss: 1.41; acc: 0.52
Batch: 660; loss: 1.37; acc: 0.56
Batch: 680; loss: 1.06; acc: 0.62
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.26; acc: 0.59
Batch: 740; loss: 1.22; acc: 0.56
Batch: 760; loss: 1.26; acc: 0.59
Batch: 780; loss: 1.28; acc: 0.56
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.59
Val Epoch over. val_loss: 1.210565592832626; val_accuracy: 0.5938495222929936 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.66
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.75
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.23; acc: 0.53
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 0.98; acc: 0.66
Batch: 200; loss: 1.34; acc: 0.58
Batch: 220; loss: 1.19; acc: 0.58
Batch: 240; loss: 1.24; acc: 0.59
Batch: 260; loss: 1.37; acc: 0.53
Batch: 280; loss: 1.54; acc: 0.48
Batch: 300; loss: 1.28; acc: 0.58
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.36; acc: 0.55
Batch: 360; loss: 1.28; acc: 0.58
Batch: 380; loss: 1.08; acc: 0.62
Batch: 400; loss: 1.15; acc: 0.61
Batch: 420; loss: 1.32; acc: 0.5
Batch: 440; loss: 1.34; acc: 0.55
Batch: 460; loss: 1.28; acc: 0.59
Batch: 480; loss: 1.29; acc: 0.56
Batch: 500; loss: 1.32; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.56
Batch: 540; loss: 1.0; acc: 0.67
Batch: 560; loss: 1.7; acc: 0.47
Batch: 580; loss: 1.43; acc: 0.53
Batch: 600; loss: 1.36; acc: 0.55
Batch: 620; loss: 1.23; acc: 0.59
Batch: 640; loss: 1.26; acc: 0.47
Batch: 660; loss: 1.43; acc: 0.58
Batch: 680; loss: 1.39; acc: 0.58
Batch: 700; loss: 1.06; acc: 0.67
Batch: 720; loss: 1.33; acc: 0.47
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 1.25; acc: 0.58
Batch: 780; loss: 1.38; acc: 0.59
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.61
Val Epoch over. val_loss: 1.2106367581209558; val_accuracy: 0.5947452229299363 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.48
Batch: 80; loss: 1.04; acc: 0.56
Batch: 100; loss: 1.4; acc: 0.52
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 1.12; acc: 0.64
Batch: 160; loss: 1.57; acc: 0.45
Batch: 180; loss: 1.15; acc: 0.61
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.17; acc: 0.61
Batch: 260; loss: 1.46; acc: 0.55
Batch: 280; loss: 1.22; acc: 0.61
Batch: 300; loss: 1.25; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.67
Batch: 340; loss: 1.18; acc: 0.61
Batch: 360; loss: 1.37; acc: 0.58
Batch: 380; loss: 1.31; acc: 0.55
Batch: 400; loss: 1.44; acc: 0.52
Batch: 420; loss: 1.45; acc: 0.55
Batch: 440; loss: 1.1; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.56
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.62; acc: 0.44
Batch: 520; loss: 1.4; acc: 0.52
Batch: 540; loss: 1.42; acc: 0.55
Batch: 560; loss: 1.36; acc: 0.55
Batch: 580; loss: 1.24; acc: 0.53
Batch: 600; loss: 1.13; acc: 0.62
Batch: 620; loss: 1.12; acc: 0.59
Batch: 640; loss: 1.03; acc: 0.66
Batch: 660; loss: 1.01; acc: 0.69
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.18; acc: 0.58
Batch: 720; loss: 1.43; acc: 0.44
Batch: 740; loss: 1.34; acc: 0.55
Batch: 760; loss: 1.39; acc: 0.62
Batch: 780; loss: 1.12; acc: 0.67
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.62
Val Epoch over. val_loss: 1.2112720168320237; val_accuracy: 0.5943471337579618 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.45; acc: 0.52
Batch: 40; loss: 1.25; acc: 0.55
Batch: 60; loss: 1.42; acc: 0.52
Batch: 80; loss: 1.46; acc: 0.5
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.48
Batch: 160; loss: 1.11; acc: 0.66
Batch: 180; loss: 1.44; acc: 0.56
Batch: 200; loss: 1.31; acc: 0.55
Batch: 220; loss: 1.33; acc: 0.61
Batch: 240; loss: 1.16; acc: 0.56
Batch: 260; loss: 1.03; acc: 0.67
Batch: 280; loss: 1.25; acc: 0.59
Batch: 300; loss: 1.09; acc: 0.61
Batch: 320; loss: 1.27; acc: 0.53
Batch: 340; loss: 1.33; acc: 0.61
Batch: 360; loss: 1.39; acc: 0.53
Batch: 380; loss: 1.06; acc: 0.67
Batch: 400; loss: 1.0; acc: 0.69
Batch: 420; loss: 1.66; acc: 0.48
Batch: 440; loss: 1.39; acc: 0.56
Batch: 460; loss: 1.39; acc: 0.55
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.28; acc: 0.53
Batch: 520; loss: 1.2; acc: 0.58
Batch: 540; loss: 1.45; acc: 0.53
Batch: 560; loss: 1.36; acc: 0.55
Batch: 580; loss: 1.44; acc: 0.53
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.48; acc: 0.56
Batch: 640; loss: 1.15; acc: 0.64
Batch: 660; loss: 1.35; acc: 0.56
Batch: 680; loss: 1.27; acc: 0.61
Batch: 700; loss: 1.49; acc: 0.48
Batch: 720; loss: 1.29; acc: 0.61
Batch: 740; loss: 1.45; acc: 0.41
Batch: 760; loss: 1.14; acc: 0.67
Batch: 780; loss: 1.34; acc: 0.58
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.42
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.64
Val Epoch over. val_loss: 1.2108032972949325; val_accuracy: 0.59484474522293 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 1.16; acc: 0.61
Batch: 60; loss: 1.06; acc: 0.62
Batch: 80; loss: 1.23; acc: 0.61
Batch: 100; loss: 1.27; acc: 0.61
Batch: 120; loss: 1.18; acc: 0.61
Batch: 140; loss: 1.37; acc: 0.5
Batch: 160; loss: 1.19; acc: 0.66
Batch: 180; loss: 1.59; acc: 0.45
Batch: 200; loss: 1.16; acc: 0.69
Batch: 220; loss: 1.58; acc: 0.55
Batch: 240; loss: 1.35; acc: 0.58
Batch: 260; loss: 1.07; acc: 0.64
Batch: 280; loss: 1.37; acc: 0.53
Batch: 300; loss: 1.13; acc: 0.58
Batch: 320; loss: 1.42; acc: 0.62
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.56
Batch: 380; loss: 1.43; acc: 0.52
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.25; acc: 0.69
Batch: 440; loss: 1.14; acc: 0.62
Batch: 460; loss: 1.13; acc: 0.66
Batch: 480; loss: 1.35; acc: 0.56
Batch: 500; loss: 1.21; acc: 0.61
Batch: 520; loss: 1.4; acc: 0.59
Batch: 540; loss: 1.37; acc: 0.5
Batch: 560; loss: 1.02; acc: 0.62
Batch: 580; loss: 1.42; acc: 0.58
Batch: 600; loss: 1.42; acc: 0.59
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 1.33; acc: 0.53
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.35; acc: 0.55
Batch: 720; loss: 1.32; acc: 0.59
Batch: 740; loss: 1.45; acc: 0.53
Batch: 760; loss: 1.14; acc: 0.66
Batch: 780; loss: 1.28; acc: 0.58
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.62
Val Epoch over. val_loss: 1.2113231204117938; val_accuracy: 0.5954418789808917 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.39; acc: 0.53
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 1.17; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.58
Batch: 80; loss: 1.28; acc: 0.56
Batch: 100; loss: 1.53; acc: 0.55
Batch: 120; loss: 1.44; acc: 0.48
Batch: 140; loss: 1.29; acc: 0.62
Batch: 160; loss: 1.2; acc: 0.56
Batch: 180; loss: 1.19; acc: 0.62
Batch: 200; loss: 1.44; acc: 0.5
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 1.4; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.55
Batch: 280; loss: 1.22; acc: 0.53
Batch: 300; loss: 1.19; acc: 0.53
Batch: 320; loss: 1.07; acc: 0.64
Batch: 340; loss: 1.65; acc: 0.5
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 1.67; acc: 0.44
Batch: 400; loss: 1.33; acc: 0.56
Batch: 420; loss: 1.18; acc: 0.62
Batch: 440; loss: 1.11; acc: 0.66
Batch: 460; loss: 1.33; acc: 0.5
Batch: 480; loss: 1.53; acc: 0.5
Batch: 500; loss: 1.54; acc: 0.5
Batch: 520; loss: 1.32; acc: 0.53
Batch: 540; loss: 1.11; acc: 0.53
Batch: 560; loss: 1.27; acc: 0.59
Batch: 580; loss: 1.39; acc: 0.56
Batch: 600; loss: 1.04; acc: 0.67
Batch: 620; loss: 1.25; acc: 0.59
Batch: 640; loss: 1.47; acc: 0.56
Batch: 660; loss: 1.31; acc: 0.48
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 1.34; acc: 0.56
Batch: 720; loss: 1.21; acc: 0.58
Batch: 740; loss: 1.15; acc: 0.61
Batch: 760; loss: 1.17; acc: 0.58
Batch: 780; loss: 1.44; acc: 0.48
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.61
Val Epoch over. val_loss: 1.2109795721473209; val_accuracy: 0.59484474522293 

plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 20960
elements in E: 4442600
fraction nonzero: 0.004717957952550309
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.06
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.06
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.23
Batch: 320; loss: 2.29; acc: 0.06
Batch: 340; loss: 2.27; acc: 0.17
Batch: 360; loss: 2.27; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.14
Batch: 400; loss: 2.29; acc: 0.06
Batch: 420; loss: 2.26; acc: 0.14
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.26; acc: 0.19
Batch: 480; loss: 2.26; acc: 0.12
Batch: 500; loss: 2.27; acc: 0.2
Batch: 520; loss: 2.28; acc: 0.08
Batch: 540; loss: 2.24; acc: 0.33
Batch: 560; loss: 2.24; acc: 0.33
Batch: 580; loss: 2.25; acc: 0.27
Batch: 600; loss: 2.23; acc: 0.36
Batch: 620; loss: 2.21; acc: 0.36
Batch: 640; loss: 2.24; acc: 0.28
Batch: 660; loss: 2.21; acc: 0.39
Batch: 680; loss: 2.22; acc: 0.27
Batch: 700; loss: 2.21; acc: 0.3
Batch: 720; loss: 2.14; acc: 0.39
Batch: 740; loss: 2.15; acc: 0.33
Batch: 760; loss: 2.1; acc: 0.34
Batch: 780; loss: 2.08; acc: 0.36
Train Epoch over. train_loss: 2.26; train_accuracy: 0.19 

Batch: 0; loss: 2.08; acc: 0.41
Batch: 20; loss: 1.96; acc: 0.31
Batch: 40; loss: 1.94; acc: 0.56
Batch: 60; loss: 2.01; acc: 0.5
Batch: 80; loss: 2.0; acc: 0.5
Batch: 100; loss: 2.0; acc: 0.45
Batch: 120; loss: 2.02; acc: 0.45
Batch: 140; loss: 1.99; acc: 0.41
Val Epoch over. val_loss: 2.052749711237136; val_accuracy: 0.39958200636942676 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.12; acc: 0.38
Batch: 20; loss: 2.03; acc: 0.34
Batch: 40; loss: 2.05; acc: 0.33
Batch: 60; loss: 1.82; acc: 0.45
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.54; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.48
Batch: 140; loss: 1.38; acc: 0.62
Batch: 160; loss: 1.21; acc: 0.66
Batch: 180; loss: 1.22; acc: 0.56
Batch: 200; loss: 1.05; acc: 0.72
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 1.4; acc: 0.61
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 0.83; acc: 0.83
Batch: 300; loss: 0.98; acc: 0.66
Batch: 320; loss: 0.81; acc: 0.75
Batch: 340; loss: 1.07; acc: 0.64
Batch: 360; loss: 1.13; acc: 0.64
Batch: 380; loss: 0.94; acc: 0.72
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 0.77; acc: 0.75
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 0.87; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.62
Batch: 520; loss: 1.09; acc: 0.64
Batch: 540; loss: 0.96; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.89; acc: 0.75
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 1.01; acc: 0.73
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.67
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 1.01; acc: 0.62
Batch: 780; loss: 0.93; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.65 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.76; acc: 0.72
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.64
Batch: 140; loss: 0.59; acc: 0.77
Val Epoch over. val_loss: 0.8056751669971807; val_accuracy: 0.7418391719745223 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 0.94; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.78; acc: 0.78
Batch: 160; loss: 0.84; acc: 0.81
Batch: 180; loss: 0.84; acc: 0.7
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.87; acc: 0.69
Batch: 240; loss: 0.93; acc: 0.7
Batch: 260; loss: 1.01; acc: 0.69
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.78
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 1.0; acc: 0.69
Batch: 360; loss: 0.89; acc: 0.7
Batch: 380; loss: 0.87; acc: 0.7
Batch: 400; loss: 0.8; acc: 0.75
Batch: 420; loss: 0.88; acc: 0.64
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.67
Batch: 480; loss: 0.81; acc: 0.72
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 0.55; acc: 0.73
Batch: 540; loss: 0.79; acc: 0.77
Batch: 560; loss: 0.91; acc: 0.69
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 0.92; acc: 0.64
Batch: 680; loss: 0.84; acc: 0.72
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.81; acc: 0.75
Batch: 740; loss: 0.9; acc: 0.72
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.99; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.76; acc: 0.69
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.72; acc: 0.75
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.48; acc: 0.81
Val Epoch over. val_loss: 0.7567143679424456; val_accuracy: 0.7540804140127388 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.82; acc: 0.73
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.83; acc: 0.67
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.75
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.86; acc: 0.75
Batch: 220; loss: 0.8; acc: 0.77
Batch: 240; loss: 1.14; acc: 0.62
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 0.82; acc: 0.75
Batch: 320; loss: 0.73; acc: 0.78
Batch: 340; loss: 1.05; acc: 0.73
Batch: 360; loss: 0.6; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.67
Batch: 400; loss: 0.78; acc: 0.81
Batch: 420; loss: 0.88; acc: 0.67
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.62
Batch: 480; loss: 0.58; acc: 0.73
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.91; acc: 0.72
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.86; acc: 0.7
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 1.03; acc: 0.77
Batch: 660; loss: 0.88; acc: 0.72
Batch: 680; loss: 0.79; acc: 0.72
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 0.8; acc: 0.61
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.8; acc: 0.67
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.55; acc: 0.81
Val Epoch over. val_loss: 0.807585378171532; val_accuracy: 0.7376592356687898 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.85; acc: 0.7
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.92; acc: 0.64
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.97; acc: 0.64
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 1.04; acc: 0.7
Batch: 240; loss: 1.0; acc: 0.69
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.79; acc: 0.73
Batch: 320; loss: 0.7; acc: 0.73
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.75; acc: 0.72
Batch: 380; loss: 0.65; acc: 0.77
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.69
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 1.06; acc: 0.69
Batch: 500; loss: 0.83; acc: 0.77
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.97; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.73
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.8; acc: 0.73
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.93; acc: 0.7
Batch: 680; loss: 0.82; acc: 0.73
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 1.19; acc: 0.62
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.89; acc: 0.66
Train Epoch over. train_loss: 0.77; train_accuracy: 0.75 

Batch: 0; loss: 1.34; acc: 0.44
Batch: 20; loss: 2.07; acc: 0.45
Batch: 40; loss: 1.16; acc: 0.59
Batch: 60; loss: 1.6; acc: 0.61
Batch: 80; loss: 1.39; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.47
Batch: 120; loss: 1.54; acc: 0.42
Batch: 140; loss: 1.66; acc: 0.61
Val Epoch over. val_loss: 1.4611252409637354; val_accuracy: 0.5480692675159236 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 1.03; acc: 0.66
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.75
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 1.0; acc: 0.72
Batch: 260; loss: 0.8; acc: 0.78
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.76; acc: 0.7
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.85; acc: 0.73
Batch: 380; loss: 0.8; acc: 0.69
Batch: 400; loss: 0.81; acc: 0.75
Batch: 420; loss: 0.8; acc: 0.81
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 0.63; acc: 0.75
Batch: 480; loss: 0.97; acc: 0.62
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.77
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.79; acc: 0.72
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.63; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.61; acc: 0.77
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.86; acc: 0.67
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.62
Batch: 140; loss: 0.64; acc: 0.8
Val Epoch over. val_loss: 0.8181604298816365; val_accuracy: 0.7384554140127388 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.88; acc: 0.7
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.7
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.94; acc: 0.7
Batch: 180; loss: 0.91; acc: 0.78
Batch: 200; loss: 0.78; acc: 0.69
Batch: 220; loss: 0.79; acc: 0.77
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.73; acc: 0.7
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.79; acc: 0.7
Batch: 340; loss: 0.61; acc: 0.75
Batch: 360; loss: 0.92; acc: 0.66
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.77; acc: 0.72
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.79; acc: 0.67
Batch: 480; loss: 0.84; acc: 0.7
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.78
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.73
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.72; acc: 0.75
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.89; acc: 0.72
Batch: 760; loss: 0.67; acc: 0.77
Batch: 780; loss: 0.75; acc: 0.75
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.65; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.64
Batch: 140; loss: 0.58; acc: 0.81
Val Epoch over. val_loss: 0.7146479052722834; val_accuracy: 0.7696058917197452 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.67
Batch: 20; loss: 0.86; acc: 0.69
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.77
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.7
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.8
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 0.82; acc: 0.7
Batch: 280; loss: 0.63; acc: 0.78
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.86; acc: 0.66
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.9; acc: 0.81
Batch: 380; loss: 0.8; acc: 0.77
Batch: 400; loss: 1.01; acc: 0.69
Batch: 420; loss: 0.79; acc: 0.73
Batch: 440; loss: 0.78; acc: 0.69
Batch: 460; loss: 0.85; acc: 0.73
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 1.03; acc: 0.64
Batch: 520; loss: 1.0; acc: 0.73
Batch: 540; loss: 0.74; acc: 0.78
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.79; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.77
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.73; acc: 0.7
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.9; acc: 0.75
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.81; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.61; acc: 0.72
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.49; acc: 0.81
Val Epoch over. val_loss: 0.7020667244674293; val_accuracy: 0.7722929936305732 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.84; acc: 0.7
Batch: 40; loss: 0.89; acc: 0.66
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.77
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.9; acc: 0.7
Batch: 160; loss: 0.66; acc: 0.77
Batch: 180; loss: 0.55; acc: 0.77
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.75
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.84; acc: 0.66
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.81; acc: 0.72
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.97; acc: 0.66
Batch: 600; loss: 0.8; acc: 0.73
Batch: 620; loss: 0.48; acc: 0.8
Batch: 640; loss: 0.81; acc: 0.7
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.91; acc: 0.73
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.75
Batch: 740; loss: 0.96; acc: 0.69
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.98; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.7
Batch: 80; loss: 1.37; acc: 0.67
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 1.28; acc: 0.59
Batch: 140; loss: 1.12; acc: 0.73
Val Epoch over. val_loss: 1.22480296215434; val_accuracy: 0.6645103503184714 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.67
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.96; acc: 0.66
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.81; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.75
Batch: 320; loss: 0.82; acc: 0.73
Batch: 340; loss: 0.81; acc: 0.73
Batch: 360; loss: 1.13; acc: 0.64
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.72
Batch: 460; loss: 0.79; acc: 0.75
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.6; acc: 0.75
Batch: 520; loss: 0.84; acc: 0.69
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.63; acc: 0.78
Batch: 580; loss: 0.88; acc: 0.66
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.73; acc: 0.72
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 0.76; acc: 0.69
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.75
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.6652144832406074; val_accuracy: 0.7891122611464968 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.0; acc: 0.66
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.72; acc: 0.75
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.73
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.65; acc: 0.77
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.77; acc: 0.75
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.69; acc: 0.77
Batch: 300; loss: 0.73; acc: 0.77
Batch: 320; loss: 0.76; acc: 0.72
Batch: 340; loss: 0.86; acc: 0.67
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.89; acc: 0.72
Batch: 440; loss: 0.53; acc: 0.8
Batch: 460; loss: 0.71; acc: 0.77
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.77
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.61; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.75
Batch: 660; loss: 1.07; acc: 0.61
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.84; acc: 0.7
Batch: 720; loss: 0.78; acc: 0.73
Batch: 740; loss: 0.71; acc: 0.72
Batch: 760; loss: 0.74; acc: 0.72
Batch: 780; loss: 0.89; acc: 0.69
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.78
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.39; acc: 0.88
Val Epoch over. val_loss: 0.6302984725138184; val_accuracy: 0.794187898089172 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.69
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.89; acc: 0.75
Batch: 240; loss: 0.87; acc: 0.73
Batch: 260; loss: 0.69; acc: 0.73
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.77
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.96; acc: 0.67
Batch: 440; loss: 0.62; acc: 0.75
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.6; acc: 0.8
Batch: 560; loss: 0.99; acc: 0.67
Batch: 580; loss: 0.7; acc: 0.7
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 0.71; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.94; acc: 0.69
Batch: 700; loss: 0.68; acc: 0.75
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.86; acc: 0.73
Batch: 760; loss: 0.81; acc: 0.73
Batch: 780; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.65; acc: 0.75
Batch: 20; loss: 0.95; acc: 0.66
Batch: 40; loss: 0.35; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.5; acc: 0.83
Val Epoch over. val_loss: 0.6850915390784573; val_accuracy: 0.775577229299363 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.88; acc: 0.78
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 0.7; acc: 0.77
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.75
Batch: 380; loss: 0.61; acc: 0.75
Batch: 400; loss: 0.88; acc: 0.75
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.77
Batch: 460; loss: 0.84; acc: 0.72
Batch: 480; loss: 0.91; acc: 0.72
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.72
Batch: 620; loss: 0.58; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.96; acc: 0.73
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 1.12; acc: 0.67
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.42; acc: 0.88
Val Epoch over. val_loss: 0.6435342564894135; val_accuracy: 0.7966759554140127 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.75
Batch: 60; loss: 0.92; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.78
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.71; acc: 0.72
Batch: 200; loss: 0.84; acc: 0.66
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.78
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.78
Batch: 340; loss: 0.81; acc: 0.72
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.73; acc: 0.7
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.78; acc: 0.7
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.77; acc: 0.62
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.75
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.6; acc: 0.73
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.58; acc: 0.8
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.75
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.86
Val Epoch over. val_loss: 0.6394030307508578; val_accuracy: 0.7940883757961783 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.79; acc: 0.72
Batch: 180; loss: 0.64; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.98; acc: 0.73
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.73
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.8
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.52; acc: 0.8
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.75
Batch: 680; loss: 0.81; acc: 0.72
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.81; acc: 0.81
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 0.67; train_accuracy: 0.78 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.6311143574061667; val_accuracy: 0.8036425159235668 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.77
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.9; acc: 0.77
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.85; acc: 0.7
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.61; acc: 0.73
Batch: 280; loss: 0.83; acc: 0.73
Batch: 300; loss: 0.83; acc: 0.69
Batch: 320; loss: 0.83; acc: 0.73
Batch: 340; loss: 0.93; acc: 0.69
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.88; acc: 0.7
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.81; acc: 0.72
Batch: 540; loss: 0.49; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.75
Batch: 660; loss: 0.83; acc: 0.75
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.5; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.67; train_accuracy: 0.78 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.83
Val Epoch over. val_loss: 0.6346629709954474; val_accuracy: 0.799562101910828 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.81; acc: 0.69
Batch: 180; loss: 0.84; acc: 0.8
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.73; acc: 0.77
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.77
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.75
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.93; acc: 0.7
Batch: 400; loss: 0.6; acc: 0.78
Batch: 420; loss: 0.68; acc: 0.73
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.84; acc: 0.75
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.59; acc: 0.78
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.75
Batch: 660; loss: 0.65; acc: 0.73
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.78 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.75
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.6094319277508243; val_accuracy: 0.8086186305732485 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.94; acc: 0.69
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.78
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.73
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.91; acc: 0.77
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.75
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.89; acc: 0.78
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.96; acc: 0.72
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.92; acc: 0.69
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.75
Batch: 600; loss: 0.8; acc: 0.75
Batch: 620; loss: 0.63; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.73
Batch: 700; loss: 0.76; acc: 0.73
Batch: 720; loss: 0.6; acc: 0.77
Batch: 740; loss: 1.13; acc: 0.73
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.619345260748438; val_accuracy: 0.8077229299363057 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.63; acc: 0.75
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.7
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 0.79; acc: 0.72
Batch: 240; loss: 0.79; acc: 0.7
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 0.69; acc: 0.77
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.75
Batch: 380; loss: 0.95; acc: 0.69
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.94; acc: 0.64
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.72
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.75
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.72; acc: 0.69
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.67; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.81
Val Epoch over. val_loss: 0.6268656165546672; val_accuracy: 0.7966759554140127 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.51; acc: 0.8
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.82; acc: 0.73
Batch: 240; loss: 0.86; acc: 0.78
Batch: 260; loss: 0.83; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.78
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.61; acc: 0.75
Batch: 360; loss: 0.62; acc: 0.73
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.77
Batch: 460; loss: 0.81; acc: 0.77
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.65; acc: 0.73
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.77
Batch: 600; loss: 0.61; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.78
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.65; acc: 0.77
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.79; acc: 0.8
Batch: 780; loss: 0.73; acc: 0.69
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.86; acc: 0.66
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.83
Val Epoch over. val_loss: 0.6353470732452003; val_accuracy: 0.7965764331210191 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.78
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.73
Batch: 240; loss: 0.72; acc: 0.73
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.62; acc: 0.78
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.83; acc: 0.73
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.78
Batch: 440; loss: 0.66; acc: 0.73
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.73
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.75; acc: 0.7
Batch: 580; loss: 0.84; acc: 0.73
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.73
Batch: 780; loss: 0.85; acc: 0.66
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.72
Batch: 140; loss: 0.42; acc: 0.84
Val Epoch over. val_loss: 0.5974287622293849; val_accuracy: 0.8130971337579618 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.67; acc: 0.73
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.73
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.96; acc: 0.67
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.86; acc: 0.72
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.8
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.94; acc: 0.77
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.8
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 1.2; acc: 0.69
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.75
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.77
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.84
Val Epoch over. val_loss: 0.5938276261281056; val_accuracy: 0.816281847133758 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.67; acc: 0.73
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.87; acc: 0.69
Batch: 220; loss: 0.73; acc: 0.77
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.77
Batch: 280; loss: 0.72; acc: 0.7
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 0.73; acc: 0.77
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.94; acc: 0.75
Batch: 380; loss: 0.65; acc: 0.75
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.83; acc: 0.73
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.66; acc: 0.75
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.89; acc: 0.72
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 1.01; acc: 0.66
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.59; acc: 0.77
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.84
Val Epoch over. val_loss: 0.597156769720612; val_accuracy: 0.8112062101910829 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.72
Batch: 180; loss: 0.86; acc: 0.75
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.73
Batch: 240; loss: 0.82; acc: 0.73
Batch: 260; loss: 0.64; acc: 0.8
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.78
Batch: 520; loss: 0.75; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.73
Batch: 560; loss: 0.74; acc: 0.72
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.8
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.75; acc: 0.73
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.65; acc: 0.81
Batch: 780; loss: 0.82; acc: 0.62
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6061059031516883; val_accuracy: 0.8092157643312102 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.99; acc: 0.72
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.67
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.75
Batch: 160; loss: 0.71; acc: 0.77
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.91; acc: 0.73
Batch: 220; loss: 0.82; acc: 0.7
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 0.62; acc: 0.77
Batch: 280; loss: 0.98; acc: 0.67
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.86; acc: 0.69
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 1.16; acc: 0.69
Batch: 400; loss: 0.65; acc: 0.75
Batch: 420; loss: 0.89; acc: 0.7
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.58; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.77
Batch: 540; loss: 0.57; acc: 0.73
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.8; acc: 0.73
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.83
Batch: 740; loss: 0.88; acc: 0.73
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.5947494853264207; val_accuracy: 0.8128980891719745 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.67; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.8
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.72
Batch: 220; loss: 1.02; acc: 0.64
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.79; acc: 0.73
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.65; acc: 0.77
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.82; acc: 0.75
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.63; acc: 0.75
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.81; acc: 0.73
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.8
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.8
Batch: 720; loss: 0.79; acc: 0.78
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.77
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.7
Batch: 140; loss: 0.42; acc: 0.86
Val Epoch over. val_loss: 0.587040236041804; val_accuracy: 0.8147890127388535 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.68; acc: 0.75
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.63; acc: 0.75
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.73
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.81; acc: 0.77
Batch: 420; loss: 0.69; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 1.06; acc: 0.72
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 1.0; acc: 0.72
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.75
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 1.15; acc: 0.75
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.82; acc: 0.69
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.72
Batch: 140; loss: 0.41; acc: 0.84
Val Epoch over. val_loss: 0.5823514381791376; val_accuracy: 0.8153861464968153 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.77
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.95; acc: 0.75
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.68; acc: 0.73
Batch: 420; loss: 0.6; acc: 0.77
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.73
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.69
Batch: 580; loss: 0.73; acc: 0.72
Batch: 600; loss: 0.53; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.62; acc: 0.77
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.83
Val Epoch over. val_loss: 0.5975720983022338; val_accuracy: 0.8094148089171974 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.92; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.72
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.73
Batch: 160; loss: 0.92; acc: 0.7
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.8
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.7
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.75
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.97; acc: 0.72
Batch: 580; loss: 0.66; acc: 0.77
Batch: 600; loss: 0.77; acc: 0.72
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.49; acc: 0.8
Batch: 700; loss: 0.82; acc: 0.73
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.73
Batch: 780; loss: 1.12; acc: 0.69
Train Epoch over. train_loss: 0.64; train_accuracy: 0.79 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.44; acc: 0.83
Val Epoch over. val_loss: 0.5905884607772159; val_accuracy: 0.8107085987261147 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.77
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.48; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.73
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.75
Batch: 300; loss: 0.9; acc: 0.67
Batch: 320; loss: 0.62; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.94; acc: 0.72
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.89
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.93; acc: 0.77
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.91; acc: 0.78
Batch: 780; loss: 0.81; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.7
Batch: 40; loss: 0.4; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.6040562788012681; val_accuracy: 0.8070262738853503 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.69
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.77
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.78; acc: 0.69
Batch: 160; loss: 1.04; acc: 0.7
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 1.01; acc: 0.73
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.9; acc: 0.75
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.73; acc: 0.73
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.75
Batch: 660; loss: 0.74; acc: 0.73
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.5811004391901052; val_accuracy: 0.8153861464968153 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.73
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.78
Batch: 160; loss: 0.67; acc: 0.77
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.82; acc: 0.78
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.79; acc: 0.72
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 1.0; acc: 0.7
Batch: 580; loss: 0.54; acc: 0.78
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.6; acc: 0.77
Batch: 660; loss: 0.67; acc: 0.72
Batch: 680; loss: 0.59; acc: 0.78
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.56; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.86
Val Epoch over. val_loss: 0.5809834549191651; val_accuracy: 0.8153861464968153 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.88; acc: 0.72
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.65; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.98; acc: 0.77
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.99; acc: 0.7
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.8
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.94; acc: 0.72
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.77
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 0.82; acc: 0.77
Batch: 560; loss: 0.63; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.72
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.73
Batch: 680; loss: 0.77; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.83; acc: 0.73
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.77
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.5794327022258643; val_accuracy: 0.8156847133757962 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.6; acc: 0.73
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.92; acc: 0.67
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.78; acc: 0.73
Batch: 280; loss: 0.85; acc: 0.67
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.75
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.81; acc: 0.73
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.8
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.75
Batch: 520; loss: 0.54; acc: 0.81
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.85; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.77
Batch: 680; loss: 0.63; acc: 0.77
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.78
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.5791520986967026; val_accuracy: 0.8157842356687898 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.95; acc: 0.66
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.99; acc: 0.72
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.73
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.73; acc: 0.73
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.78
Batch: 540; loss: 0.65; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.55; acc: 0.78
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.73
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.577763788734272; val_accuracy: 0.8137937898089171 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.75
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.78
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 0.65; acc: 0.77
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 0.73; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.78
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 0.91; acc: 0.73
Batch: 480; loss: 0.65; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.61; acc: 0.8
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.9; acc: 0.73
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.78
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.5797242135948436; val_accuracy: 0.8152866242038217 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 1.06; acc: 0.61
Batch: 300; loss: 0.81; acc: 0.73
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.64; acc: 0.73
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.65; acc: 0.78
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.8
Batch: 680; loss: 0.56; acc: 0.77
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.71; acc: 0.73
Batch: 760; loss: 0.71; acc: 0.72
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.4; acc: 0.83
Val Epoch over. val_loss: 0.579160068397689; val_accuracy: 0.8157842356687898 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.84; acc: 0.7
Batch: 300; loss: 0.9; acc: 0.7
Batch: 320; loss: 0.69; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.93; acc: 0.64
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.72; acc: 0.75
Batch: 480; loss: 0.65; acc: 0.77
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.75
Batch: 540; loss: 0.73; acc: 0.77
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.68; acc: 0.77
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.73
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.84
Val Epoch over. val_loss: 0.5784848753813725; val_accuracy: 0.8140923566878981 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.49; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.7
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.72
Batch: 360; loss: 0.69; acc: 0.77
Batch: 380; loss: 0.68; acc: 0.73
Batch: 400; loss: 0.54; acc: 0.78
Batch: 420; loss: 0.69; acc: 0.78
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.8; acc: 0.75
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.57951180622646; val_accuracy: 0.8156847133757962 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.69
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.75
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.78
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.98; acc: 0.69
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.57; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.7; acc: 0.78
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.7
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.89; acc: 0.75
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.86
Val Epoch over. val_loss: 0.5772772783971136; val_accuracy: 0.8150875796178344 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.75; acc: 0.73
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.73
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.8; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.88; acc: 0.69
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.73; acc: 0.78
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.78
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.7
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.5758718740503499; val_accuracy: 0.8165804140127388 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.61; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.8
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.8
Batch: 240; loss: 1.07; acc: 0.75
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.75
Batch: 300; loss: 0.55; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.77
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.75
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.77
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.73
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.77
Batch: 560; loss: 0.74; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.77
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.73
Batch: 640; loss: 0.82; acc: 0.72
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.84
Val Epoch over. val_loss: 0.5759406805892658; val_accuracy: 0.8152866242038217 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.8
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 0.64; acc: 0.75
Batch: 260; loss: 1.01; acc: 0.73
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.93; acc: 0.73
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.77
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.73
Batch: 560; loss: 0.98; acc: 0.66
Batch: 580; loss: 0.63; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 0.62; acc: 0.78
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.77
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.84
Val Epoch over. val_loss: 0.5756369882328495; val_accuracy: 0.8166799363057324 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.6; acc: 0.77
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.72
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.77
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.61; acc: 0.77
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.75
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.9; acc: 0.67
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.84
Val Epoch over. val_loss: 0.5762764375870395; val_accuracy: 0.8159832802547771 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.75
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.84; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.73
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.75
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.75
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.91; acc: 0.78
Batch: 460; loss: 0.75; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.96; acc: 0.7
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.64; acc: 0.78
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.75
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.74; acc: 0.77
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.5764541871798267; val_accuracy: 0.8172770700636943 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.75
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.73
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.78
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.76; acc: 0.72
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.87; acc: 0.75
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.67; acc: 0.75
Batch: 420; loss: 0.79; acc: 0.77
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.78
Batch: 500; loss: 0.79; acc: 0.73
Batch: 520; loss: 0.71; acc: 0.78
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.8; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.77
Batch: 660; loss: 0.7; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.5746632796848655; val_accuracy: 0.8186703821656051 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 1.01; acc: 0.66
Batch: 180; loss: 0.72; acc: 0.73
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.78
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.71; acc: 0.72
Batch: 400; loss: 0.85; acc: 0.69
Batch: 420; loss: 0.61; acc: 0.75
Batch: 440; loss: 0.73; acc: 0.73
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 0.78; acc: 0.78
Batch: 560; loss: 0.83; acc: 0.7
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.87; acc: 0.72
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.72
Batch: 740; loss: 0.73; acc: 0.73
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.5; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.84
Val Epoch over. val_loss: 0.5744762641324359; val_accuracy: 0.8184713375796179 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.75
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 1.09; acc: 0.72
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.77
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.66
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.87; acc: 0.7
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.8
Batch: 540; loss: 0.68; acc: 0.78
Batch: 560; loss: 0.62; acc: 0.75
Batch: 580; loss: 0.65; acc: 0.77
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.87; acc: 0.69
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.73; acc: 0.73
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.71; acc: 0.7
Batch: 760; loss: 0.6; acc: 0.8
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.84
Val Epoch over. val_loss: 0.5738958615311391; val_accuracy: 0.8182722929936306 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.82; acc: 0.72
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.78
Batch: 520; loss: 0.7; acc: 0.75
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.73
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 0.69; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.75
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.37; acc: 0.84
Val Epoch over. val_loss: 0.5745135538612202; val_accuracy: 0.8171775477707006 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.73
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.77
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.75
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.77
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.68; acc: 0.73
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.77
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.82; acc: 0.75
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.82; acc: 0.75
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.79; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.73
Batch: 740; loss: 0.72; acc: 0.73
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 1.0; acc: 0.67
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.84
Val Epoch over. val_loss: 0.5743250655615406; val_accuracy: 0.818172770700637 

plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 42083
elements in E: 8885200
fraction nonzero: 0.0047363030657722955
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.09
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.17
Batch: 220; loss: 2.26; acc: 0.23
Batch: 240; loss: 2.24; acc: 0.31
Batch: 260; loss: 2.25; acc: 0.25
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.34
Batch: 320; loss: 2.23; acc: 0.25
Batch: 340; loss: 2.22; acc: 0.34
Batch: 360; loss: 2.18; acc: 0.41
Batch: 380; loss: 2.19; acc: 0.36
Batch: 400; loss: 2.15; acc: 0.36
Batch: 420; loss: 2.11; acc: 0.39
Batch: 440; loss: 2.05; acc: 0.39
Batch: 460; loss: 2.0; acc: 0.42
Batch: 480; loss: 1.95; acc: 0.36
Batch: 500; loss: 1.85; acc: 0.42
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.4; acc: 0.59
Batch: 560; loss: 1.3; acc: 0.58
Batch: 580; loss: 1.27; acc: 0.64
Batch: 600; loss: 1.0; acc: 0.72
Batch: 620; loss: 1.0; acc: 0.67
Batch: 640; loss: 0.85; acc: 0.72
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 1.18; acc: 0.55
Batch: 700; loss: 1.07; acc: 0.58
Batch: 720; loss: 0.61; acc: 0.8
Batch: 740; loss: 0.94; acc: 0.64
Batch: 760; loss: 0.63; acc: 0.78
Batch: 780; loss: 1.08; acc: 0.62
Train Epoch over. train_loss: 1.8; train_accuracy: 0.39 

Batch: 0; loss: 0.85; acc: 0.69
Batch: 20; loss: 0.85; acc: 0.66
Batch: 40; loss: 0.44; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.7
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7196250138389078; val_accuracy: 0.7735867834394905 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.69
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 1.07; acc: 0.69
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 0.8; acc: 0.7
Batch: 140; loss: 0.85; acc: 0.72
Batch: 160; loss: 0.91; acc: 0.69
Batch: 180; loss: 0.82; acc: 0.75
Batch: 200; loss: 0.6; acc: 0.77
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 1.0; acc: 0.7
Batch: 260; loss: 0.91; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.77
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.75
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.63; acc: 0.77
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.69
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.75
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.84; acc: 0.75
Batch: 780; loss: 0.56; acc: 0.77
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 1.0; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 0.73; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.66
Batch: 80; loss: 0.79; acc: 0.67
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.59; acc: 0.81
Val Epoch over. val_loss: 0.8933421158866518; val_accuracy: 0.6991441082802548 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.55; acc: 0.77
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.66; acc: 0.72
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.52; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.64; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.91; acc: 0.7
Batch: 680; loss: 0.69; acc: 0.72
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.28; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 1.24; acc: 0.73
Batch: 140; loss: 0.65; acc: 0.83
Val Epoch over. val_loss: 0.964406487478572; val_accuracy: 0.7289012738853503 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.64
Batch: 80; loss: 0.57; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.81
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.82; acc: 0.8
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.84; acc: 0.75
Batch: 480; loss: 0.4; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.81; acc: 0.77
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.81
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 1.14; acc: 0.62
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 1.16; acc: 0.66
Batch: 140; loss: 0.43; acc: 0.83
Val Epoch over. val_loss: 0.9462291362938607; val_accuracy: 0.7126791401273885 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.77
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.73
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.75
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.04; acc: 0.69
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.91; acc: 0.8
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 1.11; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.83
Val Epoch over. val_loss: 0.841212472528409; val_accuracy: 0.7466162420382165 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.69
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.75; acc: 0.72
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 1.17; acc: 0.61
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.78
Batch: 60; loss: 0.95; acc: 0.67
Batch: 80; loss: 0.7; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 0.79; acc: 0.72
Val Epoch over. val_loss: 0.7591068465618571; val_accuracy: 0.7502985668789809 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.78
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.58; acc: 0.77
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.78
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.76; acc: 0.73
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3962324289664341; val_accuracy: 0.883359872611465 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.77
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.78
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.83; acc: 0.78
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 1.16; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.694470585436578; val_accuracy: 0.7960788216560509 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.75
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.81
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.8
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.83; acc: 0.75
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.73
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.62; acc: 0.73
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4669955732526293; val_accuracy: 0.8567874203821656 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.73
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.64; acc: 0.73
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.86
Val Epoch over. val_loss: 0.5827799192659414; val_accuracy: 0.8227507961783439 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.81
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.73
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.38267272473520536; val_accuracy: 0.884156050955414 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.78
Batch: 520; loss: 0.65; acc: 0.77
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.38153833340687354; val_accuracy: 0.8839570063694268 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.8
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.39667238840821445; val_accuracy: 0.878781847133758 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.81
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3689195000726706; val_accuracy: 0.8838574840764332 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.78
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.81
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.34547877076799705; val_accuracy: 0.8967953821656051 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.83
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.34624635238366525; val_accuracy: 0.8939092356687898 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.78
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.81
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.36264966029650086; val_accuracy: 0.89171974522293 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.67; acc: 0.73
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37969935267784033; val_accuracy: 0.8857484076433121 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.27; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3575540404695614; val_accuracy: 0.8902269108280255 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3488766890327642; val_accuracy: 0.892515923566879 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.78
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.3248098703801252; val_accuracy: 0.9028662420382165 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.33507891359982217; val_accuracy: 0.8984872611464968 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.32497188456024334; val_accuracy: 0.9052547770700637 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.81
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3263436502711788; val_accuracy: 0.9027667197452229 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.99; acc: 0.75
Batch: 260; loss: 0.36; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.68; acc: 0.77
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3354677585469689; val_accuracy: 0.8973925159235668 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.83
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.32566692224543564; val_accuracy: 0.9013734076433121 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3364018462361044; val_accuracy: 0.8974920382165605 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.37; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.38345164568371076; val_accuracy: 0.8808718152866242 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.4; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.81
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.3304006541325788; val_accuracy: 0.9028662420382165 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3406959260060529; val_accuracy: 0.8928144904458599 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.27; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3220993451726664; val_accuracy: 0.9033638535031847 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.320720143948391; val_accuracy: 0.9045581210191083 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.84
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.78
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.32182209497424447; val_accuracy: 0.9031648089171974 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3228341725886248; val_accuracy: 0.903562898089172 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.32167721473297495; val_accuracy: 0.9009753184713376 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.17; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.32422353944201376; val_accuracy: 0.902468152866242 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.31; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.32623276721899674; val_accuracy: 0.9040605095541401 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.8
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.32138754133206265; val_accuracy: 0.9022691082802548 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.81
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.32400057360434986; val_accuracy: 0.9038614649681529 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.8
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.52; acc: 0.78
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3234394280014524; val_accuracy: 0.9034633757961783 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.83
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.83
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3201623997965436; val_accuracy: 0.9031648089171974 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.31975060829501245; val_accuracy: 0.9027667197452229 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.319628912647059; val_accuracy: 0.9030652866242038 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.81
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.32002111334519784; val_accuracy: 0.9039609872611465 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.8
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.31959346506246333; val_accuracy: 0.9031648089171974 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.81
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.81
Batch: 640; loss: 0.45; acc: 0.8
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.31912654531514567; val_accuracy: 0.904359076433121 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3200642794465563; val_accuracy: 0.9029657643312102 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.3; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.63; acc: 0.8
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3198275574643141; val_accuracy: 0.902468152866242 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.81; acc: 0.77
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.31980719616648495; val_accuracy: 0.9026671974522293 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3205886554376335; val_accuracy: 0.9038614649681529 

plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 62943
elements in E: 13327800
fraction nonzero: 0.0047226849142394094
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.09
Batch: 140; loss: 2.27; acc: 0.09
Batch: 160; loss: 2.25; acc: 0.2
Batch: 180; loss: 2.27; acc: 0.22
Batch: 200; loss: 2.24; acc: 0.2
Batch: 220; loss: 2.21; acc: 0.28
Batch: 240; loss: 2.17; acc: 0.38
Batch: 260; loss: 2.14; acc: 0.3
Batch: 280; loss: 2.07; acc: 0.34
Batch: 300; loss: 1.95; acc: 0.33
Batch: 320; loss: 1.81; acc: 0.42
Batch: 340; loss: 1.55; acc: 0.53
Batch: 360; loss: 1.18; acc: 0.64
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 1.0; acc: 0.62
Batch: 420; loss: 1.07; acc: 0.61
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 1.0; acc: 0.61
Batch: 480; loss: 0.71; acc: 0.75
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.77
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.93; acc: 0.72
Batch: 700; loss: 0.83; acc: 0.72
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.78
Train Epoch over. train_loss: 1.38; train_accuracy: 0.53 

Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.5278095902910658; val_accuracy: 0.8343949044585988 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.75
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.79; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.7
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.72
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 0.628014019055731; val_accuracy: 0.7951831210191083 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.8
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.57; acc: 0.8
Batch: 200; loss: 0.4; acc: 0.81
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.4525275694526685; val_accuracy: 0.8490246815286624 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.81
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.86
Val Epoch over. val_loss: 0.4580377081207409; val_accuracy: 0.8553941082802548 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.46; acc: 0.8
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.78
Batch: 120; loss: 0.61; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.88; acc: 0.69
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4190375520165559; val_accuracy: 0.8627587579617835 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.7; acc: 0.72
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.32; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.349493532067841; val_accuracy: 0.8922173566878981 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.54; acc: 0.78
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.98
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.28311664786688084; val_accuracy: 0.9142117834394905 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.84
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.72; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.52; acc: 0.81
Val Epoch over. val_loss: 0.7225606069443332; val_accuracy: 0.7853304140127388 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.83
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.83
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3226617653943171; val_accuracy: 0.9002786624203821 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.41; acc: 0.81
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.437058951824334; val_accuracy: 0.8590764331210191 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2312048252695685; val_accuracy: 0.9311305732484076 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.08; acc: 1.0
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2537358786649765; val_accuracy: 0.9226711783439491 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2976036012931994; val_accuracy: 0.9061504777070064 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.27564235650904617; val_accuracy: 0.9147093949044586 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.56; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.22854947047248766; val_accuracy: 0.9335191082802548 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.23014671274810838; val_accuracy: 0.928343949044586 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.86
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2513061041237822; val_accuracy: 0.9231687898089171 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.25657934401255506; val_accuracy: 0.9230692675159236 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.07; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.07; acc: 1.0
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.88
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.23101408936225684; val_accuracy: 0.9277468152866242 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.28006142475138046; val_accuracy: 0.9147093949044586 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.22367931494287624; val_accuracy: 0.9342157643312102 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.22389220776166885; val_accuracy: 0.9312300955414012 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.21888978171880077; val_accuracy: 0.9346138535031847 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.84
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2162300162966464; val_accuracy: 0.9345143312101911 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.22021539706238516; val_accuracy: 0.9331210191082803 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2233242466333945; val_accuracy: 0.9327229299363057 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.22083871339441866; val_accuracy: 0.933718152866242 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.23533583752763498; val_accuracy: 0.9279458598726115 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2233714115847448; val_accuracy: 0.9323248407643312 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.22120479121804237; val_accuracy: 0.9340167197452229 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.21602315908879233; val_accuracy: 0.9345143312101911 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.21571803816659435; val_accuracy: 0.9344148089171974 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21904885282467126; val_accuracy: 0.9339171974522293 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2172938169092889; val_accuracy: 0.9363057324840764 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.21558374679012662; val_accuracy: 0.9354100318471338 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21712050487281412; val_accuracy: 0.9338176751592356 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.91
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.88
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2177774054324551; val_accuracy: 0.9355095541401274 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.89
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21518003990411; val_accuracy: 0.9353105095541401 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21431305480136234; val_accuracy: 0.9344148089171974 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2156790979918401; val_accuracy: 0.9349124203821656 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.84
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21481652761910372; val_accuracy: 0.9344148089171974 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21489380508851094; val_accuracy: 0.9354100318471338 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2150639938368539; val_accuracy: 0.9361066878980892 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.216308866218207; val_accuracy: 0.9347133757961783 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.89
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21502177903701544; val_accuracy: 0.9353105095541401 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.21505038539884955; val_accuracy: 0.9354100318471338 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.2; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.83
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2155513321375771; val_accuracy: 0.9350119426751592 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.84
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2146353205772722; val_accuracy: 0.935609076433121 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21582527797977635; val_accuracy: 0.9344148089171974 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21541392615741226; val_accuracy: 0.9336186305732485 

plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 84542
elements in E: 17770400
fraction nonzero: 0.0047574618466663665
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.28; acc: 0.17
Batch: 120; loss: 2.27; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.23
Batch: 160; loss: 2.2; acc: 0.41
Batch: 180; loss: 2.21; acc: 0.3
Batch: 200; loss: 2.16; acc: 0.28
Batch: 220; loss: 2.1; acc: 0.34
Batch: 240; loss: 1.9; acc: 0.61
Batch: 260; loss: 1.77; acc: 0.48
Batch: 280; loss: 1.33; acc: 0.61
Batch: 300; loss: 1.15; acc: 0.69
Batch: 320; loss: 1.09; acc: 0.62
Batch: 340; loss: 0.99; acc: 0.69
Batch: 360; loss: 0.81; acc: 0.7
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.72
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.8
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.7; acc: 0.73
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.78
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 1.18; train_accuracy: 0.61 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.75
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.3882189951599783; val_accuracy: 0.8791799363057324 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.81
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.3866956634031739; val_accuracy: 0.8771894904458599 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.78
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.78
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3982923030378712; val_accuracy: 0.8669386942675159 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.92
Val Epoch over. val_loss: 0.31744903539586217; val_accuracy: 0.8966958598726115 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.86
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.31686556524342036; val_accuracy: 0.903562898089172 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.21; acc: 0.88
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.30417144872770185; val_accuracy: 0.8974920382165605 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.24172462769754374; val_accuracy: 0.923765923566879 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.81
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.91 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 0.54; acc: 0.88
Val Epoch over. val_loss: 0.7809339961998022; val_accuracy: 0.7814490445859873 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.81
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.81
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.8
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.3093211843519454; val_accuracy: 0.9003781847133758 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.86
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.32768681787761156; val_accuracy: 0.8966958598726115 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20171025257771183; val_accuracy: 0.9387937898089171 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2196925519995249; val_accuracy: 0.9336186305732485 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21846953777086203; val_accuracy: 0.9338176751592356 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22844815242347444; val_accuracy: 0.9286425159235668 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.86
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21117442041920248; val_accuracy: 0.9332205414012739 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.83
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21392917170361347; val_accuracy: 0.9319267515923567 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2147125250120072; val_accuracy: 0.9307324840764332 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.83
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.84
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21670929802830813; val_accuracy: 0.931827229299363 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.86
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.20392250625570868; val_accuracy: 0.9370023885350318 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21149033179898172; val_accuracy: 0.935609076433121 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19068084362965482; val_accuracy: 0.9405851910828026 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18399216998724421; val_accuracy: 0.9437699044585988 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.19022324387055295; val_accuracy: 0.9400875796178344 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.86
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18755046385014132; val_accuracy: 0.9424761146496815 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.84
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.19184614091542115; val_accuracy: 0.9397890127388535 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.1906575848864522; val_accuracy: 0.9413813694267515 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.1915585055804936; val_accuracy: 0.9404856687898089 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.19536001648113227; val_accuracy: 0.9391918789808917 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.08; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19316705553584798; val_accuracy: 0.9427746815286624 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.18; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.19083079859424548; val_accuracy: 0.941281847133758 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18325857165607679; val_accuracy: 0.9449641719745223 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.1831809533581992; val_accuracy: 0.9435708598726115 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18384368774617554; val_accuracy: 0.9446656050955414 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.84
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18212046239311527; val_accuracy: 0.9437699044585988 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.89
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1839836351810747; val_accuracy: 0.9434713375796179 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.1; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18459183608840227; val_accuracy: 0.944765127388535 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.184486037155815; val_accuracy: 0.9434713375796179 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.84
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.1819554986372875; val_accuracy: 0.9443670382165605 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18158542120437712; val_accuracy: 0.9453622611464968 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18290073728295647; val_accuracy: 0.9450636942675159 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1811358028916037; val_accuracy: 0.945859872611465 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18158208763903114; val_accuracy: 0.9444665605095541 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.09; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.18171626730424584; val_accuracy: 0.9455613057324841 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.64; acc: 0.91
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18169263966239182; val_accuracy: 0.9443670382165605 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.181317875173631; val_accuracy: 0.9455613057324841 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1808188202179921; val_accuracy: 0.9452627388535032 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18050328729923365; val_accuracy: 0.945859872611465 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.1; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1803690818654504; val_accuracy: 0.9462579617834395 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18078256837406737; val_accuracy: 0.9457603503184714 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1830681597066533; val_accuracy: 0.944765127388535 

plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 105710
elements in E: 22213000
fraction nonzero: 0.0047589249538558505
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.22
Batch: 100; loss: 2.27; acc: 0.16
Batch: 120; loss: 2.24; acc: 0.3
Batch: 140; loss: 2.23; acc: 0.28
Batch: 160; loss: 2.15; acc: 0.52
Batch: 180; loss: 2.11; acc: 0.27
Batch: 200; loss: 1.9; acc: 0.42
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.15; acc: 0.55
Batch: 260; loss: 1.34; acc: 0.61
Batch: 280; loss: 0.9; acc: 0.73
Batch: 300; loss: 0.81; acc: 0.7
Batch: 320; loss: 0.73; acc: 0.69
Batch: 340; loss: 1.16; acc: 0.69
Batch: 360; loss: 0.78; acc: 0.7
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.7
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.82; acc: 0.83
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 1.06; train_accuracy: 0.65 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2933479428860792; val_accuracy: 0.9104299363057324 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.8
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.8
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.81
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.83
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.48072778950830936; val_accuracy: 0.8565883757961783 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.75
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.46166679215659; val_accuracy: 0.8541998407643312 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.07; acc: 1.0
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.29273279329204255; val_accuracy: 0.9082404458598726 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.55; acc: 0.73
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.44765396629738957; val_accuracy: 0.8655453821656051 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.45; acc: 0.8
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.89
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.36; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17440748537421985; val_accuracy: 0.9461584394904459 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.91
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18592355179653805; val_accuracy: 0.9456608280254777 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3693144209920221; val_accuracy: 0.8975915605095541 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2396219871036566; val_accuracy: 0.9258558917197452 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.98
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.08; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.92
Val Epoch over. val_loss: 0.4597180863949144; val_accuracy: 0.8663415605095541 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1687970929986732; val_accuracy: 0.9473527070063694 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17145650390132217; val_accuracy: 0.9488455414012739 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17756213492174058; val_accuracy: 0.9461584394904459 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16156091987137583; val_accuracy: 0.9517316878980892 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17453677036390183; val_accuracy: 0.9482484076433121 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16003208773530972; val_accuracy: 0.9519307324840764 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.89
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.168494834048543; val_accuracy: 0.9477507961783439 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.24088787211544194; val_accuracy: 0.9282444267515924 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.156324103474617; val_accuracy: 0.9543192675159236 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1912494328371279; val_accuracy: 0.943172770700637 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14970124000386828; val_accuracy: 0.9555135350318471 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14862370130362784; val_accuracy: 0.9546178343949044 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14822939612493394; val_accuracy: 0.9557125796178344 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15621820395919167; val_accuracy: 0.9535230891719745 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1475941764226385; val_accuracy: 0.9560111464968153 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15232359124406888; val_accuracy: 0.9561106687898089 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14928392800176218; val_accuracy: 0.955812101910828 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14939121050155088; val_accuracy: 0.9562101910828026 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15141181657268743; val_accuracy: 0.9549164012738853 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1468923574515209; val_accuracy: 0.9578025477707006 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.91
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.08; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14436820723638413; val_accuracy: 0.9568073248407644 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14429008426844694; val_accuracy: 0.9574044585987261 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14268378275120333; val_accuracy: 0.9581011146496815 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14454601043064125; val_accuracy: 0.9579020700636943 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.91
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14687342301105996; val_accuracy: 0.9552149681528662 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14919613225824513; val_accuracy: 0.9565087579617835 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.88
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14363779070650695; val_accuracy: 0.9583001592356688 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1448529957064018; val_accuracy: 0.9564092356687898 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.07; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14292192537408727; val_accuracy: 0.9589968152866242 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14484909675113714; val_accuracy: 0.9571058917197452 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14247129254849852; val_accuracy: 0.9581011146496815 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1439177186768146; val_accuracy: 0.9565087579617835 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14266496759121586; val_accuracy: 0.9579020700636943 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14279086273282196; val_accuracy: 0.956906847133758 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14336625904224481; val_accuracy: 0.9582006369426752 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14261690171281244; val_accuracy: 0.9574044585987261 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14253376011446023; val_accuracy: 0.9571058917197452 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14175943933000232; val_accuracy: 0.9573049363057324 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14241202550519044; val_accuracy: 0.9575039808917197 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1436844311986759; val_accuracy: 0.9566082802547771 

plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/lenet/2020-01-19 17:34:33/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4385829/slurm_script: line 25: --print_freq=20: command not found
