nonzero elements in E: 44696
elements in E: 19921000
fraction nonzero: 0.0022436624667436372
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 36.37; acc: 0.12
Batch: 20; loss: 19.85; acc: 0.19
Batch: 40; loss: 17.04; acc: 0.25
Batch: 60; loss: 19.84; acc: 0.25
Batch: 80; loss: 17.99; acc: 0.23
Batch: 100; loss: 16.37; acc: 0.3
Batch: 120; loss: 19.82; acc: 0.25
Batch: 140; loss: 16.36; acc: 0.28
Batch: 160; loss: 11.52; acc: 0.48
Batch: 180; loss: 16.15; acc: 0.39
Batch: 200; loss: 15.81; acc: 0.34
Batch: 220; loss: 16.5; acc: 0.31
Batch: 240; loss: 10.0; acc: 0.41
Batch: 260; loss: 14.48; acc: 0.33
Batch: 280; loss: 14.43; acc: 0.38
Batch: 300; loss: 14.81; acc: 0.41
Batch: 320; loss: 14.8; acc: 0.34
Batch: 340; loss: 13.97; acc: 0.39
Batch: 360; loss: 15.09; acc: 0.39
Batch: 380; loss: 17.6; acc: 0.33
Batch: 400; loss: 13.5; acc: 0.41
Batch: 420; loss: 18.12; acc: 0.36
Batch: 440; loss: 13.4; acc: 0.39
Batch: 460; loss: 16.77; acc: 0.44
Batch: 480; loss: 12.68; acc: 0.38
Batch: 500; loss: 19.59; acc: 0.34
Batch: 520; loss: 15.31; acc: 0.38
Batch: 540; loss: 14.35; acc: 0.42
Batch: 560; loss: 14.8; acc: 0.39
Batch: 580; loss: 18.24; acc: 0.36
Batch: 600; loss: 14.37; acc: 0.47
Batch: 620; loss: 14.0; acc: 0.39
Train Epoch over. train_loss: 15.33; train_accuracy: 0.36 

Batch: 0; loss: 16.03; acc: 0.33
Batch: 20; loss: 19.51; acc: 0.22
Batch: 40; loss: 17.26; acc: 0.38
Batch: 60; loss: 17.49; acc: 0.33
Batch: 80; loss: 18.59; acc: 0.28
Batch: 100; loss: 18.49; acc: 0.28
Batch: 120; loss: 15.64; acc: 0.44
Batch: 140; loss: 17.91; acc: 0.31
Val Epoch over. val_loss: 15.563458096449542; val_accuracy: 0.3638535031847134 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 13.01; acc: 0.38
Batch: 20; loss: 12.48; acc: 0.52
Batch: 40; loss: 14.63; acc: 0.31
Batch: 60; loss: 12.72; acc: 0.42
Batch: 80; loss: 16.22; acc: 0.41
Batch: 100; loss: 17.83; acc: 0.34
Batch: 120; loss: 17.69; acc: 0.44
Batch: 140; loss: 14.68; acc: 0.52
Batch: 160; loss: 15.37; acc: 0.42
Batch: 180; loss: 13.58; acc: 0.39
Batch: 200; loss: 15.28; acc: 0.34
Batch: 220; loss: 20.55; acc: 0.36
Batch: 240; loss: 12.52; acc: 0.41
Batch: 260; loss: 14.23; acc: 0.41
Batch: 280; loss: 12.57; acc: 0.48
Batch: 300; loss: 15.2; acc: 0.48
Batch: 320; loss: 13.51; acc: 0.48
Batch: 340; loss: 16.22; acc: 0.39
Batch: 360; loss: 15.55; acc: 0.36
Batch: 380; loss: 15.01; acc: 0.42
Batch: 400; loss: 17.3; acc: 0.36
Batch: 420; loss: 14.47; acc: 0.42
Batch: 440; loss: 13.77; acc: 0.44
Batch: 460; loss: 17.24; acc: 0.31
Batch: 480; loss: 17.97; acc: 0.28
Batch: 500; loss: 9.74; acc: 0.42
Batch: 520; loss: 15.82; acc: 0.41
Batch: 540; loss: 11.44; acc: 0.52
Batch: 560; loss: 14.85; acc: 0.41
Batch: 580; loss: 17.82; acc: 0.42
Batch: 600; loss: 19.26; acc: 0.38
Batch: 620; loss: 15.31; acc: 0.36
Train Epoch over. train_loss: 14.83; train_accuracy: 0.4 

Batch: 0; loss: 14.69; acc: 0.39
Batch: 20; loss: 14.9; acc: 0.38
Batch: 40; loss: 15.98; acc: 0.31
Batch: 60; loss: 15.71; acc: 0.39
Batch: 80; loss: 15.43; acc: 0.36
Batch: 100; loss: 16.64; acc: 0.39
Batch: 120; loss: 12.34; acc: 0.55
Batch: 140; loss: 16.23; acc: 0.27
Val Epoch over. val_loss: 14.451731608931427; val_accuracy: 0.3968949044585987 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 18.46; acc: 0.3
Batch: 20; loss: 16.71; acc: 0.34
Batch: 40; loss: 9.31; acc: 0.53
Batch: 60; loss: 23.1; acc: 0.2
Batch: 80; loss: 16.64; acc: 0.34
Batch: 100; loss: 12.18; acc: 0.47
Batch: 120; loss: 17.86; acc: 0.39
Batch: 140; loss: 17.68; acc: 0.44
Batch: 160; loss: 12.57; acc: 0.42
Batch: 180; loss: 15.06; acc: 0.36
Batch: 200; loss: 17.36; acc: 0.36
Batch: 220; loss: 17.36; acc: 0.33
Batch: 240; loss: 13.41; acc: 0.36
Batch: 260; loss: 10.63; acc: 0.41
Batch: 280; loss: 16.95; acc: 0.53
Batch: 300; loss: 17.44; acc: 0.42
Batch: 320; loss: 16.29; acc: 0.44
Batch: 340; loss: 11.9; acc: 0.44
Batch: 360; loss: 16.16; acc: 0.44
Batch: 380; loss: 12.4; acc: 0.48
Batch: 400; loss: 15.24; acc: 0.38
Batch: 420; loss: 13.95; acc: 0.39
Batch: 440; loss: 8.79; acc: 0.48
Batch: 460; loss: 14.27; acc: 0.39
Batch: 480; loss: 12.68; acc: 0.41
Batch: 500; loss: 15.28; acc: 0.42
Batch: 520; loss: 17.74; acc: 0.45
Batch: 540; loss: 13.58; acc: 0.41
Batch: 560; loss: 17.93; acc: 0.31
Batch: 580; loss: 14.7; acc: 0.39
Batch: 600; loss: 14.7; acc: 0.38
Batch: 620; loss: 12.87; acc: 0.38
Train Epoch over. train_loss: 14.78; train_accuracy: 0.4 

Batch: 0; loss: 15.01; acc: 0.33
Batch: 20; loss: 15.89; acc: 0.3
Batch: 40; loss: 15.12; acc: 0.39
Batch: 60; loss: 15.69; acc: 0.36
Batch: 80; loss: 14.62; acc: 0.31
Batch: 100; loss: 17.65; acc: 0.34
Batch: 120; loss: 10.2; acc: 0.59
Batch: 140; loss: 17.53; acc: 0.27
Val Epoch over. val_loss: 14.668335525852859; val_accuracy: 0.37718949044585987 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 14.31; acc: 0.44
Batch: 20; loss: 13.23; acc: 0.38
Batch: 40; loss: 12.69; acc: 0.41
Batch: 60; loss: 12.85; acc: 0.38
Batch: 80; loss: 12.64; acc: 0.42
Batch: 100; loss: 13.1; acc: 0.39
Batch: 120; loss: 13.54; acc: 0.38
Batch: 140; loss: 14.0; acc: 0.42
Batch: 160; loss: 17.6; acc: 0.34
Batch: 180; loss: 12.04; acc: 0.48
Batch: 200; loss: 13.59; acc: 0.42
Batch: 220; loss: 13.99; acc: 0.41
Batch: 240; loss: 17.79; acc: 0.23
Batch: 260; loss: 12.4; acc: 0.44
Batch: 280; loss: 10.7; acc: 0.48
Batch: 300; loss: 11.08; acc: 0.55
Batch: 320; loss: 17.56; acc: 0.34
Batch: 340; loss: 13.57; acc: 0.44
Batch: 360; loss: 13.8; acc: 0.41
Batch: 380; loss: 13.17; acc: 0.42
Batch: 400; loss: 12.91; acc: 0.42
Batch: 420; loss: 16.2; acc: 0.45
Batch: 440; loss: 15.61; acc: 0.36
Batch: 460; loss: 13.29; acc: 0.45
Batch: 480; loss: 12.29; acc: 0.45
Batch: 500; loss: 13.66; acc: 0.5
Batch: 520; loss: 13.76; acc: 0.45
Batch: 540; loss: 13.99; acc: 0.39
Batch: 560; loss: 16.34; acc: 0.39
Batch: 580; loss: 16.8; acc: 0.41
Batch: 600; loss: 13.69; acc: 0.44
Batch: 620; loss: 14.56; acc: 0.41
Train Epoch over. train_loss: 14.72; train_accuracy: 0.41 

Batch: 0; loss: 13.29; acc: 0.39
Batch: 20; loss: 14.99; acc: 0.36
Batch: 40; loss: 12.72; acc: 0.48
Batch: 60; loss: 13.55; acc: 0.41
Batch: 80; loss: 13.39; acc: 0.47
Batch: 100; loss: 16.44; acc: 0.41
Batch: 120; loss: 11.32; acc: 0.5
Batch: 140; loss: 17.2; acc: 0.28
Val Epoch over. val_loss: 14.187978213000449; val_accuracy: 0.41023089171974525 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 15.02; acc: 0.33
Batch: 20; loss: 8.01; acc: 0.53
Batch: 40; loss: 13.73; acc: 0.34
Batch: 60; loss: 16.7; acc: 0.39
Batch: 80; loss: 14.44; acc: 0.41
Batch: 100; loss: 15.8; acc: 0.48
Batch: 120; loss: 16.26; acc: 0.39
Batch: 140; loss: 14.48; acc: 0.45
Batch: 160; loss: 11.65; acc: 0.34
Batch: 180; loss: 17.56; acc: 0.3
Batch: 200; loss: 19.58; acc: 0.22
Batch: 220; loss: 12.5; acc: 0.41
Batch: 240; loss: 14.29; acc: 0.39
Batch: 260; loss: 15.06; acc: 0.41
Batch: 280; loss: 14.16; acc: 0.44
Batch: 300; loss: 18.92; acc: 0.34
Batch: 320; loss: 15.21; acc: 0.31
Batch: 340; loss: 14.89; acc: 0.42
Batch: 360; loss: 12.63; acc: 0.44
Batch: 380; loss: 11.52; acc: 0.52
Batch: 400; loss: 11.74; acc: 0.5
Batch: 420; loss: 14.25; acc: 0.44
Batch: 440; loss: 14.15; acc: 0.31
Batch: 460; loss: 13.04; acc: 0.44
Batch: 480; loss: 14.24; acc: 0.36
Batch: 500; loss: 12.07; acc: 0.52
Batch: 520; loss: 15.57; acc: 0.41
Batch: 540; loss: 13.4; acc: 0.47
Batch: 560; loss: 13.05; acc: 0.39
Batch: 580; loss: 16.81; acc: 0.33
Batch: 600; loss: 17.12; acc: 0.48
Batch: 620; loss: 15.8; acc: 0.33
Train Epoch over. train_loss: 14.69; train_accuracy: 0.41 

Batch: 0; loss: 14.71; acc: 0.42
Batch: 20; loss: 15.48; acc: 0.3
Batch: 40; loss: 13.83; acc: 0.45
Batch: 60; loss: 12.86; acc: 0.39
Batch: 80; loss: 13.96; acc: 0.41
Batch: 100; loss: 18.24; acc: 0.33
Batch: 120; loss: 10.17; acc: 0.5
Batch: 140; loss: 18.31; acc: 0.33
Val Epoch over. val_loss: 14.458835911598934; val_accuracy: 0.41789410828025475 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 14.22; acc: 0.41
Batch: 20; loss: 11.65; acc: 0.42
Batch: 40; loss: 17.66; acc: 0.36
Batch: 60; loss: 13.44; acc: 0.45
Batch: 80; loss: 19.1; acc: 0.27
Batch: 100; loss: 12.85; acc: 0.38
Batch: 120; loss: 15.98; acc: 0.41
Batch: 140; loss: 11.56; acc: 0.45
Batch: 160; loss: 16.34; acc: 0.42
Batch: 180; loss: 12.44; acc: 0.48
Batch: 200; loss: 16.31; acc: 0.36
Batch: 220; loss: 15.19; acc: 0.45
Batch: 240; loss: 14.41; acc: 0.36
Batch: 260; loss: 14.91; acc: 0.42
Batch: 280; loss: 10.67; acc: 0.53
Batch: 300; loss: 10.84; acc: 0.48
Batch: 320; loss: 13.31; acc: 0.34
Batch: 340; loss: 16.91; acc: 0.44
Batch: 360; loss: 7.92; acc: 0.55
Batch: 380; loss: 19.35; acc: 0.36
Batch: 400; loss: 16.48; acc: 0.34
Batch: 420; loss: 16.01; acc: 0.47
Batch: 440; loss: 15.11; acc: 0.42
Batch: 460; loss: 12.48; acc: 0.41
Batch: 480; loss: 13.32; acc: 0.44
Batch: 500; loss: 12.69; acc: 0.52
Batch: 520; loss: 18.27; acc: 0.36
Batch: 540; loss: 14.12; acc: 0.42
Batch: 560; loss: 18.67; acc: 0.34
Batch: 580; loss: 16.28; acc: 0.33
Batch: 600; loss: 14.86; acc: 0.44
Batch: 620; loss: 14.81; acc: 0.47
Train Epoch over. train_loss: 14.62; train_accuracy: 0.41 

Batch: 0; loss: 12.59; acc: 0.44
Batch: 20; loss: 15.76; acc: 0.34
Batch: 40; loss: 12.14; acc: 0.45
Batch: 60; loss: 15.13; acc: 0.34
Batch: 80; loss: 14.12; acc: 0.36
Batch: 100; loss: 16.46; acc: 0.41
Batch: 120; loss: 10.72; acc: 0.53
Batch: 140; loss: 17.12; acc: 0.27
Val Epoch over. val_loss: 14.366626484378887; val_accuracy: 0.40973328025477707 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 14.49; acc: 0.34
Batch: 20; loss: 17.45; acc: 0.38
Batch: 40; loss: 12.21; acc: 0.41
Batch: 60; loss: 17.46; acc: 0.33
Batch: 80; loss: 19.51; acc: 0.33
Batch: 100; loss: 15.6; acc: 0.38
Batch: 120; loss: 12.76; acc: 0.45
Batch: 140; loss: 12.81; acc: 0.42
Batch: 160; loss: 15.83; acc: 0.45
Batch: 180; loss: 14.18; acc: 0.38
Batch: 200; loss: 16.81; acc: 0.38
Batch: 220; loss: 15.04; acc: 0.39
Batch: 240; loss: 14.16; acc: 0.41
Batch: 260; loss: 12.64; acc: 0.45
Batch: 280; loss: 12.54; acc: 0.5
Batch: 300; loss: 13.19; acc: 0.41
Batch: 320; loss: 15.76; acc: 0.34
Batch: 340; loss: 14.94; acc: 0.41
Batch: 360; loss: 10.94; acc: 0.55
Batch: 380; loss: 13.52; acc: 0.44
Batch: 400; loss: 14.38; acc: 0.44
Batch: 420; loss: 12.66; acc: 0.45
Batch: 440; loss: 13.49; acc: 0.39
Batch: 460; loss: 17.76; acc: 0.34
Batch: 480; loss: 12.46; acc: 0.52
Batch: 500; loss: 12.23; acc: 0.48
Batch: 520; loss: 16.54; acc: 0.39
Batch: 540; loss: 12.88; acc: 0.45
Batch: 560; loss: 13.14; acc: 0.34
Batch: 580; loss: 13.68; acc: 0.44
Batch: 600; loss: 12.96; acc: 0.44
Batch: 620; loss: 15.54; acc: 0.42
Train Epoch over. train_loss: 14.66; train_accuracy: 0.41 

Batch: 0; loss: 12.63; acc: 0.47
Batch: 20; loss: 16.34; acc: 0.34
Batch: 40; loss: 13.18; acc: 0.5
Batch: 60; loss: 14.7; acc: 0.39
Batch: 80; loss: 17.92; acc: 0.38
Batch: 100; loss: 18.68; acc: 0.33
Batch: 120; loss: 13.8; acc: 0.55
Batch: 140; loss: 21.69; acc: 0.3
Val Epoch over. val_loss: 15.311345562054093; val_accuracy: 0.41421178343949044 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 17.57; acc: 0.31
Batch: 20; loss: 13.32; acc: 0.52
Batch: 40; loss: 17.0; acc: 0.48
Batch: 60; loss: 13.29; acc: 0.39
Batch: 80; loss: 13.21; acc: 0.42
Batch: 100; loss: 14.1; acc: 0.36
Batch: 120; loss: 12.53; acc: 0.41
Batch: 140; loss: 16.61; acc: 0.36
Batch: 160; loss: 14.97; acc: 0.41
Batch: 180; loss: 12.73; acc: 0.45
Batch: 200; loss: 13.33; acc: 0.52
Batch: 220; loss: 17.21; acc: 0.41
Batch: 240; loss: 10.19; acc: 0.5
Batch: 260; loss: 14.42; acc: 0.47
Batch: 280; loss: 18.84; acc: 0.36
Batch: 300; loss: 12.2; acc: 0.58
Batch: 320; loss: 18.58; acc: 0.28
Batch: 340; loss: 12.75; acc: 0.48
Batch: 360; loss: 15.89; acc: 0.3
Batch: 380; loss: 14.88; acc: 0.41
Batch: 400; loss: 14.9; acc: 0.41
Batch: 420; loss: 12.14; acc: 0.44
Batch: 440; loss: 12.32; acc: 0.47
Batch: 460; loss: 13.21; acc: 0.44
Batch: 480; loss: 13.64; acc: 0.42
Batch: 500; loss: 16.85; acc: 0.44
Batch: 520; loss: 14.47; acc: 0.41
Batch: 540; loss: 17.28; acc: 0.39
Batch: 560; loss: 13.07; acc: 0.45
Batch: 580; loss: 13.46; acc: 0.53
Batch: 600; loss: 14.61; acc: 0.42
Batch: 620; loss: 14.59; acc: 0.39
Train Epoch over. train_loss: 14.66; train_accuracy: 0.41 

Batch: 0; loss: 15.0; acc: 0.38
Batch: 20; loss: 17.72; acc: 0.23
Batch: 40; loss: 13.62; acc: 0.39
Batch: 60; loss: 16.26; acc: 0.34
Batch: 80; loss: 16.02; acc: 0.34
Batch: 100; loss: 15.8; acc: 0.41
Batch: 120; loss: 10.87; acc: 0.55
Batch: 140; loss: 16.9; acc: 0.28
Val Epoch over. val_loss: 14.991441331851254; val_accuracy: 0.39759156050955413 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 15.6; acc: 0.38
Batch: 20; loss: 14.36; acc: 0.44
Batch: 40; loss: 13.92; acc: 0.47
Batch: 60; loss: 13.44; acc: 0.38
Batch: 80; loss: 12.3; acc: 0.41
Batch: 100; loss: 16.96; acc: 0.44
Batch: 120; loss: 14.88; acc: 0.36
Batch: 140; loss: 10.49; acc: 0.48
Batch: 160; loss: 13.11; acc: 0.48
Batch: 180; loss: 12.33; acc: 0.45
Batch: 200; loss: 17.79; acc: 0.34
Batch: 220; loss: 13.96; acc: 0.48
Batch: 240; loss: 14.05; acc: 0.38
Batch: 260; loss: 12.69; acc: 0.39
Batch: 280; loss: 14.87; acc: 0.47
Batch: 300; loss: 12.51; acc: 0.38
Batch: 320; loss: 15.52; acc: 0.45
Batch: 340; loss: 16.83; acc: 0.34
Batch: 360; loss: 17.33; acc: 0.36
Batch: 380; loss: 12.5; acc: 0.47
Batch: 400; loss: 21.02; acc: 0.31
Batch: 420; loss: 14.11; acc: 0.45
Batch: 440; loss: 17.7; acc: 0.34
Batch: 460; loss: 17.29; acc: 0.33
Batch: 480; loss: 12.87; acc: 0.42
Batch: 500; loss: 15.76; acc: 0.3
Batch: 520; loss: 12.06; acc: 0.42
Batch: 540; loss: 12.9; acc: 0.52
Batch: 560; loss: 15.65; acc: 0.25
Batch: 580; loss: 12.72; acc: 0.5
Batch: 600; loss: 13.15; acc: 0.45
Batch: 620; loss: 16.17; acc: 0.31
Train Epoch over. train_loss: 14.7; train_accuracy: 0.41 

Batch: 0; loss: 13.24; acc: 0.41
Batch: 20; loss: 16.46; acc: 0.31
Batch: 40; loss: 13.8; acc: 0.39
Batch: 60; loss: 13.83; acc: 0.42
Batch: 80; loss: 14.31; acc: 0.42
Batch: 100; loss: 17.1; acc: 0.36
Batch: 120; loss: 9.52; acc: 0.58
Batch: 140; loss: 17.73; acc: 0.3
Val Epoch over. val_loss: 14.567926874585972; val_accuracy: 0.395203025477707 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 17.57; acc: 0.28
Batch: 20; loss: 14.01; acc: 0.38
Batch: 40; loss: 15.46; acc: 0.44
Batch: 60; loss: 19.36; acc: 0.42
Batch: 80; loss: 16.62; acc: 0.36
Batch: 100; loss: 19.59; acc: 0.3
Batch: 120; loss: 21.01; acc: 0.3
Batch: 140; loss: 12.2; acc: 0.36
Batch: 160; loss: 13.43; acc: 0.42
Batch: 180; loss: 13.01; acc: 0.48
Batch: 200; loss: 14.38; acc: 0.52
Batch: 220; loss: 20.88; acc: 0.34
Batch: 240; loss: 16.01; acc: 0.41
Batch: 260; loss: 12.33; acc: 0.45
Batch: 280; loss: 14.0; acc: 0.44
Batch: 300; loss: 19.24; acc: 0.44
Batch: 320; loss: 10.64; acc: 0.42
Batch: 340; loss: 15.64; acc: 0.47
Batch: 360; loss: 16.69; acc: 0.41
Batch: 380; loss: 10.11; acc: 0.62
Batch: 400; loss: 15.64; acc: 0.36
Batch: 420; loss: 13.89; acc: 0.44
Batch: 440; loss: 14.69; acc: 0.34
Batch: 460; loss: 21.56; acc: 0.31
Batch: 480; loss: 8.8; acc: 0.47
Batch: 500; loss: 15.31; acc: 0.41
Batch: 520; loss: 11.65; acc: 0.5
Batch: 540; loss: 12.93; acc: 0.55
Batch: 560; loss: 14.43; acc: 0.34
Batch: 580; loss: 11.1; acc: 0.48
Batch: 600; loss: 12.65; acc: 0.38
Batch: 620; loss: 15.28; acc: 0.36
Train Epoch over. train_loss: 14.64; train_accuracy: 0.41 

Batch: 0; loss: 15.38; acc: 0.38
Batch: 20; loss: 16.14; acc: 0.34
Batch: 40; loss: 13.96; acc: 0.48
Batch: 60; loss: 14.69; acc: 0.39
Batch: 80; loss: 15.44; acc: 0.36
Batch: 100; loss: 18.91; acc: 0.38
Batch: 120; loss: 13.15; acc: 0.42
Batch: 140; loss: 18.62; acc: 0.23
Val Epoch over. val_loss: 15.758866152186302; val_accuracy: 0.38505175159235666 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 13.94; acc: 0.44
Batch: 20; loss: 13.99; acc: 0.42
Batch: 40; loss: 14.83; acc: 0.36
Batch: 60; loss: 12.96; acc: 0.41
Batch: 80; loss: 11.08; acc: 0.47
Batch: 100; loss: 16.32; acc: 0.3
Batch: 120; loss: 14.89; acc: 0.34
Batch: 140; loss: 13.13; acc: 0.34
Batch: 160; loss: 14.11; acc: 0.47
Batch: 180; loss: 13.04; acc: 0.39
Batch: 200; loss: 12.41; acc: 0.44
Batch: 220; loss: 13.99; acc: 0.41
Batch: 240; loss: 13.43; acc: 0.38
Batch: 260; loss: 11.21; acc: 0.47
Batch: 280; loss: 10.84; acc: 0.5
Batch: 300; loss: 9.36; acc: 0.5
Batch: 320; loss: 16.21; acc: 0.45
Batch: 340; loss: 14.45; acc: 0.42
Batch: 360; loss: 10.99; acc: 0.52
Batch: 380; loss: 14.08; acc: 0.39
Batch: 400; loss: 16.79; acc: 0.45
Batch: 420; loss: 16.04; acc: 0.33
Batch: 440; loss: 13.61; acc: 0.45
Batch: 460; loss: 13.26; acc: 0.47
Batch: 480; loss: 10.81; acc: 0.45
Batch: 500; loss: 13.5; acc: 0.44
Batch: 520; loss: 9.31; acc: 0.58
Batch: 540; loss: 14.12; acc: 0.39
Batch: 560; loss: 14.11; acc: 0.47
Batch: 580; loss: 14.14; acc: 0.45
Batch: 600; loss: 12.35; acc: 0.44
Batch: 620; loss: 14.9; acc: 0.38
Train Epoch over. train_loss: 13.54; train_accuracy: 0.42 

Batch: 0; loss: 12.07; acc: 0.44
Batch: 20; loss: 15.04; acc: 0.31
Batch: 40; loss: 12.4; acc: 0.5
Batch: 60; loss: 12.96; acc: 0.39
Batch: 80; loss: 12.94; acc: 0.42
Batch: 100; loss: 16.24; acc: 0.42
Batch: 120; loss: 10.59; acc: 0.5
Batch: 140; loss: 17.61; acc: 0.31
Val Epoch over. val_loss: 13.617948519955775; val_accuracy: 0.4185907643312102 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 13.67; acc: 0.47
Batch: 20; loss: 12.71; acc: 0.39
Batch: 40; loss: 12.93; acc: 0.44
Batch: 60; loss: 14.15; acc: 0.38
Batch: 80; loss: 12.45; acc: 0.47
Batch: 100; loss: 14.84; acc: 0.36
Batch: 120; loss: 12.51; acc: 0.47
Batch: 140; loss: 13.81; acc: 0.42
Batch: 160; loss: 16.79; acc: 0.45
Batch: 180; loss: 13.77; acc: 0.42
Batch: 200; loss: 15.99; acc: 0.3
Batch: 220; loss: 8.88; acc: 0.53
Batch: 240; loss: 12.87; acc: 0.38
Batch: 260; loss: 12.03; acc: 0.41
Batch: 280; loss: 10.8; acc: 0.55
Batch: 300; loss: 10.61; acc: 0.5
Batch: 320; loss: 14.55; acc: 0.42
Batch: 340; loss: 15.07; acc: 0.41
Batch: 360; loss: 16.12; acc: 0.36
Batch: 380; loss: 13.45; acc: 0.5
Batch: 400; loss: 8.84; acc: 0.52
Batch: 420; loss: 14.06; acc: 0.45
Batch: 440; loss: 15.73; acc: 0.42
Batch: 460; loss: 11.52; acc: 0.47
Batch: 480; loss: 17.97; acc: 0.44
Batch: 500; loss: 12.72; acc: 0.48
Batch: 520; loss: 13.55; acc: 0.48
Batch: 540; loss: 12.98; acc: 0.41
Batch: 560; loss: 11.7; acc: 0.52
Batch: 580; loss: 11.84; acc: 0.52
Batch: 600; loss: 12.25; acc: 0.53
Batch: 620; loss: 11.5; acc: 0.52
Train Epoch over. train_loss: 13.41; train_accuracy: 0.43 

Batch: 0; loss: 12.48; acc: 0.47
Batch: 20; loss: 14.81; acc: 0.33
Batch: 40; loss: 12.34; acc: 0.5
Batch: 60; loss: 13.21; acc: 0.38
Batch: 80; loss: 12.37; acc: 0.41
Batch: 100; loss: 16.91; acc: 0.41
Batch: 120; loss: 11.04; acc: 0.52
Batch: 140; loss: 17.84; acc: 0.34
Val Epoch over. val_loss: 13.641083337698772; val_accuracy: 0.42167595541401276 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 16.24; acc: 0.42
Batch: 20; loss: 14.36; acc: 0.39
Batch: 40; loss: 13.0; acc: 0.39
Batch: 60; loss: 10.68; acc: 0.48
Batch: 80; loss: 13.98; acc: 0.44
Batch: 100; loss: 17.0; acc: 0.44
Batch: 120; loss: 9.79; acc: 0.62
Batch: 140; loss: 11.6; acc: 0.47
Batch: 160; loss: 11.57; acc: 0.52
Batch: 180; loss: 15.61; acc: 0.41
Batch: 200; loss: 14.31; acc: 0.47
Batch: 220; loss: 11.25; acc: 0.44
Batch: 240; loss: 12.98; acc: 0.42
Batch: 260; loss: 15.65; acc: 0.31
Batch: 280; loss: 14.14; acc: 0.53
Batch: 300; loss: 13.81; acc: 0.5
Batch: 320; loss: 15.12; acc: 0.38
Batch: 340; loss: 15.53; acc: 0.31
Batch: 360; loss: 9.83; acc: 0.48
Batch: 380; loss: 14.61; acc: 0.48
Batch: 400; loss: 13.41; acc: 0.38
Batch: 420; loss: 14.44; acc: 0.38
Batch: 440; loss: 10.89; acc: 0.53
Batch: 460; loss: 17.22; acc: 0.41
Batch: 480; loss: 9.97; acc: 0.39
Batch: 500; loss: 14.0; acc: 0.45
Batch: 520; loss: 11.19; acc: 0.42
Batch: 540; loss: 11.38; acc: 0.36
Batch: 560; loss: 9.71; acc: 0.47
Batch: 580; loss: 11.12; acc: 0.56
Batch: 600; loss: 15.14; acc: 0.41
Batch: 620; loss: 9.09; acc: 0.53
Train Epoch over. train_loss: 13.4; train_accuracy: 0.43 

Batch: 0; loss: 12.31; acc: 0.45
Batch: 20; loss: 14.96; acc: 0.28
Batch: 40; loss: 12.27; acc: 0.47
Batch: 60; loss: 13.1; acc: 0.39
Batch: 80; loss: 12.65; acc: 0.45
Batch: 100; loss: 16.52; acc: 0.42
Batch: 120; loss: 10.61; acc: 0.53
Batch: 140; loss: 18.06; acc: 0.33
Val Epoch over. val_loss: 13.613493275490535; val_accuracy: 0.42366640127388533 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 12.52; acc: 0.41
Batch: 20; loss: 11.93; acc: 0.52
Batch: 40; loss: 8.4; acc: 0.64
Batch: 60; loss: 17.12; acc: 0.39
Batch: 80; loss: 11.87; acc: 0.45
Batch: 100; loss: 13.28; acc: 0.44
Batch: 120; loss: 13.38; acc: 0.42
Batch: 140; loss: 13.98; acc: 0.47
Batch: 160; loss: 13.41; acc: 0.42
Batch: 180; loss: 15.32; acc: 0.28
Batch: 200; loss: 11.2; acc: 0.53
Batch: 220; loss: 14.66; acc: 0.41
Batch: 240; loss: 12.8; acc: 0.38
Batch: 260; loss: 15.39; acc: 0.41
Batch: 280; loss: 11.99; acc: 0.48
Batch: 300; loss: 11.18; acc: 0.44
Batch: 320; loss: 12.77; acc: 0.39
Batch: 340; loss: 14.81; acc: 0.41
Batch: 360; loss: 16.89; acc: 0.33
Batch: 380; loss: 15.05; acc: 0.42
Batch: 400; loss: 9.97; acc: 0.5
Batch: 420; loss: 12.02; acc: 0.47
Batch: 440; loss: 11.59; acc: 0.45
Batch: 460; loss: 15.28; acc: 0.34
Batch: 480; loss: 9.35; acc: 0.5
Batch: 500; loss: 12.11; acc: 0.52
Batch: 520; loss: 15.85; acc: 0.38
Batch: 540; loss: 12.41; acc: 0.41
Batch: 560; loss: 20.74; acc: 0.34
Batch: 580; loss: 9.42; acc: 0.44
Batch: 600; loss: 12.11; acc: 0.45
Batch: 620; loss: 14.55; acc: 0.39
Train Epoch over. train_loss: 13.39; train_accuracy: 0.43 

Batch: 0; loss: 12.22; acc: 0.45
Batch: 20; loss: 15.01; acc: 0.36
Batch: 40; loss: 12.18; acc: 0.45
Batch: 60; loss: 12.61; acc: 0.38
Batch: 80; loss: 12.74; acc: 0.42
Batch: 100; loss: 16.57; acc: 0.42
Batch: 120; loss: 10.69; acc: 0.52
Batch: 140; loss: 17.54; acc: 0.28
Val Epoch over. val_loss: 13.682288844114655; val_accuracy: 0.42177547770700635 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 17.75; acc: 0.33
Batch: 20; loss: 18.35; acc: 0.36
Batch: 40; loss: 16.09; acc: 0.44
Batch: 60; loss: 11.65; acc: 0.47
Batch: 80; loss: 15.58; acc: 0.3
Batch: 100; loss: 11.67; acc: 0.45
Batch: 120; loss: 15.77; acc: 0.44
Batch: 140; loss: 16.62; acc: 0.36
Batch: 160; loss: 12.64; acc: 0.39
Batch: 180; loss: 11.7; acc: 0.36
Batch: 200; loss: 11.22; acc: 0.47
Batch: 220; loss: 16.79; acc: 0.28
Batch: 240; loss: 13.17; acc: 0.31
Batch: 260; loss: 18.24; acc: 0.36
Batch: 280; loss: 15.3; acc: 0.33
Batch: 300; loss: 14.25; acc: 0.34
Batch: 320; loss: 13.88; acc: 0.39
Batch: 340; loss: 12.26; acc: 0.55
Batch: 360; loss: 13.88; acc: 0.5
Batch: 380; loss: 12.98; acc: 0.36
Batch: 400; loss: 14.1; acc: 0.44
Batch: 420; loss: 13.17; acc: 0.34
Batch: 440; loss: 12.5; acc: 0.53
Batch: 460; loss: 9.41; acc: 0.52
Batch: 480; loss: 14.88; acc: 0.41
Batch: 500; loss: 11.73; acc: 0.47
Batch: 520; loss: 13.9; acc: 0.39
Batch: 540; loss: 16.22; acc: 0.3
Batch: 560; loss: 8.51; acc: 0.48
Batch: 580; loss: 9.62; acc: 0.47
Batch: 600; loss: 12.17; acc: 0.48
Batch: 620; loss: 17.06; acc: 0.3
Train Epoch over. train_loss: 13.39; train_accuracy: 0.43 

Batch: 0; loss: 12.36; acc: 0.45
Batch: 20; loss: 15.37; acc: 0.3
Batch: 40; loss: 12.38; acc: 0.45
Batch: 60; loss: 12.76; acc: 0.39
Batch: 80; loss: 12.66; acc: 0.41
Batch: 100; loss: 16.84; acc: 0.44
Batch: 120; loss: 11.05; acc: 0.52
Batch: 140; loss: 18.07; acc: 0.3
Val Epoch over. val_loss: 13.654405678913092; val_accuracy: 0.4192874203821656 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 13.28; acc: 0.41
Batch: 20; loss: 17.74; acc: 0.28
Batch: 40; loss: 12.39; acc: 0.33
Batch: 60; loss: 9.47; acc: 0.44
Batch: 80; loss: 11.08; acc: 0.45
Batch: 100; loss: 16.63; acc: 0.31
Batch: 120; loss: 15.13; acc: 0.36
Batch: 140; loss: 14.09; acc: 0.5
Batch: 160; loss: 9.49; acc: 0.5
Batch: 180; loss: 13.44; acc: 0.44
Batch: 200; loss: 12.53; acc: 0.44
Batch: 220; loss: 12.56; acc: 0.31
Batch: 240; loss: 11.22; acc: 0.47
Batch: 260; loss: 10.28; acc: 0.42
Batch: 280; loss: 10.77; acc: 0.44
Batch: 300; loss: 15.4; acc: 0.34
Batch: 320; loss: 13.53; acc: 0.42
Batch: 340; loss: 11.19; acc: 0.44
Batch: 360; loss: 11.05; acc: 0.38
Batch: 380; loss: 11.63; acc: 0.44
Batch: 400; loss: 11.48; acc: 0.53
Batch: 420; loss: 16.38; acc: 0.38
Batch: 440; loss: 12.86; acc: 0.44
Batch: 460; loss: 17.8; acc: 0.38
Batch: 480; loss: 12.79; acc: 0.45
Batch: 500; loss: 14.54; acc: 0.36
Batch: 520; loss: 13.12; acc: 0.44
Batch: 540; loss: 13.59; acc: 0.45
Batch: 560; loss: 12.15; acc: 0.5
Batch: 580; loss: 14.11; acc: 0.44
Batch: 600; loss: 14.65; acc: 0.36
Batch: 620; loss: 17.68; acc: 0.38
Train Epoch over. train_loss: 13.39; train_accuracy: 0.43 

Batch: 0; loss: 12.13; acc: 0.45
Batch: 20; loss: 14.93; acc: 0.28
Batch: 40; loss: 11.8; acc: 0.44
Batch: 60; loss: 12.78; acc: 0.38
Batch: 80; loss: 12.49; acc: 0.42
Batch: 100; loss: 16.53; acc: 0.44
Batch: 120; loss: 10.5; acc: 0.55
Batch: 140; loss: 17.72; acc: 0.31
Val Epoch over. val_loss: 13.626074894218688; val_accuracy: 0.41948646496815284 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 12.38; acc: 0.47
Batch: 20; loss: 13.36; acc: 0.41
Batch: 40; loss: 12.45; acc: 0.44
Batch: 60; loss: 12.58; acc: 0.47
Batch: 80; loss: 13.67; acc: 0.44
Batch: 100; loss: 13.27; acc: 0.45
Batch: 120; loss: 9.88; acc: 0.47
Batch: 140; loss: 12.6; acc: 0.44
Batch: 160; loss: 11.2; acc: 0.41
Batch: 180; loss: 15.18; acc: 0.38
Batch: 200; loss: 13.75; acc: 0.53
Batch: 220; loss: 12.76; acc: 0.44
Batch: 240; loss: 15.23; acc: 0.44
Batch: 260; loss: 9.09; acc: 0.48
Batch: 280; loss: 11.73; acc: 0.38
Batch: 300; loss: 11.65; acc: 0.52
Batch: 320; loss: 12.8; acc: 0.39
Batch: 340; loss: 14.47; acc: 0.38
Batch: 360; loss: 7.71; acc: 0.58
Batch: 380; loss: 8.47; acc: 0.53
Batch: 400; loss: 13.72; acc: 0.42
Batch: 420; loss: 11.37; acc: 0.42
Batch: 440; loss: 11.16; acc: 0.48
Batch: 460; loss: 12.82; acc: 0.48
Batch: 480; loss: 11.46; acc: 0.45
Batch: 500; loss: 14.77; acc: 0.38
Batch: 520; loss: 12.52; acc: 0.44
Batch: 540; loss: 14.17; acc: 0.41
Batch: 560; loss: 15.43; acc: 0.39
Batch: 580; loss: 13.88; acc: 0.38
Batch: 600; loss: 14.87; acc: 0.28
Batch: 620; loss: 14.4; acc: 0.42
Train Epoch over. train_loss: 13.39; train_accuracy: 0.43 

Batch: 0; loss: 12.94; acc: 0.45
Batch: 20; loss: 14.82; acc: 0.33
Batch: 40; loss: 12.28; acc: 0.48
Batch: 60; loss: 12.69; acc: 0.42
Batch: 80; loss: 12.53; acc: 0.41
Batch: 100; loss: 17.05; acc: 0.39
Batch: 120; loss: 10.87; acc: 0.53
Batch: 140; loss: 17.81; acc: 0.31
Val Epoch over. val_loss: 13.664622194448095; val_accuracy: 0.42306926751592355 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 16.16; acc: 0.3
Batch: 20; loss: 12.86; acc: 0.41
Batch: 40; loss: 14.89; acc: 0.44
Batch: 60; loss: 12.41; acc: 0.36
Batch: 80; loss: 16.92; acc: 0.31
Batch: 100; loss: 15.79; acc: 0.36
Batch: 120; loss: 11.65; acc: 0.5
Batch: 140; loss: 17.59; acc: 0.34
Batch: 160; loss: 12.74; acc: 0.34
Batch: 180; loss: 11.99; acc: 0.5
Batch: 200; loss: 14.47; acc: 0.36
Batch: 220; loss: 13.31; acc: 0.42
Batch: 240; loss: 14.38; acc: 0.47
Batch: 260; loss: 12.09; acc: 0.42
Batch: 280; loss: 10.7; acc: 0.48
Batch: 300; loss: 11.4; acc: 0.42
Batch: 320; loss: 12.3; acc: 0.47
Batch: 340; loss: 9.47; acc: 0.52
Batch: 360; loss: 14.99; acc: 0.41
Batch: 380; loss: 13.04; acc: 0.41
Batch: 400; loss: 16.73; acc: 0.39
Batch: 420; loss: 12.9; acc: 0.47
Batch: 440; loss: 11.11; acc: 0.53
Batch: 460; loss: 8.96; acc: 0.55
Batch: 480; loss: 11.2; acc: 0.52
Batch: 500; loss: 13.47; acc: 0.38
Batch: 520; loss: 9.63; acc: 0.53
Batch: 540; loss: 11.49; acc: 0.36
Batch: 560; loss: 12.76; acc: 0.48
Batch: 580; loss: 15.1; acc: 0.44
Batch: 600; loss: 10.6; acc: 0.56
Batch: 620; loss: 12.97; acc: 0.42
Train Epoch over. train_loss: 13.39; train_accuracy: 0.43 

Batch: 0; loss: 12.48; acc: 0.44
Batch: 20; loss: 14.74; acc: 0.31
Batch: 40; loss: 12.35; acc: 0.48
Batch: 60; loss: 13.04; acc: 0.38
Batch: 80; loss: 12.49; acc: 0.41
Batch: 100; loss: 16.93; acc: 0.39
Batch: 120; loss: 10.91; acc: 0.52
Batch: 140; loss: 18.13; acc: 0.31
Val Epoch over. val_loss: 13.646392980198952; val_accuracy: 0.42436305732484075 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 10.85; acc: 0.48
Batch: 20; loss: 11.6; acc: 0.47
Batch: 40; loss: 10.14; acc: 0.47
Batch: 60; loss: 11.87; acc: 0.47
Batch: 80; loss: 17.93; acc: 0.27
Batch: 100; loss: 13.07; acc: 0.47
Batch: 120; loss: 13.67; acc: 0.42
Batch: 140; loss: 13.18; acc: 0.47
Batch: 160; loss: 11.27; acc: 0.55
Batch: 180; loss: 14.85; acc: 0.38
Batch: 200; loss: 13.15; acc: 0.34
Batch: 220; loss: 14.72; acc: 0.42
Batch: 240; loss: 16.48; acc: 0.41
Batch: 260; loss: 13.41; acc: 0.44
Batch: 280; loss: 11.99; acc: 0.44
Batch: 300; loss: 13.28; acc: 0.36
Batch: 320; loss: 12.74; acc: 0.41
Batch: 340; loss: 17.58; acc: 0.42
Batch: 360; loss: 12.57; acc: 0.39
Batch: 380; loss: 10.29; acc: 0.56
Batch: 400; loss: 15.19; acc: 0.41
Batch: 420; loss: 15.25; acc: 0.36
Batch: 440; loss: 13.25; acc: 0.45
Batch: 460; loss: 10.27; acc: 0.53
Batch: 480; loss: 15.41; acc: 0.41
Batch: 500; loss: 14.64; acc: 0.44
Batch: 520; loss: 8.13; acc: 0.52
Batch: 540; loss: 14.26; acc: 0.38
Batch: 560; loss: 10.43; acc: 0.5
Batch: 580; loss: 13.9; acc: 0.44
Batch: 600; loss: 13.94; acc: 0.39
Batch: 620; loss: 13.61; acc: 0.36
Train Epoch over. train_loss: 13.4; train_accuracy: 0.43 

Batch: 0; loss: 12.0; acc: 0.47
Batch: 20; loss: 14.96; acc: 0.31
Batch: 40; loss: 12.22; acc: 0.5
Batch: 60; loss: 12.93; acc: 0.38
Batch: 80; loss: 12.42; acc: 0.41
Batch: 100; loss: 16.52; acc: 0.41
Batch: 120; loss: 10.91; acc: 0.53
Batch: 140; loss: 17.99; acc: 0.3
Val Epoch over. val_loss: 13.59576233177428; val_accuracy: 0.42296974522292996 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 12.86; acc: 0.34
Batch: 20; loss: 13.73; acc: 0.44
Batch: 40; loss: 9.14; acc: 0.45
Batch: 60; loss: 11.71; acc: 0.53
Batch: 80; loss: 13.41; acc: 0.36
Batch: 100; loss: 12.2; acc: 0.47
Batch: 120; loss: 13.17; acc: 0.53
Batch: 140; loss: 11.58; acc: 0.53
Batch: 160; loss: 15.51; acc: 0.41
Batch: 180; loss: 12.09; acc: 0.41
Batch: 200; loss: 10.82; acc: 0.44
Batch: 220; loss: 12.88; acc: 0.44
Batch: 240; loss: 13.08; acc: 0.45
Batch: 260; loss: 13.44; acc: 0.38
Batch: 280; loss: 15.74; acc: 0.44
Batch: 300; loss: 10.83; acc: 0.39
Batch: 320; loss: 14.61; acc: 0.28
Batch: 340; loss: 18.49; acc: 0.27
Batch: 360; loss: 12.36; acc: 0.47
Batch: 380; loss: 15.3; acc: 0.42
Batch: 400; loss: 12.87; acc: 0.53
Batch: 420; loss: 15.47; acc: 0.34
Batch: 440; loss: 9.17; acc: 0.58
Batch: 460; loss: 15.53; acc: 0.39
Batch: 480; loss: 11.47; acc: 0.5
Batch: 500; loss: 17.39; acc: 0.41
Batch: 520; loss: 14.09; acc: 0.39
Batch: 540; loss: 15.77; acc: 0.36
Batch: 560; loss: 15.55; acc: 0.42
Batch: 580; loss: 15.01; acc: 0.39
Batch: 600; loss: 13.22; acc: 0.39
Batch: 620; loss: 15.59; acc: 0.36
Train Epoch over. train_loss: 13.39; train_accuracy: 0.43 

Batch: 0; loss: 12.49; acc: 0.45
Batch: 20; loss: 15.13; acc: 0.3
Batch: 40; loss: 12.16; acc: 0.5
Batch: 60; loss: 13.08; acc: 0.39
Batch: 80; loss: 12.56; acc: 0.44
Batch: 100; loss: 16.7; acc: 0.38
Batch: 120; loss: 11.07; acc: 0.53
Batch: 140; loss: 18.17; acc: 0.31
Val Epoch over. val_loss: 13.646000014748543; val_accuracy: 0.417296974522293 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 13.57; acc: 0.41
Batch: 20; loss: 11.23; acc: 0.5
Batch: 40; loss: 14.17; acc: 0.45
Batch: 60; loss: 15.9; acc: 0.34
Batch: 80; loss: 11.58; acc: 0.44
Batch: 100; loss: 16.88; acc: 0.41
Batch: 120; loss: 15.73; acc: 0.34
Batch: 140; loss: 13.37; acc: 0.42
Batch: 160; loss: 12.41; acc: 0.44
Batch: 180; loss: 12.41; acc: 0.36
Batch: 200; loss: 12.96; acc: 0.39
Batch: 220; loss: 14.35; acc: 0.38
Batch: 240; loss: 14.34; acc: 0.44
Batch: 260; loss: 13.81; acc: 0.36
Batch: 280; loss: 14.79; acc: 0.42
Batch: 300; loss: 13.09; acc: 0.39
Batch: 320; loss: 11.01; acc: 0.48
Batch: 340; loss: 14.2; acc: 0.45
Batch: 360; loss: 14.35; acc: 0.52
Batch: 380; loss: 15.47; acc: 0.5
Batch: 400; loss: 16.43; acc: 0.33
Batch: 420; loss: 15.57; acc: 0.36
Batch: 440; loss: 6.36; acc: 0.53
Batch: 460; loss: 12.7; acc: 0.41
Batch: 480; loss: 16.16; acc: 0.39
Batch: 500; loss: 16.04; acc: 0.38
Batch: 520; loss: 13.42; acc: 0.41
Batch: 540; loss: 12.88; acc: 0.42
Batch: 560; loss: 16.72; acc: 0.36
Batch: 580; loss: 14.15; acc: 0.47
Batch: 600; loss: 14.38; acc: 0.44
Batch: 620; loss: 13.67; acc: 0.44
Train Epoch over. train_loss: 13.31; train_accuracy: 0.43 

Batch: 0; loss: 12.4; acc: 0.47
Batch: 20; loss: 14.79; acc: 0.31
Batch: 40; loss: 12.0; acc: 0.5
Batch: 60; loss: 12.85; acc: 0.38
Batch: 80; loss: 12.55; acc: 0.44
Batch: 100; loss: 16.63; acc: 0.38
Batch: 120; loss: 10.74; acc: 0.55
Batch: 140; loss: 17.95; acc: 0.31
Val Epoch over. val_loss: 13.58421924007926; val_accuracy: 0.4211783439490446 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 15.12; acc: 0.42
Batch: 20; loss: 12.85; acc: 0.5
Batch: 40; loss: 14.7; acc: 0.42
Batch: 60; loss: 12.76; acc: 0.38
Batch: 80; loss: 15.45; acc: 0.38
Batch: 100; loss: 14.25; acc: 0.42
Batch: 120; loss: 10.52; acc: 0.5
Batch: 140; loss: 14.75; acc: 0.38
Batch: 160; loss: 14.41; acc: 0.41
Batch: 180; loss: 14.11; acc: 0.45
Batch: 200; loss: 12.61; acc: 0.45
Batch: 220; loss: 14.62; acc: 0.34
Batch: 240; loss: 15.05; acc: 0.33
Batch: 260; loss: 15.71; acc: 0.33
Batch: 280; loss: 14.78; acc: 0.39
Batch: 300; loss: 14.51; acc: 0.33
Batch: 320; loss: 15.23; acc: 0.39
Batch: 340; loss: 12.65; acc: 0.45
Batch: 360; loss: 12.63; acc: 0.52
Batch: 380; loss: 10.29; acc: 0.53
Batch: 400; loss: 16.77; acc: 0.41
Batch: 420; loss: 12.03; acc: 0.39
Batch: 440; loss: 11.79; acc: 0.5
Batch: 460; loss: 12.22; acc: 0.36
Batch: 480; loss: 11.97; acc: 0.47
Batch: 500; loss: 12.78; acc: 0.53
Batch: 520; loss: 14.78; acc: 0.42
Batch: 540; loss: 13.64; acc: 0.44
Batch: 560; loss: 14.37; acc: 0.38
Batch: 580; loss: 13.37; acc: 0.42
Batch: 600; loss: 15.73; acc: 0.41
Batch: 620; loss: 12.12; acc: 0.55
Train Epoch over. train_loss: 13.3; train_accuracy: 0.43 

Batch: 0; loss: 12.34; acc: 0.45
Batch: 20; loss: 14.78; acc: 0.31
Batch: 40; loss: 11.97; acc: 0.47
Batch: 60; loss: 12.84; acc: 0.38
Batch: 80; loss: 12.62; acc: 0.42
Batch: 100; loss: 16.51; acc: 0.39
Batch: 120; loss: 10.75; acc: 0.55
Batch: 140; loss: 17.92; acc: 0.31
Val Epoch over. val_loss: 13.57723996897412; val_accuracy: 0.42296974522292996 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 12.89; acc: 0.53
Batch: 20; loss: 10.23; acc: 0.53
Batch: 40; loss: 10.18; acc: 0.42
Batch: 60; loss: 12.97; acc: 0.38
Batch: 80; loss: 16.25; acc: 0.28
Batch: 100; loss: 15.78; acc: 0.45
Batch: 120; loss: 15.95; acc: 0.45
Batch: 140; loss: 14.03; acc: 0.44
Batch: 160; loss: 12.9; acc: 0.48
Batch: 180; loss: 14.1; acc: 0.39
Batch: 200; loss: 10.94; acc: 0.52
Batch: 220; loss: 11.57; acc: 0.44
Batch: 240; loss: 11.99; acc: 0.53
Batch: 260; loss: 10.35; acc: 0.48
Batch: 280; loss: 13.75; acc: 0.42
Batch: 300; loss: 12.05; acc: 0.44
Batch: 320; loss: 10.75; acc: 0.47
Batch: 340; loss: 12.75; acc: 0.44
Batch: 360; loss: 13.33; acc: 0.47
Batch: 380; loss: 13.8; acc: 0.42
Batch: 400; loss: 14.85; acc: 0.44
Batch: 420; loss: 15.14; acc: 0.42
Batch: 440; loss: 13.63; acc: 0.36
Batch: 460; loss: 14.05; acc: 0.44
Batch: 480; loss: 11.78; acc: 0.41
Batch: 500; loss: 16.78; acc: 0.28
Batch: 520; loss: 11.98; acc: 0.53
Batch: 540; loss: 16.19; acc: 0.3
Batch: 560; loss: 14.89; acc: 0.42
Batch: 580; loss: 12.62; acc: 0.45
Batch: 600; loss: 12.94; acc: 0.42
Batch: 620; loss: 10.83; acc: 0.48
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.39; acc: 0.47
Batch: 20; loss: 14.83; acc: 0.31
Batch: 40; loss: 11.98; acc: 0.45
Batch: 60; loss: 12.78; acc: 0.38
Batch: 80; loss: 12.5; acc: 0.44
Batch: 100; loss: 16.54; acc: 0.39
Batch: 120; loss: 10.81; acc: 0.53
Batch: 140; loss: 17.84; acc: 0.31
Val Epoch over. val_loss: 13.589315359759482; val_accuracy: 0.421875 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 11.68; acc: 0.48
Batch: 20; loss: 9.24; acc: 0.44
Batch: 40; loss: 15.32; acc: 0.45
Batch: 60; loss: 14.81; acc: 0.42
Batch: 80; loss: 15.6; acc: 0.33
Batch: 100; loss: 15.45; acc: 0.34
Batch: 120; loss: 15.59; acc: 0.36
Batch: 140; loss: 12.54; acc: 0.39
Batch: 160; loss: 13.92; acc: 0.38
Batch: 180; loss: 12.41; acc: 0.42
Batch: 200; loss: 14.47; acc: 0.42
Batch: 220; loss: 14.36; acc: 0.38
Batch: 240; loss: 11.24; acc: 0.38
Batch: 260; loss: 12.28; acc: 0.41
Batch: 280; loss: 16.12; acc: 0.38
Batch: 300; loss: 11.77; acc: 0.47
Batch: 320; loss: 12.68; acc: 0.42
Batch: 340; loss: 13.24; acc: 0.45
Batch: 360; loss: 13.92; acc: 0.41
Batch: 380; loss: 15.96; acc: 0.42
Batch: 400; loss: 11.41; acc: 0.41
Batch: 420; loss: 13.18; acc: 0.41
Batch: 440; loss: 12.96; acc: 0.27
Batch: 460; loss: 12.95; acc: 0.31
Batch: 480; loss: 10.23; acc: 0.48
Batch: 500; loss: 12.52; acc: 0.39
Batch: 520; loss: 13.74; acc: 0.34
Batch: 540; loss: 9.68; acc: 0.5
Batch: 560; loss: 18.84; acc: 0.38
Batch: 580; loss: 12.23; acc: 0.41
Batch: 600; loss: 14.61; acc: 0.34
Batch: 620; loss: 14.42; acc: 0.5
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.36; acc: 0.47
Batch: 20; loss: 14.78; acc: 0.31
Batch: 40; loss: 11.98; acc: 0.45
Batch: 60; loss: 12.76; acc: 0.38
Batch: 80; loss: 12.56; acc: 0.44
Batch: 100; loss: 16.45; acc: 0.39
Batch: 120; loss: 10.81; acc: 0.53
Batch: 140; loss: 17.82; acc: 0.31
Val Epoch over. val_loss: 13.580282475538315; val_accuracy: 0.42267117834394907 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 16.37; acc: 0.36
Batch: 20; loss: 15.77; acc: 0.36
Batch: 40; loss: 14.05; acc: 0.44
Batch: 60; loss: 14.1; acc: 0.45
Batch: 80; loss: 17.6; acc: 0.42
Batch: 100; loss: 10.85; acc: 0.52
Batch: 120; loss: 11.77; acc: 0.42
Batch: 140; loss: 15.77; acc: 0.47
Batch: 160; loss: 12.58; acc: 0.47
Batch: 180; loss: 12.39; acc: 0.44
Batch: 200; loss: 16.04; acc: 0.38
Batch: 220; loss: 15.64; acc: 0.36
Batch: 240; loss: 10.95; acc: 0.44
Batch: 260; loss: 9.97; acc: 0.42
Batch: 280; loss: 13.9; acc: 0.34
Batch: 300; loss: 11.41; acc: 0.52
Batch: 320; loss: 11.77; acc: 0.53
Batch: 340; loss: 16.49; acc: 0.36
Batch: 360; loss: 13.78; acc: 0.44
Batch: 380; loss: 11.32; acc: 0.47
Batch: 400; loss: 11.75; acc: 0.39
Batch: 420; loss: 12.47; acc: 0.44
Batch: 440; loss: 14.86; acc: 0.41
Batch: 460; loss: 14.61; acc: 0.36
Batch: 480; loss: 14.46; acc: 0.44
Batch: 500; loss: 13.4; acc: 0.39
Batch: 520; loss: 13.26; acc: 0.48
Batch: 540; loss: 9.86; acc: 0.53
Batch: 560; loss: 11.5; acc: 0.45
Batch: 580; loss: 11.36; acc: 0.44
Batch: 600; loss: 11.49; acc: 0.45
Batch: 620; loss: 13.56; acc: 0.34
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.31; acc: 0.47
Batch: 20; loss: 14.87; acc: 0.31
Batch: 40; loss: 12.02; acc: 0.47
Batch: 60; loss: 12.76; acc: 0.38
Batch: 80; loss: 12.65; acc: 0.41
Batch: 100; loss: 16.41; acc: 0.39
Batch: 120; loss: 10.83; acc: 0.53
Batch: 140; loss: 17.85; acc: 0.31
Val Epoch over. val_loss: 13.57953241676282; val_accuracy: 0.42197452229299365 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 15.12; acc: 0.45
Batch: 20; loss: 13.37; acc: 0.41
Batch: 40; loss: 12.27; acc: 0.39
Batch: 60; loss: 13.58; acc: 0.42
Batch: 80; loss: 12.34; acc: 0.5
Batch: 100; loss: 11.37; acc: 0.42
Batch: 120; loss: 11.4; acc: 0.5
Batch: 140; loss: 12.05; acc: 0.45
Batch: 160; loss: 15.24; acc: 0.34
Batch: 180; loss: 12.14; acc: 0.45
Batch: 200; loss: 14.86; acc: 0.41
Batch: 220; loss: 9.41; acc: 0.52
Batch: 240; loss: 10.95; acc: 0.47
Batch: 260; loss: 11.08; acc: 0.45
Batch: 280; loss: 14.29; acc: 0.53
Batch: 300; loss: 14.28; acc: 0.42
Batch: 320; loss: 14.13; acc: 0.34
Batch: 340; loss: 13.36; acc: 0.44
Batch: 360; loss: 14.68; acc: 0.38
Batch: 380; loss: 16.32; acc: 0.39
Batch: 400; loss: 9.12; acc: 0.52
Batch: 420; loss: 10.89; acc: 0.44
Batch: 440; loss: 10.04; acc: 0.5
Batch: 460; loss: 16.31; acc: 0.34
Batch: 480; loss: 13.13; acc: 0.36
Batch: 500; loss: 14.97; acc: 0.38
Batch: 520; loss: 13.49; acc: 0.36
Batch: 540; loss: 15.96; acc: 0.44
Batch: 560; loss: 16.42; acc: 0.25
Batch: 580; loss: 15.27; acc: 0.38
Batch: 600; loss: 12.2; acc: 0.3
Batch: 620; loss: 11.82; acc: 0.44
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.29; acc: 0.47
Batch: 20; loss: 14.93; acc: 0.3
Batch: 40; loss: 11.88; acc: 0.45
Batch: 60; loss: 12.89; acc: 0.38
Batch: 80; loss: 12.6; acc: 0.42
Batch: 100; loss: 16.39; acc: 0.39
Batch: 120; loss: 10.82; acc: 0.53
Batch: 140; loss: 17.76; acc: 0.31
Val Epoch over. val_loss: 13.578212242976875; val_accuracy: 0.42356687898089174 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 14.27; acc: 0.41
Batch: 20; loss: 8.71; acc: 0.45
Batch: 40; loss: 13.32; acc: 0.34
Batch: 60; loss: 13.59; acc: 0.42
Batch: 80; loss: 18.43; acc: 0.42
Batch: 100; loss: 15.91; acc: 0.38
Batch: 120; loss: 18.96; acc: 0.31
Batch: 140; loss: 15.13; acc: 0.44
Batch: 160; loss: 12.14; acc: 0.47
Batch: 180; loss: 11.91; acc: 0.42
Batch: 200; loss: 13.28; acc: 0.42
Batch: 220; loss: 15.37; acc: 0.44
Batch: 240; loss: 13.68; acc: 0.38
Batch: 260; loss: 11.82; acc: 0.47
Batch: 280; loss: 11.9; acc: 0.39
Batch: 300; loss: 14.08; acc: 0.47
Batch: 320; loss: 13.87; acc: 0.42
Batch: 340; loss: 11.77; acc: 0.5
Batch: 360; loss: 12.91; acc: 0.41
Batch: 380; loss: 14.39; acc: 0.47
Batch: 400; loss: 11.03; acc: 0.47
Batch: 420; loss: 13.51; acc: 0.47
Batch: 440; loss: 10.77; acc: 0.5
Batch: 460; loss: 13.38; acc: 0.38
Batch: 480; loss: 15.66; acc: 0.41
Batch: 500; loss: 12.83; acc: 0.45
Batch: 520; loss: 11.24; acc: 0.47
Batch: 540; loss: 15.51; acc: 0.33
Batch: 560; loss: 15.13; acc: 0.42
Batch: 580; loss: 12.3; acc: 0.5
Batch: 600; loss: 12.0; acc: 0.38
Batch: 620; loss: 12.86; acc: 0.34
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.34; acc: 0.45
Batch: 20; loss: 14.82; acc: 0.31
Batch: 40; loss: 12.06; acc: 0.47
Batch: 60; loss: 12.77; acc: 0.38
Batch: 80; loss: 12.6; acc: 0.41
Batch: 100; loss: 16.47; acc: 0.39
Batch: 120; loss: 10.9; acc: 0.53
Batch: 140; loss: 17.86; acc: 0.31
Val Epoch over. val_loss: 13.579420332696028; val_accuracy: 0.42366640127388533 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 10.27; acc: 0.39
Batch: 20; loss: 13.01; acc: 0.41
Batch: 40; loss: 10.21; acc: 0.52
Batch: 60; loss: 13.64; acc: 0.41
Batch: 80; loss: 8.18; acc: 0.56
Batch: 100; loss: 11.97; acc: 0.42
Batch: 120; loss: 16.78; acc: 0.41
Batch: 140; loss: 13.72; acc: 0.53
Batch: 160; loss: 8.76; acc: 0.48
Batch: 180; loss: 11.87; acc: 0.47
Batch: 200; loss: 11.26; acc: 0.47
Batch: 220; loss: 11.3; acc: 0.45
Batch: 240; loss: 13.21; acc: 0.45
Batch: 260; loss: 12.37; acc: 0.39
Batch: 280; loss: 14.82; acc: 0.36
Batch: 300; loss: 11.63; acc: 0.44
Batch: 320; loss: 11.53; acc: 0.41
Batch: 340; loss: 13.93; acc: 0.41
Batch: 360; loss: 16.01; acc: 0.39
Batch: 380; loss: 16.03; acc: 0.39
Batch: 400; loss: 12.43; acc: 0.5
Batch: 420; loss: 11.13; acc: 0.52
Batch: 440; loss: 10.28; acc: 0.47
Batch: 460; loss: 13.07; acc: 0.42
Batch: 480; loss: 14.89; acc: 0.36
Batch: 500; loss: 12.36; acc: 0.48
Batch: 520; loss: 11.96; acc: 0.44
Batch: 540; loss: 11.59; acc: 0.53
Batch: 560; loss: 18.03; acc: 0.41
Batch: 580; loss: 14.24; acc: 0.48
Batch: 600; loss: 10.93; acc: 0.45
Batch: 620; loss: 16.01; acc: 0.34
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.3; acc: 0.47
Batch: 20; loss: 14.88; acc: 0.3
Batch: 40; loss: 11.94; acc: 0.45
Batch: 60; loss: 12.81; acc: 0.38
Batch: 80; loss: 12.54; acc: 0.42
Batch: 100; loss: 16.45; acc: 0.39
Batch: 120; loss: 10.86; acc: 0.53
Batch: 140; loss: 17.8; acc: 0.31
Val Epoch over. val_loss: 13.576762241922367; val_accuracy: 0.42326831210191085 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 14.42; acc: 0.45
Batch: 20; loss: 10.73; acc: 0.48
Batch: 40; loss: 15.99; acc: 0.36
Batch: 60; loss: 16.76; acc: 0.45
Batch: 80; loss: 10.44; acc: 0.39
Batch: 100; loss: 14.57; acc: 0.36
Batch: 120; loss: 13.11; acc: 0.41
Batch: 140; loss: 11.51; acc: 0.42
Batch: 160; loss: 10.14; acc: 0.56
Batch: 180; loss: 12.9; acc: 0.36
Batch: 200; loss: 15.2; acc: 0.38
Batch: 220; loss: 13.11; acc: 0.45
Batch: 240; loss: 16.72; acc: 0.39
Batch: 260; loss: 13.04; acc: 0.39
Batch: 280; loss: 13.51; acc: 0.41
Batch: 300; loss: 9.69; acc: 0.58
Batch: 320; loss: 15.39; acc: 0.34
Batch: 340; loss: 12.59; acc: 0.52
Batch: 360; loss: 13.32; acc: 0.5
Batch: 380; loss: 10.47; acc: 0.5
Batch: 400; loss: 11.78; acc: 0.39
Batch: 420; loss: 10.81; acc: 0.36
Batch: 440; loss: 13.65; acc: 0.42
Batch: 460; loss: 13.66; acc: 0.45
Batch: 480; loss: 9.7; acc: 0.48
Batch: 500; loss: 14.61; acc: 0.41
Batch: 520; loss: 11.64; acc: 0.48
Batch: 540; loss: 9.96; acc: 0.5
Batch: 560; loss: 14.61; acc: 0.38
Batch: 580; loss: 13.57; acc: 0.53
Batch: 600; loss: 15.32; acc: 0.41
Batch: 620; loss: 13.63; acc: 0.39
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.27; acc: 0.48
Batch: 20; loss: 14.88; acc: 0.3
Batch: 40; loss: 11.86; acc: 0.47
Batch: 60; loss: 12.84; acc: 0.38
Batch: 80; loss: 12.56; acc: 0.41
Batch: 100; loss: 16.41; acc: 0.39
Batch: 120; loss: 10.8; acc: 0.53
Batch: 140; loss: 17.74; acc: 0.31
Val Epoch over. val_loss: 13.575028425568988; val_accuracy: 0.42486066878980894 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 11.04; acc: 0.53
Batch: 20; loss: 11.85; acc: 0.39
Batch: 40; loss: 10.61; acc: 0.45
Batch: 60; loss: 11.39; acc: 0.53
Batch: 80; loss: 14.07; acc: 0.39
Batch: 100; loss: 12.41; acc: 0.39
Batch: 120; loss: 16.09; acc: 0.33
Batch: 140; loss: 11.71; acc: 0.45
Batch: 160; loss: 13.5; acc: 0.41
Batch: 180; loss: 13.47; acc: 0.52
Batch: 200; loss: 10.9; acc: 0.47
Batch: 220; loss: 12.65; acc: 0.47
Batch: 240; loss: 13.27; acc: 0.41
Batch: 260; loss: 12.91; acc: 0.39
Batch: 280; loss: 11.8; acc: 0.47
Batch: 300; loss: 11.51; acc: 0.39
Batch: 320; loss: 15.41; acc: 0.36
Batch: 340; loss: 11.34; acc: 0.47
Batch: 360; loss: 17.22; acc: 0.39
Batch: 380; loss: 10.54; acc: 0.42
Batch: 400; loss: 11.29; acc: 0.48
Batch: 420; loss: 13.67; acc: 0.45
Batch: 440; loss: 14.26; acc: 0.48
Batch: 460; loss: 16.39; acc: 0.41
Batch: 480; loss: 15.12; acc: 0.41
Batch: 500; loss: 13.83; acc: 0.42
Batch: 520; loss: 12.78; acc: 0.38
Batch: 540; loss: 12.88; acc: 0.45
Batch: 560; loss: 12.2; acc: 0.5
Batch: 580; loss: 16.61; acc: 0.42
Batch: 600; loss: 9.49; acc: 0.53
Batch: 620; loss: 13.12; acc: 0.41
Train Epoch over. train_loss: 13.29; train_accuracy: 0.43 

Batch: 0; loss: 12.26; acc: 0.47
Batch: 20; loss: 14.78; acc: 0.3
Batch: 40; loss: 11.99; acc: 0.47
Batch: 60; loss: 12.76; acc: 0.38
Batch: 80; loss: 12.65; acc: 0.41
Batch: 100; loss: 16.43; acc: 0.39
Batch: 120; loss: 10.84; acc: 0.53
Batch: 140; loss: 17.81; acc: 0.31
Val Epoch over. val_loss: 13.571235838969042; val_accuracy: 0.42406449044585987 

plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_100_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 89357
elements in E: 39842000
fraction nonzero: 0.0022427839967873097
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 36.37; acc: 0.12
Batch: 20; loss: 11.97; acc: 0.38
Batch: 40; loss: 12.1; acc: 0.48
Batch: 60; loss: 8.85; acc: 0.44
Batch: 80; loss: 9.73; acc: 0.45
Batch: 100; loss: 8.77; acc: 0.5
Batch: 120; loss: 15.75; acc: 0.44
Batch: 140; loss: 8.14; acc: 0.55
Batch: 160; loss: 8.76; acc: 0.61
Batch: 180; loss: 9.41; acc: 0.56
Batch: 200; loss: 6.09; acc: 0.69
Batch: 220; loss: 11.82; acc: 0.47
Batch: 240; loss: 6.06; acc: 0.56
Batch: 260; loss: 9.14; acc: 0.47
Batch: 280; loss: 11.26; acc: 0.5
Batch: 300; loss: 9.22; acc: 0.44
Batch: 320; loss: 10.45; acc: 0.53
Batch: 340; loss: 8.23; acc: 0.59
Batch: 360; loss: 8.92; acc: 0.64
Batch: 380; loss: 7.82; acc: 0.53
Batch: 400; loss: 9.38; acc: 0.48
Batch: 420; loss: 10.1; acc: 0.56
Batch: 440; loss: 11.14; acc: 0.55
Batch: 460; loss: 10.16; acc: 0.56
Batch: 480; loss: 8.57; acc: 0.55
Batch: 500; loss: 8.05; acc: 0.62
Batch: 520; loss: 9.96; acc: 0.55
Batch: 540; loss: 7.36; acc: 0.64
Batch: 560; loss: 9.62; acc: 0.58
Batch: 580; loss: 8.29; acc: 0.59
Batch: 600; loss: 6.17; acc: 0.7
Batch: 620; loss: 8.88; acc: 0.53
Train Epoch over. train_loss: 9.4; train_accuracy: 0.53 

Batch: 0; loss: 7.64; acc: 0.59
Batch: 20; loss: 14.31; acc: 0.42
Batch: 40; loss: 5.98; acc: 0.67
Batch: 60; loss: 9.34; acc: 0.55
Batch: 80; loss: 8.87; acc: 0.53
Batch: 100; loss: 9.29; acc: 0.5
Batch: 120; loss: 8.19; acc: 0.64
Batch: 140; loss: 13.68; acc: 0.45
Val Epoch over. val_loss: 8.39206220997367; val_accuracy: 0.566281847133758 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 5.65; acc: 0.61
Batch: 20; loss: 7.76; acc: 0.67
Batch: 40; loss: 6.69; acc: 0.62
Batch: 60; loss: 7.61; acc: 0.64
Batch: 80; loss: 10.66; acc: 0.52
Batch: 100; loss: 8.82; acc: 0.5
Batch: 120; loss: 8.25; acc: 0.62
Batch: 140; loss: 11.76; acc: 0.5
Batch: 160; loss: 9.65; acc: 0.62
Batch: 180; loss: 9.95; acc: 0.58
Batch: 200; loss: 10.21; acc: 0.45
Batch: 220; loss: 9.65; acc: 0.48
Batch: 240; loss: 10.29; acc: 0.52
Batch: 260; loss: 9.11; acc: 0.56
Batch: 280; loss: 4.13; acc: 0.73
Batch: 300; loss: 7.86; acc: 0.61
Batch: 320; loss: 9.93; acc: 0.56
Batch: 340; loss: 7.39; acc: 0.64
Batch: 360; loss: 10.37; acc: 0.53
Batch: 380; loss: 9.87; acc: 0.55
Batch: 400; loss: 10.26; acc: 0.59
Batch: 420; loss: 11.46; acc: 0.59
Batch: 440; loss: 6.56; acc: 0.59
Batch: 460; loss: 13.37; acc: 0.5
Batch: 480; loss: 8.22; acc: 0.55
Batch: 500; loss: 6.81; acc: 0.66
Batch: 520; loss: 8.22; acc: 0.64
Batch: 540; loss: 8.17; acc: 0.55
Batch: 560; loss: 6.46; acc: 0.69
Batch: 580; loss: 9.78; acc: 0.52
Batch: 600; loss: 8.89; acc: 0.58
Batch: 620; loss: 10.21; acc: 0.47
Train Epoch over. train_loss: 8.52; train_accuracy: 0.58 

Batch: 0; loss: 6.13; acc: 0.64
Batch: 20; loss: 14.09; acc: 0.39
Batch: 40; loss: 7.08; acc: 0.64
Batch: 60; loss: 8.18; acc: 0.53
Batch: 80; loss: 7.83; acc: 0.55
Batch: 100; loss: 12.32; acc: 0.55
Batch: 120; loss: 7.8; acc: 0.69
Batch: 140; loss: 12.36; acc: 0.53
Val Epoch over. val_loss: 8.656548302644378; val_accuracy: 0.5952428343949044 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 8.78; acc: 0.66
Batch: 20; loss: 9.81; acc: 0.53
Batch: 40; loss: 8.98; acc: 0.62
Batch: 60; loss: 7.7; acc: 0.61
Batch: 80; loss: 7.57; acc: 0.59
Batch: 100; loss: 7.53; acc: 0.62
Batch: 120; loss: 9.66; acc: 0.62
Batch: 140; loss: 6.86; acc: 0.62
Batch: 160; loss: 6.58; acc: 0.66
Batch: 180; loss: 7.17; acc: 0.62
Batch: 200; loss: 8.93; acc: 0.59
Batch: 220; loss: 8.87; acc: 0.52
Batch: 240; loss: 10.15; acc: 0.47
Batch: 260; loss: 8.57; acc: 0.55
Batch: 280; loss: 11.11; acc: 0.62
Batch: 300; loss: 10.14; acc: 0.56
Batch: 320; loss: 9.22; acc: 0.52
Batch: 340; loss: 8.6; acc: 0.56
Batch: 360; loss: 8.71; acc: 0.58
Batch: 380; loss: 8.41; acc: 0.59
Batch: 400; loss: 9.2; acc: 0.56
Batch: 420; loss: 6.86; acc: 0.67
Batch: 440; loss: 6.09; acc: 0.62
Batch: 460; loss: 9.28; acc: 0.64
Batch: 480; loss: 11.72; acc: 0.53
Batch: 500; loss: 8.1; acc: 0.56
Batch: 520; loss: 8.35; acc: 0.53
Batch: 540; loss: 6.34; acc: 0.61
Batch: 560; loss: 11.63; acc: 0.59
Batch: 580; loss: 10.71; acc: 0.52
Batch: 600; loss: 10.53; acc: 0.55
Batch: 620; loss: 9.27; acc: 0.52
Train Epoch over. train_loss: 8.47; train_accuracy: 0.6 

Batch: 0; loss: 7.01; acc: 0.62
Batch: 20; loss: 13.07; acc: 0.42
Batch: 40; loss: 6.15; acc: 0.61
Batch: 60; loss: 7.67; acc: 0.58
Batch: 80; loss: 8.3; acc: 0.53
Batch: 100; loss: 11.14; acc: 0.56
Batch: 120; loss: 8.12; acc: 0.7
Batch: 140; loss: 12.61; acc: 0.48
Val Epoch over. val_loss: 8.57409892416304; val_accuracy: 0.5774283439490446 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 7.64; acc: 0.55
Batch: 20; loss: 13.74; acc: 0.41
Batch: 40; loss: 6.39; acc: 0.62
Batch: 60; loss: 6.08; acc: 0.67
Batch: 80; loss: 10.47; acc: 0.55
Batch: 100; loss: 8.35; acc: 0.56
Batch: 120; loss: 6.47; acc: 0.62
Batch: 140; loss: 7.29; acc: 0.56
Batch: 160; loss: 9.71; acc: 0.64
Batch: 180; loss: 7.79; acc: 0.59
Batch: 200; loss: 5.98; acc: 0.64
Batch: 220; loss: 6.86; acc: 0.58
Batch: 240; loss: 6.93; acc: 0.66
Batch: 260; loss: 9.56; acc: 0.58
Batch: 280; loss: 7.14; acc: 0.66
Batch: 300; loss: 8.67; acc: 0.64
Batch: 320; loss: 7.84; acc: 0.56
Batch: 340; loss: 10.18; acc: 0.64
Batch: 360; loss: 6.21; acc: 0.69
Batch: 380; loss: 10.29; acc: 0.64
Batch: 400; loss: 10.45; acc: 0.59
Batch: 420; loss: 8.43; acc: 0.58
Batch: 440; loss: 14.06; acc: 0.47
Batch: 460; loss: 9.49; acc: 0.66
Batch: 480; loss: 7.34; acc: 0.62
Batch: 500; loss: 9.16; acc: 0.58
Batch: 520; loss: 11.66; acc: 0.52
Batch: 540; loss: 7.72; acc: 0.56
Batch: 560; loss: 6.47; acc: 0.7
Batch: 580; loss: 7.71; acc: 0.62
Batch: 600; loss: 10.41; acc: 0.67
Batch: 620; loss: 6.63; acc: 0.56
Train Epoch over. train_loss: 8.48; train_accuracy: 0.6 

Batch: 0; loss: 6.38; acc: 0.61
Batch: 20; loss: 16.56; acc: 0.33
Batch: 40; loss: 4.92; acc: 0.78
Batch: 60; loss: 10.77; acc: 0.48
Batch: 80; loss: 9.55; acc: 0.53
Batch: 100; loss: 10.94; acc: 0.59
Batch: 120; loss: 8.15; acc: 0.66
Batch: 140; loss: 11.02; acc: 0.42
Val Epoch over. val_loss: 9.075547625304788; val_accuracy: 0.5791202229299363 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 12.61; acc: 0.47
Batch: 20; loss: 5.2; acc: 0.62
Batch: 40; loss: 8.14; acc: 0.55
Batch: 60; loss: 12.08; acc: 0.5
Batch: 80; loss: 5.71; acc: 0.69
Batch: 100; loss: 8.17; acc: 0.56
Batch: 120; loss: 6.92; acc: 0.67
Batch: 140; loss: 9.52; acc: 0.55
Batch: 160; loss: 9.35; acc: 0.52
Batch: 180; loss: 8.57; acc: 0.62
Batch: 200; loss: 8.82; acc: 0.55
Batch: 220; loss: 12.4; acc: 0.58
Batch: 240; loss: 9.29; acc: 0.62
Batch: 260; loss: 9.56; acc: 0.56
Batch: 280; loss: 10.41; acc: 0.62
Batch: 300; loss: 10.09; acc: 0.59
Batch: 320; loss: 8.52; acc: 0.62
Batch: 340; loss: 8.14; acc: 0.55
Batch: 360; loss: 7.57; acc: 0.66
Batch: 380; loss: 6.04; acc: 0.61
Batch: 400; loss: 6.85; acc: 0.61
Batch: 420; loss: 6.9; acc: 0.5
Batch: 440; loss: 7.93; acc: 0.69
Batch: 460; loss: 8.52; acc: 0.59
Batch: 480; loss: 7.32; acc: 0.67
Batch: 500; loss: 3.58; acc: 0.67
Batch: 520; loss: 9.32; acc: 0.66
Batch: 540; loss: 10.27; acc: 0.5
Batch: 560; loss: 11.82; acc: 0.56
Batch: 580; loss: 10.0; acc: 0.52
Batch: 600; loss: 12.72; acc: 0.42
Batch: 620; loss: 5.35; acc: 0.69
Train Epoch over. train_loss: 8.42; train_accuracy: 0.61 

Batch: 0; loss: 5.85; acc: 0.64
Batch: 20; loss: 13.33; acc: 0.38
Batch: 40; loss: 7.28; acc: 0.62
Batch: 60; loss: 8.23; acc: 0.59
Batch: 80; loss: 10.58; acc: 0.53
Batch: 100; loss: 13.08; acc: 0.53
Batch: 120; loss: 7.86; acc: 0.67
Batch: 140; loss: 8.52; acc: 0.55
Val Epoch over. val_loss: 8.754629701565785; val_accuracy: 0.5929538216560509 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 7.78; acc: 0.58
Batch: 20; loss: 7.37; acc: 0.58
Batch: 40; loss: 10.37; acc: 0.52
Batch: 60; loss: 5.75; acc: 0.69
Batch: 80; loss: 6.9; acc: 0.62
Batch: 100; loss: 5.34; acc: 0.64
Batch: 120; loss: 7.46; acc: 0.62
Batch: 140; loss: 8.06; acc: 0.58
Batch: 160; loss: 9.4; acc: 0.56
Batch: 180; loss: 8.45; acc: 0.58
Batch: 200; loss: 8.76; acc: 0.59
Batch: 220; loss: 8.85; acc: 0.55
Batch: 240; loss: 7.7; acc: 0.53
Batch: 260; loss: 6.64; acc: 0.64
Batch: 280; loss: 10.23; acc: 0.55
Batch: 300; loss: 7.95; acc: 0.56
Batch: 320; loss: 9.03; acc: 0.56
Batch: 340; loss: 5.76; acc: 0.7
Batch: 360; loss: 4.98; acc: 0.72
Batch: 380; loss: 9.51; acc: 0.62
Batch: 400; loss: 12.03; acc: 0.56
Batch: 420; loss: 10.09; acc: 0.55
Batch: 440; loss: 9.24; acc: 0.53
Batch: 460; loss: 9.03; acc: 0.52
Batch: 480; loss: 9.84; acc: 0.55
Batch: 500; loss: 5.53; acc: 0.7
Batch: 520; loss: 8.44; acc: 0.61
Batch: 540; loss: 9.39; acc: 0.45
Batch: 560; loss: 10.06; acc: 0.55
Batch: 580; loss: 6.4; acc: 0.61
Batch: 600; loss: 7.0; acc: 0.64
Batch: 620; loss: 8.86; acc: 0.55
Train Epoch over. train_loss: 8.4; train_accuracy: 0.61 

Batch: 0; loss: 6.95; acc: 0.69
Batch: 20; loss: 13.86; acc: 0.39
Batch: 40; loss: 8.59; acc: 0.64
Batch: 60; loss: 8.69; acc: 0.66
Batch: 80; loss: 9.61; acc: 0.56
Batch: 100; loss: 10.79; acc: 0.58
Batch: 120; loss: 6.25; acc: 0.7
Batch: 140; loss: 9.27; acc: 0.45
Val Epoch over. val_loss: 8.615663124497528; val_accuracy: 0.6115644904458599 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 11.73; acc: 0.52
Batch: 20; loss: 7.73; acc: 0.69
Batch: 40; loss: 7.89; acc: 0.56
Batch: 60; loss: 5.65; acc: 0.67
Batch: 80; loss: 11.39; acc: 0.52
Batch: 100; loss: 7.2; acc: 0.69
Batch: 120; loss: 8.63; acc: 0.58
Batch: 140; loss: 7.26; acc: 0.61
Batch: 160; loss: 14.3; acc: 0.55
Batch: 180; loss: 6.78; acc: 0.66
Batch: 200; loss: 5.57; acc: 0.59
Batch: 220; loss: 9.75; acc: 0.52
Batch: 240; loss: 9.42; acc: 0.52
Batch: 260; loss: 11.09; acc: 0.58
Batch: 280; loss: 10.21; acc: 0.61
Batch: 300; loss: 10.76; acc: 0.48
Batch: 320; loss: 8.97; acc: 0.64
Batch: 340; loss: 7.18; acc: 0.67
Batch: 360; loss: 6.6; acc: 0.7
Batch: 380; loss: 9.85; acc: 0.56
Batch: 400; loss: 7.16; acc: 0.62
Batch: 420; loss: 5.7; acc: 0.66
Batch: 440; loss: 12.02; acc: 0.59
Batch: 460; loss: 7.72; acc: 0.67
Batch: 480; loss: 7.79; acc: 0.62
Batch: 500; loss: 7.62; acc: 0.55
Batch: 520; loss: 10.06; acc: 0.62
Batch: 540; loss: 10.48; acc: 0.62
Batch: 560; loss: 9.15; acc: 0.59
Batch: 580; loss: 8.08; acc: 0.64
Batch: 600; loss: 10.5; acc: 0.53
Batch: 620; loss: 9.95; acc: 0.56
Train Epoch over. train_loss: 8.44; train_accuracy: 0.61 

Batch: 0; loss: 5.55; acc: 0.69
Batch: 20; loss: 15.53; acc: 0.41
Batch: 40; loss: 6.2; acc: 0.67
Batch: 60; loss: 9.5; acc: 0.58
Batch: 80; loss: 7.82; acc: 0.56
Batch: 100; loss: 12.71; acc: 0.56
Batch: 120; loss: 7.76; acc: 0.69
Batch: 140; loss: 9.55; acc: 0.55
Val Epoch over. val_loss: 8.767785928811238; val_accuracy: 0.5971337579617835 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 7.1; acc: 0.58
Batch: 20; loss: 5.84; acc: 0.61
Batch: 40; loss: 11.25; acc: 0.64
Batch: 60; loss: 8.84; acc: 0.62
Batch: 80; loss: 9.95; acc: 0.64
Batch: 100; loss: 8.87; acc: 0.53
Batch: 120; loss: 7.76; acc: 0.67
Batch: 140; loss: 8.97; acc: 0.58
Batch: 160; loss: 7.06; acc: 0.72
Batch: 180; loss: 8.04; acc: 0.61
Batch: 200; loss: 3.65; acc: 0.67
Batch: 220; loss: 9.47; acc: 0.66
Batch: 240; loss: 8.97; acc: 0.61
Batch: 260; loss: 8.22; acc: 0.66
Batch: 280; loss: 6.69; acc: 0.64
Batch: 300; loss: 8.49; acc: 0.56
Batch: 320; loss: 8.67; acc: 0.67
Batch: 340; loss: 11.35; acc: 0.52
Batch: 360; loss: 7.9; acc: 0.56
Batch: 380; loss: 5.92; acc: 0.75
Batch: 400; loss: 6.76; acc: 0.59
Batch: 420; loss: 10.36; acc: 0.58
Batch: 440; loss: 4.56; acc: 0.72
Batch: 460; loss: 8.4; acc: 0.61
Batch: 480; loss: 6.69; acc: 0.67
Batch: 500; loss: 5.56; acc: 0.58
Batch: 520; loss: 7.28; acc: 0.66
Batch: 540; loss: 8.09; acc: 0.53
Batch: 560; loss: 10.05; acc: 0.55
Batch: 580; loss: 9.37; acc: 0.62
Batch: 600; loss: 10.19; acc: 0.53
Batch: 620; loss: 8.25; acc: 0.56
Train Epoch over. train_loss: 8.42; train_accuracy: 0.61 

Batch: 0; loss: 5.45; acc: 0.66
Batch: 20; loss: 14.21; acc: 0.38
Batch: 40; loss: 5.37; acc: 0.7
Batch: 60; loss: 9.69; acc: 0.61
Batch: 80; loss: 9.55; acc: 0.52
Batch: 100; loss: 10.78; acc: 0.59
Batch: 120; loss: 9.21; acc: 0.67
Batch: 140; loss: 9.13; acc: 0.53
Val Epoch over. val_loss: 8.638456163892322; val_accuracy: 0.6005175159235668 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 8.26; acc: 0.56
Batch: 20; loss: 4.76; acc: 0.72
Batch: 40; loss: 5.2; acc: 0.7
Batch: 60; loss: 6.64; acc: 0.67
Batch: 80; loss: 6.3; acc: 0.67
Batch: 100; loss: 10.05; acc: 0.55
Batch: 120; loss: 7.73; acc: 0.72
Batch: 140; loss: 6.49; acc: 0.72
Batch: 160; loss: 7.31; acc: 0.56
Batch: 180; loss: 7.11; acc: 0.69
Batch: 200; loss: 10.07; acc: 0.58
Batch: 220; loss: 9.74; acc: 0.56
Batch: 240; loss: 5.89; acc: 0.7
Batch: 260; loss: 8.87; acc: 0.61
Batch: 280; loss: 7.67; acc: 0.64
Batch: 300; loss: 8.12; acc: 0.62
Batch: 320; loss: 6.46; acc: 0.59
Batch: 340; loss: 9.27; acc: 0.53
Batch: 360; loss: 10.02; acc: 0.61
Batch: 380; loss: 7.84; acc: 0.64
Batch: 400; loss: 7.25; acc: 0.59
Batch: 420; loss: 7.24; acc: 0.59
Batch: 440; loss: 9.11; acc: 0.67
Batch: 460; loss: 7.03; acc: 0.62
Batch: 480; loss: 6.27; acc: 0.61
Batch: 500; loss: 8.16; acc: 0.62
Batch: 520; loss: 8.19; acc: 0.56
Batch: 540; loss: 8.05; acc: 0.59
Batch: 560; loss: 8.58; acc: 0.59
Batch: 580; loss: 5.24; acc: 0.75
Batch: 600; loss: 6.07; acc: 0.7
Batch: 620; loss: 11.67; acc: 0.61
Train Epoch over. train_loss: 8.45; train_accuracy: 0.61 

Batch: 0; loss: 6.65; acc: 0.61
Batch: 20; loss: 16.01; acc: 0.38
Batch: 40; loss: 6.7; acc: 0.59
Batch: 60; loss: 9.17; acc: 0.59
Batch: 80; loss: 8.12; acc: 0.58
Batch: 100; loss: 10.6; acc: 0.56
Batch: 120; loss: 7.73; acc: 0.7
Batch: 140; loss: 9.14; acc: 0.53
Val Epoch over. val_loss: 8.915866576941909; val_accuracy: 0.5804140127388535 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 7.98; acc: 0.58
Batch: 20; loss: 9.12; acc: 0.66
Batch: 40; loss: 10.21; acc: 0.58
Batch: 60; loss: 9.81; acc: 0.62
Batch: 80; loss: 7.98; acc: 0.59
Batch: 100; loss: 10.03; acc: 0.59
Batch: 120; loss: 12.13; acc: 0.47
Batch: 140; loss: 9.19; acc: 0.59
Batch: 160; loss: 8.54; acc: 0.58
Batch: 180; loss: 10.71; acc: 0.52
Batch: 200; loss: 8.07; acc: 0.66
Batch: 220; loss: 9.01; acc: 0.66
Batch: 240; loss: 5.99; acc: 0.69
Batch: 260; loss: 6.71; acc: 0.77
Batch: 280; loss: 8.86; acc: 0.59
Batch: 300; loss: 7.6; acc: 0.66
Batch: 320; loss: 4.33; acc: 0.72
Batch: 340; loss: 9.96; acc: 0.56
Batch: 360; loss: 6.93; acc: 0.7
Batch: 380; loss: 4.84; acc: 0.66
Batch: 400; loss: 11.13; acc: 0.56
Batch: 420; loss: 7.06; acc: 0.58
Batch: 440; loss: 8.49; acc: 0.64
Batch: 460; loss: 12.2; acc: 0.55
Batch: 480; loss: 6.66; acc: 0.64
Batch: 500; loss: 8.01; acc: 0.58
Batch: 520; loss: 6.15; acc: 0.64
Batch: 540; loss: 6.74; acc: 0.67
Batch: 560; loss: 6.97; acc: 0.67
Batch: 580; loss: 6.44; acc: 0.73
Batch: 600; loss: 5.7; acc: 0.59
Batch: 620; loss: 12.52; acc: 0.47
Train Epoch over. train_loss: 8.42; train_accuracy: 0.61 

Batch: 0; loss: 7.76; acc: 0.62
Batch: 20; loss: 12.59; acc: 0.36
Batch: 40; loss: 4.58; acc: 0.72
Batch: 60; loss: 8.98; acc: 0.52
Batch: 80; loss: 8.34; acc: 0.5
Batch: 100; loss: 12.04; acc: 0.52
Batch: 120; loss: 6.89; acc: 0.67
Batch: 140; loss: 9.55; acc: 0.48
Val Epoch over. val_loss: 8.177862896281443; val_accuracy: 0.6049960191082803 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 10.56; acc: 0.53
Batch: 20; loss: 7.95; acc: 0.7
Batch: 40; loss: 10.0; acc: 0.45
Batch: 60; loss: 8.49; acc: 0.59
Batch: 80; loss: 7.57; acc: 0.69
Batch: 100; loss: 7.97; acc: 0.62
Batch: 120; loss: 9.23; acc: 0.62
Batch: 140; loss: 4.65; acc: 0.69
Batch: 160; loss: 9.21; acc: 0.61
Batch: 180; loss: 7.12; acc: 0.61
Batch: 200; loss: 8.6; acc: 0.59
Batch: 220; loss: 10.04; acc: 0.61
Batch: 240; loss: 6.87; acc: 0.7
Batch: 260; loss: 4.84; acc: 0.69
Batch: 280; loss: 5.98; acc: 0.66
Batch: 300; loss: 3.86; acc: 0.72
Batch: 320; loss: 7.13; acc: 0.61
Batch: 340; loss: 10.41; acc: 0.62
Batch: 360; loss: 5.47; acc: 0.69
Batch: 380; loss: 7.34; acc: 0.56
Batch: 400; loss: 6.35; acc: 0.64
Batch: 420; loss: 7.39; acc: 0.61
Batch: 440; loss: 8.18; acc: 0.67
Batch: 460; loss: 8.91; acc: 0.53
Batch: 480; loss: 7.03; acc: 0.62
Batch: 500; loss: 6.58; acc: 0.66
Batch: 520; loss: 6.24; acc: 0.77
Batch: 540; loss: 4.7; acc: 0.72
Batch: 560; loss: 6.35; acc: 0.7
Batch: 580; loss: 10.38; acc: 0.61
Batch: 600; loss: 8.92; acc: 0.56
Batch: 620; loss: 10.45; acc: 0.53
Train Epoch over. train_loss: 7.19; train_accuracy: 0.64 

Batch: 0; loss: 5.73; acc: 0.64
Batch: 20; loss: 13.27; acc: 0.36
Batch: 40; loss: 4.14; acc: 0.73
Batch: 60; loss: 8.84; acc: 0.58
Batch: 80; loss: 7.5; acc: 0.58
Batch: 100; loss: 10.66; acc: 0.53
Batch: 120; loss: 6.27; acc: 0.7
Batch: 140; loss: 8.55; acc: 0.52
Val Epoch over. val_loss: 7.400156731818132; val_accuracy: 0.6286823248407644 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 8.45; acc: 0.52
Batch: 20; loss: 5.78; acc: 0.69
Batch: 40; loss: 8.99; acc: 0.58
Batch: 60; loss: 7.39; acc: 0.62
Batch: 80; loss: 8.37; acc: 0.56
Batch: 100; loss: 8.75; acc: 0.69
Batch: 120; loss: 3.35; acc: 0.73
Batch: 140; loss: 6.96; acc: 0.61
Batch: 160; loss: 6.93; acc: 0.61
Batch: 180; loss: 4.96; acc: 0.73
Batch: 200; loss: 7.72; acc: 0.64
Batch: 220; loss: 5.22; acc: 0.67
Batch: 240; loss: 6.81; acc: 0.64
Batch: 260; loss: 4.88; acc: 0.7
Batch: 280; loss: 5.19; acc: 0.75
Batch: 300; loss: 4.38; acc: 0.64
Batch: 320; loss: 8.56; acc: 0.61
Batch: 340; loss: 4.38; acc: 0.64
Batch: 360; loss: 5.04; acc: 0.67
Batch: 380; loss: 8.21; acc: 0.59
Batch: 400; loss: 6.37; acc: 0.64
Batch: 420; loss: 5.53; acc: 0.66
Batch: 440; loss: 9.04; acc: 0.67
Batch: 460; loss: 2.42; acc: 0.8
Batch: 480; loss: 9.16; acc: 0.61
Batch: 500; loss: 7.19; acc: 0.61
Batch: 520; loss: 7.35; acc: 0.67
Batch: 540; loss: 7.0; acc: 0.62
Batch: 560; loss: 8.64; acc: 0.67
Batch: 580; loss: 8.4; acc: 0.56
Batch: 600; loss: 8.99; acc: 0.61
Batch: 620; loss: 4.39; acc: 0.73
Train Epoch over. train_loss: 7.05; train_accuracy: 0.64 

Batch: 0; loss: 5.03; acc: 0.67
Batch: 20; loss: 13.54; acc: 0.33
Batch: 40; loss: 4.58; acc: 0.7
Batch: 60; loss: 9.41; acc: 0.58
Batch: 80; loss: 7.05; acc: 0.62
Batch: 100; loss: 10.62; acc: 0.53
Batch: 120; loss: 6.5; acc: 0.7
Batch: 140; loss: 8.37; acc: 0.47
Val Epoch over. val_loss: 7.510697821902621; val_accuracy: 0.6252985668789809 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 8.79; acc: 0.61
Batch: 20; loss: 7.78; acc: 0.69
Batch: 40; loss: 4.45; acc: 0.72
Batch: 60; loss: 5.08; acc: 0.7
Batch: 80; loss: 5.88; acc: 0.61
Batch: 100; loss: 6.57; acc: 0.67
Batch: 120; loss: 6.95; acc: 0.7
Batch: 140; loss: 6.47; acc: 0.66
Batch: 160; loss: 5.35; acc: 0.62
Batch: 180; loss: 6.49; acc: 0.64
Batch: 200; loss: 5.44; acc: 0.75
Batch: 220; loss: 5.72; acc: 0.69
Batch: 240; loss: 6.42; acc: 0.64
Batch: 260; loss: 6.66; acc: 0.62
Batch: 280; loss: 7.54; acc: 0.66
Batch: 300; loss: 7.26; acc: 0.64
Batch: 320; loss: 5.67; acc: 0.7
Batch: 340; loss: 9.66; acc: 0.56
Batch: 360; loss: 5.14; acc: 0.69
Batch: 380; loss: 8.53; acc: 0.62
Batch: 400; loss: 8.17; acc: 0.61
Batch: 420; loss: 5.98; acc: 0.67
Batch: 440; loss: 6.63; acc: 0.66
Batch: 460; loss: 7.36; acc: 0.61
Batch: 480; loss: 7.13; acc: 0.61
Batch: 500; loss: 6.45; acc: 0.64
Batch: 520; loss: 7.8; acc: 0.67
Batch: 540; loss: 8.45; acc: 0.61
Batch: 560; loss: 7.59; acc: 0.52
Batch: 580; loss: 8.94; acc: 0.58
Batch: 600; loss: 6.69; acc: 0.67
Batch: 620; loss: 5.89; acc: 0.62
Train Epoch over. train_loss: 7.04; train_accuracy: 0.64 

Batch: 0; loss: 5.11; acc: 0.64
Batch: 20; loss: 13.65; acc: 0.36
Batch: 40; loss: 4.3; acc: 0.73
Batch: 60; loss: 9.35; acc: 0.56
Batch: 80; loss: 7.22; acc: 0.58
Batch: 100; loss: 10.6; acc: 0.55
Batch: 120; loss: 6.28; acc: 0.72
Batch: 140; loss: 8.39; acc: 0.47
Val Epoch over. val_loss: 7.469952896142462; val_accuracy: 0.6252985668789809 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 7.56; acc: 0.64
Batch: 20; loss: 5.81; acc: 0.72
Batch: 40; loss: 3.79; acc: 0.77
Batch: 60; loss: 4.16; acc: 0.69
Batch: 80; loss: 6.13; acc: 0.72
Batch: 100; loss: 6.3; acc: 0.59
Batch: 120; loss: 9.3; acc: 0.59
Batch: 140; loss: 4.81; acc: 0.66
Batch: 160; loss: 4.82; acc: 0.69
Batch: 180; loss: 9.26; acc: 0.58
Batch: 200; loss: 6.82; acc: 0.58
Batch: 220; loss: 5.85; acc: 0.67
Batch: 240; loss: 7.81; acc: 0.66
Batch: 260; loss: 5.73; acc: 0.66
Batch: 280; loss: 4.96; acc: 0.7
Batch: 300; loss: 7.57; acc: 0.62
Batch: 320; loss: 6.61; acc: 0.72
Batch: 340; loss: 7.19; acc: 0.67
Batch: 360; loss: 9.38; acc: 0.58
Batch: 380; loss: 5.82; acc: 0.66
Batch: 400; loss: 6.05; acc: 0.59
Batch: 420; loss: 7.25; acc: 0.59
Batch: 440; loss: 6.98; acc: 0.64
Batch: 460; loss: 4.21; acc: 0.69
Batch: 480; loss: 7.38; acc: 0.61
Batch: 500; loss: 3.6; acc: 0.69
Batch: 520; loss: 7.91; acc: 0.56
Batch: 540; loss: 5.05; acc: 0.66
Batch: 560; loss: 8.33; acc: 0.58
Batch: 580; loss: 4.92; acc: 0.8
Batch: 600; loss: 7.88; acc: 0.66
Batch: 620; loss: 4.88; acc: 0.61
Train Epoch over. train_loss: 7.03; train_accuracy: 0.64 

Batch: 0; loss: 5.32; acc: 0.64
Batch: 20; loss: 13.39; acc: 0.36
Batch: 40; loss: 4.33; acc: 0.69
Batch: 60; loss: 9.7; acc: 0.55
Batch: 80; loss: 7.49; acc: 0.61
Batch: 100; loss: 10.67; acc: 0.55
Batch: 120; loss: 6.34; acc: 0.72
Batch: 140; loss: 8.37; acc: 0.5
Val Epoch over. val_loss: 7.473766803741455; val_accuracy: 0.6289808917197452 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.63; acc: 0.69
Batch: 20; loss: 10.22; acc: 0.55
Batch: 40; loss: 8.83; acc: 0.55
Batch: 60; loss: 6.02; acc: 0.64
Batch: 80; loss: 6.23; acc: 0.62
Batch: 100; loss: 3.58; acc: 0.7
Batch: 120; loss: 8.76; acc: 0.64
Batch: 140; loss: 10.29; acc: 0.52
Batch: 160; loss: 5.66; acc: 0.7
Batch: 180; loss: 6.68; acc: 0.61
Batch: 200; loss: 4.43; acc: 0.62
Batch: 220; loss: 9.39; acc: 0.58
Batch: 240; loss: 5.77; acc: 0.62
Batch: 260; loss: 7.29; acc: 0.58
Batch: 280; loss: 7.14; acc: 0.62
Batch: 300; loss: 6.01; acc: 0.73
Batch: 320; loss: 8.33; acc: 0.59
Batch: 340; loss: 7.35; acc: 0.66
Batch: 360; loss: 4.86; acc: 0.69
Batch: 380; loss: 6.24; acc: 0.69
Batch: 400; loss: 6.02; acc: 0.72
Batch: 420; loss: 5.65; acc: 0.67
Batch: 440; loss: 8.73; acc: 0.61
Batch: 460; loss: 5.63; acc: 0.73
Batch: 480; loss: 6.16; acc: 0.62
Batch: 500; loss: 6.51; acc: 0.56
Batch: 520; loss: 7.8; acc: 0.64
Batch: 540; loss: 7.06; acc: 0.58
Batch: 560; loss: 3.36; acc: 0.75
Batch: 580; loss: 7.45; acc: 0.64
Batch: 600; loss: 5.85; acc: 0.62
Batch: 620; loss: 7.75; acc: 0.58
Train Epoch over. train_loss: 7.03; train_accuracy: 0.64 

Batch: 0; loss: 4.86; acc: 0.66
Batch: 20; loss: 14.03; acc: 0.38
Batch: 40; loss: 4.46; acc: 0.72
Batch: 60; loss: 9.36; acc: 0.56
Batch: 80; loss: 7.05; acc: 0.59
Batch: 100; loss: 10.52; acc: 0.56
Batch: 120; loss: 6.31; acc: 0.72
Batch: 140; loss: 8.4; acc: 0.5
Val Epoch over. val_loss: 7.458207097023156; val_accuracy: 0.6320660828025477 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 7.48; acc: 0.67
Batch: 20; loss: 10.0; acc: 0.58
Batch: 40; loss: 2.23; acc: 0.77
Batch: 60; loss: 5.19; acc: 0.66
Batch: 80; loss: 4.86; acc: 0.72
Batch: 100; loss: 7.59; acc: 0.64
Batch: 120; loss: 8.27; acc: 0.59
Batch: 140; loss: 7.7; acc: 0.66
Batch: 160; loss: 6.32; acc: 0.69
Batch: 180; loss: 8.64; acc: 0.61
Batch: 200; loss: 5.19; acc: 0.73
Batch: 220; loss: 5.43; acc: 0.69
Batch: 240; loss: 5.46; acc: 0.62
Batch: 260; loss: 7.93; acc: 0.66
Batch: 280; loss: 6.26; acc: 0.69
Batch: 300; loss: 7.05; acc: 0.66
Batch: 320; loss: 6.08; acc: 0.67
Batch: 340; loss: 7.08; acc: 0.72
Batch: 360; loss: 7.06; acc: 0.62
Batch: 380; loss: 7.12; acc: 0.58
Batch: 400; loss: 6.84; acc: 0.7
Batch: 420; loss: 9.23; acc: 0.59
Batch: 440; loss: 8.24; acc: 0.64
Batch: 460; loss: 6.99; acc: 0.66
Batch: 480; loss: 8.08; acc: 0.59
Batch: 500; loss: 4.79; acc: 0.72
Batch: 520; loss: 4.47; acc: 0.66
Batch: 540; loss: 7.8; acc: 0.7
Batch: 560; loss: 6.39; acc: 0.7
Batch: 580; loss: 8.32; acc: 0.61
Batch: 600; loss: 7.83; acc: 0.61
Batch: 620; loss: 4.68; acc: 0.66
Train Epoch over. train_loss: 7.03; train_accuracy: 0.64 

Batch: 0; loss: 5.03; acc: 0.66
Batch: 20; loss: 13.22; acc: 0.36
Batch: 40; loss: 4.57; acc: 0.75
Batch: 60; loss: 9.5; acc: 0.56
Batch: 80; loss: 7.18; acc: 0.59
Batch: 100; loss: 10.32; acc: 0.53
Batch: 120; loss: 6.39; acc: 0.7
Batch: 140; loss: 7.71; acc: 0.53
Val Epoch over. val_loss: 7.437338566324513; val_accuracy: 0.6323646496815286 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 7.56; acc: 0.69
Batch: 20; loss: 9.91; acc: 0.62
Batch: 40; loss: 3.04; acc: 0.73
Batch: 60; loss: 5.77; acc: 0.66
Batch: 80; loss: 6.38; acc: 0.66
Batch: 100; loss: 6.83; acc: 0.64
Batch: 120; loss: 6.95; acc: 0.64
Batch: 140; loss: 6.87; acc: 0.7
Batch: 160; loss: 6.03; acc: 0.72
Batch: 180; loss: 8.07; acc: 0.55
Batch: 200; loss: 7.18; acc: 0.66
Batch: 220; loss: 6.93; acc: 0.64
Batch: 240; loss: 6.49; acc: 0.69
Batch: 260; loss: 7.29; acc: 0.61
Batch: 280; loss: 6.6; acc: 0.7
Batch: 300; loss: 7.14; acc: 0.61
Batch: 320; loss: 9.8; acc: 0.55
Batch: 340; loss: 8.37; acc: 0.56
Batch: 360; loss: 2.81; acc: 0.77
Batch: 380; loss: 7.0; acc: 0.66
Batch: 400; loss: 5.59; acc: 0.69
Batch: 420; loss: 8.75; acc: 0.72
Batch: 440; loss: 5.12; acc: 0.66
Batch: 460; loss: 6.18; acc: 0.73
Batch: 480; loss: 8.03; acc: 0.61
Batch: 500; loss: 9.3; acc: 0.58
Batch: 520; loss: 9.92; acc: 0.53
Batch: 540; loss: 8.14; acc: 0.62
Batch: 560; loss: 7.85; acc: 0.62
Batch: 580; loss: 7.64; acc: 0.61
Batch: 600; loss: 9.22; acc: 0.59
Batch: 620; loss: 6.04; acc: 0.64
Train Epoch over. train_loss: 7.02; train_accuracy: 0.64 

Batch: 0; loss: 5.13; acc: 0.64
Batch: 20; loss: 14.32; acc: 0.39
Batch: 40; loss: 4.39; acc: 0.72
Batch: 60; loss: 9.26; acc: 0.56
Batch: 80; loss: 7.23; acc: 0.62
Batch: 100; loss: 10.84; acc: 0.56
Batch: 120; loss: 6.53; acc: 0.73
Batch: 140; loss: 7.95; acc: 0.5
Val Epoch over. val_loss: 7.50186880986402; val_accuracy: 0.6291799363057324 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 9.4; acc: 0.55
Batch: 20; loss: 5.78; acc: 0.67
Batch: 40; loss: 5.9; acc: 0.67
Batch: 60; loss: 8.92; acc: 0.56
Batch: 80; loss: 8.76; acc: 0.55
Batch: 100; loss: 6.51; acc: 0.7
Batch: 120; loss: 7.25; acc: 0.73
Batch: 140; loss: 10.78; acc: 0.58
Batch: 160; loss: 7.07; acc: 0.62
Batch: 180; loss: 4.85; acc: 0.69
Batch: 200; loss: 5.79; acc: 0.77
Batch: 220; loss: 7.26; acc: 0.64
Batch: 240; loss: 6.08; acc: 0.72
Batch: 260; loss: 6.96; acc: 0.56
Batch: 280; loss: 6.19; acc: 0.64
Batch: 300; loss: 5.59; acc: 0.72
Batch: 320; loss: 6.85; acc: 0.72
Batch: 340; loss: 4.14; acc: 0.77
Batch: 360; loss: 5.63; acc: 0.75
Batch: 380; loss: 5.36; acc: 0.67
Batch: 400; loss: 5.89; acc: 0.55
Batch: 420; loss: 5.06; acc: 0.64
Batch: 440; loss: 5.3; acc: 0.69
Batch: 460; loss: 7.66; acc: 0.52
Batch: 480; loss: 5.09; acc: 0.62
Batch: 500; loss: 6.08; acc: 0.69
Batch: 520; loss: 6.25; acc: 0.66
Batch: 540; loss: 6.48; acc: 0.62
Batch: 560; loss: 4.96; acc: 0.72
Batch: 580; loss: 6.51; acc: 0.61
Batch: 600; loss: 4.33; acc: 0.73
Batch: 620; loss: 5.03; acc: 0.67
Train Epoch over. train_loss: 7.02; train_accuracy: 0.64 

Batch: 0; loss: 5.55; acc: 0.69
Batch: 20; loss: 13.79; acc: 0.36
Batch: 40; loss: 4.31; acc: 0.73
Batch: 60; loss: 8.98; acc: 0.55
Batch: 80; loss: 6.97; acc: 0.62
Batch: 100; loss: 10.64; acc: 0.56
Batch: 120; loss: 6.73; acc: 0.7
Batch: 140; loss: 7.73; acc: 0.52
Val Epoch over. val_loss: 7.454341762384791; val_accuracy: 0.634156050955414 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.52; acc: 0.64
Batch: 20; loss: 5.6; acc: 0.62
Batch: 40; loss: 6.43; acc: 0.72
Batch: 60; loss: 5.83; acc: 0.66
Batch: 80; loss: 10.6; acc: 0.55
Batch: 100; loss: 9.0; acc: 0.59
Batch: 120; loss: 7.17; acc: 0.69
Batch: 140; loss: 5.85; acc: 0.67
Batch: 160; loss: 7.81; acc: 0.62
Batch: 180; loss: 6.26; acc: 0.72
Batch: 200; loss: 8.87; acc: 0.58
Batch: 220; loss: 5.78; acc: 0.62
Batch: 240; loss: 8.26; acc: 0.61
Batch: 260; loss: 6.06; acc: 0.7
Batch: 280; loss: 7.4; acc: 0.61
Batch: 300; loss: 7.98; acc: 0.7
Batch: 320; loss: 8.11; acc: 0.66
Batch: 340; loss: 10.81; acc: 0.53
Batch: 360; loss: 5.6; acc: 0.69
Batch: 380; loss: 6.67; acc: 0.67
Batch: 400; loss: 4.9; acc: 0.67
Batch: 420; loss: 5.84; acc: 0.61
Batch: 440; loss: 5.27; acc: 0.66
Batch: 460; loss: 8.07; acc: 0.59
Batch: 480; loss: 14.02; acc: 0.48
Batch: 500; loss: 10.28; acc: 0.62
Batch: 520; loss: 9.47; acc: 0.58
Batch: 540; loss: 7.28; acc: 0.58
Batch: 560; loss: 6.7; acc: 0.66
Batch: 580; loss: 6.02; acc: 0.64
Batch: 600; loss: 5.2; acc: 0.59
Batch: 620; loss: 8.33; acc: 0.58
Train Epoch over. train_loss: 7.02; train_accuracy: 0.64 

Batch: 0; loss: 5.18; acc: 0.67
Batch: 20; loss: 14.04; acc: 0.36
Batch: 40; loss: 4.29; acc: 0.7
Batch: 60; loss: 9.02; acc: 0.58
Batch: 80; loss: 7.04; acc: 0.62
Batch: 100; loss: 10.46; acc: 0.55
Batch: 120; loss: 6.72; acc: 0.7
Batch: 140; loss: 8.31; acc: 0.48
Val Epoch over. val_loss: 7.438358845984101; val_accuracy: 0.6308718152866242 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 7.46; acc: 0.66
Batch: 20; loss: 8.1; acc: 0.62
Batch: 40; loss: 5.47; acc: 0.67
Batch: 60; loss: 5.05; acc: 0.73
Batch: 80; loss: 9.88; acc: 0.53
Batch: 100; loss: 7.69; acc: 0.62
Batch: 120; loss: 5.63; acc: 0.69
Batch: 140; loss: 8.93; acc: 0.55
Batch: 160; loss: 4.29; acc: 0.73
Batch: 180; loss: 7.12; acc: 0.61
Batch: 200; loss: 7.08; acc: 0.67
Batch: 220; loss: 7.2; acc: 0.62
Batch: 240; loss: 7.09; acc: 0.67
Batch: 260; loss: 6.06; acc: 0.66
Batch: 280; loss: 7.44; acc: 0.59
Batch: 300; loss: 7.95; acc: 0.62
Batch: 320; loss: 7.96; acc: 0.59
Batch: 340; loss: 10.55; acc: 0.56
Batch: 360; loss: 5.8; acc: 0.67
Batch: 380; loss: 8.12; acc: 0.59
Batch: 400; loss: 5.71; acc: 0.7
Batch: 420; loss: 6.42; acc: 0.61
Batch: 440; loss: 7.48; acc: 0.64
Batch: 460; loss: 7.92; acc: 0.69
Batch: 480; loss: 4.17; acc: 0.75
Batch: 500; loss: 5.94; acc: 0.67
Batch: 520; loss: 12.2; acc: 0.47
Batch: 540; loss: 8.46; acc: 0.62
Batch: 560; loss: 6.89; acc: 0.67
Batch: 580; loss: 7.14; acc: 0.64
Batch: 600; loss: 6.07; acc: 0.66
Batch: 620; loss: 6.54; acc: 0.66
Train Epoch over. train_loss: 7.02; train_accuracy: 0.64 

Batch: 0; loss: 5.12; acc: 0.64
Batch: 20; loss: 14.34; acc: 0.36
Batch: 40; loss: 4.12; acc: 0.75
Batch: 60; loss: 9.91; acc: 0.55
Batch: 80; loss: 6.82; acc: 0.61
Batch: 100; loss: 10.36; acc: 0.53
Batch: 120; loss: 6.87; acc: 0.69
Batch: 140; loss: 8.09; acc: 0.5
Val Epoch over. val_loss: 7.4917605363639295; val_accuracy: 0.6281847133757962 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 8.66; acc: 0.62
Batch: 20; loss: 8.57; acc: 0.58
Batch: 40; loss: 4.48; acc: 0.66
Batch: 60; loss: 4.28; acc: 0.69
Batch: 80; loss: 6.59; acc: 0.61
Batch: 100; loss: 8.83; acc: 0.59
Batch: 120; loss: 9.74; acc: 0.59
Batch: 140; loss: 5.45; acc: 0.66
Batch: 160; loss: 8.1; acc: 0.61
Batch: 180; loss: 11.02; acc: 0.64
Batch: 200; loss: 10.7; acc: 0.53
Batch: 220; loss: 5.32; acc: 0.64
Batch: 240; loss: 8.82; acc: 0.61
Batch: 260; loss: 7.87; acc: 0.59
Batch: 280; loss: 7.64; acc: 0.66
Batch: 300; loss: 6.3; acc: 0.58
Batch: 320; loss: 9.03; acc: 0.61
Batch: 340; loss: 7.28; acc: 0.62
Batch: 360; loss: 5.06; acc: 0.77
Batch: 380; loss: 5.92; acc: 0.73
Batch: 400; loss: 10.06; acc: 0.67
Batch: 420; loss: 6.26; acc: 0.73
Batch: 440; loss: 3.4; acc: 0.7
Batch: 460; loss: 6.12; acc: 0.64
Batch: 480; loss: 7.62; acc: 0.61
Batch: 500; loss: 7.27; acc: 0.7
Batch: 520; loss: 4.06; acc: 0.69
Batch: 540; loss: 9.59; acc: 0.55
Batch: 560; loss: 8.64; acc: 0.59
Batch: 580; loss: 6.89; acc: 0.66
Batch: 600; loss: 7.5; acc: 0.56
Batch: 620; loss: 5.86; acc: 0.62
Train Epoch over. train_loss: 6.93; train_accuracy: 0.64 

Batch: 0; loss: 5.04; acc: 0.66
Batch: 20; loss: 13.83; acc: 0.34
Batch: 40; loss: 4.17; acc: 0.73
Batch: 60; loss: 9.25; acc: 0.55
Batch: 80; loss: 6.92; acc: 0.59
Batch: 100; loss: 10.51; acc: 0.56
Batch: 120; loss: 6.52; acc: 0.7
Batch: 140; loss: 8.01; acc: 0.5
Val Epoch over. val_loss: 7.411244075009777; val_accuracy: 0.6299761146496815 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.72; acc: 0.66
Batch: 20; loss: 5.29; acc: 0.7
Batch: 40; loss: 6.6; acc: 0.67
Batch: 60; loss: 7.45; acc: 0.72
Batch: 80; loss: 8.92; acc: 0.66
Batch: 100; loss: 3.85; acc: 0.67
Batch: 120; loss: 6.28; acc: 0.73
Batch: 140; loss: 7.79; acc: 0.61
Batch: 160; loss: 7.94; acc: 0.58
Batch: 180; loss: 6.71; acc: 0.62
Batch: 200; loss: 4.06; acc: 0.69
Batch: 220; loss: 7.89; acc: 0.66
Batch: 240; loss: 8.11; acc: 0.66
Batch: 260; loss: 5.8; acc: 0.69
Batch: 280; loss: 7.73; acc: 0.61
Batch: 300; loss: 6.61; acc: 0.61
Batch: 320; loss: 4.97; acc: 0.64
Batch: 340; loss: 6.82; acc: 0.69
Batch: 360; loss: 2.28; acc: 0.8
Batch: 380; loss: 9.32; acc: 0.52
Batch: 400; loss: 8.86; acc: 0.45
Batch: 420; loss: 6.58; acc: 0.61
Batch: 440; loss: 9.56; acc: 0.66
Batch: 460; loss: 9.52; acc: 0.58
Batch: 480; loss: 9.29; acc: 0.59
Batch: 500; loss: 10.32; acc: 0.53
Batch: 520; loss: 6.66; acc: 0.66
Batch: 540; loss: 6.18; acc: 0.62
Batch: 560; loss: 6.3; acc: 0.64
Batch: 580; loss: 11.02; acc: 0.55
Batch: 600; loss: 7.42; acc: 0.66
Batch: 620; loss: 6.48; acc: 0.69
Train Epoch over. train_loss: 6.91; train_accuracy: 0.64 

Batch: 0; loss: 5.08; acc: 0.66
Batch: 20; loss: 13.66; acc: 0.34
Batch: 40; loss: 4.29; acc: 0.73
Batch: 60; loss: 9.06; acc: 0.56
Batch: 80; loss: 7.05; acc: 0.61
Batch: 100; loss: 10.57; acc: 0.56
Batch: 120; loss: 6.44; acc: 0.7
Batch: 140; loss: 7.97; acc: 0.5
Val Epoch over. val_loss: 7.4079332837633265; val_accuracy: 0.6297770700636943 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.4; acc: 0.66
Batch: 20; loss: 2.83; acc: 0.72
Batch: 40; loss: 3.83; acc: 0.73
Batch: 60; loss: 5.68; acc: 0.62
Batch: 80; loss: 7.72; acc: 0.59
Batch: 100; loss: 7.91; acc: 0.62
Batch: 120; loss: 5.36; acc: 0.7
Batch: 140; loss: 7.9; acc: 0.69
Batch: 160; loss: 9.61; acc: 0.58
Batch: 180; loss: 7.96; acc: 0.59
Batch: 200; loss: 4.73; acc: 0.69
Batch: 220; loss: 7.08; acc: 0.66
Batch: 240; loss: 8.95; acc: 0.58
Batch: 260; loss: 4.75; acc: 0.67
Batch: 280; loss: 9.37; acc: 0.59
Batch: 300; loss: 8.09; acc: 0.62
Batch: 320; loss: 7.06; acc: 0.73
Batch: 340; loss: 6.18; acc: 0.58
Batch: 360; loss: 5.19; acc: 0.7
Batch: 380; loss: 6.74; acc: 0.69
Batch: 400; loss: 7.42; acc: 0.61
Batch: 420; loss: 6.97; acc: 0.67
Batch: 440; loss: 5.79; acc: 0.7
Batch: 460; loss: 5.31; acc: 0.7
Batch: 480; loss: 7.28; acc: 0.69
Batch: 500; loss: 8.62; acc: 0.59
Batch: 520; loss: 7.32; acc: 0.64
Batch: 540; loss: 8.18; acc: 0.64
Batch: 560; loss: 6.01; acc: 0.61
Batch: 580; loss: 4.44; acc: 0.69
Batch: 600; loss: 6.91; acc: 0.67
Batch: 620; loss: 6.97; acc: 0.66
Train Epoch over. train_loss: 6.9; train_accuracy: 0.64 

Batch: 0; loss: 5.07; acc: 0.66
Batch: 20; loss: 13.76; acc: 0.34
Batch: 40; loss: 4.24; acc: 0.73
Batch: 60; loss: 9.06; acc: 0.56
Batch: 80; loss: 6.97; acc: 0.59
Batch: 100; loss: 10.47; acc: 0.56
Batch: 120; loss: 6.52; acc: 0.7
Batch: 140; loss: 7.95; acc: 0.52
Val Epoch over. val_loss: 7.402437216157367; val_accuracy: 0.6319665605095541 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 6.32; acc: 0.66
Batch: 20; loss: 7.4; acc: 0.64
Batch: 40; loss: 5.62; acc: 0.69
Batch: 60; loss: 5.73; acc: 0.64
Batch: 80; loss: 6.26; acc: 0.67
Batch: 100; loss: 6.13; acc: 0.67
Batch: 120; loss: 6.49; acc: 0.69
Batch: 140; loss: 8.09; acc: 0.61
Batch: 160; loss: 7.27; acc: 0.66
Batch: 180; loss: 7.53; acc: 0.66
Batch: 200; loss: 4.33; acc: 0.69
Batch: 220; loss: 6.77; acc: 0.64
Batch: 240; loss: 7.85; acc: 0.56
Batch: 260; loss: 8.11; acc: 0.59
Batch: 280; loss: 9.58; acc: 0.64
Batch: 300; loss: 7.04; acc: 0.66
Batch: 320; loss: 6.23; acc: 0.61
Batch: 340; loss: 8.15; acc: 0.66
Batch: 360; loss: 4.9; acc: 0.72
Batch: 380; loss: 6.41; acc: 0.72
Batch: 400; loss: 8.94; acc: 0.61
Batch: 420; loss: 10.61; acc: 0.61
Batch: 440; loss: 4.19; acc: 0.69
Batch: 460; loss: 7.13; acc: 0.64
Batch: 480; loss: 8.59; acc: 0.66
Batch: 500; loss: 5.1; acc: 0.7
Batch: 520; loss: 6.69; acc: 0.62
Batch: 540; loss: 3.89; acc: 0.78
Batch: 560; loss: 8.41; acc: 0.58
Batch: 580; loss: 8.95; acc: 0.66
Batch: 600; loss: 5.98; acc: 0.66
Batch: 620; loss: 5.5; acc: 0.64
Train Epoch over. train_loss: 6.9; train_accuracy: 0.64 

Batch: 0; loss: 5.03; acc: 0.66
Batch: 20; loss: 13.76; acc: 0.36
Batch: 40; loss: 4.26; acc: 0.73
Batch: 60; loss: 9.08; acc: 0.56
Batch: 80; loss: 6.98; acc: 0.59
Batch: 100; loss: 10.49; acc: 0.56
Batch: 120; loss: 6.49; acc: 0.7
Batch: 140; loss: 7.98; acc: 0.52
Val Epoch over. val_loss: 7.406088576954641; val_accuracy: 0.6316679936305732 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 9.53; acc: 0.52
Batch: 20; loss: 8.56; acc: 0.67
Batch: 40; loss: 7.64; acc: 0.61
Batch: 60; loss: 4.03; acc: 0.81
Batch: 80; loss: 9.7; acc: 0.62
Batch: 100; loss: 6.02; acc: 0.67
Batch: 120; loss: 7.8; acc: 0.69
Batch: 140; loss: 8.01; acc: 0.58
Batch: 160; loss: 5.26; acc: 0.67
Batch: 180; loss: 5.57; acc: 0.77
Batch: 200; loss: 9.06; acc: 0.66
Batch: 220; loss: 7.06; acc: 0.61
Batch: 240; loss: 4.92; acc: 0.73
Batch: 260; loss: 4.96; acc: 0.64
Batch: 280; loss: 4.37; acc: 0.72
Batch: 300; loss: 6.15; acc: 0.67
Batch: 320; loss: 6.8; acc: 0.59
Batch: 340; loss: 6.74; acc: 0.66
Batch: 360; loss: 7.28; acc: 0.56
Batch: 380; loss: 6.77; acc: 0.66
Batch: 400; loss: 6.71; acc: 0.62
Batch: 420; loss: 7.4; acc: 0.59
Batch: 440; loss: 4.01; acc: 0.72
Batch: 460; loss: 8.47; acc: 0.61
Batch: 480; loss: 8.88; acc: 0.66
Batch: 500; loss: 6.29; acc: 0.64
Batch: 520; loss: 5.19; acc: 0.62
Batch: 540; loss: 6.22; acc: 0.7
Batch: 560; loss: 5.25; acc: 0.69
Batch: 580; loss: 7.22; acc: 0.64
Batch: 600; loss: 5.23; acc: 0.67
Batch: 620; loss: 4.82; acc: 0.69
Train Epoch over. train_loss: 6.9; train_accuracy: 0.64 

Batch: 0; loss: 5.02; acc: 0.66
Batch: 20; loss: 13.81; acc: 0.36
Batch: 40; loss: 4.24; acc: 0.73
Batch: 60; loss: 9.07; acc: 0.56
Batch: 80; loss: 7.0; acc: 0.59
Batch: 100; loss: 10.45; acc: 0.56
Batch: 120; loss: 6.46; acc: 0.7
Batch: 140; loss: 7.99; acc: 0.52
Val Epoch over. val_loss: 7.404616919292766; val_accuracy: 0.6324641719745223 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.16; acc: 0.67
Batch: 20; loss: 7.5; acc: 0.67
Batch: 40; loss: 7.81; acc: 0.66
Batch: 60; loss: 8.33; acc: 0.56
Batch: 80; loss: 6.57; acc: 0.62
Batch: 100; loss: 5.08; acc: 0.62
Batch: 120; loss: 4.72; acc: 0.66
Batch: 140; loss: 5.71; acc: 0.61
Batch: 160; loss: 7.69; acc: 0.58
Batch: 180; loss: 7.41; acc: 0.59
Batch: 200; loss: 6.74; acc: 0.66
Batch: 220; loss: 5.2; acc: 0.72
Batch: 240; loss: 5.07; acc: 0.73
Batch: 260; loss: 7.07; acc: 0.62
Batch: 280; loss: 8.75; acc: 0.53
Batch: 300; loss: 7.77; acc: 0.59
Batch: 320; loss: 9.29; acc: 0.62
Batch: 340; loss: 7.19; acc: 0.66
Batch: 360; loss: 7.68; acc: 0.64
Batch: 380; loss: 5.2; acc: 0.69
Batch: 400; loss: 5.61; acc: 0.69
Batch: 420; loss: 6.36; acc: 0.56
Batch: 440; loss: 5.81; acc: 0.66
Batch: 460; loss: 12.74; acc: 0.45
Batch: 480; loss: 6.12; acc: 0.77
Batch: 500; loss: 7.29; acc: 0.58
Batch: 520; loss: 6.54; acc: 0.7
Batch: 540; loss: 7.14; acc: 0.64
Batch: 560; loss: 9.41; acc: 0.48
Batch: 580; loss: 10.38; acc: 0.56
Batch: 600; loss: 7.82; acc: 0.58
Batch: 620; loss: 3.17; acc: 0.7
Train Epoch over. train_loss: 6.9; train_accuracy: 0.64 

Batch: 0; loss: 5.08; acc: 0.66
Batch: 20; loss: 13.83; acc: 0.36
Batch: 40; loss: 4.33; acc: 0.73
Batch: 60; loss: 9.16; acc: 0.55
Batch: 80; loss: 7.16; acc: 0.62
Batch: 100; loss: 10.52; acc: 0.56
Batch: 120; loss: 6.44; acc: 0.69
Batch: 140; loss: 8.03; acc: 0.48
Val Epoch over. val_loss: 7.411176754410859; val_accuracy: 0.630672770700637 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 9.74; acc: 0.5
Batch: 20; loss: 5.22; acc: 0.64
Batch: 40; loss: 7.55; acc: 0.56
Batch: 60; loss: 7.9; acc: 0.53
Batch: 80; loss: 8.96; acc: 0.62
Batch: 100; loss: 7.36; acc: 0.62
Batch: 120; loss: 9.13; acc: 0.53
Batch: 140; loss: 5.87; acc: 0.7
Batch: 160; loss: 6.36; acc: 0.64
Batch: 180; loss: 6.18; acc: 0.67
Batch: 200; loss: 4.93; acc: 0.62
Batch: 220; loss: 6.83; acc: 0.64
Batch: 240; loss: 9.64; acc: 0.62
Batch: 260; loss: 7.97; acc: 0.59
Batch: 280; loss: 8.09; acc: 0.61
Batch: 300; loss: 7.23; acc: 0.66
Batch: 320; loss: 8.41; acc: 0.55
Batch: 340; loss: 6.74; acc: 0.7
Batch: 360; loss: 6.82; acc: 0.55
Batch: 380; loss: 6.93; acc: 0.67
Batch: 400; loss: 8.35; acc: 0.59
Batch: 420; loss: 8.74; acc: 0.52
Batch: 440; loss: 5.24; acc: 0.66
Batch: 460; loss: 5.73; acc: 0.61
Batch: 480; loss: 8.07; acc: 0.66
Batch: 500; loss: 8.17; acc: 0.66
Batch: 520; loss: 7.31; acc: 0.69
Batch: 540; loss: 5.11; acc: 0.73
Batch: 560; loss: 6.5; acc: 0.62
Batch: 580; loss: 4.34; acc: 0.75
Batch: 600; loss: 6.5; acc: 0.67
Batch: 620; loss: 6.98; acc: 0.66
Train Epoch over. train_loss: 6.9; train_accuracy: 0.64 

Batch: 0; loss: 5.06; acc: 0.67
Batch: 20; loss: 13.84; acc: 0.34
Batch: 40; loss: 4.37; acc: 0.73
Batch: 60; loss: 9.08; acc: 0.56
Batch: 80; loss: 7.09; acc: 0.59
Batch: 100; loss: 10.52; acc: 0.56
Batch: 120; loss: 6.49; acc: 0.72
Batch: 140; loss: 7.97; acc: 0.5
Val Epoch over. val_loss: 7.417539932165935; val_accuracy: 0.6323646496815286 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 6.88; acc: 0.62
Batch: 20; loss: 7.28; acc: 0.58
Batch: 40; loss: 7.35; acc: 0.66
Batch: 60; loss: 7.06; acc: 0.55
Batch: 80; loss: 6.55; acc: 0.61
Batch: 100; loss: 5.95; acc: 0.64
Batch: 120; loss: 7.81; acc: 0.64
Batch: 140; loss: 7.65; acc: 0.66
Batch: 160; loss: 5.31; acc: 0.7
Batch: 180; loss: 8.75; acc: 0.61
Batch: 200; loss: 6.59; acc: 0.58
Batch: 220; loss: 5.87; acc: 0.64
Batch: 240; loss: 5.16; acc: 0.67
Batch: 260; loss: 7.24; acc: 0.64
Batch: 280; loss: 6.59; acc: 0.64
Batch: 300; loss: 6.25; acc: 0.69
Batch: 320; loss: 6.73; acc: 0.67
Batch: 340; loss: 5.43; acc: 0.61
Batch: 360; loss: 5.33; acc: 0.62
Batch: 380; loss: 6.8; acc: 0.69
Batch: 400; loss: 7.49; acc: 0.62
Batch: 420; loss: 7.11; acc: 0.61
Batch: 440; loss: 6.82; acc: 0.64
Batch: 460; loss: 7.49; acc: 0.58
Batch: 480; loss: 5.99; acc: 0.66
Batch: 500; loss: 7.29; acc: 0.62
Batch: 520; loss: 8.46; acc: 0.61
Batch: 540; loss: 4.49; acc: 0.69
Batch: 560; loss: 5.55; acc: 0.73
Batch: 580; loss: 6.35; acc: 0.67
Batch: 600; loss: 8.72; acc: 0.66
Batch: 620; loss: 6.54; acc: 0.58
Train Epoch over. train_loss: 6.9; train_accuracy: 0.65 

Batch: 0; loss: 5.04; acc: 0.67
Batch: 20; loss: 13.9; acc: 0.34
Batch: 40; loss: 4.32; acc: 0.73
Batch: 60; loss: 9.11; acc: 0.56
Batch: 80; loss: 7.07; acc: 0.59
Batch: 100; loss: 10.49; acc: 0.58
Batch: 120; loss: 6.48; acc: 0.72
Batch: 140; loss: 7.99; acc: 0.5
Val Epoch over. val_loss: 7.414069005638171; val_accuracy: 0.6327627388535032 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 7.35; acc: 0.62
Batch: 20; loss: 5.2; acc: 0.72
Batch: 40; loss: 7.67; acc: 0.59
Batch: 60; loss: 8.69; acc: 0.58
Batch: 80; loss: 8.07; acc: 0.62
Batch: 100; loss: 9.71; acc: 0.58
Batch: 120; loss: 8.64; acc: 0.62
Batch: 140; loss: 8.54; acc: 0.58
Batch: 160; loss: 6.31; acc: 0.64
Batch: 180; loss: 4.22; acc: 0.75
Batch: 200; loss: 6.25; acc: 0.73
Batch: 220; loss: 4.04; acc: 0.7
Batch: 240; loss: 7.07; acc: 0.69
Batch: 260; loss: 6.29; acc: 0.73
Batch: 280; loss: 11.84; acc: 0.64
Batch: 300; loss: 4.84; acc: 0.72
Batch: 320; loss: 9.12; acc: 0.58
Batch: 340; loss: 4.78; acc: 0.73
Batch: 360; loss: 7.14; acc: 0.61
Batch: 380; loss: 6.48; acc: 0.64
Batch: 400; loss: 4.92; acc: 0.73
Batch: 420; loss: 5.64; acc: 0.67
Batch: 440; loss: 5.45; acc: 0.7
Batch: 460; loss: 4.42; acc: 0.7
Batch: 480; loss: 6.63; acc: 0.73
Batch: 500; loss: 5.97; acc: 0.7
Batch: 520; loss: 7.25; acc: 0.55
Batch: 540; loss: 4.76; acc: 0.75
Batch: 560; loss: 9.47; acc: 0.59
Batch: 580; loss: 6.62; acc: 0.7
Batch: 600; loss: 4.96; acc: 0.7
Batch: 620; loss: 4.34; acc: 0.69
Train Epoch over. train_loss: 6.9; train_accuracy: 0.65 

Batch: 0; loss: 4.99; acc: 0.67
Batch: 20; loss: 13.86; acc: 0.36
Batch: 40; loss: 4.3; acc: 0.73
Batch: 60; loss: 9.23; acc: 0.55
Batch: 80; loss: 7.08; acc: 0.59
Batch: 100; loss: 10.52; acc: 0.56
Batch: 120; loss: 6.46; acc: 0.72
Batch: 140; loss: 8.03; acc: 0.5
Val Epoch over. val_loss: 7.411474408617445; val_accuracy: 0.6321656050955414 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 6.37; acc: 0.67
Batch: 20; loss: 5.62; acc: 0.62
Batch: 40; loss: 3.99; acc: 0.69
Batch: 60; loss: 6.81; acc: 0.69
Batch: 80; loss: 7.32; acc: 0.66
Batch: 100; loss: 6.43; acc: 0.61
Batch: 120; loss: 7.19; acc: 0.56
Batch: 140; loss: 6.86; acc: 0.67
Batch: 160; loss: 5.14; acc: 0.66
Batch: 180; loss: 7.9; acc: 0.62
Batch: 200; loss: 8.56; acc: 0.66
Batch: 220; loss: 5.97; acc: 0.67
Batch: 240; loss: 7.85; acc: 0.58
Batch: 260; loss: 6.85; acc: 0.61
Batch: 280; loss: 7.26; acc: 0.75
Batch: 300; loss: 4.22; acc: 0.62
Batch: 320; loss: 9.58; acc: 0.64
Batch: 340; loss: 4.9; acc: 0.73
Batch: 360; loss: 8.14; acc: 0.59
Batch: 380; loss: 5.18; acc: 0.69
Batch: 400; loss: 5.18; acc: 0.73
Batch: 420; loss: 8.28; acc: 0.69
Batch: 440; loss: 9.86; acc: 0.53
Batch: 460; loss: 7.0; acc: 0.58
Batch: 480; loss: 8.46; acc: 0.56
Batch: 500; loss: 9.47; acc: 0.58
Batch: 520; loss: 8.96; acc: 0.59
Batch: 540; loss: 5.75; acc: 0.62
Batch: 560; loss: 6.92; acc: 0.61
Batch: 580; loss: 8.91; acc: 0.55
Batch: 600; loss: 5.77; acc: 0.69
Batch: 620; loss: 5.27; acc: 0.77
Train Epoch over. train_loss: 6.9; train_accuracy: 0.65 

Batch: 0; loss: 5.08; acc: 0.66
Batch: 20; loss: 13.69; acc: 0.36
Batch: 40; loss: 4.31; acc: 0.73
Batch: 60; loss: 8.89; acc: 0.56
Batch: 80; loss: 7.02; acc: 0.59
Batch: 100; loss: 10.48; acc: 0.56
Batch: 120; loss: 6.47; acc: 0.72
Batch: 140; loss: 7.95; acc: 0.52
Val Epoch over. val_loss: 7.407818074438982; val_accuracy: 0.6332603503184714 

plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_200_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 178798
elements in E: 79684000
fraction nonzero: 0.002243838160734903
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 36.37; acc: 0.12
Batch: 20; loss: 12.69; acc: 0.36
Batch: 40; loss: 6.31; acc: 0.58
Batch: 60; loss: 6.81; acc: 0.56
Batch: 80; loss: 5.89; acc: 0.61
Batch: 100; loss: 4.35; acc: 0.69
Batch: 120; loss: 8.29; acc: 0.59
Batch: 140; loss: 6.04; acc: 0.62
Batch: 160; loss: 3.92; acc: 0.73
Batch: 180; loss: 7.27; acc: 0.72
Batch: 200; loss: 4.9; acc: 0.72
Batch: 220; loss: 4.68; acc: 0.64
Batch: 240; loss: 3.84; acc: 0.69
Batch: 260; loss: 7.06; acc: 0.62
Batch: 280; loss: 6.67; acc: 0.67
Batch: 300; loss: 10.56; acc: 0.58
Batch: 320; loss: 3.29; acc: 0.8
Batch: 340; loss: 4.7; acc: 0.77
Batch: 360; loss: 6.02; acc: 0.72
Batch: 380; loss: 4.21; acc: 0.72
Batch: 400; loss: 7.35; acc: 0.58
Batch: 420; loss: 9.61; acc: 0.56
Batch: 440; loss: 6.78; acc: 0.59
Batch: 460; loss: 5.76; acc: 0.66
Batch: 480; loss: 6.24; acc: 0.7
Batch: 500; loss: 4.33; acc: 0.7
Batch: 520; loss: 6.36; acc: 0.69
Batch: 540; loss: 4.65; acc: 0.69
Batch: 560; loss: 8.27; acc: 0.55
Batch: 580; loss: 4.55; acc: 0.69
Batch: 600; loss: 3.05; acc: 0.72
Batch: 620; loss: 7.47; acc: 0.67
Train Epoch over. train_loss: 6.2; train_accuracy: 0.65 

Batch: 0; loss: 4.94; acc: 0.62
Batch: 20; loss: 9.82; acc: 0.56
Batch: 40; loss: 2.84; acc: 0.84
Batch: 60; loss: 6.49; acc: 0.62
Batch: 80; loss: 8.36; acc: 0.67
Batch: 100; loss: 9.28; acc: 0.64
Batch: 120; loss: 3.55; acc: 0.75
Batch: 140; loss: 8.95; acc: 0.61
Val Epoch over. val_loss: 5.680966488115347; val_accuracy: 0.6905851910828026 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 4.82; acc: 0.72
Batch: 20; loss: 5.38; acc: 0.7
Batch: 40; loss: 3.02; acc: 0.73
Batch: 60; loss: 4.93; acc: 0.7
Batch: 80; loss: 6.39; acc: 0.62
Batch: 100; loss: 7.85; acc: 0.62
Batch: 120; loss: 3.19; acc: 0.83
Batch: 140; loss: 7.8; acc: 0.58
Batch: 160; loss: 4.44; acc: 0.67
Batch: 180; loss: 8.43; acc: 0.62
Batch: 200; loss: 6.09; acc: 0.67
Batch: 220; loss: 5.91; acc: 0.7
Batch: 240; loss: 6.98; acc: 0.64
Batch: 260; loss: 5.43; acc: 0.66
Batch: 280; loss: 3.86; acc: 0.8
Batch: 300; loss: 5.62; acc: 0.62
Batch: 320; loss: 3.18; acc: 0.81
Batch: 340; loss: 6.42; acc: 0.67
Batch: 360; loss: 5.89; acc: 0.67
Batch: 380; loss: 6.44; acc: 0.69
Batch: 400; loss: 4.34; acc: 0.72
Batch: 420; loss: 6.91; acc: 0.61
Batch: 440; loss: 4.13; acc: 0.75
Batch: 460; loss: 6.12; acc: 0.69
Batch: 480; loss: 5.11; acc: 0.75
Batch: 500; loss: 4.4; acc: 0.72
Batch: 520; loss: 2.75; acc: 0.78
Batch: 540; loss: 3.87; acc: 0.66
Batch: 560; loss: 4.57; acc: 0.75
Batch: 580; loss: 8.97; acc: 0.59
Batch: 600; loss: 12.39; acc: 0.48
Batch: 620; loss: 10.13; acc: 0.59
Train Epoch over. train_loss: 5.49; train_accuracy: 0.7 

Batch: 0; loss: 2.94; acc: 0.81
Batch: 20; loss: 5.93; acc: 0.64
Batch: 40; loss: 2.61; acc: 0.83
Batch: 60; loss: 4.1; acc: 0.7
Batch: 80; loss: 6.99; acc: 0.72
Batch: 100; loss: 7.39; acc: 0.64
Batch: 120; loss: 3.82; acc: 0.83
Batch: 140; loss: 7.6; acc: 0.64
Val Epoch over. val_loss: 5.025158009711345; val_accuracy: 0.7219347133757962 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 3.93; acc: 0.75
Batch: 20; loss: 9.31; acc: 0.64
Batch: 40; loss: 6.39; acc: 0.75
Batch: 60; loss: 4.14; acc: 0.66
Batch: 80; loss: 4.54; acc: 0.69
Batch: 100; loss: 4.44; acc: 0.7
Batch: 120; loss: 5.08; acc: 0.7
Batch: 140; loss: 5.28; acc: 0.69
Batch: 160; loss: 3.83; acc: 0.7
Batch: 180; loss: 4.26; acc: 0.8
Batch: 200; loss: 6.66; acc: 0.62
Batch: 220; loss: 4.56; acc: 0.72
Batch: 240; loss: 5.83; acc: 0.67
Batch: 260; loss: 3.0; acc: 0.78
Batch: 280; loss: 6.04; acc: 0.77
Batch: 300; loss: 5.83; acc: 0.72
Batch: 320; loss: 4.9; acc: 0.75
Batch: 340; loss: 4.72; acc: 0.69
Batch: 360; loss: 6.84; acc: 0.66
Batch: 380; loss: 6.4; acc: 0.75
Batch: 400; loss: 6.76; acc: 0.73
Batch: 420; loss: 4.01; acc: 0.78
Batch: 440; loss: 4.03; acc: 0.8
Batch: 460; loss: 4.41; acc: 0.73
Batch: 480; loss: 3.08; acc: 0.77
Batch: 500; loss: 8.78; acc: 0.62
Batch: 520; loss: 7.06; acc: 0.56
Batch: 540; loss: 2.89; acc: 0.84
Batch: 560; loss: 4.89; acc: 0.64
Batch: 580; loss: 4.42; acc: 0.78
Batch: 600; loss: 5.29; acc: 0.72
Batch: 620; loss: 4.84; acc: 0.66
Train Epoch over. train_loss: 5.43; train_accuracy: 0.71 

Batch: 0; loss: 5.25; acc: 0.75
Batch: 20; loss: 8.32; acc: 0.55
Batch: 40; loss: 3.49; acc: 0.78
Batch: 60; loss: 4.93; acc: 0.72
Batch: 80; loss: 8.03; acc: 0.61
Batch: 100; loss: 8.01; acc: 0.62
Batch: 120; loss: 4.21; acc: 0.81
Batch: 140; loss: 7.32; acc: 0.66
Val Epoch over. val_loss: 5.3462433970657885; val_accuracy: 0.7051154458598726 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 5.5; acc: 0.72
Batch: 20; loss: 6.62; acc: 0.67
Batch: 40; loss: 3.78; acc: 0.77
Batch: 60; loss: 3.91; acc: 0.77
Batch: 80; loss: 3.74; acc: 0.81
Batch: 100; loss: 5.88; acc: 0.67
Batch: 120; loss: 5.44; acc: 0.69
Batch: 140; loss: 5.83; acc: 0.73
Batch: 160; loss: 3.15; acc: 0.8
Batch: 180; loss: 4.41; acc: 0.8
Batch: 200; loss: 3.28; acc: 0.75
Batch: 220; loss: 3.52; acc: 0.75
Batch: 240; loss: 2.27; acc: 0.83
Batch: 260; loss: 5.84; acc: 0.69
Batch: 280; loss: 3.37; acc: 0.8
Batch: 300; loss: 3.72; acc: 0.78
Batch: 320; loss: 7.88; acc: 0.62
Batch: 340; loss: 3.76; acc: 0.72
Batch: 360; loss: 3.81; acc: 0.78
Batch: 380; loss: 6.04; acc: 0.72
Batch: 400; loss: 4.84; acc: 0.78
Batch: 420; loss: 3.57; acc: 0.8
Batch: 440; loss: 7.19; acc: 0.64
Batch: 460; loss: 4.42; acc: 0.64
Batch: 480; loss: 4.64; acc: 0.78
Batch: 500; loss: 5.41; acc: 0.73
Batch: 520; loss: 10.18; acc: 0.64
Batch: 540; loss: 3.01; acc: 0.8
Batch: 560; loss: 6.61; acc: 0.73
Batch: 580; loss: 2.85; acc: 0.8
Batch: 600; loss: 5.86; acc: 0.78
Batch: 620; loss: 4.16; acc: 0.78
Train Epoch over. train_loss: 5.4; train_accuracy: 0.72 

Batch: 0; loss: 3.52; acc: 0.77
Batch: 20; loss: 6.72; acc: 0.66
Batch: 40; loss: 2.62; acc: 0.81
Batch: 60; loss: 5.71; acc: 0.64
Batch: 80; loss: 8.13; acc: 0.67
Batch: 100; loss: 9.04; acc: 0.58
Batch: 120; loss: 4.47; acc: 0.8
Batch: 140; loss: 9.32; acc: 0.62
Val Epoch over. val_loss: 5.634404777721235; val_accuracy: 0.6992436305732485 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 4.01; acc: 0.69
Batch: 20; loss: 4.01; acc: 0.73
Batch: 40; loss: 6.94; acc: 0.69
Batch: 60; loss: 8.91; acc: 0.64
Batch: 80; loss: 4.77; acc: 0.7
Batch: 100; loss: 5.03; acc: 0.66
Batch: 120; loss: 4.46; acc: 0.77
Batch: 140; loss: 6.11; acc: 0.69
Batch: 160; loss: 4.18; acc: 0.73
Batch: 180; loss: 5.29; acc: 0.73
Batch: 200; loss: 6.09; acc: 0.64
Batch: 220; loss: 8.17; acc: 0.66
Batch: 240; loss: 7.18; acc: 0.67
Batch: 260; loss: 3.68; acc: 0.8
Batch: 280; loss: 4.66; acc: 0.73
Batch: 300; loss: 5.5; acc: 0.72
Batch: 320; loss: 5.03; acc: 0.73
Batch: 340; loss: 2.03; acc: 0.75
Batch: 360; loss: 5.42; acc: 0.7
Batch: 380; loss: 4.24; acc: 0.73
Batch: 400; loss: 5.06; acc: 0.78
Batch: 420; loss: 4.81; acc: 0.75
Batch: 440; loss: 6.43; acc: 0.66
Batch: 460; loss: 3.32; acc: 0.77
Batch: 480; loss: 3.48; acc: 0.75
Batch: 500; loss: 3.08; acc: 0.72
Batch: 520; loss: 5.87; acc: 0.75
Batch: 540; loss: 7.24; acc: 0.69
Batch: 560; loss: 7.16; acc: 0.67
Batch: 580; loss: 5.45; acc: 0.73
Batch: 600; loss: 5.54; acc: 0.75
Batch: 620; loss: 1.32; acc: 0.81
Train Epoch over. train_loss: 5.37; train_accuracy: 0.72 

Batch: 0; loss: 3.59; acc: 0.83
Batch: 20; loss: 6.83; acc: 0.64
Batch: 40; loss: 3.47; acc: 0.83
Batch: 60; loss: 5.65; acc: 0.72
Batch: 80; loss: 4.97; acc: 0.73
Batch: 100; loss: 7.16; acc: 0.61
Batch: 120; loss: 5.22; acc: 0.78
Batch: 140; loss: 7.01; acc: 0.62
Val Epoch over. val_loss: 5.342855101178406; val_accuracy: 0.7146695859872612 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 5.76; acc: 0.77
Batch: 20; loss: 5.67; acc: 0.7
Batch: 40; loss: 3.94; acc: 0.73
Batch: 60; loss: 4.03; acc: 0.78
Batch: 80; loss: 4.93; acc: 0.77
Batch: 100; loss: 4.86; acc: 0.81
Batch: 120; loss: 3.85; acc: 0.77
Batch: 140; loss: 6.28; acc: 0.7
Batch: 160; loss: 6.1; acc: 0.66
Batch: 180; loss: 6.48; acc: 0.7
Batch: 200; loss: 4.69; acc: 0.75
Batch: 220; loss: 6.29; acc: 0.64
Batch: 240; loss: 4.02; acc: 0.77
Batch: 260; loss: 4.47; acc: 0.77
Batch: 280; loss: 4.95; acc: 0.69
Batch: 300; loss: 4.33; acc: 0.78
Batch: 320; loss: 4.55; acc: 0.78
Batch: 340; loss: 7.53; acc: 0.75
Batch: 360; loss: 3.71; acc: 0.81
Batch: 380; loss: 3.62; acc: 0.77
Batch: 400; loss: 6.97; acc: 0.67
Batch: 420; loss: 6.93; acc: 0.66
Batch: 440; loss: 3.55; acc: 0.81
Batch: 460; loss: 7.57; acc: 0.61
Batch: 480; loss: 5.05; acc: 0.78
Batch: 500; loss: 4.0; acc: 0.75
Batch: 520; loss: 4.79; acc: 0.75
Batch: 540; loss: 6.08; acc: 0.67
Batch: 560; loss: 6.02; acc: 0.58
Batch: 580; loss: 5.74; acc: 0.7
Batch: 600; loss: 4.99; acc: 0.8
Batch: 620; loss: 5.05; acc: 0.67
Train Epoch over. train_loss: 5.36; train_accuracy: 0.72 

Batch: 0; loss: 3.98; acc: 0.78
Batch: 20; loss: 6.63; acc: 0.58
Batch: 40; loss: 2.91; acc: 0.81
Batch: 60; loss: 3.84; acc: 0.7
Batch: 80; loss: 6.81; acc: 0.61
Batch: 100; loss: 7.77; acc: 0.7
Batch: 120; loss: 4.86; acc: 0.77
Batch: 140; loss: 7.6; acc: 0.64
Val Epoch over. val_loss: 5.1240806875714835; val_accuracy: 0.7192476114649682 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 4.19; acc: 0.7
Batch: 20; loss: 6.08; acc: 0.69
Batch: 40; loss: 7.6; acc: 0.75
Batch: 60; loss: 2.17; acc: 0.84
Batch: 80; loss: 9.98; acc: 0.61
Batch: 100; loss: 7.23; acc: 0.67
Batch: 120; loss: 6.27; acc: 0.7
Batch: 140; loss: 4.54; acc: 0.75
Batch: 160; loss: 6.08; acc: 0.8
Batch: 180; loss: 2.91; acc: 0.8
Batch: 200; loss: 3.6; acc: 0.83
Batch: 220; loss: 5.95; acc: 0.72
Batch: 240; loss: 7.51; acc: 0.62
Batch: 260; loss: 4.79; acc: 0.75
Batch: 280; loss: 5.64; acc: 0.72
Batch: 300; loss: 4.86; acc: 0.67
Batch: 320; loss: 7.59; acc: 0.69
Batch: 340; loss: 2.86; acc: 0.81
Batch: 360; loss: 8.24; acc: 0.73
Batch: 380; loss: 7.98; acc: 0.64
Batch: 400; loss: 3.28; acc: 0.75
Batch: 420; loss: 2.65; acc: 0.83
Batch: 440; loss: 7.77; acc: 0.67
Batch: 460; loss: 3.65; acc: 0.73
Batch: 480; loss: 3.01; acc: 0.8
Batch: 500; loss: 2.46; acc: 0.77
Batch: 520; loss: 4.95; acc: 0.69
Batch: 540; loss: 2.86; acc: 0.73
Batch: 560; loss: 5.59; acc: 0.72
Batch: 580; loss: 5.21; acc: 0.72
Batch: 600; loss: 5.77; acc: 0.75
Batch: 620; loss: 6.42; acc: 0.72
Train Epoch over. train_loss: 5.41; train_accuracy: 0.72 

Batch: 0; loss: 4.28; acc: 0.73
Batch: 20; loss: 7.16; acc: 0.59
Batch: 40; loss: 4.64; acc: 0.73
Batch: 60; loss: 4.91; acc: 0.69
Batch: 80; loss: 7.38; acc: 0.67
Batch: 100; loss: 8.55; acc: 0.69
Batch: 120; loss: 3.51; acc: 0.8
Batch: 140; loss: 7.19; acc: 0.7
Val Epoch over. val_loss: 5.257783297520534; val_accuracy: 0.7146695859872612 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 4.76; acc: 0.72
Batch: 20; loss: 3.61; acc: 0.73
Batch: 40; loss: 4.51; acc: 0.75
Batch: 60; loss: 3.38; acc: 0.73
Batch: 80; loss: 4.41; acc: 0.73
Batch: 100; loss: 5.8; acc: 0.73
Batch: 120; loss: 6.02; acc: 0.69
Batch: 140; loss: 6.14; acc: 0.75
Batch: 160; loss: 4.0; acc: 0.81
Batch: 180; loss: 3.98; acc: 0.72
Batch: 200; loss: 2.58; acc: 0.84
Batch: 220; loss: 6.26; acc: 0.77
Batch: 240; loss: 4.78; acc: 0.77
Batch: 260; loss: 4.49; acc: 0.81
Batch: 280; loss: 4.55; acc: 0.75
Batch: 300; loss: 5.1; acc: 0.73
Batch: 320; loss: 6.83; acc: 0.62
Batch: 340; loss: 5.11; acc: 0.69
Batch: 360; loss: 7.25; acc: 0.72
Batch: 380; loss: 6.35; acc: 0.64
Batch: 400; loss: 7.01; acc: 0.73
Batch: 420; loss: 5.07; acc: 0.73
Batch: 440; loss: 3.08; acc: 0.78
Batch: 460; loss: 5.02; acc: 0.77
Batch: 480; loss: 3.71; acc: 0.75
Batch: 500; loss: 5.33; acc: 0.8
Batch: 520; loss: 4.9; acc: 0.73
Batch: 540; loss: 5.05; acc: 0.67
Batch: 560; loss: 7.55; acc: 0.58
Batch: 580; loss: 4.7; acc: 0.7
Batch: 600; loss: 5.38; acc: 0.72
Batch: 620; loss: 4.23; acc: 0.73
Train Epoch over. train_loss: 5.38; train_accuracy: 0.72 

Batch: 0; loss: 4.64; acc: 0.73
Batch: 20; loss: 10.31; acc: 0.61
Batch: 40; loss: 4.66; acc: 0.77
Batch: 60; loss: 7.34; acc: 0.62
Batch: 80; loss: 11.14; acc: 0.55
Batch: 100; loss: 6.66; acc: 0.66
Batch: 120; loss: 6.08; acc: 0.78
Batch: 140; loss: 10.09; acc: 0.62
Val Epoch over. val_loss: 7.539145281360407; val_accuracy: 0.6685907643312102 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 6.82; acc: 0.64
Batch: 20; loss: 4.09; acc: 0.75
Batch: 40; loss: 5.02; acc: 0.73
Batch: 60; loss: 3.23; acc: 0.8
Batch: 80; loss: 4.47; acc: 0.75
Batch: 100; loss: 4.52; acc: 0.73
Batch: 120; loss: 3.29; acc: 0.81
Batch: 140; loss: 4.38; acc: 0.73
Batch: 160; loss: 4.85; acc: 0.73
Batch: 180; loss: 2.22; acc: 0.83
Batch: 200; loss: 3.39; acc: 0.81
Batch: 220; loss: 4.76; acc: 0.77
Batch: 240; loss: 2.24; acc: 0.77
Batch: 260; loss: 2.99; acc: 0.78
Batch: 280; loss: 5.14; acc: 0.8
Batch: 300; loss: 3.34; acc: 0.8
Batch: 320; loss: 5.88; acc: 0.7
Batch: 340; loss: 6.51; acc: 0.66
Batch: 360; loss: 4.62; acc: 0.72
Batch: 380; loss: 6.56; acc: 0.67
Batch: 400; loss: 3.57; acc: 0.78
Batch: 420; loss: 6.45; acc: 0.64
Batch: 440; loss: 6.71; acc: 0.66
Batch: 460; loss: 5.79; acc: 0.67
Batch: 480; loss: 5.75; acc: 0.7
Batch: 500; loss: 4.62; acc: 0.77
Batch: 520; loss: 3.84; acc: 0.73
Batch: 540; loss: 4.66; acc: 0.73
Batch: 560; loss: 6.32; acc: 0.7
Batch: 580; loss: 1.92; acc: 0.86
Batch: 600; loss: 6.24; acc: 0.67
Batch: 620; loss: 7.41; acc: 0.66
Train Epoch over. train_loss: 5.44; train_accuracy: 0.72 

Batch: 0; loss: 3.33; acc: 0.75
Batch: 20; loss: 8.36; acc: 0.66
Batch: 40; loss: 2.6; acc: 0.81
Batch: 60; loss: 7.88; acc: 0.67
Batch: 80; loss: 6.66; acc: 0.72
Batch: 100; loss: 7.34; acc: 0.62
Batch: 120; loss: 4.77; acc: 0.78
Batch: 140; loss: 7.16; acc: 0.64
Val Epoch over. val_loss: 5.505961557102811; val_accuracy: 0.7064092356687898 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 4.54; acc: 0.7
Batch: 20; loss: 5.44; acc: 0.66
Batch: 40; loss: 5.36; acc: 0.67
Batch: 60; loss: 4.75; acc: 0.73
Batch: 80; loss: 3.6; acc: 0.81
Batch: 100; loss: 3.9; acc: 0.78
Batch: 120; loss: 7.52; acc: 0.66
Batch: 140; loss: 5.39; acc: 0.72
Batch: 160; loss: 7.18; acc: 0.64
Batch: 180; loss: 8.42; acc: 0.62
Batch: 200; loss: 5.15; acc: 0.75
Batch: 220; loss: 5.8; acc: 0.73
Batch: 240; loss: 4.48; acc: 0.75
Batch: 260; loss: 3.77; acc: 0.7
Batch: 280; loss: 7.7; acc: 0.69
Batch: 300; loss: 4.81; acc: 0.69
Batch: 320; loss: 3.71; acc: 0.81
Batch: 340; loss: 5.28; acc: 0.7
Batch: 360; loss: 5.33; acc: 0.73
Batch: 380; loss: 3.59; acc: 0.77
Batch: 400; loss: 8.01; acc: 0.64
Batch: 420; loss: 5.01; acc: 0.7
Batch: 440; loss: 5.1; acc: 0.73
Batch: 460; loss: 6.3; acc: 0.75
Batch: 480; loss: 6.77; acc: 0.75
Batch: 500; loss: 4.89; acc: 0.7
Batch: 520; loss: 6.78; acc: 0.67
Batch: 540; loss: 3.23; acc: 0.81
Batch: 560; loss: 3.7; acc: 0.77
Batch: 580; loss: 5.13; acc: 0.75
Batch: 600; loss: 2.5; acc: 0.89
Batch: 620; loss: 6.66; acc: 0.67
Train Epoch over. train_loss: 5.31; train_accuracy: 0.72 

Batch: 0; loss: 4.37; acc: 0.73
Batch: 20; loss: 7.78; acc: 0.66
Batch: 40; loss: 2.87; acc: 0.83
Batch: 60; loss: 7.42; acc: 0.62
Batch: 80; loss: 7.57; acc: 0.72
Batch: 100; loss: 6.83; acc: 0.72
Batch: 120; loss: 4.9; acc: 0.75
Batch: 140; loss: 6.37; acc: 0.62
Val Epoch over. val_loss: 6.051222577216519; val_accuracy: 0.700437898089172 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 6.84; acc: 0.55
Batch: 20; loss: 5.02; acc: 0.73
Batch: 40; loss: 3.38; acc: 0.8
Batch: 60; loss: 2.69; acc: 0.77
Batch: 80; loss: 3.87; acc: 0.77
Batch: 100; loss: 5.33; acc: 0.7
Batch: 120; loss: 4.27; acc: 0.77
Batch: 140; loss: 2.44; acc: 0.83
Batch: 160; loss: 3.83; acc: 0.75
Batch: 180; loss: 4.93; acc: 0.75
Batch: 200; loss: 2.38; acc: 0.8
Batch: 220; loss: 6.03; acc: 0.73
Batch: 240; loss: 4.74; acc: 0.7
Batch: 260; loss: 2.34; acc: 0.81
Batch: 280; loss: 2.61; acc: 0.8
Batch: 300; loss: 3.12; acc: 0.84
Batch: 320; loss: 3.2; acc: 0.78
Batch: 340; loss: 3.77; acc: 0.73
Batch: 360; loss: 3.21; acc: 0.78
Batch: 380; loss: 5.64; acc: 0.67
Batch: 400; loss: 5.21; acc: 0.81
Batch: 420; loss: 3.12; acc: 0.78
Batch: 440; loss: 4.16; acc: 0.81
Batch: 460; loss: 3.95; acc: 0.78
Batch: 480; loss: 3.8; acc: 0.81
Batch: 500; loss: 5.33; acc: 0.78
Batch: 520; loss: 5.5; acc: 0.7
Batch: 540; loss: 2.62; acc: 0.81
Batch: 560; loss: 3.82; acc: 0.83
Batch: 580; loss: 3.38; acc: 0.77
Batch: 600; loss: 4.24; acc: 0.69
Batch: 620; loss: 3.34; acc: 0.73
Train Epoch over. train_loss: 3.98; train_accuracy: 0.76 

Batch: 0; loss: 2.98; acc: 0.8
Batch: 20; loss: 6.03; acc: 0.66
Batch: 40; loss: 2.51; acc: 0.83
Batch: 60; loss: 4.02; acc: 0.73
Batch: 80; loss: 4.62; acc: 0.77
Batch: 100; loss: 5.67; acc: 0.7
Batch: 120; loss: 3.47; acc: 0.88
Batch: 140; loss: 5.12; acc: 0.72
Val Epoch over. val_loss: 4.099954502597736; val_accuracy: 0.7530851910828026 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.39; acc: 0.78
Batch: 20; loss: 4.28; acc: 0.75
Batch: 40; loss: 3.69; acc: 0.75
Batch: 60; loss: 4.4; acc: 0.73
Batch: 80; loss: 5.2; acc: 0.62
Batch: 100; loss: 5.61; acc: 0.72
Batch: 120; loss: 2.42; acc: 0.8
Batch: 140; loss: 2.92; acc: 0.81
Batch: 160; loss: 3.99; acc: 0.73
Batch: 180; loss: 3.86; acc: 0.78
Batch: 200; loss: 3.9; acc: 0.77
Batch: 220; loss: 2.42; acc: 0.83
Batch: 240; loss: 3.27; acc: 0.75
Batch: 260; loss: 3.29; acc: 0.75
Batch: 280; loss: 2.91; acc: 0.77
Batch: 300; loss: 2.59; acc: 0.84
Batch: 320; loss: 4.01; acc: 0.72
Batch: 340; loss: 5.34; acc: 0.73
Batch: 360; loss: 2.99; acc: 0.83
Batch: 380; loss: 4.07; acc: 0.78
Batch: 400; loss: 1.7; acc: 0.89
Batch: 420; loss: 4.04; acc: 0.75
Batch: 440; loss: 5.5; acc: 0.73
Batch: 460; loss: 2.43; acc: 0.81
Batch: 480; loss: 4.77; acc: 0.7
Batch: 500; loss: 4.76; acc: 0.75
Batch: 520; loss: 2.53; acc: 0.84
Batch: 540; loss: 3.02; acc: 0.77
Batch: 560; loss: 3.95; acc: 0.77
Batch: 580; loss: 5.57; acc: 0.73
Batch: 600; loss: 2.95; acc: 0.83
Batch: 620; loss: 2.56; acc: 0.83
Train Epoch over. train_loss: 3.71; train_accuracy: 0.77 

Batch: 0; loss: 2.61; acc: 0.81
Batch: 20; loss: 5.88; acc: 0.67
Batch: 40; loss: 2.35; acc: 0.8
Batch: 60; loss: 4.38; acc: 0.67
Batch: 80; loss: 4.48; acc: 0.78
Batch: 100; loss: 5.77; acc: 0.7
Batch: 120; loss: 3.44; acc: 0.83
Batch: 140; loss: 5.34; acc: 0.73
Val Epoch over. val_loss: 4.019242577492052; val_accuracy: 0.7519904458598726 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.13; acc: 0.73
Batch: 20; loss: 2.29; acc: 0.83
Batch: 40; loss: 4.46; acc: 0.67
Batch: 60; loss: 2.99; acc: 0.8
Batch: 80; loss: 2.93; acc: 0.8
Batch: 100; loss: 4.19; acc: 0.72
Batch: 120; loss: 1.97; acc: 0.77
Batch: 140; loss: 2.64; acc: 0.8
Batch: 160; loss: 3.69; acc: 0.75
Batch: 180; loss: 3.24; acc: 0.75
Batch: 200; loss: 5.31; acc: 0.75
Batch: 220; loss: 4.34; acc: 0.77
Batch: 240; loss: 4.0; acc: 0.8
Batch: 260; loss: 5.72; acc: 0.77
Batch: 280; loss: 7.24; acc: 0.62
Batch: 300; loss: 3.74; acc: 0.73
Batch: 320; loss: 3.93; acc: 0.75
Batch: 340; loss: 2.93; acc: 0.83
Batch: 360; loss: 3.1; acc: 0.78
Batch: 380; loss: 4.03; acc: 0.7
Batch: 400; loss: 2.46; acc: 0.86
Batch: 420; loss: 4.49; acc: 0.77
Batch: 440; loss: 3.32; acc: 0.78
Batch: 460; loss: 4.85; acc: 0.73
Batch: 480; loss: 3.99; acc: 0.75
Batch: 500; loss: 4.57; acc: 0.59
Batch: 520; loss: 3.94; acc: 0.77
Batch: 540; loss: 4.11; acc: 0.78
Batch: 560; loss: 4.12; acc: 0.77
Batch: 580; loss: 2.78; acc: 0.77
Batch: 600; loss: 1.52; acc: 0.81
Batch: 620; loss: 3.51; acc: 0.8
Train Epoch over. train_loss: 3.65; train_accuracy: 0.77 

Batch: 0; loss: 2.63; acc: 0.81
Batch: 20; loss: 5.49; acc: 0.67
Batch: 40; loss: 2.13; acc: 0.86
Batch: 60; loss: 4.07; acc: 0.67
Batch: 80; loss: 4.38; acc: 0.77
Batch: 100; loss: 5.66; acc: 0.7
Batch: 120; loss: 3.39; acc: 0.83
Batch: 140; loss: 5.52; acc: 0.72
Val Epoch over. val_loss: 3.9444561779119405; val_accuracy: 0.75 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.18; acc: 0.81
Batch: 20; loss: 2.91; acc: 0.75
Batch: 40; loss: 2.59; acc: 0.8
Batch: 60; loss: 2.72; acc: 0.86
Batch: 80; loss: 3.89; acc: 0.77
Batch: 100; loss: 3.62; acc: 0.81
Batch: 120; loss: 3.7; acc: 0.77
Batch: 140; loss: 3.33; acc: 0.83
Batch: 160; loss: 3.22; acc: 0.77
Batch: 180; loss: 3.68; acc: 0.78
Batch: 200; loss: 4.59; acc: 0.75
Batch: 220; loss: 3.11; acc: 0.84
Batch: 240; loss: 4.4; acc: 0.66
Batch: 260; loss: 4.06; acc: 0.8
Batch: 280; loss: 2.68; acc: 0.81
Batch: 300; loss: 4.39; acc: 0.75
Batch: 320; loss: 2.82; acc: 0.77
Batch: 340; loss: 3.92; acc: 0.62
Batch: 360; loss: 3.24; acc: 0.77
Batch: 380; loss: 2.8; acc: 0.8
Batch: 400; loss: 3.64; acc: 0.78
Batch: 420; loss: 3.15; acc: 0.77
Batch: 440; loss: 3.82; acc: 0.77
Batch: 460; loss: 4.32; acc: 0.75
Batch: 480; loss: 2.98; acc: 0.73
Batch: 500; loss: 1.89; acc: 0.84
Batch: 520; loss: 4.76; acc: 0.75
Batch: 540; loss: 1.54; acc: 0.84
Batch: 560; loss: 5.16; acc: 0.64
Batch: 580; loss: 3.97; acc: 0.77
Batch: 600; loss: 3.82; acc: 0.67
Batch: 620; loss: 3.22; acc: 0.78
Train Epoch over. train_loss: 3.63; train_accuracy: 0.76 

Batch: 0; loss: 2.64; acc: 0.8
Batch: 20; loss: 5.05; acc: 0.62
Batch: 40; loss: 2.42; acc: 0.83
Batch: 60; loss: 3.67; acc: 0.69
Batch: 80; loss: 4.76; acc: 0.77
Batch: 100; loss: 5.81; acc: 0.69
Batch: 120; loss: 3.62; acc: 0.84
Batch: 140; loss: 5.79; acc: 0.7
Val Epoch over. val_loss: 3.998660272094095; val_accuracy: 0.7463176751592356 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.76; acc: 0.75
Batch: 20; loss: 3.29; acc: 0.75
Batch: 40; loss: 4.27; acc: 0.73
Batch: 60; loss: 2.39; acc: 0.8
Batch: 80; loss: 4.0; acc: 0.77
Batch: 100; loss: 1.6; acc: 0.86
Batch: 120; loss: 3.18; acc: 0.81
Batch: 140; loss: 4.76; acc: 0.73
Batch: 160; loss: 3.18; acc: 0.75
Batch: 180; loss: 5.87; acc: 0.64
Batch: 200; loss: 2.85; acc: 0.81
Batch: 220; loss: 2.94; acc: 0.81
Batch: 240; loss: 2.15; acc: 0.73
Batch: 260; loss: 5.06; acc: 0.72
Batch: 280; loss: 3.93; acc: 0.72
Batch: 300; loss: 4.84; acc: 0.7
Batch: 320; loss: 5.84; acc: 0.72
Batch: 340; loss: 3.12; acc: 0.81
Batch: 360; loss: 3.73; acc: 0.75
Batch: 380; loss: 2.5; acc: 0.77
Batch: 400; loss: 1.98; acc: 0.8
Batch: 420; loss: 1.98; acc: 0.83
Batch: 440; loss: 4.22; acc: 0.77
Batch: 460; loss: 3.31; acc: 0.83
Batch: 480; loss: 1.77; acc: 0.83
Batch: 500; loss: 3.31; acc: 0.77
Batch: 520; loss: 3.77; acc: 0.77
Batch: 540; loss: 3.62; acc: 0.77
Batch: 560; loss: 1.5; acc: 0.88
Batch: 580; loss: 2.14; acc: 0.7
Batch: 600; loss: 3.68; acc: 0.78
Batch: 620; loss: 3.99; acc: 0.75
Train Epoch over. train_loss: 3.61; train_accuracy: 0.76 

Batch: 0; loss: 2.39; acc: 0.81
Batch: 20; loss: 5.42; acc: 0.61
Batch: 40; loss: 2.39; acc: 0.84
Batch: 60; loss: 3.83; acc: 0.73
Batch: 80; loss: 4.34; acc: 0.77
Batch: 100; loss: 5.98; acc: 0.67
Batch: 120; loss: 3.42; acc: 0.81
Batch: 140; loss: 5.61; acc: 0.72
Val Epoch over. val_loss: 3.900022830173468; val_accuracy: 0.7476114649681529 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.25; acc: 0.78
Batch: 20; loss: 4.86; acc: 0.69
Batch: 40; loss: 2.46; acc: 0.84
Batch: 60; loss: 1.43; acc: 0.88
Batch: 80; loss: 2.36; acc: 0.88
Batch: 100; loss: 3.67; acc: 0.77
Batch: 120; loss: 2.51; acc: 0.81
Batch: 140; loss: 3.89; acc: 0.81
Batch: 160; loss: 3.3; acc: 0.78
Batch: 180; loss: 4.0; acc: 0.69
Batch: 200; loss: 3.13; acc: 0.83
Batch: 220; loss: 1.74; acc: 0.83
Batch: 240; loss: 3.1; acc: 0.78
Batch: 260; loss: 4.14; acc: 0.77
Batch: 280; loss: 1.78; acc: 0.83
Batch: 300; loss: 3.35; acc: 0.78
Batch: 320; loss: 2.94; acc: 0.78
Batch: 340; loss: 4.04; acc: 0.78
Batch: 360; loss: 3.87; acc: 0.77
Batch: 380; loss: 5.66; acc: 0.69
Batch: 400; loss: 3.77; acc: 0.72
Batch: 420; loss: 4.74; acc: 0.67
Batch: 440; loss: 3.16; acc: 0.81
Batch: 460; loss: 3.9; acc: 0.69
Batch: 480; loss: 2.6; acc: 0.77
Batch: 500; loss: 3.8; acc: 0.75
Batch: 520; loss: 4.58; acc: 0.72
Batch: 540; loss: 5.23; acc: 0.7
Batch: 560; loss: 2.3; acc: 0.84
Batch: 580; loss: 4.64; acc: 0.73
Batch: 600; loss: 6.96; acc: 0.73
Batch: 620; loss: 3.13; acc: 0.77
Train Epoch over. train_loss: 3.6; train_accuracy: 0.76 

Batch: 0; loss: 2.28; acc: 0.81
Batch: 20; loss: 5.77; acc: 0.62
Batch: 40; loss: 2.35; acc: 0.83
Batch: 60; loss: 3.42; acc: 0.77
Batch: 80; loss: 4.62; acc: 0.8
Batch: 100; loss: 6.02; acc: 0.67
Batch: 120; loss: 3.73; acc: 0.8
Batch: 140; loss: 5.58; acc: 0.72
Val Epoch over. val_loss: 3.8854905640243724; val_accuracy: 0.7483081210191083 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.4; acc: 0.84
Batch: 20; loss: 3.93; acc: 0.77
Batch: 40; loss: 4.07; acc: 0.73
Batch: 60; loss: 3.1; acc: 0.78
Batch: 80; loss: 3.78; acc: 0.78
Batch: 100; loss: 3.94; acc: 0.8
Batch: 120; loss: 4.9; acc: 0.73
Batch: 140; loss: 3.0; acc: 0.8
Batch: 160; loss: 3.06; acc: 0.8
Batch: 180; loss: 3.59; acc: 0.73
Batch: 200; loss: 2.93; acc: 0.77
Batch: 220; loss: 1.93; acc: 0.83
Batch: 240; loss: 4.0; acc: 0.67
Batch: 260; loss: 3.11; acc: 0.73
Batch: 280; loss: 4.15; acc: 0.77
Batch: 300; loss: 3.26; acc: 0.78
Batch: 320; loss: 3.17; acc: 0.77
Batch: 340; loss: 4.22; acc: 0.8
Batch: 360; loss: 2.82; acc: 0.8
Batch: 380; loss: 3.09; acc: 0.86
Batch: 400; loss: 3.14; acc: 0.78
Batch: 420; loss: 2.98; acc: 0.81
Batch: 440; loss: 2.74; acc: 0.75
Batch: 460; loss: 6.67; acc: 0.7
Batch: 480; loss: 2.05; acc: 0.83
Batch: 500; loss: 3.64; acc: 0.7
Batch: 520; loss: 4.66; acc: 0.72
Batch: 540; loss: 7.03; acc: 0.73
Batch: 560; loss: 3.79; acc: 0.81
Batch: 580; loss: 5.5; acc: 0.7
Batch: 600; loss: 4.16; acc: 0.72
Batch: 620; loss: 4.9; acc: 0.7
Train Epoch over. train_loss: 3.59; train_accuracy: 0.77 

Batch: 0; loss: 2.31; acc: 0.8
Batch: 20; loss: 5.51; acc: 0.64
Batch: 40; loss: 1.98; acc: 0.84
Batch: 60; loss: 3.43; acc: 0.72
Batch: 80; loss: 4.51; acc: 0.8
Batch: 100; loss: 5.62; acc: 0.69
Batch: 120; loss: 3.65; acc: 0.83
Batch: 140; loss: 5.65; acc: 0.73
Val Epoch over. val_loss: 3.89531407386634; val_accuracy: 0.7478105095541401 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.8; acc: 0.73
Batch: 20; loss: 3.11; acc: 0.83
Batch: 40; loss: 1.89; acc: 0.8
Batch: 60; loss: 4.16; acc: 0.77
Batch: 80; loss: 4.87; acc: 0.69
Batch: 100; loss: 4.51; acc: 0.72
Batch: 120; loss: 2.92; acc: 0.81
Batch: 140; loss: 4.9; acc: 0.73
Batch: 160; loss: 2.41; acc: 0.78
Batch: 180; loss: 2.98; acc: 0.78
Batch: 200; loss: 3.51; acc: 0.8
Batch: 220; loss: 5.55; acc: 0.67
Batch: 240; loss: 2.54; acc: 0.84
Batch: 260; loss: 4.36; acc: 0.78
Batch: 280; loss: 2.94; acc: 0.78
Batch: 300; loss: 2.61; acc: 0.8
Batch: 320; loss: 4.35; acc: 0.72
Batch: 340; loss: 3.32; acc: 0.81
Batch: 360; loss: 4.51; acc: 0.69
Batch: 380; loss: 3.31; acc: 0.78
Batch: 400; loss: 2.66; acc: 0.81
Batch: 420; loss: 3.46; acc: 0.77
Batch: 440; loss: 3.02; acc: 0.73
Batch: 460; loss: 4.25; acc: 0.75
Batch: 480; loss: 1.99; acc: 0.83
Batch: 500; loss: 3.32; acc: 0.8
Batch: 520; loss: 4.63; acc: 0.78
Batch: 540; loss: 4.13; acc: 0.73
Batch: 560; loss: 3.23; acc: 0.86
Batch: 580; loss: 6.08; acc: 0.69
Batch: 600; loss: 2.6; acc: 0.81
Batch: 620; loss: 4.36; acc: 0.77
Train Epoch over. train_loss: 3.58; train_accuracy: 0.77 

Batch: 0; loss: 2.25; acc: 0.83
Batch: 20; loss: 5.77; acc: 0.64
Batch: 40; loss: 1.9; acc: 0.86
Batch: 60; loss: 3.72; acc: 0.72
Batch: 80; loss: 4.55; acc: 0.78
Batch: 100; loss: 5.8; acc: 0.69
Batch: 120; loss: 3.58; acc: 0.84
Batch: 140; loss: 6.22; acc: 0.69
Val Epoch over. val_loss: 3.8804954343540654; val_accuracy: 0.7487062101910829 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.86; acc: 0.88
Batch: 20; loss: 4.74; acc: 0.69
Batch: 40; loss: 3.22; acc: 0.83
Batch: 60; loss: 3.26; acc: 0.8
Batch: 80; loss: 4.63; acc: 0.7
Batch: 100; loss: 3.71; acc: 0.78
Batch: 120; loss: 3.5; acc: 0.72
Batch: 140; loss: 3.74; acc: 0.81
Batch: 160; loss: 5.67; acc: 0.66
Batch: 180; loss: 4.9; acc: 0.72
Batch: 200; loss: 3.98; acc: 0.73
Batch: 220; loss: 3.02; acc: 0.77
Batch: 240; loss: 3.21; acc: 0.78
Batch: 260; loss: 3.03; acc: 0.8
Batch: 280; loss: 3.29; acc: 0.83
Batch: 300; loss: 2.89; acc: 0.8
Batch: 320; loss: 3.96; acc: 0.78
Batch: 340; loss: 4.57; acc: 0.72
Batch: 360; loss: 4.12; acc: 0.67
Batch: 380; loss: 3.63; acc: 0.8
Batch: 400; loss: 3.93; acc: 0.69
Batch: 420; loss: 3.09; acc: 0.75
Batch: 440; loss: 2.62; acc: 0.77
Batch: 460; loss: 5.52; acc: 0.72
Batch: 480; loss: 5.36; acc: 0.73
Batch: 500; loss: 5.04; acc: 0.75
Batch: 520; loss: 2.83; acc: 0.78
Batch: 540; loss: 3.69; acc: 0.66
Batch: 560; loss: 2.28; acc: 0.81
Batch: 580; loss: 3.25; acc: 0.72
Batch: 600; loss: 2.78; acc: 0.83
Batch: 620; loss: 3.72; acc: 0.69
Train Epoch over. train_loss: 3.58; train_accuracy: 0.76 

Batch: 0; loss: 2.1; acc: 0.81
Batch: 20; loss: 5.37; acc: 0.64
Batch: 40; loss: 1.96; acc: 0.84
Batch: 60; loss: 3.41; acc: 0.75
Batch: 80; loss: 4.59; acc: 0.78
Batch: 100; loss: 5.69; acc: 0.7
Batch: 120; loss: 3.44; acc: 0.83
Batch: 140; loss: 5.83; acc: 0.72
Val Epoch over. val_loss: 3.8678397031346705; val_accuracy: 0.75109474522293 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.98; acc: 0.78
Batch: 20; loss: 3.13; acc: 0.78
Batch: 40; loss: 2.88; acc: 0.83
Batch: 60; loss: 2.51; acc: 0.77
Batch: 80; loss: 4.1; acc: 0.77
Batch: 100; loss: 2.17; acc: 0.8
Batch: 120; loss: 3.66; acc: 0.81
Batch: 140; loss: 3.38; acc: 0.75
Batch: 160; loss: 2.61; acc: 0.88
Batch: 180; loss: 1.35; acc: 0.88
Batch: 200; loss: 2.27; acc: 0.8
Batch: 220; loss: 3.27; acc: 0.77
Batch: 240; loss: 4.56; acc: 0.77
Batch: 260; loss: 2.84; acc: 0.83
Batch: 280; loss: 5.58; acc: 0.69
Batch: 300; loss: 3.33; acc: 0.78
Batch: 320; loss: 4.04; acc: 0.67
Batch: 340; loss: 7.82; acc: 0.61
Batch: 360; loss: 3.0; acc: 0.81
Batch: 380; loss: 3.91; acc: 0.78
Batch: 400; loss: 3.36; acc: 0.83
Batch: 420; loss: 5.19; acc: 0.59
Batch: 440; loss: 3.32; acc: 0.78
Batch: 460; loss: 2.87; acc: 0.84
Batch: 480; loss: 3.38; acc: 0.78
Batch: 500; loss: 5.47; acc: 0.77
Batch: 520; loss: 3.64; acc: 0.67
Batch: 540; loss: 3.88; acc: 0.72
Batch: 560; loss: 3.66; acc: 0.75
Batch: 580; loss: 3.25; acc: 0.75
Batch: 600; loss: 4.01; acc: 0.8
Batch: 620; loss: 5.11; acc: 0.66
Train Epoch over. train_loss: 3.57; train_accuracy: 0.77 

Batch: 0; loss: 2.35; acc: 0.8
Batch: 20; loss: 5.34; acc: 0.64
Batch: 40; loss: 2.01; acc: 0.86
Batch: 60; loss: 3.16; acc: 0.78
Batch: 80; loss: 4.66; acc: 0.75
Batch: 100; loss: 6.28; acc: 0.72
Batch: 120; loss: 3.71; acc: 0.83
Batch: 140; loss: 6.13; acc: 0.7
Val Epoch over. val_loss: 3.9331385360401905; val_accuracy: 0.7521894904458599 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.37; acc: 0.77
Batch: 20; loss: 4.91; acc: 0.67
Batch: 40; loss: 3.03; acc: 0.8
Batch: 60; loss: 2.01; acc: 0.88
Batch: 80; loss: 3.76; acc: 0.73
Batch: 100; loss: 5.0; acc: 0.78
Batch: 120; loss: 6.31; acc: 0.72
Batch: 140; loss: 4.04; acc: 0.77
Batch: 160; loss: 2.63; acc: 0.78
Batch: 180; loss: 3.35; acc: 0.77
Batch: 200; loss: 4.99; acc: 0.69
Batch: 220; loss: 0.95; acc: 0.88
Batch: 240; loss: 4.71; acc: 0.78
Batch: 260; loss: 3.99; acc: 0.72
Batch: 280; loss: 4.51; acc: 0.78
Batch: 300; loss: 4.25; acc: 0.77
Batch: 320; loss: 4.58; acc: 0.7
Batch: 340; loss: 2.93; acc: 0.81
Batch: 360; loss: 3.63; acc: 0.81
Batch: 380; loss: 2.3; acc: 0.81
Batch: 400; loss: 4.81; acc: 0.77
Batch: 420; loss: 4.02; acc: 0.75
Batch: 440; loss: 2.31; acc: 0.84
Batch: 460; loss: 3.36; acc: 0.73
Batch: 480; loss: 4.9; acc: 0.72
Batch: 500; loss: 3.87; acc: 0.72
Batch: 520; loss: 2.83; acc: 0.8
Batch: 540; loss: 2.8; acc: 0.81
Batch: 560; loss: 2.8; acc: 0.73
Batch: 580; loss: 4.68; acc: 0.78
Batch: 600; loss: 3.89; acc: 0.77
Batch: 620; loss: 2.46; acc: 0.78
Train Epoch over. train_loss: 3.47; train_accuracy: 0.77 

Batch: 0; loss: 2.14; acc: 0.83
Batch: 20; loss: 5.6; acc: 0.64
Batch: 40; loss: 1.9; acc: 0.84
Batch: 60; loss: 3.16; acc: 0.75
Batch: 80; loss: 4.51; acc: 0.75
Batch: 100; loss: 6.05; acc: 0.72
Batch: 120; loss: 3.55; acc: 0.83
Batch: 140; loss: 6.13; acc: 0.7
Val Epoch over. val_loss: 3.857303359705931; val_accuracy: 0.7532842356687898 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.47; acc: 0.75
Batch: 20; loss: 3.38; acc: 0.77
Batch: 40; loss: 2.36; acc: 0.81
Batch: 60; loss: 4.27; acc: 0.81
Batch: 80; loss: 5.17; acc: 0.69
Batch: 100; loss: 3.52; acc: 0.81
Batch: 120; loss: 3.4; acc: 0.75
Batch: 140; loss: 4.31; acc: 0.78
Batch: 160; loss: 2.12; acc: 0.81
Batch: 180; loss: 3.51; acc: 0.77
Batch: 200; loss: 2.29; acc: 0.81
Batch: 220; loss: 1.69; acc: 0.86
Batch: 240; loss: 3.61; acc: 0.78
Batch: 260; loss: 2.9; acc: 0.81
Batch: 280; loss: 3.63; acc: 0.78
Batch: 300; loss: 4.65; acc: 0.73
Batch: 320; loss: 6.15; acc: 0.7
Batch: 340; loss: 3.35; acc: 0.78
Batch: 360; loss: 2.64; acc: 0.81
Batch: 380; loss: 3.63; acc: 0.72
Batch: 400; loss: 4.37; acc: 0.7
Batch: 420; loss: 2.5; acc: 0.73
Batch: 440; loss: 3.41; acc: 0.78
Batch: 460; loss: 1.95; acc: 0.81
Batch: 480; loss: 4.08; acc: 0.73
Batch: 500; loss: 6.12; acc: 0.72
Batch: 520; loss: 4.43; acc: 0.73
Batch: 540; loss: 5.8; acc: 0.62
Batch: 560; loss: 4.06; acc: 0.73
Batch: 580; loss: 3.66; acc: 0.67
Batch: 600; loss: 3.19; acc: 0.73
Batch: 620; loss: 3.9; acc: 0.72
Train Epoch over. train_loss: 3.45; train_accuracy: 0.77 

Batch: 0; loss: 2.15; acc: 0.81
Batch: 20; loss: 5.61; acc: 0.64
Batch: 40; loss: 1.92; acc: 0.84
Batch: 60; loss: 2.99; acc: 0.77
Batch: 80; loss: 4.53; acc: 0.78
Batch: 100; loss: 6.01; acc: 0.72
Batch: 120; loss: 3.56; acc: 0.84
Batch: 140; loss: 6.0; acc: 0.72
Val Epoch over. val_loss: 3.852355578902421; val_accuracy: 0.753781847133758 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.96; acc: 0.77
Batch: 20; loss: 2.17; acc: 0.84
Batch: 40; loss: 2.94; acc: 0.78
Batch: 60; loss: 2.95; acc: 0.72
Batch: 80; loss: 3.94; acc: 0.72
Batch: 100; loss: 3.57; acc: 0.81
Batch: 120; loss: 4.38; acc: 0.7
Batch: 140; loss: 6.77; acc: 0.66
Batch: 160; loss: 3.71; acc: 0.77
Batch: 180; loss: 3.24; acc: 0.77
Batch: 200; loss: 3.37; acc: 0.78
Batch: 220; loss: 3.88; acc: 0.75
Batch: 240; loss: 4.66; acc: 0.69
Batch: 260; loss: 4.08; acc: 0.75
Batch: 280; loss: 4.4; acc: 0.77
Batch: 300; loss: 4.0; acc: 0.78
Batch: 320; loss: 3.63; acc: 0.72
Batch: 340; loss: 2.94; acc: 0.75
Batch: 360; loss: 3.71; acc: 0.75
Batch: 380; loss: 3.28; acc: 0.75
Batch: 400; loss: 4.33; acc: 0.75
Batch: 420; loss: 4.45; acc: 0.67
Batch: 440; loss: 3.62; acc: 0.81
Batch: 460; loss: 2.21; acc: 0.84
Batch: 480; loss: 1.31; acc: 0.89
Batch: 500; loss: 4.73; acc: 0.78
Batch: 520; loss: 4.53; acc: 0.77
Batch: 540; loss: 5.45; acc: 0.64
Batch: 560; loss: 2.32; acc: 0.8
Batch: 580; loss: 2.49; acc: 0.83
Batch: 600; loss: 3.85; acc: 0.77
Batch: 620; loss: 4.62; acc: 0.75
Train Epoch over. train_loss: 3.45; train_accuracy: 0.77 

Batch: 0; loss: 2.11; acc: 0.83
Batch: 20; loss: 5.59; acc: 0.64
Batch: 40; loss: 1.89; acc: 0.88
Batch: 60; loss: 3.1; acc: 0.75
Batch: 80; loss: 4.46; acc: 0.77
Batch: 100; loss: 5.93; acc: 0.72
Batch: 120; loss: 3.5; acc: 0.84
Batch: 140; loss: 6.06; acc: 0.7
Val Epoch over. val_loss: 3.844051752120826; val_accuracy: 0.7529856687898089 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.56; acc: 0.77
Batch: 20; loss: 2.46; acc: 0.81
Batch: 40; loss: 4.76; acc: 0.67
Batch: 60; loss: 2.77; acc: 0.78
Batch: 80; loss: 3.06; acc: 0.8
Batch: 100; loss: 3.12; acc: 0.75
Batch: 120; loss: 2.59; acc: 0.78
Batch: 140; loss: 3.91; acc: 0.75
Batch: 160; loss: 2.92; acc: 0.73
Batch: 180; loss: 2.95; acc: 0.81
Batch: 200; loss: 3.16; acc: 0.81
Batch: 220; loss: 2.54; acc: 0.73
Batch: 240; loss: 3.51; acc: 0.78
Batch: 260; loss: 3.52; acc: 0.72
Batch: 280; loss: 4.02; acc: 0.77
Batch: 300; loss: 2.65; acc: 0.81
Batch: 320; loss: 1.69; acc: 0.84
Batch: 340; loss: 3.78; acc: 0.72
Batch: 360; loss: 1.93; acc: 0.83
Batch: 380; loss: 2.08; acc: 0.73
Batch: 400; loss: 4.33; acc: 0.75
Batch: 420; loss: 3.8; acc: 0.81
Batch: 440; loss: 3.51; acc: 0.8
Batch: 460; loss: 5.38; acc: 0.66
Batch: 480; loss: 5.6; acc: 0.7
Batch: 500; loss: 1.67; acc: 0.75
Batch: 520; loss: 3.17; acc: 0.77
Batch: 540; loss: 1.69; acc: 0.89
Batch: 560; loss: 3.74; acc: 0.75
Batch: 580; loss: 4.4; acc: 0.78
Batch: 600; loss: 2.01; acc: 0.84
Batch: 620; loss: 3.86; acc: 0.72
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.13; acc: 0.83
Batch: 20; loss: 5.63; acc: 0.64
Batch: 40; loss: 1.89; acc: 0.84
Batch: 60; loss: 3.04; acc: 0.77
Batch: 80; loss: 4.49; acc: 0.77
Batch: 100; loss: 5.97; acc: 0.72
Batch: 120; loss: 3.48; acc: 0.84
Batch: 140; loss: 6.02; acc: 0.7
Val Epoch over. val_loss: 3.845294840016942; val_accuracy: 0.7531847133757962 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.14; acc: 0.78
Batch: 20; loss: 3.66; acc: 0.75
Batch: 40; loss: 4.43; acc: 0.72
Batch: 60; loss: 2.42; acc: 0.8
Batch: 80; loss: 3.81; acc: 0.73
Batch: 100; loss: 2.99; acc: 0.78
Batch: 120; loss: 2.78; acc: 0.8
Batch: 140; loss: 2.56; acc: 0.77
Batch: 160; loss: 2.14; acc: 0.83
Batch: 180; loss: 2.6; acc: 0.83
Batch: 200; loss: 4.01; acc: 0.78
Batch: 220; loss: 4.77; acc: 0.77
Batch: 240; loss: 3.52; acc: 0.8
Batch: 260; loss: 2.59; acc: 0.77
Batch: 280; loss: 4.29; acc: 0.73
Batch: 300; loss: 2.05; acc: 0.86
Batch: 320; loss: 1.29; acc: 0.84
Batch: 340; loss: 5.1; acc: 0.73
Batch: 360; loss: 4.54; acc: 0.72
Batch: 380; loss: 5.28; acc: 0.73
Batch: 400; loss: 2.92; acc: 0.81
Batch: 420; loss: 2.56; acc: 0.89
Batch: 440; loss: 4.32; acc: 0.75
Batch: 460; loss: 5.79; acc: 0.72
Batch: 480; loss: 4.69; acc: 0.7
Batch: 500; loss: 2.53; acc: 0.8
Batch: 520; loss: 2.88; acc: 0.8
Batch: 540; loss: 3.34; acc: 0.81
Batch: 560; loss: 3.72; acc: 0.75
Batch: 580; loss: 3.97; acc: 0.83
Batch: 600; loss: 3.1; acc: 0.84
Batch: 620; loss: 1.63; acc: 0.86
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.16; acc: 0.83
Batch: 20; loss: 5.6; acc: 0.64
Batch: 40; loss: 1.89; acc: 0.84
Batch: 60; loss: 3.01; acc: 0.77
Batch: 80; loss: 4.45; acc: 0.75
Batch: 100; loss: 6.02; acc: 0.72
Batch: 120; loss: 3.48; acc: 0.84
Batch: 140; loss: 5.98; acc: 0.72
Val Epoch over. val_loss: 3.841479820051011; val_accuracy: 0.7542794585987261 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.08; acc: 0.7
Batch: 20; loss: 3.15; acc: 0.75
Batch: 40; loss: 3.54; acc: 0.81
Batch: 60; loss: 2.86; acc: 0.75
Batch: 80; loss: 3.07; acc: 0.78
Batch: 100; loss: 3.57; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.84
Batch: 140; loss: 3.87; acc: 0.77
Batch: 160; loss: 3.52; acc: 0.77
Batch: 180; loss: 2.89; acc: 0.78
Batch: 200; loss: 4.94; acc: 0.72
Batch: 220; loss: 3.25; acc: 0.78
Batch: 240; loss: 1.53; acc: 0.86
Batch: 260; loss: 4.35; acc: 0.78
Batch: 280; loss: 4.37; acc: 0.81
Batch: 300; loss: 4.17; acc: 0.73
Batch: 320; loss: 4.62; acc: 0.72
Batch: 340; loss: 4.19; acc: 0.83
Batch: 360; loss: 2.22; acc: 0.78
Batch: 380; loss: 2.08; acc: 0.83
Batch: 400; loss: 4.35; acc: 0.73
Batch: 420; loss: 2.08; acc: 0.78
Batch: 440; loss: 2.21; acc: 0.86
Batch: 460; loss: 5.3; acc: 0.67
Batch: 480; loss: 2.64; acc: 0.83
Batch: 500; loss: 4.21; acc: 0.75
Batch: 520; loss: 4.06; acc: 0.77
Batch: 540; loss: 2.11; acc: 0.75
Batch: 560; loss: 4.33; acc: 0.78
Batch: 580; loss: 3.38; acc: 0.78
Batch: 600; loss: 3.55; acc: 0.8
Batch: 620; loss: 4.22; acc: 0.73
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.19; acc: 0.81
Batch: 20; loss: 5.59; acc: 0.64
Batch: 40; loss: 1.86; acc: 0.86
Batch: 60; loss: 3.13; acc: 0.77
Batch: 80; loss: 4.53; acc: 0.75
Batch: 100; loss: 6.08; acc: 0.72
Batch: 120; loss: 3.46; acc: 0.84
Batch: 140; loss: 6.0; acc: 0.7
Val Epoch over. val_loss: 3.853355845827965; val_accuracy: 0.7535828025477707 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 6.31; acc: 0.67
Batch: 20; loss: 2.0; acc: 0.81
Batch: 40; loss: 3.08; acc: 0.75
Batch: 60; loss: 3.32; acc: 0.81
Batch: 80; loss: 5.77; acc: 0.75
Batch: 100; loss: 3.11; acc: 0.73
Batch: 120; loss: 5.49; acc: 0.7
Batch: 140; loss: 3.25; acc: 0.77
Batch: 160; loss: 2.46; acc: 0.81
Batch: 180; loss: 1.96; acc: 0.83
Batch: 200; loss: 4.74; acc: 0.69
Batch: 220; loss: 4.12; acc: 0.72
Batch: 240; loss: 2.8; acc: 0.78
Batch: 260; loss: 2.08; acc: 0.83
Batch: 280; loss: 2.34; acc: 0.81
Batch: 300; loss: 2.29; acc: 0.8
Batch: 320; loss: 3.8; acc: 0.75
Batch: 340; loss: 1.71; acc: 0.8
Batch: 360; loss: 1.06; acc: 0.86
Batch: 380; loss: 5.64; acc: 0.8
Batch: 400; loss: 4.44; acc: 0.73
Batch: 420; loss: 3.87; acc: 0.75
Batch: 440; loss: 1.96; acc: 0.81
Batch: 460; loss: 4.66; acc: 0.73
Batch: 480; loss: 3.68; acc: 0.75
Batch: 500; loss: 4.56; acc: 0.83
Batch: 520; loss: 3.16; acc: 0.77
Batch: 540; loss: 1.23; acc: 0.84
Batch: 560; loss: 3.22; acc: 0.77
Batch: 580; loss: 2.59; acc: 0.78
Batch: 600; loss: 3.22; acc: 0.81
Batch: 620; loss: 3.78; acc: 0.73
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.23; acc: 0.81
Batch: 20; loss: 5.56; acc: 0.62
Batch: 40; loss: 1.89; acc: 0.88
Batch: 60; loss: 2.95; acc: 0.8
Batch: 80; loss: 4.58; acc: 0.75
Batch: 100; loss: 6.04; acc: 0.72
Batch: 120; loss: 3.5; acc: 0.84
Batch: 140; loss: 5.89; acc: 0.7
Val Epoch over. val_loss: 3.8550011967397797; val_accuracy: 0.7540804140127388 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.59; acc: 0.83
Batch: 20; loss: 2.4; acc: 0.77
Batch: 40; loss: 2.8; acc: 0.72
Batch: 60; loss: 2.41; acc: 0.81
Batch: 80; loss: 4.58; acc: 0.75
Batch: 100; loss: 3.57; acc: 0.81
Batch: 120; loss: 3.89; acc: 0.75
Batch: 140; loss: 3.92; acc: 0.78
Batch: 160; loss: 1.62; acc: 0.84
Batch: 180; loss: 4.71; acc: 0.77
Batch: 200; loss: 4.79; acc: 0.72
Batch: 220; loss: 2.81; acc: 0.81
Batch: 240; loss: 2.47; acc: 0.78
Batch: 260; loss: 4.23; acc: 0.7
Batch: 280; loss: 4.56; acc: 0.7
Batch: 300; loss: 3.51; acc: 0.8
Batch: 320; loss: 1.86; acc: 0.75
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 3.62; acc: 0.78
Batch: 380; loss: 2.71; acc: 0.77
Batch: 400; loss: 1.75; acc: 0.88
Batch: 420; loss: 2.85; acc: 0.84
Batch: 440; loss: 3.26; acc: 0.77
Batch: 460; loss: 3.58; acc: 0.8
Batch: 480; loss: 2.15; acc: 0.8
Batch: 500; loss: 2.67; acc: 0.83
Batch: 520; loss: 4.22; acc: 0.69
Batch: 540; loss: 1.17; acc: 0.89
Batch: 560; loss: 4.04; acc: 0.78
Batch: 580; loss: 3.19; acc: 0.83
Batch: 600; loss: 6.95; acc: 0.73
Batch: 620; loss: 2.24; acc: 0.75
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.16; acc: 0.81
Batch: 20; loss: 5.62; acc: 0.64
Batch: 40; loss: 1.88; acc: 0.86
Batch: 60; loss: 3.12; acc: 0.77
Batch: 80; loss: 4.55; acc: 0.75
Batch: 100; loss: 6.02; acc: 0.72
Batch: 120; loss: 3.46; acc: 0.84
Batch: 140; loss: 5.97; acc: 0.7
Val Epoch over. val_loss: 3.8514346431015403; val_accuracy: 0.7529856687898089 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.23; acc: 0.81
Batch: 20; loss: 1.41; acc: 0.88
Batch: 40; loss: 1.75; acc: 0.81
Batch: 60; loss: 3.52; acc: 0.7
Batch: 80; loss: 5.25; acc: 0.72
Batch: 100; loss: 3.55; acc: 0.7
Batch: 120; loss: 4.79; acc: 0.67
Batch: 140; loss: 3.8; acc: 0.8
Batch: 160; loss: 2.68; acc: 0.83
Batch: 180; loss: 2.17; acc: 0.81
Batch: 200; loss: 2.32; acc: 0.81
Batch: 220; loss: 3.5; acc: 0.72
Batch: 240; loss: 2.31; acc: 0.81
Batch: 260; loss: 5.21; acc: 0.75
Batch: 280; loss: 5.91; acc: 0.73
Batch: 300; loss: 3.18; acc: 0.78
Batch: 320; loss: 3.11; acc: 0.75
Batch: 340; loss: 2.6; acc: 0.81
Batch: 360; loss: 5.57; acc: 0.66
Batch: 380; loss: 2.56; acc: 0.78
Batch: 400; loss: 1.46; acc: 0.84
Batch: 420; loss: 4.81; acc: 0.72
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 2.43; acc: 0.81
Batch: 480; loss: 2.51; acc: 0.83
Batch: 500; loss: 3.65; acc: 0.8
Batch: 520; loss: 2.13; acc: 0.81
Batch: 540; loss: 2.91; acc: 0.78
Batch: 560; loss: 5.71; acc: 0.66
Batch: 580; loss: 3.14; acc: 0.75
Batch: 600; loss: 2.25; acc: 0.84
Batch: 620; loss: 2.69; acc: 0.8
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.13; acc: 0.83
Batch: 20; loss: 5.58; acc: 0.64
Batch: 40; loss: 1.88; acc: 0.86
Batch: 60; loss: 3.16; acc: 0.75
Batch: 80; loss: 4.41; acc: 0.75
Batch: 100; loss: 5.95; acc: 0.7
Batch: 120; loss: 3.42; acc: 0.84
Batch: 140; loss: 6.04; acc: 0.69
Val Epoch over. val_loss: 3.8423537342411698; val_accuracy: 0.7516918789808917 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.14; acc: 0.78
Batch: 20; loss: 2.04; acc: 0.84
Batch: 40; loss: 2.29; acc: 0.83
Batch: 60; loss: 5.77; acc: 0.75
Batch: 80; loss: 3.32; acc: 0.75
Batch: 100; loss: 1.94; acc: 0.77
Batch: 120; loss: 3.19; acc: 0.7
Batch: 140; loss: 3.62; acc: 0.72
Batch: 160; loss: 1.6; acc: 0.94
Batch: 180; loss: 3.49; acc: 0.8
Batch: 200; loss: 3.59; acc: 0.72
Batch: 220; loss: 1.83; acc: 0.86
Batch: 240; loss: 3.46; acc: 0.75
Batch: 260; loss: 5.41; acc: 0.69
Batch: 280; loss: 4.6; acc: 0.75
Batch: 300; loss: 2.43; acc: 0.8
Batch: 320; loss: 4.87; acc: 0.69
Batch: 340; loss: 2.19; acc: 0.86
Batch: 360; loss: 3.68; acc: 0.77
Batch: 380; loss: 3.36; acc: 0.72
Batch: 400; loss: 3.02; acc: 0.81
Batch: 420; loss: 4.98; acc: 0.69
Batch: 440; loss: 4.96; acc: 0.61
Batch: 460; loss: 4.33; acc: 0.78
Batch: 480; loss: 4.46; acc: 0.77
Batch: 500; loss: 5.2; acc: 0.72
Batch: 520; loss: 4.16; acc: 0.72
Batch: 540; loss: 3.34; acc: 0.69
Batch: 560; loss: 3.51; acc: 0.81
Batch: 580; loss: 4.27; acc: 0.66
Batch: 600; loss: 3.13; acc: 0.73
Batch: 620; loss: 3.11; acc: 0.72
Train Epoch over. train_loss: 3.44; train_accuracy: 0.77 

Batch: 0; loss: 2.17; acc: 0.81
Batch: 20; loss: 5.59; acc: 0.62
Batch: 40; loss: 1.91; acc: 0.86
Batch: 60; loss: 2.99; acc: 0.77
Batch: 80; loss: 4.52; acc: 0.75
Batch: 100; loss: 5.93; acc: 0.7
Batch: 120; loss: 3.46; acc: 0.83
Batch: 140; loss: 5.92; acc: 0.7
Val Epoch over. val_loss: 3.84549491116955; val_accuracy: 0.7538813694267515 

plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_400_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 268055
elements in E: 119526000
fraction nonzero: 0.0022426501346987267
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 36.37; acc: 0.12
Batch: 20; loss: 7.56; acc: 0.52
Batch: 40; loss: 5.74; acc: 0.67
Batch: 60; loss: 7.64; acc: 0.56
Batch: 80; loss: 6.63; acc: 0.64
Batch: 100; loss: 3.24; acc: 0.7
Batch: 120; loss: 8.23; acc: 0.58
Batch: 140; loss: 5.16; acc: 0.72
Batch: 160; loss: 6.28; acc: 0.7
Batch: 180; loss: 5.48; acc: 0.75
Batch: 200; loss: 3.2; acc: 0.8
Batch: 220; loss: 5.52; acc: 0.69
Batch: 240; loss: 3.25; acc: 0.84
Batch: 260; loss: 4.9; acc: 0.7
Batch: 280; loss: 4.97; acc: 0.72
Batch: 300; loss: 8.69; acc: 0.67
Batch: 320; loss: 5.39; acc: 0.73
Batch: 340; loss: 2.88; acc: 0.8
Batch: 360; loss: 4.37; acc: 0.8
Batch: 380; loss: 7.09; acc: 0.66
Batch: 400; loss: 9.99; acc: 0.67
Batch: 420; loss: 4.42; acc: 0.8
Batch: 440; loss: 4.21; acc: 0.81
Batch: 460; loss: 5.86; acc: 0.72
Batch: 480; loss: 5.73; acc: 0.72
Batch: 500; loss: 4.43; acc: 0.77
Batch: 520; loss: 3.78; acc: 0.77
Batch: 540; loss: 5.71; acc: 0.81
Batch: 560; loss: 3.97; acc: 0.78
Batch: 580; loss: 5.5; acc: 0.73
Batch: 600; loss: 4.75; acc: 0.7
Batch: 620; loss: 4.33; acc: 0.72
Train Epoch over. train_loss: 5.49; train_accuracy: 0.71 

Batch: 0; loss: 2.45; acc: 0.83
Batch: 20; loss: 8.69; acc: 0.55
Batch: 40; loss: 2.01; acc: 0.89
Batch: 60; loss: 4.48; acc: 0.7
Batch: 80; loss: 4.2; acc: 0.77
Batch: 100; loss: 5.35; acc: 0.75
Batch: 120; loss: 5.75; acc: 0.78
Batch: 140; loss: 5.87; acc: 0.67
Val Epoch over. val_loss: 4.481929735393281; val_accuracy: 0.7624402866242038 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 3.0; acc: 0.81
Batch: 20; loss: 3.75; acc: 0.77
Batch: 40; loss: 2.01; acc: 0.8
Batch: 60; loss: 5.26; acc: 0.77
Batch: 80; loss: 5.19; acc: 0.78
Batch: 100; loss: 4.44; acc: 0.77
Batch: 120; loss: 3.09; acc: 0.75
Batch: 140; loss: 5.84; acc: 0.67
Batch: 160; loss: 2.02; acc: 0.8
Batch: 180; loss: 6.84; acc: 0.75
Batch: 200; loss: 5.27; acc: 0.83
Batch: 220; loss: 8.48; acc: 0.66
Batch: 240; loss: 5.08; acc: 0.69
Batch: 260; loss: 4.94; acc: 0.77
Batch: 280; loss: 2.56; acc: 0.84
Batch: 300; loss: 2.79; acc: 0.86
Batch: 320; loss: 2.5; acc: 0.8
Batch: 340; loss: 4.53; acc: 0.72
Batch: 360; loss: 5.05; acc: 0.7
Batch: 380; loss: 4.62; acc: 0.8
Batch: 400; loss: 6.25; acc: 0.75
Batch: 420; loss: 4.38; acc: 0.84
Batch: 440; loss: 2.47; acc: 0.83
Batch: 460; loss: 7.17; acc: 0.69
Batch: 480; loss: 4.85; acc: 0.72
Batch: 500; loss: 3.46; acc: 0.77
Batch: 520; loss: 4.64; acc: 0.8
Batch: 540; loss: 3.78; acc: 0.78
Batch: 560; loss: 4.18; acc: 0.75
Batch: 580; loss: 6.19; acc: 0.72
Batch: 600; loss: 5.82; acc: 0.73
Batch: 620; loss: 4.63; acc: 0.73
Train Epoch over. train_loss: 4.54; train_accuracy: 0.77 

Batch: 0; loss: 3.38; acc: 0.73
Batch: 20; loss: 7.04; acc: 0.7
Batch: 40; loss: 2.59; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.8
Batch: 80; loss: 5.26; acc: 0.7
Batch: 100; loss: 4.36; acc: 0.75
Batch: 120; loss: 2.51; acc: 0.86
Batch: 140; loss: 4.89; acc: 0.73
Val Epoch over. val_loss: 4.544097934558892; val_accuracy: 0.7627388535031847 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 4.5; acc: 0.75
Batch: 20; loss: 8.23; acc: 0.66
Batch: 40; loss: 3.71; acc: 0.81
Batch: 60; loss: 3.63; acc: 0.8
Batch: 80; loss: 4.34; acc: 0.77
Batch: 100; loss: 3.06; acc: 0.83
Batch: 120; loss: 5.48; acc: 0.7
Batch: 140; loss: 2.89; acc: 0.81
Batch: 160; loss: 4.68; acc: 0.77
Batch: 180; loss: 3.29; acc: 0.81
Batch: 200; loss: 4.42; acc: 0.72
Batch: 220; loss: 5.07; acc: 0.75
Batch: 240; loss: 3.67; acc: 0.83
Batch: 260; loss: 2.33; acc: 0.89
Batch: 280; loss: 6.66; acc: 0.78
Batch: 300; loss: 3.84; acc: 0.84
Batch: 320; loss: 4.27; acc: 0.77
Batch: 340; loss: 4.77; acc: 0.77
Batch: 360; loss: 7.04; acc: 0.7
Batch: 380; loss: 4.48; acc: 0.78
Batch: 400; loss: 5.18; acc: 0.81
Batch: 420; loss: 2.69; acc: 0.78
Batch: 440; loss: 2.09; acc: 0.84
Batch: 460; loss: 3.84; acc: 0.75
Batch: 480; loss: 5.19; acc: 0.78
Batch: 500; loss: 9.06; acc: 0.66
Batch: 520; loss: 5.91; acc: 0.72
Batch: 540; loss: 2.15; acc: 0.81
Batch: 560; loss: 3.46; acc: 0.8
Batch: 580; loss: 4.22; acc: 0.8
Batch: 600; loss: 4.13; acc: 0.8
Batch: 620; loss: 6.6; acc: 0.69
Train Epoch over. train_loss: 4.57; train_accuracy: 0.77 

Batch: 0; loss: 3.51; acc: 0.8
Batch: 20; loss: 6.03; acc: 0.66
Batch: 40; loss: 2.04; acc: 0.84
Batch: 60; loss: 4.95; acc: 0.75
Batch: 80; loss: 4.4; acc: 0.77
Batch: 100; loss: 3.87; acc: 0.73
Batch: 120; loss: 3.4; acc: 0.84
Batch: 140; loss: 5.11; acc: 0.72
Val Epoch over. val_loss: 4.2698418167745995; val_accuracy: 0.7759753184713376 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 3.42; acc: 0.77
Batch: 20; loss: 6.03; acc: 0.69
Batch: 40; loss: 1.78; acc: 0.86
Batch: 60; loss: 6.03; acc: 0.7
Batch: 80; loss: 2.59; acc: 0.77
Batch: 100; loss: 4.57; acc: 0.7
Batch: 120; loss: 5.16; acc: 0.81
Batch: 140; loss: 3.73; acc: 0.78
Batch: 160; loss: 4.43; acc: 0.83
Batch: 180; loss: 2.22; acc: 0.89
Batch: 200; loss: 4.37; acc: 0.8
Batch: 220; loss: 5.97; acc: 0.72
Batch: 240; loss: 4.02; acc: 0.77
Batch: 260; loss: 5.33; acc: 0.73
Batch: 280; loss: 3.72; acc: 0.81
Batch: 300; loss: 3.96; acc: 0.77
Batch: 320; loss: 3.92; acc: 0.75
Batch: 340; loss: 3.73; acc: 0.72
Batch: 360; loss: 3.28; acc: 0.83
Batch: 380; loss: 5.41; acc: 0.77
Batch: 400; loss: 3.73; acc: 0.8
Batch: 420; loss: 3.85; acc: 0.75
Batch: 440; loss: 8.18; acc: 0.59
Batch: 460; loss: 3.36; acc: 0.77
Batch: 480; loss: 4.81; acc: 0.8
Batch: 500; loss: 3.52; acc: 0.83
Batch: 520; loss: 5.77; acc: 0.72
Batch: 540; loss: 2.2; acc: 0.81
Batch: 560; loss: 5.54; acc: 0.73
Batch: 580; loss: 2.16; acc: 0.89
Batch: 600; loss: 3.79; acc: 0.81
Batch: 620; loss: 4.11; acc: 0.81
Train Epoch over. train_loss: 4.49; train_accuracy: 0.77 

Batch: 0; loss: 2.3; acc: 0.81
Batch: 20; loss: 7.84; acc: 0.72
Batch: 40; loss: 2.44; acc: 0.89
Batch: 60; loss: 6.53; acc: 0.7
Batch: 80; loss: 5.97; acc: 0.7
Batch: 100; loss: 4.98; acc: 0.8
Batch: 120; loss: 5.1; acc: 0.84
Batch: 140; loss: 6.23; acc: 0.7
Val Epoch over. val_loss: 4.638191734529604; val_accuracy: 0.7738853503184714 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 5.42; acc: 0.8
Batch: 20; loss: 2.41; acc: 0.84
Batch: 40; loss: 6.66; acc: 0.67
Batch: 60; loss: 6.65; acc: 0.69
Batch: 80; loss: 6.97; acc: 0.72
Batch: 100; loss: 2.72; acc: 0.86
Batch: 120; loss: 2.7; acc: 0.83
Batch: 140; loss: 5.49; acc: 0.73
Batch: 160; loss: 2.53; acc: 0.88
Batch: 180; loss: 5.26; acc: 0.75
Batch: 200; loss: 3.93; acc: 0.77
Batch: 220; loss: 4.68; acc: 0.69
Batch: 240; loss: 8.4; acc: 0.69
Batch: 260; loss: 4.07; acc: 0.7
Batch: 280; loss: 3.54; acc: 0.81
Batch: 300; loss: 3.93; acc: 0.77
Batch: 320; loss: 3.72; acc: 0.83
Batch: 340; loss: 5.16; acc: 0.72
Batch: 360; loss: 3.22; acc: 0.8
Batch: 380; loss: 4.08; acc: 0.69
Batch: 400; loss: 5.29; acc: 0.67
Batch: 420; loss: 5.02; acc: 0.8
Batch: 440; loss: 6.08; acc: 0.66
Batch: 460; loss: 3.03; acc: 0.84
Batch: 480; loss: 3.36; acc: 0.8
Batch: 500; loss: 4.66; acc: 0.8
Batch: 520; loss: 4.82; acc: 0.72
Batch: 540; loss: 7.19; acc: 0.67
Batch: 560; loss: 6.32; acc: 0.73
Batch: 580; loss: 0.86; acc: 0.95
Batch: 600; loss: 6.47; acc: 0.78
Batch: 620; loss: 2.96; acc: 0.81
Train Epoch over. train_loss: 4.48; train_accuracy: 0.78 

Batch: 0; loss: 2.31; acc: 0.8
Batch: 20; loss: 7.35; acc: 0.66
Batch: 40; loss: 2.15; acc: 0.86
Batch: 60; loss: 3.1; acc: 0.8
Batch: 80; loss: 3.29; acc: 0.84
Batch: 100; loss: 4.55; acc: 0.81
Batch: 120; loss: 3.82; acc: 0.84
Batch: 140; loss: 6.96; acc: 0.67
Val Epoch over. val_loss: 4.376118129985348; val_accuracy: 0.7780652866242038 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 3.34; acc: 0.81
Batch: 20; loss: 5.6; acc: 0.78
Batch: 40; loss: 2.96; acc: 0.78
Batch: 60; loss: 3.48; acc: 0.83
Batch: 80; loss: 5.49; acc: 0.7
Batch: 100; loss: 2.27; acc: 0.88
Batch: 120; loss: 4.81; acc: 0.75
Batch: 140; loss: 1.49; acc: 0.84
Batch: 160; loss: 3.5; acc: 0.78
Batch: 180; loss: 5.65; acc: 0.7
Batch: 200; loss: 10.43; acc: 0.66
Batch: 220; loss: 4.82; acc: 0.73
Batch: 240; loss: 3.2; acc: 0.81
Batch: 260; loss: 3.43; acc: 0.75
Batch: 280; loss: 4.82; acc: 0.78
Batch: 300; loss: 4.6; acc: 0.73
Batch: 320; loss: 2.14; acc: 0.83
Batch: 340; loss: 7.55; acc: 0.67
Batch: 360; loss: 2.59; acc: 0.83
Batch: 380; loss: 2.69; acc: 0.83
Batch: 400; loss: 7.03; acc: 0.73
Batch: 420; loss: 4.11; acc: 0.77
Batch: 440; loss: 2.62; acc: 0.78
Batch: 460; loss: 5.41; acc: 0.77
Batch: 480; loss: 4.92; acc: 0.72
Batch: 500; loss: 5.19; acc: 0.7
Batch: 520; loss: 2.45; acc: 0.84
Batch: 540; loss: 6.02; acc: 0.75
Batch: 560; loss: 4.33; acc: 0.77
Batch: 580; loss: 1.96; acc: 0.86
Batch: 600; loss: 2.61; acc: 0.77
Batch: 620; loss: 3.74; acc: 0.75
Train Epoch over. train_loss: 4.45; train_accuracy: 0.78 

Batch: 0; loss: 3.32; acc: 0.75
Batch: 20; loss: 8.66; acc: 0.62
Batch: 40; loss: 4.98; acc: 0.84
Batch: 60; loss: 5.72; acc: 0.73
Batch: 80; loss: 7.58; acc: 0.73
Batch: 100; loss: 4.5; acc: 0.77
Batch: 120; loss: 3.98; acc: 0.81
Batch: 140; loss: 5.08; acc: 0.75
Val Epoch over. val_loss: 4.756079993430217; val_accuracy: 0.7668192675159236 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 3.64; acc: 0.77
Batch: 20; loss: 3.6; acc: 0.75
Batch: 40; loss: 5.05; acc: 0.78
Batch: 60; loss: 5.38; acc: 0.75
Batch: 80; loss: 5.97; acc: 0.67
Batch: 100; loss: 8.12; acc: 0.73
Batch: 120; loss: 4.43; acc: 0.77
Batch: 140; loss: 3.25; acc: 0.75
Batch: 160; loss: 5.54; acc: 0.8
Batch: 180; loss: 2.85; acc: 0.73
Batch: 200; loss: 3.49; acc: 0.88
Batch: 220; loss: 4.88; acc: 0.77
Batch: 240; loss: 4.7; acc: 0.75
Batch: 260; loss: 3.68; acc: 0.81
Batch: 280; loss: 4.01; acc: 0.73
Batch: 300; loss: 4.26; acc: 0.8
Batch: 320; loss: 4.15; acc: 0.77
Batch: 340; loss: 3.31; acc: 0.78
Batch: 360; loss: 3.02; acc: 0.8
Batch: 380; loss: 4.55; acc: 0.72
Batch: 400; loss: 3.68; acc: 0.78
Batch: 420; loss: 3.42; acc: 0.78
Batch: 440; loss: 6.39; acc: 0.72
Batch: 460; loss: 6.38; acc: 0.7
Batch: 480; loss: 3.53; acc: 0.83
Batch: 500; loss: 4.03; acc: 0.77
Batch: 520; loss: 3.82; acc: 0.78
Batch: 540; loss: 4.14; acc: 0.77
Batch: 560; loss: 7.67; acc: 0.62
Batch: 580; loss: 4.98; acc: 0.78
Batch: 600; loss: 4.96; acc: 0.78
Batch: 620; loss: 5.0; acc: 0.81
Train Epoch over. train_loss: 4.36; train_accuracy: 0.78 

Batch: 0; loss: 2.47; acc: 0.81
Batch: 20; loss: 9.67; acc: 0.61
Batch: 40; loss: 3.7; acc: 0.83
Batch: 60; loss: 5.73; acc: 0.75
Batch: 80; loss: 3.52; acc: 0.78
Batch: 100; loss: 4.73; acc: 0.72
Batch: 120; loss: 4.44; acc: 0.86
Batch: 140; loss: 6.08; acc: 0.77
Val Epoch over. val_loss: 4.458662656841764; val_accuracy: 0.779359076433121 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 1.87; acc: 0.86
Batch: 20; loss: 3.86; acc: 0.81
Batch: 40; loss: 4.64; acc: 0.8
Batch: 60; loss: 4.79; acc: 0.8
Batch: 80; loss: 6.64; acc: 0.7
Batch: 100; loss: 6.62; acc: 0.72
Batch: 120; loss: 3.14; acc: 0.78
Batch: 140; loss: 4.33; acc: 0.8
Batch: 160; loss: 1.92; acc: 0.88
Batch: 180; loss: 3.69; acc: 0.81
Batch: 200; loss: 4.17; acc: 0.84
Batch: 220; loss: 7.4; acc: 0.72
Batch: 240; loss: 4.8; acc: 0.78
Batch: 260; loss: 3.63; acc: 0.8
Batch: 280; loss: 3.36; acc: 0.78
Batch: 300; loss: 2.95; acc: 0.81
Batch: 320; loss: 4.92; acc: 0.8
Batch: 340; loss: 6.21; acc: 0.75
Batch: 360; loss: 3.95; acc: 0.8
Batch: 380; loss: 5.87; acc: 0.72
Batch: 400; loss: 2.61; acc: 0.73
Batch: 420; loss: 5.14; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.92
Batch: 460; loss: 5.01; acc: 0.81
Batch: 480; loss: 4.78; acc: 0.8
Batch: 500; loss: 2.89; acc: 0.86
Batch: 520; loss: 2.68; acc: 0.83
Batch: 540; loss: 4.58; acc: 0.78
Batch: 560; loss: 5.16; acc: 0.72
Batch: 580; loss: 3.19; acc: 0.8
Batch: 600; loss: 8.52; acc: 0.69
Batch: 620; loss: 7.36; acc: 0.66
Train Epoch over. train_loss: 4.31; train_accuracy: 0.78 

Batch: 0; loss: 2.37; acc: 0.83
Batch: 20; loss: 7.3; acc: 0.66
Batch: 40; loss: 3.81; acc: 0.84
Batch: 60; loss: 4.19; acc: 0.77
Batch: 80; loss: 5.46; acc: 0.72
Batch: 100; loss: 5.04; acc: 0.72
Batch: 120; loss: 2.85; acc: 0.83
Batch: 140; loss: 6.22; acc: 0.73
Val Epoch over. val_loss: 4.603984349852155; val_accuracy: 0.7715963375796179 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 3.43; acc: 0.8
Batch: 20; loss: 3.97; acc: 0.83
Batch: 40; loss: 3.84; acc: 0.84
Batch: 60; loss: 1.41; acc: 0.89
Batch: 80; loss: 3.2; acc: 0.77
Batch: 100; loss: 4.35; acc: 0.8
Batch: 120; loss: 4.12; acc: 0.84
Batch: 140; loss: 2.57; acc: 0.8
Batch: 160; loss: 2.13; acc: 0.84
Batch: 180; loss: 4.37; acc: 0.78
Batch: 200; loss: 4.26; acc: 0.73
Batch: 220; loss: 5.25; acc: 0.77
Batch: 240; loss: 1.51; acc: 0.91
Batch: 260; loss: 3.98; acc: 0.75
Batch: 280; loss: 5.64; acc: 0.84
Batch: 300; loss: 0.76; acc: 0.88
Batch: 320; loss: 4.98; acc: 0.78
Batch: 340; loss: 2.84; acc: 0.88
Batch: 360; loss: 2.96; acc: 0.83
Batch: 380; loss: 6.97; acc: 0.75
Batch: 400; loss: 8.06; acc: 0.73
Batch: 420; loss: 4.47; acc: 0.78
Batch: 440; loss: 4.95; acc: 0.75
Batch: 460; loss: 4.28; acc: 0.78
Batch: 480; loss: 7.21; acc: 0.77
Batch: 500; loss: 4.7; acc: 0.83
Batch: 520; loss: 6.6; acc: 0.72
Batch: 540; loss: 4.54; acc: 0.81
Batch: 560; loss: 5.07; acc: 0.73
Batch: 580; loss: 2.94; acc: 0.78
Batch: 600; loss: 3.46; acc: 0.8
Batch: 620; loss: 7.52; acc: 0.69
Train Epoch over. train_loss: 4.35; train_accuracy: 0.78 

Batch: 0; loss: 3.56; acc: 0.78
Batch: 20; loss: 8.55; acc: 0.62
Batch: 40; loss: 3.91; acc: 0.78
Batch: 60; loss: 5.97; acc: 0.67
Batch: 80; loss: 4.92; acc: 0.7
Batch: 100; loss: 5.31; acc: 0.72
Batch: 120; loss: 3.52; acc: 0.8
Batch: 140; loss: 7.35; acc: 0.67
Val Epoch over. val_loss: 5.2323125623593665; val_accuracy: 0.7467157643312102 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 5.35; acc: 0.72
Batch: 20; loss: 4.12; acc: 0.7
Batch: 40; loss: 2.85; acc: 0.84
Batch: 60; loss: 4.35; acc: 0.72
Batch: 80; loss: 4.9; acc: 0.81
Batch: 100; loss: 4.06; acc: 0.83
Batch: 120; loss: 6.24; acc: 0.73
Batch: 140; loss: 4.86; acc: 0.77
Batch: 160; loss: 4.3; acc: 0.73
Batch: 180; loss: 7.08; acc: 0.7
Batch: 200; loss: 3.08; acc: 0.83
Batch: 220; loss: 4.9; acc: 0.69
Batch: 240; loss: 2.05; acc: 0.83
Batch: 260; loss: 2.27; acc: 0.8
Batch: 280; loss: 5.21; acc: 0.8
Batch: 300; loss: 6.59; acc: 0.66
Batch: 320; loss: 3.05; acc: 0.86
Batch: 340; loss: 6.18; acc: 0.66
Batch: 360; loss: 5.67; acc: 0.78
Batch: 380; loss: 1.9; acc: 0.88
Batch: 400; loss: 5.05; acc: 0.72
Batch: 420; loss: 6.1; acc: 0.78
Batch: 440; loss: 6.41; acc: 0.73
Batch: 460; loss: 3.04; acc: 0.8
Batch: 480; loss: 5.4; acc: 0.72
Batch: 500; loss: 4.68; acc: 0.77
Batch: 520; loss: 6.62; acc: 0.77
Batch: 540; loss: 2.22; acc: 0.84
Batch: 560; loss: 5.25; acc: 0.72
Batch: 580; loss: 3.66; acc: 0.81
Batch: 600; loss: 2.67; acc: 0.83
Batch: 620; loss: 5.87; acc: 0.75
Train Epoch over. train_loss: 4.42; train_accuracy: 0.78 

Batch: 0; loss: 3.16; acc: 0.81
Batch: 20; loss: 8.33; acc: 0.64
Batch: 40; loss: 3.43; acc: 0.89
Batch: 60; loss: 5.04; acc: 0.73
Batch: 80; loss: 3.9; acc: 0.77
Batch: 100; loss: 6.26; acc: 0.72
Batch: 120; loss: 3.67; acc: 0.88
Batch: 140; loss: 5.74; acc: 0.73
Val Epoch over. val_loss: 4.710805754752675; val_accuracy: 0.7745820063694268 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.76; acc: 0.8
Batch: 20; loss: 2.37; acc: 0.81
Batch: 40; loss: 3.03; acc: 0.81
Batch: 60; loss: 3.12; acc: 0.84
Batch: 80; loss: 1.61; acc: 0.88
Batch: 100; loss: 2.81; acc: 0.86
Batch: 120; loss: 3.28; acc: 0.88
Batch: 140; loss: 1.93; acc: 0.84
Batch: 160; loss: 1.27; acc: 0.89
Batch: 180; loss: 3.46; acc: 0.75
Batch: 200; loss: 1.43; acc: 0.88
Batch: 220; loss: 5.03; acc: 0.75
Batch: 240; loss: 3.48; acc: 0.75
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 2.43; acc: 0.83
Batch: 300; loss: 2.24; acc: 0.88
Batch: 320; loss: 3.86; acc: 0.81
Batch: 340; loss: 4.71; acc: 0.78
Batch: 360; loss: 4.06; acc: 0.83
Batch: 380; loss: 2.52; acc: 0.8
Batch: 400; loss: 2.07; acc: 0.86
Batch: 420; loss: 2.97; acc: 0.84
Batch: 440; loss: 2.9; acc: 0.81
Batch: 460; loss: 4.68; acc: 0.78
Batch: 480; loss: 1.65; acc: 0.83
Batch: 500; loss: 3.44; acc: 0.83
Batch: 520; loss: 2.61; acc: 0.84
Batch: 540; loss: 0.85; acc: 0.92
Batch: 560; loss: 3.36; acc: 0.8
Batch: 580; loss: 2.67; acc: 0.84
Batch: 600; loss: 1.5; acc: 0.86
Batch: 620; loss: 3.71; acc: 0.78
Train Epoch over. train_loss: 2.9; train_accuracy: 0.83 

Batch: 0; loss: 3.04; acc: 0.81
Batch: 20; loss: 5.22; acc: 0.66
Batch: 40; loss: 2.03; acc: 0.92
Batch: 60; loss: 2.34; acc: 0.81
Batch: 80; loss: 1.91; acc: 0.83
Batch: 100; loss: 3.29; acc: 0.73
Batch: 120; loss: 2.44; acc: 0.86
Batch: 140; loss: 4.35; acc: 0.69
Val Epoch over. val_loss: 3.06178479209827; val_accuracy: 0.8146894904458599 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.33; acc: 0.75
Batch: 20; loss: 2.47; acc: 0.83
Batch: 40; loss: 2.17; acc: 0.84
Batch: 60; loss: 3.21; acc: 0.75
Batch: 80; loss: 4.01; acc: 0.73
Batch: 100; loss: 3.02; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.92
Batch: 140; loss: 1.53; acc: 0.81
Batch: 160; loss: 3.31; acc: 0.81
Batch: 180; loss: 3.05; acc: 0.83
Batch: 200; loss: 2.0; acc: 0.8
Batch: 220; loss: 1.69; acc: 0.89
Batch: 240; loss: 3.03; acc: 0.8
Batch: 260; loss: 1.05; acc: 0.95
Batch: 280; loss: 2.92; acc: 0.84
Batch: 300; loss: 2.41; acc: 0.8
Batch: 320; loss: 3.27; acc: 0.84
Batch: 340; loss: 2.2; acc: 0.83
Batch: 360; loss: 2.12; acc: 0.84
Batch: 380; loss: 4.54; acc: 0.81
Batch: 400; loss: 2.46; acc: 0.81
Batch: 420; loss: 3.42; acc: 0.81
Batch: 440; loss: 2.55; acc: 0.84
Batch: 460; loss: 1.53; acc: 0.86
Batch: 480; loss: 3.48; acc: 0.77
Batch: 500; loss: 3.14; acc: 0.81
Batch: 520; loss: 0.96; acc: 0.94
Batch: 540; loss: 1.67; acc: 0.88
Batch: 560; loss: 1.18; acc: 0.94
Batch: 580; loss: 4.2; acc: 0.78
Batch: 600; loss: 2.78; acc: 0.81
Batch: 620; loss: 3.3; acc: 0.81
Train Epoch over. train_loss: 2.56; train_accuracy: 0.83 

Batch: 0; loss: 3.09; acc: 0.78
Batch: 20; loss: 5.26; acc: 0.69
Batch: 40; loss: 1.84; acc: 0.91
Batch: 60; loss: 2.41; acc: 0.83
Batch: 80; loss: 2.25; acc: 0.84
Batch: 100; loss: 3.1; acc: 0.8
Batch: 120; loss: 2.46; acc: 0.86
Batch: 140; loss: 3.79; acc: 0.67
Val Epoch over. val_loss: 2.9649382336124495; val_accuracy: 0.8144904458598726 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.11; acc: 0.84
Batch: 20; loss: 1.79; acc: 0.86
Batch: 40; loss: 1.87; acc: 0.88
Batch: 60; loss: 2.47; acc: 0.81
Batch: 80; loss: 1.32; acc: 0.89
Batch: 100; loss: 2.18; acc: 0.83
Batch: 120; loss: 1.29; acc: 0.88
Batch: 140; loss: 1.79; acc: 0.84
Batch: 160; loss: 1.87; acc: 0.89
Batch: 180; loss: 1.35; acc: 0.89
Batch: 200; loss: 2.11; acc: 0.83
Batch: 220; loss: 2.89; acc: 0.72
Batch: 240; loss: 2.46; acc: 0.86
Batch: 260; loss: 3.95; acc: 0.73
Batch: 280; loss: 2.66; acc: 0.78
Batch: 300; loss: 1.33; acc: 0.81
Batch: 320; loss: 2.79; acc: 0.81
Batch: 340; loss: 2.66; acc: 0.84
Batch: 360; loss: 2.38; acc: 0.83
Batch: 380; loss: 3.0; acc: 0.78
Batch: 400; loss: 1.39; acc: 0.89
Batch: 420; loss: 4.35; acc: 0.78
Batch: 440; loss: 0.72; acc: 0.94
Batch: 460; loss: 2.1; acc: 0.91
Batch: 480; loss: 1.97; acc: 0.89
Batch: 500; loss: 1.83; acc: 0.89
Batch: 520; loss: 2.19; acc: 0.81
Batch: 540; loss: 1.68; acc: 0.84
Batch: 560; loss: 2.11; acc: 0.84
Batch: 580; loss: 1.93; acc: 0.91
Batch: 600; loss: 1.09; acc: 0.91
Batch: 620; loss: 1.68; acc: 0.84
Train Epoch over. train_loss: 2.48; train_accuracy: 0.83 

Batch: 0; loss: 2.97; acc: 0.8
Batch: 20; loss: 5.71; acc: 0.7
Batch: 40; loss: 2.01; acc: 0.91
Batch: 60; loss: 2.17; acc: 0.84
Batch: 80; loss: 1.8; acc: 0.86
Batch: 100; loss: 3.29; acc: 0.73
Batch: 120; loss: 1.79; acc: 0.91
Batch: 140; loss: 4.18; acc: 0.69
Val Epoch over. val_loss: 2.9091028394592797; val_accuracy: 0.814390923566879 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.17; acc: 0.83
Batch: 20; loss: 3.38; acc: 0.81
Batch: 40; loss: 2.73; acc: 0.84
Batch: 60; loss: 1.41; acc: 0.89
Batch: 80; loss: 1.15; acc: 0.89
Batch: 100; loss: 2.58; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 2.52; acc: 0.81
Batch: 160; loss: 2.26; acc: 0.83
Batch: 180; loss: 2.79; acc: 0.8
Batch: 200; loss: 2.75; acc: 0.8
Batch: 220; loss: 2.47; acc: 0.86
Batch: 240; loss: 2.59; acc: 0.81
Batch: 260; loss: 3.15; acc: 0.84
Batch: 280; loss: 1.36; acc: 0.86
Batch: 300; loss: 1.95; acc: 0.84
Batch: 320; loss: 2.12; acc: 0.89
Batch: 340; loss: 1.32; acc: 0.83
Batch: 360; loss: 1.81; acc: 0.8
Batch: 380; loss: 1.76; acc: 0.86
Batch: 400; loss: 2.17; acc: 0.83
Batch: 420; loss: 1.72; acc: 0.88
Batch: 440; loss: 4.46; acc: 0.73
Batch: 460; loss: 2.86; acc: 0.83
Batch: 480; loss: 1.26; acc: 0.86
Batch: 500; loss: 1.32; acc: 0.89
Batch: 520; loss: 4.07; acc: 0.77
Batch: 540; loss: 1.51; acc: 0.89
Batch: 560; loss: 3.11; acc: 0.84
Batch: 580; loss: 3.05; acc: 0.84
Batch: 600; loss: 1.93; acc: 0.84
Batch: 620; loss: 1.35; acc: 0.86
Train Epoch over. train_loss: 2.43; train_accuracy: 0.83 

Batch: 0; loss: 3.11; acc: 0.8
Batch: 20; loss: 4.98; acc: 0.72
Batch: 40; loss: 1.78; acc: 0.91
Batch: 60; loss: 1.8; acc: 0.84
Batch: 80; loss: 1.61; acc: 0.84
Batch: 100; loss: 3.11; acc: 0.77
Batch: 120; loss: 1.78; acc: 0.91
Batch: 140; loss: 4.04; acc: 0.72
Val Epoch over. val_loss: 2.8603885369316027; val_accuracy: 0.8165804140127388 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.81; acc: 0.77
Batch: 20; loss: 5.15; acc: 0.75
Batch: 40; loss: 1.36; acc: 0.88
Batch: 60; loss: 0.99; acc: 0.91
Batch: 80; loss: 1.86; acc: 0.86
Batch: 100; loss: 1.21; acc: 0.92
Batch: 120; loss: 2.58; acc: 0.77
Batch: 140; loss: 2.6; acc: 0.84
Batch: 160; loss: 2.36; acc: 0.84
Batch: 180; loss: 1.79; acc: 0.89
Batch: 200; loss: 2.74; acc: 0.8
Batch: 220; loss: 1.93; acc: 0.81
Batch: 240; loss: 1.9; acc: 0.83
Batch: 260; loss: 2.26; acc: 0.84
Batch: 280; loss: 2.26; acc: 0.89
Batch: 300; loss: 2.9; acc: 0.8
Batch: 320; loss: 3.22; acc: 0.8
Batch: 340; loss: 3.61; acc: 0.86
Batch: 360; loss: 2.62; acc: 0.75
Batch: 380; loss: 1.53; acc: 0.81
Batch: 400; loss: 3.12; acc: 0.77
Batch: 420; loss: 0.87; acc: 0.89
Batch: 440; loss: 2.13; acc: 0.84
Batch: 460; loss: 1.77; acc: 0.89
Batch: 480; loss: 2.25; acc: 0.86
Batch: 500; loss: 1.57; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.92
Batch: 540; loss: 1.79; acc: 0.81
Batch: 560; loss: 1.04; acc: 0.88
Batch: 580; loss: 2.18; acc: 0.8
Batch: 600; loss: 1.42; acc: 0.86
Batch: 620; loss: 3.28; acc: 0.77
Train Epoch over. train_loss: 2.4; train_accuracy: 0.83 

Batch: 0; loss: 2.74; acc: 0.78
Batch: 20; loss: 4.75; acc: 0.72
Batch: 40; loss: 1.98; acc: 0.89
Batch: 60; loss: 1.79; acc: 0.84
Batch: 80; loss: 1.9; acc: 0.86
Batch: 100; loss: 3.07; acc: 0.8
Batch: 120; loss: 1.7; acc: 0.88
Batch: 140; loss: 3.81; acc: 0.72
Val Epoch over. val_loss: 2.817934792892189; val_accuracy: 0.8180732484076433 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.43; acc: 0.89
Batch: 20; loss: 1.79; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.95
Batch: 80; loss: 1.54; acc: 0.88
Batch: 100; loss: 4.92; acc: 0.75
Batch: 120; loss: 2.18; acc: 0.84
Batch: 140; loss: 2.49; acc: 0.83
Batch: 160; loss: 1.42; acc: 0.88
Batch: 180; loss: 2.35; acc: 0.86
Batch: 200; loss: 1.38; acc: 0.89
Batch: 220; loss: 2.24; acc: 0.77
Batch: 240; loss: 1.6; acc: 0.89
Batch: 260; loss: 1.26; acc: 0.92
Batch: 280; loss: 2.92; acc: 0.84
Batch: 300; loss: 3.17; acc: 0.77
Batch: 320; loss: 1.84; acc: 0.8
Batch: 340; loss: 1.33; acc: 0.89
Batch: 360; loss: 2.43; acc: 0.77
Batch: 380; loss: 4.65; acc: 0.78
Batch: 400; loss: 1.95; acc: 0.91
Batch: 420; loss: 3.61; acc: 0.75
Batch: 440; loss: 2.78; acc: 0.86
Batch: 460; loss: 2.82; acc: 0.81
Batch: 480; loss: 2.3; acc: 0.81
Batch: 500; loss: 1.35; acc: 0.83
Batch: 520; loss: 2.58; acc: 0.84
Batch: 540; loss: 4.14; acc: 0.88
Batch: 560; loss: 2.13; acc: 0.92
Batch: 580; loss: 2.69; acc: 0.84
Batch: 600; loss: 4.67; acc: 0.77
Batch: 620; loss: 2.57; acc: 0.84
Train Epoch over. train_loss: 2.38; train_accuracy: 0.83 

Batch: 0; loss: 2.61; acc: 0.8
Batch: 20; loss: 5.25; acc: 0.69
Batch: 40; loss: 1.81; acc: 0.89
Batch: 60; loss: 1.98; acc: 0.81
Batch: 80; loss: 1.58; acc: 0.81
Batch: 100; loss: 3.16; acc: 0.78
Batch: 120; loss: 1.68; acc: 0.91
Batch: 140; loss: 3.68; acc: 0.73
Val Epoch over. val_loss: 2.7760919579275094; val_accuracy: 0.8174761146496815 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.6; acc: 0.83
Batch: 20; loss: 3.18; acc: 0.78
Batch: 40; loss: 1.36; acc: 0.84
Batch: 60; loss: 2.38; acc: 0.84
Batch: 80; loss: 2.7; acc: 0.86
Batch: 100; loss: 1.61; acc: 0.89
Batch: 120; loss: 2.46; acc: 0.81
Batch: 140; loss: 2.76; acc: 0.8
Batch: 160; loss: 2.75; acc: 0.83
Batch: 180; loss: 1.76; acc: 0.81
Batch: 200; loss: 1.09; acc: 0.88
Batch: 220; loss: 2.23; acc: 0.86
Batch: 240; loss: 2.85; acc: 0.8
Batch: 260; loss: 2.2; acc: 0.89
Batch: 280; loss: 1.81; acc: 0.86
Batch: 300; loss: 2.93; acc: 0.83
Batch: 320; loss: 1.98; acc: 0.84
Batch: 340; loss: 3.54; acc: 0.8
Batch: 360; loss: 0.72; acc: 0.94
Batch: 380; loss: 1.14; acc: 0.92
Batch: 400; loss: 2.07; acc: 0.88
Batch: 420; loss: 1.85; acc: 0.86
Batch: 440; loss: 0.86; acc: 0.88
Batch: 460; loss: 2.62; acc: 0.81
Batch: 480; loss: 1.81; acc: 0.8
Batch: 500; loss: 3.96; acc: 0.81
Batch: 520; loss: 3.45; acc: 0.83
Batch: 540; loss: 4.32; acc: 0.78
Batch: 560; loss: 2.35; acc: 0.8
Batch: 580; loss: 3.12; acc: 0.88
Batch: 600; loss: 1.81; acc: 0.84
Batch: 620; loss: 3.79; acc: 0.83
Train Epoch over. train_loss: 2.37; train_accuracy: 0.83 

Batch: 0; loss: 2.81; acc: 0.78
Batch: 20; loss: 5.27; acc: 0.7
Batch: 40; loss: 1.78; acc: 0.89
Batch: 60; loss: 2.09; acc: 0.83
Batch: 80; loss: 1.86; acc: 0.8
Batch: 100; loss: 2.91; acc: 0.8
Batch: 120; loss: 1.63; acc: 0.89
Batch: 140; loss: 3.57; acc: 0.75
Val Epoch over. val_loss: 2.789239965426694; val_accuracy: 0.8174761146496815 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.36; acc: 0.78
Batch: 20; loss: 3.63; acc: 0.78
Batch: 40; loss: 2.09; acc: 0.84
Batch: 60; loss: 1.43; acc: 0.88
Batch: 80; loss: 3.27; acc: 0.78
Batch: 100; loss: 1.87; acc: 0.88
Batch: 120; loss: 1.48; acc: 0.88
Batch: 140; loss: 2.89; acc: 0.78
Batch: 160; loss: 1.91; acc: 0.84
Batch: 180; loss: 1.17; acc: 0.92
Batch: 200; loss: 1.83; acc: 0.89
Batch: 220; loss: 3.14; acc: 0.75
Batch: 240; loss: 2.73; acc: 0.81
Batch: 260; loss: 1.38; acc: 0.91
Batch: 280; loss: 1.61; acc: 0.81
Batch: 300; loss: 2.4; acc: 0.86
Batch: 320; loss: 2.27; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.91
Batch: 360; loss: 3.01; acc: 0.83
Batch: 380; loss: 1.16; acc: 0.94
Batch: 400; loss: 1.67; acc: 0.81
Batch: 420; loss: 2.19; acc: 0.84
Batch: 440; loss: 2.73; acc: 0.84
Batch: 460; loss: 2.38; acc: 0.8
Batch: 480; loss: 1.99; acc: 0.81
Batch: 500; loss: 2.81; acc: 0.83
Batch: 520; loss: 2.65; acc: 0.81
Batch: 540; loss: 1.84; acc: 0.84
Batch: 560; loss: 2.55; acc: 0.88
Batch: 580; loss: 1.36; acc: 0.88
Batch: 600; loss: 2.44; acc: 0.83
Batch: 620; loss: 0.74; acc: 0.91
Train Epoch over. train_loss: 2.36; train_accuracy: 0.83 

Batch: 0; loss: 2.76; acc: 0.78
Batch: 20; loss: 5.22; acc: 0.72
Batch: 40; loss: 1.57; acc: 0.91
Batch: 60; loss: 2.21; acc: 0.83
Batch: 80; loss: 1.76; acc: 0.83
Batch: 100; loss: 2.75; acc: 0.77
Batch: 120; loss: 1.38; acc: 0.89
Batch: 140; loss: 3.88; acc: 0.78
Val Epoch over. val_loss: 2.76567877477901; val_accuracy: 0.8198646496815286 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.12; acc: 0.86
Batch: 20; loss: 3.44; acc: 0.83
Batch: 40; loss: 1.29; acc: 0.88
Batch: 60; loss: 1.54; acc: 0.88
Batch: 80; loss: 3.9; acc: 0.81
Batch: 100; loss: 1.65; acc: 0.86
Batch: 120; loss: 3.45; acc: 0.78
Batch: 140; loss: 1.16; acc: 0.89
Batch: 160; loss: 1.44; acc: 0.83
Batch: 180; loss: 2.97; acc: 0.81
Batch: 200; loss: 3.69; acc: 0.77
Batch: 220; loss: 2.68; acc: 0.77
Batch: 240; loss: 1.43; acc: 0.8
Batch: 260; loss: 0.52; acc: 0.94
Batch: 280; loss: 2.52; acc: 0.84
Batch: 300; loss: 1.76; acc: 0.89
Batch: 320; loss: 2.15; acc: 0.84
Batch: 340; loss: 3.77; acc: 0.7
Batch: 360; loss: 3.65; acc: 0.8
Batch: 380; loss: 2.83; acc: 0.78
Batch: 400; loss: 3.15; acc: 0.78
Batch: 420; loss: 2.96; acc: 0.78
Batch: 440; loss: 1.1; acc: 0.86
Batch: 460; loss: 1.98; acc: 0.83
Batch: 480; loss: 4.04; acc: 0.78
Batch: 500; loss: 5.38; acc: 0.75
Batch: 520; loss: 1.39; acc: 0.88
Batch: 540; loss: 1.82; acc: 0.8
Batch: 560; loss: 1.0; acc: 0.91
Batch: 580; loss: 2.15; acc: 0.83
Batch: 600; loss: 1.55; acc: 0.89
Batch: 620; loss: 2.15; acc: 0.84
Train Epoch over. train_loss: 2.34; train_accuracy: 0.83 

Batch: 0; loss: 2.76; acc: 0.8
Batch: 20; loss: 5.27; acc: 0.69
Batch: 40; loss: 1.58; acc: 0.91
Batch: 60; loss: 2.16; acc: 0.81
Batch: 80; loss: 1.94; acc: 0.84
Batch: 100; loss: 3.17; acc: 0.78
Batch: 120; loss: 1.71; acc: 0.91
Batch: 140; loss: 3.88; acc: 0.77
Val Epoch over. val_loss: 2.761341582437989; val_accuracy: 0.8191679936305732 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.52; acc: 0.83
Batch: 20; loss: 2.63; acc: 0.84
Batch: 40; loss: 2.75; acc: 0.77
Batch: 60; loss: 1.45; acc: 0.77
Batch: 80; loss: 4.06; acc: 0.81
Batch: 100; loss: 1.22; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.95
Batch: 140; loss: 0.71; acc: 0.94
Batch: 160; loss: 0.77; acc: 0.88
Batch: 180; loss: 1.16; acc: 0.84
Batch: 200; loss: 1.35; acc: 0.89
Batch: 220; loss: 2.53; acc: 0.78
Batch: 240; loss: 1.86; acc: 0.8
Batch: 260; loss: 3.15; acc: 0.86
Batch: 280; loss: 3.67; acc: 0.8
Batch: 300; loss: 1.94; acc: 0.84
Batch: 320; loss: 3.77; acc: 0.77
Batch: 340; loss: 3.61; acc: 0.75
Batch: 360; loss: 2.6; acc: 0.84
Batch: 380; loss: 2.18; acc: 0.89
Batch: 400; loss: 2.05; acc: 0.86
Batch: 420; loss: 4.58; acc: 0.72
Batch: 440; loss: 3.4; acc: 0.81
Batch: 460; loss: 2.02; acc: 0.86
Batch: 480; loss: 1.85; acc: 0.83
Batch: 500; loss: 3.26; acc: 0.8
Batch: 520; loss: 2.07; acc: 0.75
Batch: 540; loss: 2.75; acc: 0.8
Batch: 560; loss: 2.59; acc: 0.81
Batch: 580; loss: 1.98; acc: 0.86
Batch: 600; loss: 1.27; acc: 0.83
Batch: 620; loss: 3.09; acc: 0.77
Train Epoch over. train_loss: 2.34; train_accuracy: 0.83 

Batch: 0; loss: 2.49; acc: 0.8
Batch: 20; loss: 5.35; acc: 0.7
Batch: 40; loss: 1.77; acc: 0.91
Batch: 60; loss: 2.09; acc: 0.81
Batch: 80; loss: 1.73; acc: 0.84
Batch: 100; loss: 2.86; acc: 0.77
Batch: 120; loss: 1.45; acc: 0.91
Batch: 140; loss: 3.8; acc: 0.77
Val Epoch over. val_loss: 2.778721325716395; val_accuracy: 0.8161823248407644 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.85; acc: 0.84
Batch: 20; loss: 4.03; acc: 0.78
Batch: 40; loss: 2.17; acc: 0.78
Batch: 60; loss: 1.76; acc: 0.81
Batch: 80; loss: 1.88; acc: 0.84
Batch: 100; loss: 2.01; acc: 0.81
Batch: 120; loss: 2.05; acc: 0.81
Batch: 140; loss: 2.54; acc: 0.8
Batch: 160; loss: 1.75; acc: 0.84
Batch: 180; loss: 3.19; acc: 0.78
Batch: 200; loss: 1.93; acc: 0.75
Batch: 220; loss: 2.5; acc: 0.83
Batch: 240; loss: 3.38; acc: 0.81
Batch: 260; loss: 2.41; acc: 0.84
Batch: 280; loss: 1.56; acc: 0.84
Batch: 300; loss: 2.22; acc: 0.89
Batch: 320; loss: 3.62; acc: 0.75
Batch: 340; loss: 2.09; acc: 0.88
Batch: 360; loss: 1.8; acc: 0.91
Batch: 380; loss: 2.54; acc: 0.84
Batch: 400; loss: 4.04; acc: 0.8
Batch: 420; loss: 1.91; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 1.04; acc: 0.91
Batch: 480; loss: 3.29; acc: 0.86
Batch: 500; loss: 2.85; acc: 0.83
Batch: 520; loss: 3.04; acc: 0.86
Batch: 540; loss: 3.33; acc: 0.75
Batch: 560; loss: 3.64; acc: 0.81
Batch: 580; loss: 0.99; acc: 0.91
Batch: 600; loss: 2.1; acc: 0.84
Batch: 620; loss: 1.74; acc: 0.86
Train Epoch over. train_loss: 2.24; train_accuracy: 0.84 

Batch: 0; loss: 2.71; acc: 0.78
Batch: 20; loss: 5.37; acc: 0.67
Batch: 40; loss: 1.67; acc: 0.91
Batch: 60; loss: 2.02; acc: 0.83
Batch: 80; loss: 1.74; acc: 0.86
Batch: 100; loss: 2.81; acc: 0.78
Batch: 120; loss: 1.42; acc: 0.88
Batch: 140; loss: 3.77; acc: 0.78
Val Epoch over. val_loss: 2.717903954208277; val_accuracy: 0.8209593949044586 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.35; acc: 0.88
Batch: 20; loss: 1.25; acc: 0.91
Batch: 40; loss: 2.01; acc: 0.88
Batch: 60; loss: 2.12; acc: 0.81
Batch: 80; loss: 3.45; acc: 0.77
Batch: 100; loss: 2.24; acc: 0.84
Batch: 120; loss: 3.17; acc: 0.8
Batch: 140; loss: 1.88; acc: 0.86
Batch: 160; loss: 2.22; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.95
Batch: 200; loss: 2.29; acc: 0.84
Batch: 220; loss: 3.11; acc: 0.86
Batch: 240; loss: 2.68; acc: 0.84
Batch: 260; loss: 1.9; acc: 0.88
Batch: 280; loss: 3.4; acc: 0.81
Batch: 300; loss: 2.97; acc: 0.81
Batch: 320; loss: 3.6; acc: 0.77
Batch: 340; loss: 2.15; acc: 0.83
Batch: 360; loss: 2.24; acc: 0.84
Batch: 380; loss: 1.71; acc: 0.83
Batch: 400; loss: 3.09; acc: 0.77
Batch: 420; loss: 2.77; acc: 0.81
Batch: 440; loss: 1.83; acc: 0.86
Batch: 460; loss: 0.77; acc: 0.86
Batch: 480; loss: 1.9; acc: 0.86
Batch: 500; loss: 3.13; acc: 0.8
Batch: 520; loss: 2.47; acc: 0.84
Batch: 540; loss: 3.3; acc: 0.75
Batch: 560; loss: 3.85; acc: 0.83
Batch: 580; loss: 5.04; acc: 0.77
Batch: 600; loss: 4.34; acc: 0.8
Batch: 620; loss: 2.26; acc: 0.83
Train Epoch over. train_loss: 2.22; train_accuracy: 0.84 

Batch: 0; loss: 2.85; acc: 0.77
Batch: 20; loss: 5.18; acc: 0.69
Batch: 40; loss: 1.7; acc: 0.89
Batch: 60; loss: 1.93; acc: 0.81
Batch: 80; loss: 1.72; acc: 0.84
Batch: 100; loss: 2.81; acc: 0.8
Batch: 120; loss: 1.41; acc: 0.89
Batch: 140; loss: 3.79; acc: 0.78
Val Epoch over. val_loss: 2.712395049963787; val_accuracy: 0.8190684713375797 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.95; acc: 0.88
Batch: 20; loss: 2.61; acc: 0.83
Batch: 40; loss: 1.56; acc: 0.84
Batch: 60; loss: 2.04; acc: 0.78
Batch: 80; loss: 4.08; acc: 0.78
Batch: 100; loss: 1.17; acc: 0.86
Batch: 120; loss: 2.68; acc: 0.86
Batch: 140; loss: 2.02; acc: 0.86
Batch: 160; loss: 3.33; acc: 0.83
Batch: 180; loss: 2.78; acc: 0.83
Batch: 200; loss: 3.36; acc: 0.8
Batch: 220; loss: 1.93; acc: 0.84
Batch: 240; loss: 3.34; acc: 0.81
Batch: 260; loss: 2.04; acc: 0.89
Batch: 280; loss: 2.2; acc: 0.84
Batch: 300; loss: 2.94; acc: 0.8
Batch: 320; loss: 1.54; acc: 0.84
Batch: 340; loss: 2.87; acc: 0.81
Batch: 360; loss: 1.59; acc: 0.88
Batch: 380; loss: 3.45; acc: 0.78
Batch: 400; loss: 1.57; acc: 0.88
Batch: 420; loss: 2.78; acc: 0.89
Batch: 440; loss: 2.1; acc: 0.88
Batch: 460; loss: 2.25; acc: 0.86
Batch: 480; loss: 1.04; acc: 0.89
Batch: 500; loss: 4.28; acc: 0.8
Batch: 520; loss: 3.31; acc: 0.78
Batch: 540; loss: 2.6; acc: 0.81
Batch: 560; loss: 1.6; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 1.01; acc: 0.86
Batch: 620; loss: 2.31; acc: 0.81
Train Epoch over. train_loss: 2.22; train_accuracy: 0.84 

Batch: 0; loss: 2.83; acc: 0.77
Batch: 20; loss: 5.19; acc: 0.69
Batch: 40; loss: 1.64; acc: 0.89
Batch: 60; loss: 2.0; acc: 0.83
Batch: 80; loss: 1.72; acc: 0.84
Batch: 100; loss: 2.79; acc: 0.8
Batch: 120; loss: 1.42; acc: 0.88
Batch: 140; loss: 3.79; acc: 0.77
Val Epoch over. val_loss: 2.7087860316227954; val_accuracy: 0.8200636942675159 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.12; acc: 0.84
Batch: 20; loss: 0.97; acc: 0.92
Batch: 40; loss: 1.36; acc: 0.91
Batch: 60; loss: 1.39; acc: 0.91
Batch: 80; loss: 2.4; acc: 0.84
Batch: 100; loss: 2.18; acc: 0.86
Batch: 120; loss: 2.38; acc: 0.83
Batch: 140; loss: 2.35; acc: 0.83
Batch: 160; loss: 2.51; acc: 0.89
Batch: 180; loss: 2.53; acc: 0.78
Batch: 200; loss: 2.76; acc: 0.83
Batch: 220; loss: 2.54; acc: 0.83
Batch: 240; loss: 2.26; acc: 0.81
Batch: 260; loss: 2.5; acc: 0.81
Batch: 280; loss: 3.96; acc: 0.81
Batch: 300; loss: 2.21; acc: 0.8
Batch: 320; loss: 2.25; acc: 0.83
Batch: 340; loss: 2.82; acc: 0.84
Batch: 360; loss: 1.11; acc: 0.88
Batch: 380; loss: 0.73; acc: 0.89
Batch: 400; loss: 4.21; acc: 0.72
Batch: 420; loss: 1.85; acc: 0.84
Batch: 440; loss: 2.35; acc: 0.78
Batch: 460; loss: 3.42; acc: 0.81
Batch: 480; loss: 4.28; acc: 0.75
Batch: 500; loss: 0.88; acc: 0.84
Batch: 520; loss: 2.2; acc: 0.81
Batch: 540; loss: 0.96; acc: 0.91
Batch: 560; loss: 2.27; acc: 0.81
Batch: 580; loss: 2.73; acc: 0.88
Batch: 600; loss: 1.06; acc: 0.91
Batch: 620; loss: 3.26; acc: 0.78
Train Epoch over. train_loss: 2.21; train_accuracy: 0.84 

Batch: 0; loss: 2.78; acc: 0.77
Batch: 20; loss: 5.25; acc: 0.69
Batch: 40; loss: 1.64; acc: 0.89
Batch: 60; loss: 2.08; acc: 0.83
Batch: 80; loss: 1.75; acc: 0.84
Batch: 100; loss: 2.78; acc: 0.78
Batch: 120; loss: 1.43; acc: 0.88
Batch: 140; loss: 3.74; acc: 0.78
Val Epoch over. val_loss: 2.7135230895060642; val_accuracy: 0.8202627388535032 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.74; acc: 0.86
Batch: 20; loss: 3.28; acc: 0.78
Batch: 40; loss: 2.31; acc: 0.84
Batch: 60; loss: 1.64; acc: 0.86
Batch: 80; loss: 1.75; acc: 0.78
Batch: 100; loss: 1.28; acc: 0.88
Batch: 120; loss: 1.8; acc: 0.86
Batch: 140; loss: 2.37; acc: 0.86
Batch: 160; loss: 1.74; acc: 0.84
Batch: 180; loss: 1.88; acc: 0.84
Batch: 200; loss: 2.35; acc: 0.81
Batch: 220; loss: 1.92; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.92
Batch: 260; loss: 1.69; acc: 0.83
Batch: 280; loss: 1.69; acc: 0.91
Batch: 300; loss: 2.08; acc: 0.88
Batch: 320; loss: 1.94; acc: 0.86
Batch: 340; loss: 4.62; acc: 0.73
Batch: 360; loss: 2.56; acc: 0.84
Batch: 380; loss: 2.72; acc: 0.83
Batch: 400; loss: 2.64; acc: 0.83
Batch: 420; loss: 1.82; acc: 0.88
Batch: 440; loss: 1.15; acc: 0.89
Batch: 460; loss: 3.91; acc: 0.77
Batch: 480; loss: 4.82; acc: 0.77
Batch: 500; loss: 2.1; acc: 0.84
Batch: 520; loss: 1.28; acc: 0.84
Batch: 540; loss: 2.3; acc: 0.91
Batch: 560; loss: 2.0; acc: 0.86
Batch: 580; loss: 2.54; acc: 0.86
Batch: 600; loss: 2.14; acc: 0.88
Batch: 620; loss: 0.97; acc: 0.95
Train Epoch over. train_loss: 2.21; train_accuracy: 0.84 

Batch: 0; loss: 2.78; acc: 0.77
Batch: 20; loss: 5.3; acc: 0.67
Batch: 40; loss: 1.63; acc: 0.89
Batch: 60; loss: 2.11; acc: 0.83
Batch: 80; loss: 1.73; acc: 0.84
Batch: 100; loss: 2.82; acc: 0.78
Batch: 120; loss: 1.46; acc: 0.88
Batch: 140; loss: 3.76; acc: 0.78
Val Epoch over. val_loss: 2.712560255436381; val_accuracy: 0.8200636942675159 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.8; acc: 0.86
Batch: 20; loss: 1.79; acc: 0.83
Batch: 40; loss: 1.76; acc: 0.88
Batch: 60; loss: 1.98; acc: 0.77
Batch: 80; loss: 2.34; acc: 0.83
Batch: 100; loss: 1.98; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.94
Batch: 140; loss: 2.82; acc: 0.84
Batch: 160; loss: 2.36; acc: 0.77
Batch: 180; loss: 2.27; acc: 0.77
Batch: 200; loss: 2.46; acc: 0.83
Batch: 220; loss: 1.8; acc: 0.89
Batch: 240; loss: 0.74; acc: 0.92
Batch: 260; loss: 2.4; acc: 0.83
Batch: 280; loss: 1.97; acc: 0.78
Batch: 300; loss: 2.0; acc: 0.84
Batch: 320; loss: 2.99; acc: 0.83
Batch: 340; loss: 2.63; acc: 0.84
Batch: 360; loss: 2.39; acc: 0.81
Batch: 380; loss: 1.41; acc: 0.86
Batch: 400; loss: 4.42; acc: 0.75
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 4.15; acc: 0.75
Batch: 480; loss: 2.01; acc: 0.84
Batch: 500; loss: 2.96; acc: 0.77
Batch: 520; loss: 2.38; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 2.01; acc: 0.83
Batch: 580; loss: 1.37; acc: 0.84
Batch: 600; loss: 3.2; acc: 0.75
Batch: 620; loss: 3.02; acc: 0.77
Train Epoch over. train_loss: 2.21; train_accuracy: 0.84 

Batch: 0; loss: 2.81; acc: 0.77
Batch: 20; loss: 5.26; acc: 0.67
Batch: 40; loss: 1.57; acc: 0.89
Batch: 60; loss: 2.09; acc: 0.83
Batch: 80; loss: 1.74; acc: 0.84
Batch: 100; loss: 2.83; acc: 0.78
Batch: 120; loss: 1.47; acc: 0.89
Batch: 140; loss: 3.82; acc: 0.78
Val Epoch over. val_loss: 2.7163870163783908; val_accuracy: 0.8199641719745223 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.17; acc: 0.81
Batch: 20; loss: 1.0; acc: 0.91
Batch: 40; loss: 2.52; acc: 0.8
Batch: 60; loss: 2.93; acc: 0.8
Batch: 80; loss: 3.59; acc: 0.72
Batch: 100; loss: 1.5; acc: 0.78
Batch: 120; loss: 1.7; acc: 0.86
Batch: 140; loss: 2.25; acc: 0.86
Batch: 160; loss: 2.7; acc: 0.86
Batch: 180; loss: 1.98; acc: 0.89
Batch: 200; loss: 1.97; acc: 0.84
Batch: 220; loss: 3.3; acc: 0.83
Batch: 240; loss: 1.16; acc: 0.88
Batch: 260; loss: 1.87; acc: 0.89
Batch: 280; loss: 2.34; acc: 0.83
Batch: 300; loss: 2.38; acc: 0.88
Batch: 320; loss: 1.58; acc: 0.88
Batch: 340; loss: 1.67; acc: 0.86
Batch: 360; loss: 2.12; acc: 0.84
Batch: 380; loss: 2.79; acc: 0.86
Batch: 400; loss: 4.11; acc: 0.84
Batch: 420; loss: 2.6; acc: 0.81
Batch: 440; loss: 1.17; acc: 0.81
Batch: 460; loss: 1.72; acc: 0.83
Batch: 480; loss: 2.63; acc: 0.8
Batch: 500; loss: 3.41; acc: 0.8
Batch: 520; loss: 1.87; acc: 0.83
Batch: 540; loss: 2.74; acc: 0.8
Batch: 560; loss: 1.11; acc: 0.86
Batch: 580; loss: 1.09; acc: 0.91
Batch: 600; loss: 1.63; acc: 0.84
Batch: 620; loss: 1.76; acc: 0.86
Train Epoch over. train_loss: 2.21; train_accuracy: 0.84 

Batch: 0; loss: 2.84; acc: 0.77
Batch: 20; loss: 5.1; acc: 0.69
Batch: 40; loss: 1.61; acc: 0.89
Batch: 60; loss: 2.03; acc: 0.83
Batch: 80; loss: 1.72; acc: 0.84
Batch: 100; loss: 2.78; acc: 0.78
Batch: 120; loss: 1.46; acc: 0.89
Batch: 140; loss: 3.81; acc: 0.78
Val Epoch over. val_loss: 2.7089294312865873; val_accuracy: 0.820859872611465 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.64; acc: 0.94
Batch: 20; loss: 1.5; acc: 0.89
Batch: 40; loss: 3.16; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.88
Batch: 80; loss: 2.08; acc: 0.78
Batch: 100; loss: 2.32; acc: 0.81
Batch: 120; loss: 2.97; acc: 0.78
Batch: 140; loss: 2.38; acc: 0.86
Batch: 160; loss: 1.86; acc: 0.83
Batch: 180; loss: 2.19; acc: 0.91
Batch: 200; loss: 3.1; acc: 0.81
Batch: 220; loss: 2.24; acc: 0.81
Batch: 240; loss: 1.25; acc: 0.88
Batch: 260; loss: 2.06; acc: 0.83
Batch: 280; loss: 2.3; acc: 0.83
Batch: 300; loss: 2.0; acc: 0.84
Batch: 320; loss: 1.23; acc: 0.84
Batch: 340; loss: 2.74; acc: 0.81
Batch: 360; loss: 1.27; acc: 0.88
Batch: 380; loss: 1.59; acc: 0.91
Batch: 400; loss: 2.25; acc: 0.78
Batch: 420; loss: 1.56; acc: 0.83
Batch: 440; loss: 1.0; acc: 0.84
Batch: 460; loss: 2.51; acc: 0.77
Batch: 480; loss: 1.77; acc: 0.89
Batch: 500; loss: 2.22; acc: 0.8
Batch: 520; loss: 3.52; acc: 0.73
Batch: 540; loss: 2.13; acc: 0.86
Batch: 560; loss: 2.23; acc: 0.88
Batch: 580; loss: 1.83; acc: 0.8
Batch: 600; loss: 2.58; acc: 0.8
Batch: 620; loss: 2.12; acc: 0.81
Train Epoch over. train_loss: 2.21; train_accuracy: 0.84 

Batch: 0; loss: 2.8; acc: 0.77
Batch: 20; loss: 5.24; acc: 0.67
Batch: 40; loss: 1.59; acc: 0.89
Batch: 60; loss: 2.13; acc: 0.83
Batch: 80; loss: 1.74; acc: 0.84
Batch: 100; loss: 2.78; acc: 0.78
Batch: 120; loss: 1.46; acc: 0.88
Batch: 140; loss: 3.73; acc: 0.78
Val Epoch over. val_loss: 2.7134758442830127; val_accuracy: 0.8212579617834395 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.93; acc: 0.91
Batch: 20; loss: 0.98; acc: 0.91
Batch: 40; loss: 2.51; acc: 0.84
Batch: 60; loss: 2.9; acc: 0.86
Batch: 80; loss: 2.45; acc: 0.88
Batch: 100; loss: 2.35; acc: 0.78
Batch: 120; loss: 2.37; acc: 0.84
Batch: 140; loss: 2.16; acc: 0.8
Batch: 160; loss: 1.56; acc: 0.92
Batch: 180; loss: 0.69; acc: 0.92
Batch: 200; loss: 2.11; acc: 0.84
Batch: 220; loss: 1.04; acc: 0.88
Batch: 240; loss: 2.25; acc: 0.78
Batch: 260; loss: 2.63; acc: 0.88
Batch: 280; loss: 3.34; acc: 0.84
Batch: 300; loss: 1.37; acc: 0.88
Batch: 320; loss: 2.57; acc: 0.84
Batch: 340; loss: 2.73; acc: 0.86
Batch: 360; loss: 2.34; acc: 0.77
Batch: 380; loss: 0.66; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.92
Batch: 420; loss: 4.21; acc: 0.78
Batch: 440; loss: 1.42; acc: 0.86
Batch: 460; loss: 1.61; acc: 0.84
Batch: 480; loss: 1.01; acc: 0.91
Batch: 500; loss: 2.26; acc: 0.81
Batch: 520; loss: 2.31; acc: 0.84
Batch: 540; loss: 1.1; acc: 0.86
Batch: 560; loss: 3.34; acc: 0.8
Batch: 580; loss: 1.95; acc: 0.77
Batch: 600; loss: 1.14; acc: 0.8
Batch: 620; loss: 2.31; acc: 0.84
Train Epoch over. train_loss: 2.21; train_accuracy: 0.84 

Batch: 0; loss: 2.88; acc: 0.77
Batch: 20; loss: 5.12; acc: 0.67
Batch: 40; loss: 1.63; acc: 0.89
Batch: 60; loss: 2.05; acc: 0.83
Batch: 80; loss: 1.71; acc: 0.84
Batch: 100; loss: 2.77; acc: 0.78
Batch: 120; loss: 1.45; acc: 0.89
Batch: 140; loss: 3.81; acc: 0.78
Val Epoch over. val_loss: 2.710138191083434; val_accuracy: 0.8196656050955414 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.36; acc: 0.89
Batch: 20; loss: 2.24; acc: 0.84
Batch: 40; loss: 1.4; acc: 0.84
Batch: 60; loss: 2.01; acc: 0.89
Batch: 80; loss: 3.54; acc: 0.81
Batch: 100; loss: 1.69; acc: 0.89
Batch: 120; loss: 1.39; acc: 0.84
Batch: 140; loss: 3.06; acc: 0.78
Batch: 160; loss: 1.66; acc: 0.83
Batch: 180; loss: 1.41; acc: 0.89
Batch: 200; loss: 3.27; acc: 0.73
Batch: 220; loss: 2.18; acc: 0.81
Batch: 240; loss: 3.36; acc: 0.81
Batch: 260; loss: 3.4; acc: 0.77
Batch: 280; loss: 2.78; acc: 0.77
Batch: 300; loss: 1.9; acc: 0.84
Batch: 320; loss: 2.22; acc: 0.8
Batch: 340; loss: 0.98; acc: 0.86
Batch: 360; loss: 2.49; acc: 0.81
Batch: 380; loss: 1.88; acc: 0.89
Batch: 400; loss: 2.52; acc: 0.83
Batch: 420; loss: 2.34; acc: 0.83
Batch: 440; loss: 1.44; acc: 0.8
Batch: 460; loss: 2.14; acc: 0.86
Batch: 480; loss: 1.79; acc: 0.86
Batch: 500; loss: 2.99; acc: 0.83
Batch: 520; loss: 2.09; acc: 0.81
Batch: 540; loss: 1.44; acc: 0.86
Batch: 560; loss: 2.45; acc: 0.83
Batch: 580; loss: 2.7; acc: 0.84
Batch: 600; loss: 2.01; acc: 0.77
Batch: 620; loss: 2.31; acc: 0.86
Train Epoch over. train_loss: 2.2; train_accuracy: 0.84 

Batch: 0; loss: 2.86; acc: 0.77
Batch: 20; loss: 5.07; acc: 0.69
Batch: 40; loss: 1.64; acc: 0.89
Batch: 60; loss: 2.07; acc: 0.81
Batch: 80; loss: 1.72; acc: 0.84
Batch: 100; loss: 2.75; acc: 0.8
Batch: 120; loss: 1.48; acc: 0.89
Batch: 140; loss: 3.81; acc: 0.78
Val Epoch over. val_loss: 2.709750273804756; val_accuracy: 0.8211584394904459 

plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_600_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 357455
elements in E: 159368000
fraction nonzero: 0.002242953415993173
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 36.37; acc: 0.12
Batch: 20; loss: 8.48; acc: 0.42
Batch: 40; loss: 8.39; acc: 0.55
Batch: 60; loss: 5.44; acc: 0.58
Batch: 80; loss: 3.37; acc: 0.72
Batch: 100; loss: 3.62; acc: 0.69
Batch: 120; loss: 6.42; acc: 0.58
Batch: 140; loss: 5.1; acc: 0.72
Batch: 160; loss: 5.17; acc: 0.72
Batch: 180; loss: 3.79; acc: 0.83
Batch: 200; loss: 3.55; acc: 0.75
Batch: 220; loss: 4.68; acc: 0.73
Batch: 240; loss: 2.06; acc: 0.81
Batch: 260; loss: 4.91; acc: 0.69
Batch: 280; loss: 3.93; acc: 0.69
Batch: 300; loss: 4.5; acc: 0.7
Batch: 320; loss: 2.59; acc: 0.78
Batch: 340; loss: 2.6; acc: 0.78
Batch: 360; loss: 3.88; acc: 0.81
Batch: 380; loss: 2.91; acc: 0.77
Batch: 400; loss: 4.05; acc: 0.72
Batch: 420; loss: 5.13; acc: 0.72
Batch: 440; loss: 5.48; acc: 0.72
Batch: 460; loss: 3.39; acc: 0.8
Batch: 480; loss: 3.07; acc: 0.77
Batch: 500; loss: 5.24; acc: 0.8
Batch: 520; loss: 5.72; acc: 0.72
Batch: 540; loss: 5.75; acc: 0.81
Batch: 560; loss: 4.88; acc: 0.77
Batch: 580; loss: 2.88; acc: 0.78
Batch: 600; loss: 2.21; acc: 0.81
Batch: 620; loss: 6.06; acc: 0.7
Train Epoch over. train_loss: 4.79; train_accuracy: 0.73 

Batch: 0; loss: 2.15; acc: 0.86
Batch: 20; loss: 3.24; acc: 0.7
Batch: 40; loss: 2.38; acc: 0.8
Batch: 60; loss: 2.24; acc: 0.86
Batch: 80; loss: 2.48; acc: 0.8
Batch: 100; loss: 3.32; acc: 0.8
Batch: 120; loss: 3.12; acc: 0.83
Batch: 140; loss: 6.41; acc: 0.64
Val Epoch over. val_loss: 3.6212015281057663; val_accuracy: 0.783937101910828 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 2.27; acc: 0.83
Batch: 20; loss: 3.62; acc: 0.78
Batch: 40; loss: 1.27; acc: 0.84
Batch: 60; loss: 4.6; acc: 0.66
Batch: 80; loss: 3.76; acc: 0.8
Batch: 100; loss: 2.58; acc: 0.8
Batch: 120; loss: 2.65; acc: 0.84
Batch: 140; loss: 7.13; acc: 0.67
Batch: 160; loss: 4.98; acc: 0.72
Batch: 180; loss: 4.64; acc: 0.73
Batch: 200; loss: 3.0; acc: 0.83
Batch: 220; loss: 3.7; acc: 0.78
Batch: 240; loss: 2.02; acc: 0.86
Batch: 260; loss: 4.1; acc: 0.72
Batch: 280; loss: 1.73; acc: 0.89
Batch: 300; loss: 4.47; acc: 0.77
Batch: 320; loss: 3.31; acc: 0.91
Batch: 340; loss: 4.19; acc: 0.88
Batch: 360; loss: 5.44; acc: 0.75
Batch: 380; loss: 4.05; acc: 0.72
Batch: 400; loss: 5.31; acc: 0.84
Batch: 420; loss: 3.94; acc: 0.77
Batch: 440; loss: 2.6; acc: 0.86
Batch: 460; loss: 4.44; acc: 0.78
Batch: 480; loss: 5.09; acc: 0.73
Batch: 500; loss: 1.09; acc: 0.89
Batch: 520; loss: 3.19; acc: 0.77
Batch: 540; loss: 2.81; acc: 0.83
Batch: 560; loss: 4.39; acc: 0.7
Batch: 580; loss: 4.04; acc: 0.72
Batch: 600; loss: 3.95; acc: 0.75
Batch: 620; loss: 3.46; acc: 0.81
Train Epoch over. train_loss: 3.75; train_accuracy: 0.79 

Batch: 0; loss: 3.37; acc: 0.84
Batch: 20; loss: 3.72; acc: 0.7
Batch: 40; loss: 2.12; acc: 0.91
Batch: 60; loss: 3.33; acc: 0.73
Batch: 80; loss: 3.77; acc: 0.81
Batch: 100; loss: 5.24; acc: 0.66
Batch: 120; loss: 4.49; acc: 0.77
Batch: 140; loss: 4.81; acc: 0.75
Val Epoch over. val_loss: 3.6366056082354987; val_accuracy: 0.7929936305732485 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 2.52; acc: 0.73
Batch: 20; loss: 4.63; acc: 0.73
Batch: 40; loss: 4.29; acc: 0.81
Batch: 60; loss: 4.15; acc: 0.78
Batch: 80; loss: 2.57; acc: 0.8
Batch: 100; loss: 4.85; acc: 0.73
Batch: 120; loss: 3.55; acc: 0.75
Batch: 140; loss: 2.79; acc: 0.83
Batch: 160; loss: 4.83; acc: 0.83
Batch: 180; loss: 3.29; acc: 0.83
Batch: 200; loss: 5.34; acc: 0.73
Batch: 220; loss: 2.54; acc: 0.84
Batch: 240; loss: 2.26; acc: 0.86
Batch: 260; loss: 3.48; acc: 0.77
Batch: 280; loss: 5.26; acc: 0.78
Batch: 300; loss: 2.53; acc: 0.83
Batch: 320; loss: 3.02; acc: 0.75
Batch: 340; loss: 1.94; acc: 0.8
Batch: 360; loss: 6.06; acc: 0.75
Batch: 380; loss: 3.6; acc: 0.77
Batch: 400; loss: 4.64; acc: 0.69
Batch: 420; loss: 1.67; acc: 0.88
Batch: 440; loss: 2.66; acc: 0.83
Batch: 460; loss: 2.62; acc: 0.8
Batch: 480; loss: 4.07; acc: 0.78
Batch: 500; loss: 2.53; acc: 0.81
Batch: 520; loss: 6.38; acc: 0.78
Batch: 540; loss: 2.15; acc: 0.84
Batch: 560; loss: 4.63; acc: 0.7
Batch: 580; loss: 3.69; acc: 0.77
Batch: 600; loss: 4.53; acc: 0.78
Batch: 620; loss: 2.71; acc: 0.77
Train Epoch over. train_loss: 3.69; train_accuracy: 0.8 

Batch: 0; loss: 3.88; acc: 0.83
Batch: 20; loss: 3.78; acc: 0.73
Batch: 40; loss: 4.13; acc: 0.81
Batch: 60; loss: 4.06; acc: 0.77
Batch: 80; loss: 4.49; acc: 0.77
Batch: 100; loss: 5.01; acc: 0.75
Batch: 120; loss: 5.47; acc: 0.75
Batch: 140; loss: 7.08; acc: 0.62
Val Epoch over. val_loss: 4.174154672653052; val_accuracy: 0.7679140127388535 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 4.06; acc: 0.81
Batch: 20; loss: 5.68; acc: 0.77
Batch: 40; loss: 2.17; acc: 0.91
Batch: 60; loss: 5.07; acc: 0.73
Batch: 80; loss: 3.16; acc: 0.8
Batch: 100; loss: 1.49; acc: 0.84
Batch: 120; loss: 3.32; acc: 0.81
Batch: 140; loss: 2.49; acc: 0.81
Batch: 160; loss: 2.21; acc: 0.8
Batch: 180; loss: 3.22; acc: 0.8
Batch: 200; loss: 2.02; acc: 0.81
Batch: 220; loss: 3.77; acc: 0.77
Batch: 240; loss: 1.54; acc: 0.88
Batch: 260; loss: 4.81; acc: 0.75
Batch: 280; loss: 3.21; acc: 0.73
Batch: 300; loss: 2.7; acc: 0.83
Batch: 320; loss: 2.08; acc: 0.77
Batch: 340; loss: 5.14; acc: 0.78
Batch: 360; loss: 2.71; acc: 0.86
Batch: 380; loss: 2.67; acc: 0.81
Batch: 400; loss: 5.17; acc: 0.64
Batch: 420; loss: 3.04; acc: 0.83
Batch: 440; loss: 8.16; acc: 0.73
Batch: 460; loss: 4.04; acc: 0.75
Batch: 480; loss: 4.46; acc: 0.75
Batch: 500; loss: 3.56; acc: 0.73
Batch: 520; loss: 6.03; acc: 0.77
Batch: 540; loss: 3.64; acc: 0.86
Batch: 560; loss: 5.94; acc: 0.73
Batch: 580; loss: 3.55; acc: 0.75
Batch: 600; loss: 3.55; acc: 0.86
Batch: 620; loss: 4.11; acc: 0.78
Train Epoch over. train_loss: 3.69; train_accuracy: 0.79 

Batch: 0; loss: 3.66; acc: 0.78
Batch: 20; loss: 6.86; acc: 0.62
Batch: 40; loss: 1.76; acc: 0.78
Batch: 60; loss: 6.66; acc: 0.69
Batch: 80; loss: 2.76; acc: 0.8
Batch: 100; loss: 6.0; acc: 0.66
Batch: 120; loss: 5.23; acc: 0.81
Batch: 140; loss: 6.07; acc: 0.61
Val Epoch over. val_loss: 4.7306261495420125; val_accuracy: 0.7560708598726115 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 5.39; acc: 0.7
Batch: 20; loss: 1.64; acc: 0.84
Batch: 40; loss: 4.25; acc: 0.75
Batch: 60; loss: 5.65; acc: 0.75
Batch: 80; loss: 1.19; acc: 0.92
Batch: 100; loss: 3.98; acc: 0.78
Batch: 120; loss: 5.36; acc: 0.77
Batch: 140; loss: 4.35; acc: 0.8
Batch: 160; loss: 4.83; acc: 0.8
Batch: 180; loss: 3.76; acc: 0.84
Batch: 200; loss: 4.39; acc: 0.84
Batch: 220; loss: 3.38; acc: 0.8
Batch: 240; loss: 2.08; acc: 0.81
Batch: 260; loss: 3.31; acc: 0.83
Batch: 280; loss: 2.9; acc: 0.8
Batch: 300; loss: 5.23; acc: 0.78
Batch: 320; loss: 3.68; acc: 0.72
Batch: 340; loss: 2.34; acc: 0.83
Batch: 360; loss: 3.25; acc: 0.83
Batch: 380; loss: 2.83; acc: 0.84
Batch: 400; loss: 1.98; acc: 0.88
Batch: 420; loss: 3.89; acc: 0.83
Batch: 440; loss: 4.39; acc: 0.75
Batch: 460; loss: 2.08; acc: 0.84
Batch: 480; loss: 2.14; acc: 0.84
Batch: 500; loss: 2.64; acc: 0.83
Batch: 520; loss: 3.98; acc: 0.81
Batch: 540; loss: 3.55; acc: 0.83
Batch: 560; loss: 4.58; acc: 0.75
Batch: 580; loss: 1.99; acc: 0.81
Batch: 600; loss: 2.05; acc: 0.84
Batch: 620; loss: 4.49; acc: 0.81
Train Epoch over. train_loss: 3.63; train_accuracy: 0.8 

Batch: 0; loss: 2.11; acc: 0.84
Batch: 20; loss: 6.04; acc: 0.64
Batch: 40; loss: 3.03; acc: 0.8
Batch: 60; loss: 4.96; acc: 0.8
Batch: 80; loss: 4.83; acc: 0.77
Batch: 100; loss: 3.47; acc: 0.83
Batch: 120; loss: 3.28; acc: 0.83
Batch: 140; loss: 7.62; acc: 0.67
Val Epoch over. val_loss: 4.164956858393493; val_accuracy: 0.7871218152866242 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 4.22; acc: 0.73
Batch: 20; loss: 3.14; acc: 0.8
Batch: 40; loss: 2.3; acc: 0.81
Batch: 60; loss: 3.01; acc: 0.83
Batch: 80; loss: 2.97; acc: 0.86
Batch: 100; loss: 1.24; acc: 0.84
Batch: 120; loss: 1.76; acc: 0.94
Batch: 140; loss: 3.19; acc: 0.8
Batch: 160; loss: 3.53; acc: 0.86
Batch: 180; loss: 2.04; acc: 0.78
Batch: 200; loss: 3.21; acc: 0.83
Batch: 220; loss: 5.03; acc: 0.83
Batch: 240; loss: 3.45; acc: 0.81
Batch: 260; loss: 2.33; acc: 0.89
Batch: 280; loss: 5.31; acc: 0.84
Batch: 300; loss: 2.65; acc: 0.8
Batch: 320; loss: 4.35; acc: 0.78
Batch: 340; loss: 3.83; acc: 0.77
Batch: 360; loss: 2.97; acc: 0.83
Batch: 380; loss: 3.67; acc: 0.89
Batch: 400; loss: 4.57; acc: 0.77
Batch: 420; loss: 3.92; acc: 0.78
Batch: 440; loss: 2.78; acc: 0.83
Batch: 460; loss: 4.46; acc: 0.8
Batch: 480; loss: 4.03; acc: 0.72
Batch: 500; loss: 4.05; acc: 0.83
Batch: 520; loss: 3.13; acc: 0.88
Batch: 540; loss: 3.11; acc: 0.77
Batch: 560; loss: 2.79; acc: 0.75
Batch: 580; loss: 2.71; acc: 0.86
Batch: 600; loss: 3.2; acc: 0.78
Batch: 620; loss: 2.7; acc: 0.83
Train Epoch over. train_loss: 3.65; train_accuracy: 0.8 

Batch: 0; loss: 4.19; acc: 0.8
Batch: 20; loss: 5.29; acc: 0.69
Batch: 40; loss: 2.39; acc: 0.89
Batch: 60; loss: 4.66; acc: 0.8
Batch: 80; loss: 3.46; acc: 0.78
Batch: 100; loss: 4.49; acc: 0.81
Batch: 120; loss: 2.81; acc: 0.83
Batch: 140; loss: 4.18; acc: 0.75
Val Epoch over. val_loss: 3.7981141615825096; val_accuracy: 0.7859275477707006 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 2.24; acc: 0.88
Batch: 20; loss: 5.31; acc: 0.77
Batch: 40; loss: 3.41; acc: 0.81
Batch: 60; loss: 2.76; acc: 0.83
Batch: 80; loss: 3.49; acc: 0.8
Batch: 100; loss: 2.32; acc: 0.81
Batch: 120; loss: 3.23; acc: 0.78
Batch: 140; loss: 4.07; acc: 0.84
Batch: 160; loss: 2.98; acc: 0.84
Batch: 180; loss: 3.2; acc: 0.83
Batch: 200; loss: 3.57; acc: 0.88
Batch: 220; loss: 3.17; acc: 0.81
Batch: 240; loss: 5.72; acc: 0.72
Batch: 260; loss: 3.25; acc: 0.81
Batch: 280; loss: 1.35; acc: 0.89
Batch: 300; loss: 2.05; acc: 0.83
Batch: 320; loss: 3.94; acc: 0.78
Batch: 340; loss: 3.01; acc: 0.81
Batch: 360; loss: 2.32; acc: 0.83
Batch: 380; loss: 6.2; acc: 0.75
Batch: 400; loss: 3.23; acc: 0.81
Batch: 420; loss: 1.36; acc: 0.88
Batch: 440; loss: 4.56; acc: 0.78
Batch: 460; loss: 6.48; acc: 0.75
Batch: 480; loss: 3.27; acc: 0.81
Batch: 500; loss: 3.94; acc: 0.77
Batch: 520; loss: 3.56; acc: 0.81
Batch: 540; loss: 3.47; acc: 0.83
Batch: 560; loss: 3.65; acc: 0.8
Batch: 580; loss: 5.23; acc: 0.8
Batch: 600; loss: 3.86; acc: 0.78
Batch: 620; loss: 3.24; acc: 0.81
Train Epoch over. train_loss: 3.49; train_accuracy: 0.81 

Batch: 0; loss: 2.58; acc: 0.84
Batch: 20; loss: 8.14; acc: 0.61
Batch: 40; loss: 3.44; acc: 0.89
Batch: 60; loss: 4.02; acc: 0.78
Batch: 80; loss: 4.92; acc: 0.72
Batch: 100; loss: 5.56; acc: 0.77
Batch: 120; loss: 3.19; acc: 0.83
Batch: 140; loss: 6.54; acc: 0.66
Val Epoch over. val_loss: 4.019055549126522; val_accuracy: 0.7856289808917197 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 3.21; acc: 0.81
Batch: 20; loss: 2.86; acc: 0.78
Batch: 40; loss: 3.36; acc: 0.77
Batch: 60; loss: 2.83; acc: 0.84
Batch: 80; loss: 3.81; acc: 0.81
Batch: 100; loss: 3.32; acc: 0.78
Batch: 120; loss: 2.45; acc: 0.83
Batch: 140; loss: 3.04; acc: 0.81
Batch: 160; loss: 3.87; acc: 0.8
Batch: 180; loss: 2.29; acc: 0.8
Batch: 200; loss: 2.13; acc: 0.88
Batch: 220; loss: 4.41; acc: 0.8
Batch: 240; loss: 2.82; acc: 0.81
Batch: 260; loss: 1.66; acc: 0.86
Batch: 280; loss: 1.9; acc: 0.86
Batch: 300; loss: 5.41; acc: 0.77
Batch: 320; loss: 2.46; acc: 0.84
Batch: 340; loss: 3.98; acc: 0.78
Batch: 360; loss: 5.72; acc: 0.67
Batch: 380; loss: 3.13; acc: 0.77
Batch: 400; loss: 3.04; acc: 0.84
Batch: 420; loss: 2.59; acc: 0.8
Batch: 440; loss: 1.44; acc: 0.88
Batch: 460; loss: 2.91; acc: 0.83
Batch: 480; loss: 2.94; acc: 0.81
Batch: 500; loss: 2.75; acc: 0.83
Batch: 520; loss: 3.04; acc: 0.86
Batch: 540; loss: 3.4; acc: 0.78
Batch: 560; loss: 4.97; acc: 0.84
Batch: 580; loss: 1.94; acc: 0.83
Batch: 600; loss: 5.09; acc: 0.75
Batch: 620; loss: 3.51; acc: 0.77
Train Epoch over. train_loss: 3.5; train_accuracy: 0.8 

Batch: 0; loss: 1.8; acc: 0.84
Batch: 20; loss: 6.3; acc: 0.66
Batch: 40; loss: 2.47; acc: 0.86
Batch: 60; loss: 7.05; acc: 0.75
Batch: 80; loss: 6.2; acc: 0.72
Batch: 100; loss: 7.78; acc: 0.72
Batch: 120; loss: 4.08; acc: 0.88
Batch: 140; loss: 5.53; acc: 0.72
Val Epoch over. val_loss: 4.288306063907162; val_accuracy: 0.7730891719745223 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 4.19; acc: 0.73
Batch: 20; loss: 2.08; acc: 0.84
Batch: 40; loss: 2.71; acc: 0.86
Batch: 60; loss: 2.87; acc: 0.78
Batch: 80; loss: 3.65; acc: 0.8
Batch: 100; loss: 4.51; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.86
Batch: 140; loss: 1.86; acc: 0.91
Batch: 160; loss: 1.54; acc: 0.88
Batch: 180; loss: 2.53; acc: 0.83
Batch: 200; loss: 3.47; acc: 0.81
Batch: 220; loss: 3.16; acc: 0.84
Batch: 240; loss: 3.44; acc: 0.84
Batch: 260; loss: 3.79; acc: 0.78
Batch: 280; loss: 3.3; acc: 0.88
Batch: 300; loss: 1.04; acc: 0.88
Batch: 320; loss: 4.1; acc: 0.77
Batch: 340; loss: 3.48; acc: 0.75
Batch: 360; loss: 4.71; acc: 0.75
Batch: 380; loss: 3.7; acc: 0.78
Batch: 400; loss: 4.34; acc: 0.83
Batch: 420; loss: 4.16; acc: 0.84
Batch: 440; loss: 3.79; acc: 0.78
Batch: 460; loss: 3.07; acc: 0.83
Batch: 480; loss: 2.95; acc: 0.78
Batch: 500; loss: 3.39; acc: 0.83
Batch: 520; loss: 2.34; acc: 0.78
Batch: 540; loss: 3.02; acc: 0.78
Batch: 560; loss: 3.66; acc: 0.77
Batch: 580; loss: 2.24; acc: 0.84
Batch: 600; loss: 5.79; acc: 0.77
Batch: 620; loss: 2.6; acc: 0.84
Train Epoch over. train_loss: 3.54; train_accuracy: 0.8 

Batch: 0; loss: 3.58; acc: 0.73
Batch: 20; loss: 6.99; acc: 0.62
Batch: 40; loss: 3.17; acc: 0.83
Batch: 60; loss: 5.11; acc: 0.77
Batch: 80; loss: 5.5; acc: 0.69
Batch: 100; loss: 5.22; acc: 0.8
Batch: 120; loss: 4.99; acc: 0.78
Batch: 140; loss: 6.55; acc: 0.67
Val Epoch over. val_loss: 4.9386245824728805; val_accuracy: 0.7379578025477707 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 4.0; acc: 0.7
Batch: 20; loss: 1.82; acc: 0.88
Batch: 40; loss: 5.3; acc: 0.73
Batch: 60; loss: 3.46; acc: 0.81
Batch: 80; loss: 1.74; acc: 0.81
Batch: 100; loss: 1.81; acc: 0.86
Batch: 120; loss: 6.71; acc: 0.67
Batch: 140; loss: 6.66; acc: 0.75
Batch: 160; loss: 3.8; acc: 0.8
Batch: 180; loss: 3.17; acc: 0.83
Batch: 200; loss: 3.6; acc: 0.91
Batch: 220; loss: 3.77; acc: 0.83
Batch: 240; loss: 1.33; acc: 0.91
Batch: 260; loss: 1.78; acc: 0.8
Batch: 280; loss: 6.56; acc: 0.73
Batch: 300; loss: 7.03; acc: 0.67
Batch: 320; loss: 1.6; acc: 0.86
Batch: 340; loss: 3.29; acc: 0.78
Batch: 360; loss: 4.0; acc: 0.75
Batch: 380; loss: 1.5; acc: 0.86
Batch: 400; loss: 4.34; acc: 0.72
Batch: 420; loss: 3.23; acc: 0.78
Batch: 440; loss: 2.96; acc: 0.8
Batch: 460; loss: 4.97; acc: 0.81
Batch: 480; loss: 4.33; acc: 0.75
Batch: 500; loss: 3.04; acc: 0.78
Batch: 520; loss: 2.03; acc: 0.86
Batch: 540; loss: 6.92; acc: 0.83
Batch: 560; loss: 2.72; acc: 0.81
Batch: 580; loss: 4.17; acc: 0.8
Batch: 600; loss: 2.2; acc: 0.84
Batch: 620; loss: 6.02; acc: 0.7
Train Epoch over. train_loss: 3.52; train_accuracy: 0.8 

Batch: 0; loss: 3.95; acc: 0.77
Batch: 20; loss: 6.79; acc: 0.7
Batch: 40; loss: 2.48; acc: 0.8
Batch: 60; loss: 5.62; acc: 0.78
Batch: 80; loss: 5.3; acc: 0.69
Batch: 100; loss: 5.05; acc: 0.75
Batch: 120; loss: 2.24; acc: 0.84
Batch: 140; loss: 6.22; acc: 0.73
Val Epoch over. val_loss: 4.357929118119987; val_accuracy: 0.7726910828025477 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.65; acc: 0.8
Batch: 20; loss: 2.07; acc: 0.81
Batch: 40; loss: 1.84; acc: 0.84
Batch: 60; loss: 2.76; acc: 0.8
Batch: 80; loss: 2.77; acc: 0.94
Batch: 100; loss: 3.7; acc: 0.84
Batch: 120; loss: 2.37; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 3.23; acc: 0.81
Batch: 200; loss: 1.9; acc: 0.91
Batch: 220; loss: 4.46; acc: 0.77
Batch: 240; loss: 2.13; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.97
Batch: 280; loss: 1.75; acc: 0.86
Batch: 300; loss: 1.59; acc: 0.88
Batch: 320; loss: 3.5; acc: 0.7
Batch: 340; loss: 3.4; acc: 0.81
Batch: 360; loss: 2.18; acc: 0.83
Batch: 380; loss: 5.01; acc: 0.78
Batch: 400; loss: 2.39; acc: 0.89
Batch: 420; loss: 1.69; acc: 0.88
Batch: 440; loss: 1.47; acc: 0.89
Batch: 460; loss: 1.43; acc: 0.86
Batch: 480; loss: 1.21; acc: 0.91
Batch: 500; loss: 2.96; acc: 0.83
Batch: 520; loss: 2.64; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 2.01; acc: 0.88
Batch: 580; loss: 3.03; acc: 0.84
Batch: 600; loss: 1.91; acc: 0.86
Batch: 620; loss: 2.73; acc: 0.84
Train Epoch over. train_loss: 2.25; train_accuracy: 0.85 

Batch: 0; loss: 1.19; acc: 0.89
Batch: 20; loss: 3.6; acc: 0.75
Batch: 40; loss: 1.04; acc: 0.92
Batch: 60; loss: 4.2; acc: 0.86
Batch: 80; loss: 2.63; acc: 0.8
Batch: 100; loss: 3.5; acc: 0.81
Batch: 120; loss: 1.82; acc: 0.88
Batch: 140; loss: 4.17; acc: 0.75
Val Epoch over. val_loss: 2.475496267247352; val_accuracy: 0.8351910828025477 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.15; acc: 0.81
Batch: 20; loss: 1.42; acc: 0.91
Batch: 40; loss: 2.95; acc: 0.84
Batch: 60; loss: 3.05; acc: 0.73
Batch: 80; loss: 3.25; acc: 0.83
Batch: 100; loss: 2.2; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.89
Batch: 140; loss: 2.27; acc: 0.88
Batch: 160; loss: 2.62; acc: 0.88
Batch: 180; loss: 1.67; acc: 0.88
Batch: 200; loss: 1.85; acc: 0.83
Batch: 220; loss: 2.36; acc: 0.86
Batch: 240; loss: 2.79; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.91
Batch: 280; loss: 1.43; acc: 0.84
Batch: 300; loss: 2.93; acc: 0.84
Batch: 320; loss: 2.08; acc: 0.89
Batch: 340; loss: 2.56; acc: 0.8
Batch: 360; loss: 0.61; acc: 0.92
Batch: 380; loss: 4.08; acc: 0.77
Batch: 400; loss: 2.52; acc: 0.8
Batch: 420; loss: 1.76; acc: 0.88
Batch: 440; loss: 2.64; acc: 0.84
Batch: 460; loss: 0.97; acc: 0.92
Batch: 480; loss: 2.14; acc: 0.89
Batch: 500; loss: 2.7; acc: 0.84
Batch: 520; loss: 1.35; acc: 0.89
Batch: 540; loss: 1.19; acc: 0.86
Batch: 560; loss: 1.84; acc: 0.91
Batch: 580; loss: 3.09; acc: 0.75
Batch: 600; loss: 0.73; acc: 0.92
Batch: 620; loss: 2.7; acc: 0.86
Train Epoch over. train_loss: 1.96; train_accuracy: 0.85 

Batch: 0; loss: 1.03; acc: 0.88
Batch: 20; loss: 3.45; acc: 0.75
Batch: 40; loss: 1.15; acc: 0.88
Batch: 60; loss: 3.9; acc: 0.84
Batch: 80; loss: 2.34; acc: 0.81
Batch: 100; loss: 3.53; acc: 0.78
Batch: 120; loss: 2.28; acc: 0.86
Batch: 140; loss: 3.97; acc: 0.73
Val Epoch over. val_loss: 2.3999609884562765; val_accuracy: 0.8347929936305732 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.19; acc: 0.81
Batch: 20; loss: 1.88; acc: 0.86
Batch: 40; loss: 1.79; acc: 0.84
Batch: 60; loss: 1.86; acc: 0.81
Batch: 80; loss: 1.09; acc: 0.92
Batch: 100; loss: 2.56; acc: 0.81
Batch: 120; loss: 1.11; acc: 0.89
Batch: 140; loss: 1.15; acc: 0.89
Batch: 160; loss: 0.95; acc: 0.91
Batch: 180; loss: 2.74; acc: 0.84
Batch: 200; loss: 1.94; acc: 0.88
Batch: 220; loss: 1.41; acc: 0.84
Batch: 240; loss: 1.88; acc: 0.83
Batch: 260; loss: 3.69; acc: 0.8
Batch: 280; loss: 1.78; acc: 0.83
Batch: 300; loss: 1.65; acc: 0.86
Batch: 320; loss: 2.11; acc: 0.91
Batch: 340; loss: 1.55; acc: 0.88
Batch: 360; loss: 2.65; acc: 0.86
Batch: 380; loss: 2.31; acc: 0.78
Batch: 400; loss: 1.38; acc: 0.84
Batch: 420; loss: 1.88; acc: 0.86
Batch: 440; loss: 1.58; acc: 0.91
Batch: 460; loss: 1.8; acc: 0.91
Batch: 480; loss: 2.23; acc: 0.86
Batch: 500; loss: 1.82; acc: 0.77
Batch: 520; loss: 1.75; acc: 0.86
Batch: 540; loss: 2.31; acc: 0.84
Batch: 560; loss: 1.61; acc: 0.91
Batch: 580; loss: 1.67; acc: 0.86
Batch: 600; loss: 2.74; acc: 0.89
Batch: 620; loss: 2.59; acc: 0.83
Train Epoch over. train_loss: 1.88; train_accuracy: 0.86 

Batch: 0; loss: 0.75; acc: 0.89
Batch: 20; loss: 3.46; acc: 0.73
Batch: 40; loss: 1.19; acc: 0.88
Batch: 60; loss: 4.03; acc: 0.84
Batch: 80; loss: 2.47; acc: 0.77
Batch: 100; loss: 3.54; acc: 0.78
Batch: 120; loss: 2.06; acc: 0.86
Batch: 140; loss: 3.91; acc: 0.75
Val Epoch over. val_loss: 2.3367029155135914; val_accuracy: 0.8309116242038217 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.72; acc: 0.81
Batch: 20; loss: 1.15; acc: 0.91
Batch: 40; loss: 1.23; acc: 0.92
Batch: 60; loss: 2.17; acc: 0.88
Batch: 80; loss: 0.79; acc: 0.89
Batch: 100; loss: 2.44; acc: 0.86
Batch: 120; loss: 1.49; acc: 0.81
Batch: 140; loss: 2.3; acc: 0.81
Batch: 160; loss: 2.63; acc: 0.86
Batch: 180; loss: 1.97; acc: 0.81
Batch: 200; loss: 2.1; acc: 0.78
Batch: 220; loss: 1.85; acc: 0.89
Batch: 240; loss: 2.65; acc: 0.77
Batch: 260; loss: 0.69; acc: 0.91
Batch: 280; loss: 1.23; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.92
Batch: 320; loss: 1.28; acc: 0.86
Batch: 340; loss: 0.96; acc: 0.86
Batch: 360; loss: 2.36; acc: 0.88
Batch: 380; loss: 1.08; acc: 0.91
Batch: 400; loss: 2.46; acc: 0.86
Batch: 420; loss: 1.84; acc: 0.84
Batch: 440; loss: 1.83; acc: 0.83
Batch: 460; loss: 2.82; acc: 0.77
Batch: 480; loss: 1.26; acc: 0.88
Batch: 500; loss: 1.76; acc: 0.83
Batch: 520; loss: 2.79; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 1.35; acc: 0.88
Batch: 580; loss: 1.86; acc: 0.83
Batch: 600; loss: 2.28; acc: 0.83
Batch: 620; loss: 2.08; acc: 0.86
Train Epoch over. train_loss: 1.84; train_accuracy: 0.85 

Batch: 0; loss: 0.91; acc: 0.91
Batch: 20; loss: 2.78; acc: 0.77
Batch: 40; loss: 1.46; acc: 0.89
Batch: 60; loss: 3.71; acc: 0.83
Batch: 80; loss: 2.54; acc: 0.78
Batch: 100; loss: 3.24; acc: 0.78
Batch: 120; loss: 1.99; acc: 0.88
Batch: 140; loss: 3.68; acc: 0.75
Val Epoch over. val_loss: 2.33273217765389; val_accuracy: 0.8318073248407644 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.1; acc: 0.83
Batch: 20; loss: 2.2; acc: 0.83
Batch: 40; loss: 1.86; acc: 0.91
Batch: 60; loss: 0.98; acc: 0.92
Batch: 80; loss: 2.31; acc: 0.88
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 1.97; acc: 0.92
Batch: 160; loss: 1.76; acc: 0.88
Batch: 180; loss: 2.45; acc: 0.83
Batch: 200; loss: 2.37; acc: 0.8
Batch: 220; loss: 1.19; acc: 0.86
Batch: 240; loss: 1.6; acc: 0.86
Batch: 260; loss: 2.48; acc: 0.84
Batch: 280; loss: 1.81; acc: 0.84
Batch: 300; loss: 1.58; acc: 0.86
Batch: 320; loss: 2.17; acc: 0.81
Batch: 340; loss: 2.48; acc: 0.86
Batch: 360; loss: 2.5; acc: 0.78
Batch: 380; loss: 1.44; acc: 0.86
Batch: 400; loss: 1.13; acc: 0.84
Batch: 420; loss: 1.29; acc: 0.84
Batch: 440; loss: 1.12; acc: 0.91
Batch: 460; loss: 2.84; acc: 0.84
Batch: 480; loss: 1.07; acc: 0.89
Batch: 500; loss: 1.13; acc: 0.86
Batch: 520; loss: 1.83; acc: 0.86
Batch: 540; loss: 1.33; acc: 0.89
Batch: 560; loss: 1.11; acc: 0.89
Batch: 580; loss: 2.97; acc: 0.86
Batch: 600; loss: 2.04; acc: 0.86
Batch: 620; loss: 3.29; acc: 0.78
Train Epoch over. train_loss: 1.82; train_accuracy: 0.85 

Batch: 0; loss: 0.65; acc: 0.91
Batch: 20; loss: 2.9; acc: 0.8
Batch: 40; loss: 1.62; acc: 0.89
Batch: 60; loss: 3.78; acc: 0.84
Batch: 80; loss: 2.27; acc: 0.8
Batch: 100; loss: 3.3; acc: 0.8
Batch: 120; loss: 2.0; acc: 0.86
Batch: 140; loss: 3.77; acc: 0.8
Val Epoch over. val_loss: 2.2814667368200934; val_accuracy: 0.8313097133757962 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.85; acc: 0.92
Batch: 20; loss: 2.7; acc: 0.84
Batch: 40; loss: 1.29; acc: 0.88
Batch: 60; loss: 1.36; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 3.52; acc: 0.86
Batch: 120; loss: 1.29; acc: 0.88
Batch: 140; loss: 1.37; acc: 0.83
Batch: 160; loss: 2.0; acc: 0.86
Batch: 180; loss: 2.05; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.95
Batch: 220; loss: 1.94; acc: 0.89
Batch: 240; loss: 1.44; acc: 0.88
Batch: 260; loss: 2.13; acc: 0.89
Batch: 280; loss: 1.53; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 1.05; acc: 0.84
Batch: 340; loss: 2.06; acc: 0.89
Batch: 360; loss: 1.37; acc: 0.88
Batch: 380; loss: 2.43; acc: 0.8
Batch: 400; loss: 3.39; acc: 0.83
Batch: 420; loss: 1.85; acc: 0.81
Batch: 440; loss: 2.13; acc: 0.81
Batch: 460; loss: 1.61; acc: 0.8
Batch: 480; loss: 1.42; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.88
Batch: 520; loss: 2.49; acc: 0.83
Batch: 540; loss: 1.38; acc: 0.91
Batch: 560; loss: 2.37; acc: 0.86
Batch: 580; loss: 2.34; acc: 0.78
Batch: 600; loss: 3.05; acc: 0.78
Batch: 620; loss: 0.85; acc: 0.86
Train Epoch over. train_loss: 1.8; train_accuracy: 0.85 

Batch: 0; loss: 0.93; acc: 0.88
Batch: 20; loss: 2.84; acc: 0.78
Batch: 40; loss: 1.6; acc: 0.88
Batch: 60; loss: 3.59; acc: 0.83
Batch: 80; loss: 2.09; acc: 0.8
Batch: 100; loss: 3.37; acc: 0.78
Batch: 120; loss: 2.02; acc: 0.86
Batch: 140; loss: 3.81; acc: 0.8
Val Epoch over. val_loss: 2.292969749611654; val_accuracy: 0.8289211783439491 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.22; acc: 0.84
Batch: 20; loss: 1.69; acc: 0.88
Batch: 40; loss: 1.48; acc: 0.84
Batch: 60; loss: 1.94; acc: 0.86
Batch: 80; loss: 2.11; acc: 0.84
Batch: 100; loss: 1.33; acc: 0.81
Batch: 120; loss: 2.64; acc: 0.81
Batch: 140; loss: 2.21; acc: 0.84
Batch: 160; loss: 2.05; acc: 0.88
Batch: 180; loss: 1.75; acc: 0.84
Batch: 200; loss: 1.07; acc: 0.83
Batch: 220; loss: 1.65; acc: 0.84
Batch: 240; loss: 3.1; acc: 0.83
Batch: 260; loss: 1.58; acc: 0.84
Batch: 280; loss: 1.94; acc: 0.91
Batch: 300; loss: 1.67; acc: 0.92
Batch: 320; loss: 0.89; acc: 0.91
Batch: 340; loss: 2.54; acc: 0.86
Batch: 360; loss: 1.25; acc: 0.91
Batch: 380; loss: 1.27; acc: 0.94
Batch: 400; loss: 3.22; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.92
Batch: 440; loss: 1.67; acc: 0.84
Batch: 460; loss: 2.22; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 1.94; acc: 0.8
Batch: 520; loss: 2.68; acc: 0.75
Batch: 540; loss: 2.03; acc: 0.84
Batch: 560; loss: 2.56; acc: 0.83
Batch: 580; loss: 4.48; acc: 0.75
Batch: 600; loss: 1.63; acc: 0.81
Batch: 620; loss: 3.09; acc: 0.81
Train Epoch over. train_loss: 1.78; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 2.89; acc: 0.78
Batch: 40; loss: 1.41; acc: 0.89
Batch: 60; loss: 3.82; acc: 0.83
Batch: 80; loss: 2.25; acc: 0.8
Batch: 100; loss: 3.1; acc: 0.78
Batch: 120; loss: 2.05; acc: 0.84
Batch: 140; loss: 3.9; acc: 0.78
Val Epoch over. val_loss: 2.2799292945178453; val_accuracy: 0.8293192675159236 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.53; acc: 0.83
Batch: 20; loss: 2.01; acc: 0.83
Batch: 40; loss: 1.11; acc: 0.86
Batch: 60; loss: 0.89; acc: 0.95
Batch: 80; loss: 2.78; acc: 0.83
Batch: 100; loss: 1.64; acc: 0.83
Batch: 120; loss: 1.57; acc: 0.86
Batch: 140; loss: 1.75; acc: 0.86
Batch: 160; loss: 1.79; acc: 0.88
Batch: 180; loss: 0.94; acc: 0.89
Batch: 200; loss: 1.32; acc: 0.88
Batch: 220; loss: 1.84; acc: 0.86
Batch: 240; loss: 1.17; acc: 0.88
Batch: 260; loss: 2.17; acc: 0.83
Batch: 280; loss: 1.3; acc: 0.91
Batch: 300; loss: 2.16; acc: 0.86
Batch: 320; loss: 1.77; acc: 0.84
Batch: 340; loss: 1.3; acc: 0.91
Batch: 360; loss: 1.75; acc: 0.92
Batch: 380; loss: 1.62; acc: 0.83
Batch: 400; loss: 1.17; acc: 0.91
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 1.19; acc: 0.84
Batch: 460; loss: 1.64; acc: 0.77
Batch: 480; loss: 2.41; acc: 0.81
Batch: 500; loss: 2.0; acc: 0.84
Batch: 520; loss: 1.81; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.95
Batch: 560; loss: 1.64; acc: 0.89
Batch: 580; loss: 1.38; acc: 0.84
Batch: 600; loss: 1.88; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.92
Train Epoch over. train_loss: 1.77; train_accuracy: 0.85 

Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 2.76; acc: 0.81
Batch: 40; loss: 1.57; acc: 0.89
Batch: 60; loss: 3.54; acc: 0.84
Batch: 80; loss: 1.97; acc: 0.8
Batch: 100; loss: 2.88; acc: 0.8
Batch: 120; loss: 1.88; acc: 0.83
Batch: 140; loss: 3.78; acc: 0.78
Val Epoch over. val_loss: 2.242953449013127; val_accuracy: 0.8348925159235668 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.3; acc: 0.88
Batch: 20; loss: 1.57; acc: 0.84
Batch: 40; loss: 1.9; acc: 0.88
Batch: 60; loss: 1.85; acc: 0.83
Batch: 80; loss: 3.32; acc: 0.8
Batch: 100; loss: 1.73; acc: 0.92
Batch: 120; loss: 2.84; acc: 0.8
Batch: 140; loss: 1.11; acc: 0.91
Batch: 160; loss: 2.22; acc: 0.89
Batch: 180; loss: 1.25; acc: 0.86
Batch: 200; loss: 2.53; acc: 0.86
Batch: 220; loss: 1.87; acc: 0.84
Batch: 240; loss: 1.69; acc: 0.86
Batch: 260; loss: 1.26; acc: 0.84
Batch: 280; loss: 2.85; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.89
Batch: 320; loss: 1.32; acc: 0.78
Batch: 340; loss: 3.54; acc: 0.77
Batch: 360; loss: 1.52; acc: 0.86
Batch: 380; loss: 1.57; acc: 0.8
Batch: 400; loss: 2.52; acc: 0.88
Batch: 420; loss: 1.2; acc: 0.91
Batch: 440; loss: 1.59; acc: 0.91
Batch: 460; loss: 1.74; acc: 0.8
Batch: 480; loss: 2.23; acc: 0.83
Batch: 500; loss: 4.18; acc: 0.72
Batch: 520; loss: 0.71; acc: 0.91
Batch: 540; loss: 2.14; acc: 0.75
Batch: 560; loss: 1.56; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.91
Batch: 600; loss: 1.01; acc: 0.88
Batch: 620; loss: 1.76; acc: 0.86
Train Epoch over. train_loss: 1.76; train_accuracy: 0.85 

Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 2.91; acc: 0.77
Batch: 40; loss: 1.66; acc: 0.89
Batch: 60; loss: 3.77; acc: 0.84
Batch: 80; loss: 2.07; acc: 0.8
Batch: 100; loss: 2.92; acc: 0.8
Batch: 120; loss: 1.95; acc: 0.83
Batch: 140; loss: 3.72; acc: 0.75
Val Epoch over. val_loss: 2.2458405035316567; val_accuracy: 0.8311106687898089 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.42; acc: 0.86
Batch: 20; loss: 2.11; acc: 0.83
Batch: 40; loss: 2.02; acc: 0.83
Batch: 60; loss: 1.03; acc: 0.86
Batch: 80; loss: 1.8; acc: 0.84
Batch: 100; loss: 1.42; acc: 0.86
Batch: 120; loss: 2.3; acc: 0.84
Batch: 140; loss: 2.06; acc: 0.83
Batch: 160; loss: 1.31; acc: 0.88
Batch: 180; loss: 0.76; acc: 0.91
Batch: 200; loss: 1.77; acc: 0.81
Batch: 220; loss: 2.78; acc: 0.81
Batch: 240; loss: 1.52; acc: 0.91
Batch: 260; loss: 1.68; acc: 0.86
Batch: 280; loss: 1.8; acc: 0.86
Batch: 300; loss: 2.84; acc: 0.86
Batch: 320; loss: 2.42; acc: 0.81
Batch: 340; loss: 2.02; acc: 0.8
Batch: 360; loss: 0.72; acc: 0.94
Batch: 380; loss: 2.02; acc: 0.83
Batch: 400; loss: 0.65; acc: 0.91
Batch: 420; loss: 1.58; acc: 0.88
Batch: 440; loss: 2.71; acc: 0.81
Batch: 460; loss: 2.85; acc: 0.86
Batch: 480; loss: 2.41; acc: 0.84
Batch: 500; loss: 2.45; acc: 0.86
Batch: 520; loss: 1.46; acc: 0.88
Batch: 540; loss: 3.27; acc: 0.84
Batch: 560; loss: 1.5; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.91
Batch: 600; loss: 1.02; acc: 0.91
Batch: 620; loss: 2.38; acc: 0.86
Train Epoch over. train_loss: 1.75; train_accuracy: 0.85 

Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 2.89; acc: 0.77
Batch: 40; loss: 1.91; acc: 0.83
Batch: 60; loss: 3.89; acc: 0.81
Batch: 80; loss: 1.96; acc: 0.78
Batch: 100; loss: 3.13; acc: 0.78
Batch: 120; loss: 2.37; acc: 0.86
Batch: 140; loss: 3.88; acc: 0.78
Val Epoch over. val_loss: 2.2717286944389343; val_accuracy: 0.8250398089171974 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.19; acc: 0.91
Batch: 20; loss: 2.26; acc: 0.78
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 2.53; acc: 0.78
Batch: 80; loss: 1.37; acc: 0.88
Batch: 100; loss: 1.42; acc: 0.88
Batch: 120; loss: 1.86; acc: 0.91
Batch: 140; loss: 1.25; acc: 0.88
Batch: 160; loss: 1.34; acc: 0.94
Batch: 180; loss: 1.69; acc: 0.84
Batch: 200; loss: 2.21; acc: 0.81
Batch: 220; loss: 1.38; acc: 0.89
Batch: 240; loss: 4.48; acc: 0.78
Batch: 260; loss: 1.58; acc: 0.89
Batch: 280; loss: 1.49; acc: 0.84
Batch: 300; loss: 2.89; acc: 0.84
Batch: 320; loss: 1.59; acc: 0.83
Batch: 340; loss: 1.21; acc: 0.89
Batch: 360; loss: 1.39; acc: 0.89
Batch: 380; loss: 2.63; acc: 0.81
Batch: 400; loss: 2.47; acc: 0.78
Batch: 420; loss: 3.48; acc: 0.73
Batch: 440; loss: 0.68; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.94
Batch: 480; loss: 2.64; acc: 0.83
Batch: 500; loss: 1.71; acc: 0.91
Batch: 520; loss: 1.74; acc: 0.86
Batch: 540; loss: 1.56; acc: 0.88
Batch: 560; loss: 2.69; acc: 0.84
Batch: 580; loss: 2.27; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.99; acc: 0.88
Train Epoch over. train_loss: 1.66; train_accuracy: 0.86 

Batch: 0; loss: 0.63; acc: 0.91
Batch: 20; loss: 2.76; acc: 0.8
Batch: 40; loss: 1.77; acc: 0.89
Batch: 60; loss: 3.86; acc: 0.81
Batch: 80; loss: 1.9; acc: 0.8
Batch: 100; loss: 2.85; acc: 0.8
Batch: 120; loss: 2.0; acc: 0.83
Batch: 140; loss: 3.83; acc: 0.78
Val Epoch over. val_loss: 2.2246077432753935; val_accuracy: 0.8331011146496815 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.15; acc: 0.81
Batch: 20; loss: 1.75; acc: 0.88
Batch: 40; loss: 1.65; acc: 0.88
Batch: 60; loss: 1.6; acc: 0.81
Batch: 80; loss: 2.98; acc: 0.83
Batch: 100; loss: 1.53; acc: 0.8
Batch: 120; loss: 2.68; acc: 0.81
Batch: 140; loss: 1.5; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.94
Batch: 180; loss: 1.33; acc: 0.91
Batch: 200; loss: 1.62; acc: 0.81
Batch: 220; loss: 4.14; acc: 0.8
Batch: 240; loss: 2.73; acc: 0.89
Batch: 260; loss: 2.62; acc: 0.86
Batch: 280; loss: 1.34; acc: 0.81
Batch: 300; loss: 1.97; acc: 0.81
Batch: 320; loss: 1.36; acc: 0.91
Batch: 340; loss: 1.71; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 1.95; acc: 0.77
Batch: 400; loss: 1.68; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.88
Batch: 440; loss: 0.79; acc: 0.95
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 2.2; acc: 0.8
Batch: 520; loss: 1.78; acc: 0.86
Batch: 540; loss: 2.68; acc: 0.84
Batch: 560; loss: 4.45; acc: 0.81
Batch: 580; loss: 1.2; acc: 0.92
Batch: 600; loss: 1.67; acc: 0.88
Batch: 620; loss: 2.31; acc: 0.83
Train Epoch over. train_loss: 1.64; train_accuracy: 0.86 

Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 2.72; acc: 0.8
Batch: 40; loss: 1.86; acc: 0.89
Batch: 60; loss: 3.75; acc: 0.81
Batch: 80; loss: 1.92; acc: 0.78
Batch: 100; loss: 2.8; acc: 0.78
Batch: 120; loss: 1.91; acc: 0.81
Batch: 140; loss: 3.85; acc: 0.78
Val Epoch over. val_loss: 2.2177105588707953; val_accuracy: 0.8330015923566879 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.81; acc: 0.84
Batch: 20; loss: 3.15; acc: 0.8
Batch: 40; loss: 1.25; acc: 0.89
Batch: 60; loss: 2.44; acc: 0.8
Batch: 80; loss: 2.65; acc: 0.83
Batch: 100; loss: 2.4; acc: 0.81
Batch: 120; loss: 1.45; acc: 0.86
Batch: 140; loss: 2.78; acc: 0.89
Batch: 160; loss: 2.27; acc: 0.86
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 1.15; acc: 0.86
Batch: 240; loss: 0.89; acc: 0.88
Batch: 260; loss: 1.34; acc: 0.88
Batch: 280; loss: 2.69; acc: 0.88
Batch: 300; loss: 1.05; acc: 0.92
Batch: 320; loss: 2.06; acc: 0.8
Batch: 340; loss: 1.61; acc: 0.83
Batch: 360; loss: 1.24; acc: 0.88
Batch: 380; loss: 1.86; acc: 0.88
Batch: 400; loss: 2.82; acc: 0.78
Batch: 420; loss: 1.45; acc: 0.91
Batch: 440; loss: 1.5; acc: 0.88
Batch: 460; loss: 0.86; acc: 0.91
Batch: 480; loss: 1.14; acc: 0.88
Batch: 500; loss: 2.37; acc: 0.81
Batch: 520; loss: 1.73; acc: 0.88
Batch: 540; loss: 1.53; acc: 0.88
Batch: 560; loss: 1.11; acc: 0.88
Batch: 580; loss: 0.67; acc: 0.94
Batch: 600; loss: 1.88; acc: 0.84
Batch: 620; loss: 0.99; acc: 0.84
Train Epoch over. train_loss: 1.64; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 2.75; acc: 0.81
Batch: 40; loss: 1.81; acc: 0.89
Batch: 60; loss: 3.75; acc: 0.83
Batch: 80; loss: 1.92; acc: 0.8
Batch: 100; loss: 2.78; acc: 0.78
Batch: 120; loss: 1.93; acc: 0.81
Batch: 140; loss: 3.83; acc: 0.78
Val Epoch over. val_loss: 2.2157328386025825; val_accuracy: 0.8342953821656051 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.67; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 1.32; acc: 0.86
Batch: 60; loss: 1.2; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.91
Batch: 120; loss: 1.78; acc: 0.8
Batch: 140; loss: 1.27; acc: 0.86
Batch: 160; loss: 2.89; acc: 0.8
Batch: 180; loss: 1.09; acc: 0.94
Batch: 200; loss: 1.23; acc: 0.84
Batch: 220; loss: 1.92; acc: 0.86
Batch: 240; loss: 1.73; acc: 0.81
Batch: 260; loss: 1.08; acc: 0.89
Batch: 280; loss: 1.79; acc: 0.89
Batch: 300; loss: 2.06; acc: 0.88
Batch: 320; loss: 1.85; acc: 0.86
Batch: 340; loss: 2.53; acc: 0.84
Batch: 360; loss: 1.08; acc: 0.88
Batch: 380; loss: 1.35; acc: 0.88
Batch: 400; loss: 1.76; acc: 0.84
Batch: 420; loss: 2.46; acc: 0.81
Batch: 440; loss: 1.54; acc: 0.8
Batch: 460; loss: 2.38; acc: 0.77
Batch: 480; loss: 3.79; acc: 0.73
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 1.86; acc: 0.83
Batch: 540; loss: 1.11; acc: 0.86
Batch: 560; loss: 2.68; acc: 0.77
Batch: 580; loss: 1.18; acc: 0.88
Batch: 600; loss: 1.69; acc: 0.91
Batch: 620; loss: 2.08; acc: 0.88
Train Epoch over. train_loss: 1.64; train_accuracy: 0.86 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 2.75; acc: 0.81
Batch: 40; loss: 1.8; acc: 0.89
Batch: 60; loss: 3.76; acc: 0.83
Batch: 80; loss: 1.96; acc: 0.78
Batch: 100; loss: 2.81; acc: 0.78
Batch: 120; loss: 1.9; acc: 0.81
Batch: 140; loss: 3.77; acc: 0.78
Val Epoch over. val_loss: 2.217136400330598; val_accuracy: 0.8314092356687898 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.58; acc: 0.72
Batch: 20; loss: 2.59; acc: 0.89
Batch: 40; loss: 3.75; acc: 0.78
Batch: 60; loss: 1.63; acc: 0.84
Batch: 80; loss: 1.13; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.84
Batch: 120; loss: 1.42; acc: 0.86
Batch: 140; loss: 1.32; acc: 0.81
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.63; acc: 0.95
Batch: 200; loss: 1.53; acc: 0.89
Batch: 220; loss: 0.91; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.95
Batch: 260; loss: 0.77; acc: 0.86
Batch: 280; loss: 2.21; acc: 0.81
Batch: 300; loss: 0.97; acc: 0.92
Batch: 320; loss: 1.0; acc: 0.84
Batch: 340; loss: 3.4; acc: 0.8
Batch: 360; loss: 1.44; acc: 0.84
Batch: 380; loss: 1.61; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.92
Batch: 420; loss: 0.92; acc: 0.88
Batch: 440; loss: 1.36; acc: 0.92
Batch: 460; loss: 4.07; acc: 0.78
Batch: 480; loss: 4.0; acc: 0.81
Batch: 500; loss: 1.57; acc: 0.84
Batch: 520; loss: 1.14; acc: 0.88
Batch: 540; loss: 1.4; acc: 0.89
Batch: 560; loss: 2.9; acc: 0.84
Batch: 580; loss: 1.25; acc: 0.86
Batch: 600; loss: 0.58; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 1.63; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 2.78; acc: 0.81
Batch: 40; loss: 1.84; acc: 0.89
Batch: 60; loss: 3.7; acc: 0.83
Batch: 80; loss: 1.97; acc: 0.78
Batch: 100; loss: 2.82; acc: 0.8
Batch: 120; loss: 2.0; acc: 0.81
Batch: 140; loss: 3.8; acc: 0.78
Val Epoch over. val_loss: 2.2120828601015603; val_accuracy: 0.8325039808917197 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.58; acc: 0.81
Batch: 20; loss: 1.06; acc: 0.84
Batch: 40; loss: 2.36; acc: 0.84
Batch: 60; loss: 2.9; acc: 0.84
Batch: 80; loss: 2.36; acc: 0.8
Batch: 100; loss: 1.7; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.89
Batch: 140; loss: 1.83; acc: 0.86
Batch: 160; loss: 1.51; acc: 0.91
Batch: 180; loss: 1.66; acc: 0.86
Batch: 200; loss: 2.07; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 1.96; acc: 0.84
Batch: 280; loss: 2.19; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.91
Batch: 320; loss: 3.32; acc: 0.84
Batch: 340; loss: 1.92; acc: 0.84
Batch: 360; loss: 2.53; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 2.29; acc: 0.84
Batch: 420; loss: 1.09; acc: 0.91
Batch: 440; loss: 1.36; acc: 0.88
Batch: 460; loss: 2.88; acc: 0.78
Batch: 480; loss: 1.48; acc: 0.83
Batch: 500; loss: 1.62; acc: 0.83
Batch: 520; loss: 2.17; acc: 0.72
Batch: 540; loss: 1.08; acc: 0.86
Batch: 560; loss: 1.73; acc: 0.81
Batch: 580; loss: 1.12; acc: 0.88
Batch: 600; loss: 3.51; acc: 0.77
Batch: 620; loss: 0.91; acc: 0.91
Train Epoch over. train_loss: 1.63; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 2.78; acc: 0.81
Batch: 40; loss: 1.8; acc: 0.89
Batch: 60; loss: 3.76; acc: 0.83
Batch: 80; loss: 1.94; acc: 0.78
Batch: 100; loss: 2.79; acc: 0.78
Batch: 120; loss: 1.92; acc: 0.81
Batch: 140; loss: 3.79; acc: 0.78
Val Epoch over. val_loss: 2.2186019874302443; val_accuracy: 0.832703025477707 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.38; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.81; acc: 0.89
Batch: 60; loss: 1.45; acc: 0.86
Batch: 80; loss: 3.36; acc: 0.77
Batch: 100; loss: 2.19; acc: 0.86
Batch: 120; loss: 2.49; acc: 0.78
Batch: 140; loss: 2.47; acc: 0.81
Batch: 160; loss: 1.19; acc: 0.88
Batch: 180; loss: 1.0; acc: 0.88
Batch: 200; loss: 1.87; acc: 0.86
Batch: 220; loss: 2.57; acc: 0.78
Batch: 240; loss: 1.47; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 1.12; acc: 0.91
Batch: 300; loss: 1.84; acc: 0.88
Batch: 320; loss: 1.01; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.92
Batch: 360; loss: 2.51; acc: 0.89
Batch: 380; loss: 2.07; acc: 0.84
Batch: 400; loss: 2.79; acc: 0.81
Batch: 420; loss: 1.09; acc: 0.89
Batch: 440; loss: 1.32; acc: 0.89
Batch: 460; loss: 2.07; acc: 0.83
Batch: 480; loss: 3.52; acc: 0.66
Batch: 500; loss: 1.36; acc: 0.89
Batch: 520; loss: 1.92; acc: 0.8
Batch: 540; loss: 1.63; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 1.19; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 1.63; train_accuracy: 0.86 

Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 2.77; acc: 0.81
Batch: 40; loss: 1.83; acc: 0.89
Batch: 60; loss: 3.74; acc: 0.83
Batch: 80; loss: 1.95; acc: 0.78
Batch: 100; loss: 2.79; acc: 0.8
Batch: 120; loss: 1.9; acc: 0.81
Batch: 140; loss: 3.77; acc: 0.78
Val Epoch over. val_loss: 2.2184378260829645; val_accuracy: 0.8329020700636943 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.91; acc: 0.94
Batch: 20; loss: 1.0; acc: 0.91
Batch: 40; loss: 0.95; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.92
Batch: 80; loss: 1.8; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.92
Batch: 120; loss: 1.78; acc: 0.83
Batch: 140; loss: 1.08; acc: 0.92
Batch: 160; loss: 1.13; acc: 0.83
Batch: 180; loss: 1.47; acc: 0.89
Batch: 200; loss: 1.51; acc: 0.88
Batch: 220; loss: 1.85; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 2.62; acc: 0.72
Batch: 280; loss: 1.1; acc: 0.92
Batch: 300; loss: 0.73; acc: 0.94
Batch: 320; loss: 1.79; acc: 0.86
Batch: 340; loss: 0.91; acc: 0.88
Batch: 360; loss: 1.8; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.97
Batch: 400; loss: 0.55; acc: 0.92
Batch: 420; loss: 1.77; acc: 0.84
Batch: 440; loss: 1.31; acc: 0.84
Batch: 460; loss: 1.52; acc: 0.88
Batch: 480; loss: 1.08; acc: 0.89
Batch: 500; loss: 2.0; acc: 0.86
Batch: 520; loss: 3.36; acc: 0.8
Batch: 540; loss: 1.13; acc: 0.86
Batch: 560; loss: 0.97; acc: 0.89
Batch: 580; loss: 1.85; acc: 0.84
Batch: 600; loss: 1.03; acc: 0.91
Batch: 620; loss: 1.54; acc: 0.88
Train Epoch over. train_loss: 1.63; train_accuracy: 0.86 

Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 2.82; acc: 0.8
Batch: 40; loss: 1.8; acc: 0.89
Batch: 60; loss: 3.77; acc: 0.81
Batch: 80; loss: 1.94; acc: 0.81
Batch: 100; loss: 2.85; acc: 0.78
Batch: 120; loss: 1.97; acc: 0.83
Batch: 140; loss: 3.77; acc: 0.77
Val Epoch over. val_loss: 2.2209032728413867; val_accuracy: 0.8304140127388535 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.45; acc: 0.91
Batch: 20; loss: 1.79; acc: 0.84
Batch: 40; loss: 1.82; acc: 0.8
Batch: 60; loss: 2.07; acc: 0.81
Batch: 80; loss: 2.27; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.89
Batch: 120; loss: 1.69; acc: 0.8
Batch: 140; loss: 2.38; acc: 0.84
Batch: 160; loss: 1.35; acc: 0.83
Batch: 180; loss: 1.03; acc: 0.91
Batch: 200; loss: 1.69; acc: 0.83
Batch: 220; loss: 2.17; acc: 0.78
Batch: 240; loss: 1.26; acc: 0.83
Batch: 260; loss: 1.94; acc: 0.88
Batch: 280; loss: 2.56; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 1.98; acc: 0.78
Batch: 340; loss: 1.37; acc: 0.84
Batch: 360; loss: 0.95; acc: 0.88
Batch: 380; loss: 1.23; acc: 0.84
Batch: 400; loss: 0.86; acc: 0.91
Batch: 420; loss: 2.06; acc: 0.86
Batch: 440; loss: 0.92; acc: 0.91
Batch: 460; loss: 1.19; acc: 0.83
Batch: 480; loss: 0.86; acc: 0.92
Batch: 500; loss: 1.84; acc: 0.86
Batch: 520; loss: 2.03; acc: 0.83
Batch: 540; loss: 1.85; acc: 0.89
Batch: 560; loss: 2.45; acc: 0.88
Batch: 580; loss: 1.64; acc: 0.77
Batch: 600; loss: 0.71; acc: 0.94
Batch: 620; loss: 1.01; acc: 0.91
Train Epoch over. train_loss: 1.63; train_accuracy: 0.86 

Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 2.77; acc: 0.81
Batch: 40; loss: 1.86; acc: 0.89
Batch: 60; loss: 3.71; acc: 0.81
Batch: 80; loss: 1.91; acc: 0.8
Batch: 100; loss: 2.81; acc: 0.78
Batch: 120; loss: 1.97; acc: 0.83
Batch: 140; loss: 3.75; acc: 0.78
Val Epoch over. val_loss: 2.21266327257369; val_accuracy: 0.8311106687898089 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.22; acc: 0.88
Batch: 20; loss: 1.07; acc: 0.88
Batch: 40; loss: 1.65; acc: 0.89
Batch: 60; loss: 2.4; acc: 0.83
Batch: 80; loss: 1.74; acc: 0.84
Batch: 100; loss: 1.38; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.89
Batch: 140; loss: 2.35; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 2.25; acc: 0.78
Batch: 200; loss: 2.15; acc: 0.73
Batch: 220; loss: 1.08; acc: 0.92
Batch: 240; loss: 1.99; acc: 0.84
Batch: 260; loss: 2.04; acc: 0.86
Batch: 280; loss: 2.06; acc: 0.84
Batch: 300; loss: 1.55; acc: 0.88
Batch: 320; loss: 2.62; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.95
Batch: 360; loss: 3.18; acc: 0.75
Batch: 380; loss: 0.9; acc: 0.88
Batch: 400; loss: 1.29; acc: 0.86
Batch: 420; loss: 1.9; acc: 0.86
Batch: 440; loss: 1.42; acc: 0.86
Batch: 460; loss: 1.3; acc: 0.78
Batch: 480; loss: 1.28; acc: 0.84
Batch: 500; loss: 2.28; acc: 0.81
Batch: 520; loss: 1.87; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 2.08; acc: 0.84
Batch: 580; loss: 2.38; acc: 0.83
Batch: 600; loss: 0.99; acc: 0.88
Batch: 620; loss: 1.87; acc: 0.84
Train Epoch over. train_loss: 1.63; train_accuracy: 0.86 

Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 2.77; acc: 0.81
Batch: 40; loss: 1.84; acc: 0.89
Batch: 60; loss: 3.74; acc: 0.81
Batch: 80; loss: 1.95; acc: 0.8
Batch: 100; loss: 2.77; acc: 0.8
Batch: 120; loss: 1.91; acc: 0.81
Batch: 140; loss: 3.75; acc: 0.78
Val Epoch over. val_loss: 2.21823152842795; val_accuracy: 0.8322054140127388 

plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_800_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 446542
elements in E: 199210000
fraction nonzero: 0.002241564178505095
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 36.37; acc: 0.12
Batch: 20; loss: 5.71; acc: 0.55
Batch: 40; loss: 7.8; acc: 0.58
Batch: 60; loss: 4.48; acc: 0.56
Batch: 80; loss: 2.01; acc: 0.77
Batch: 100; loss: 3.9; acc: 0.7
Batch: 120; loss: 4.31; acc: 0.69
Batch: 140; loss: 3.93; acc: 0.73
Batch: 160; loss: 3.69; acc: 0.77
Batch: 180; loss: 2.08; acc: 0.81
Batch: 200; loss: 4.06; acc: 0.73
Batch: 220; loss: 2.5; acc: 0.81
Batch: 240; loss: 2.35; acc: 0.81
Batch: 260; loss: 4.58; acc: 0.8
Batch: 280; loss: 2.55; acc: 0.78
Batch: 300; loss: 9.15; acc: 0.5
Batch: 320; loss: 3.73; acc: 0.78
Batch: 340; loss: 3.93; acc: 0.81
Batch: 360; loss: 3.0; acc: 0.88
Batch: 380; loss: 2.2; acc: 0.8
Batch: 400; loss: 2.51; acc: 0.83
Batch: 420; loss: 3.13; acc: 0.78
Batch: 440; loss: 4.67; acc: 0.75
Batch: 460; loss: 2.42; acc: 0.78
Batch: 480; loss: 3.12; acc: 0.75
Batch: 500; loss: 4.1; acc: 0.7
Batch: 520; loss: 1.92; acc: 0.83
Batch: 540; loss: 2.55; acc: 0.78
Batch: 560; loss: 5.94; acc: 0.69
Batch: 580; loss: 4.99; acc: 0.7
Batch: 600; loss: 3.51; acc: 0.81
Batch: 620; loss: 3.88; acc: 0.7
Train Epoch over. train_loss: 4.58; train_accuracy: 0.75 

Batch: 0; loss: 2.16; acc: 0.83
Batch: 20; loss: 5.37; acc: 0.73
Batch: 40; loss: 2.99; acc: 0.86
Batch: 60; loss: 5.17; acc: 0.78
Batch: 80; loss: 3.16; acc: 0.81
Batch: 100; loss: 4.45; acc: 0.75
Batch: 120; loss: 3.57; acc: 0.81
Batch: 140; loss: 5.19; acc: 0.7
Val Epoch over. val_loss: 3.362861761621609; val_accuracy: 0.7953821656050956 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 4.03; acc: 0.78
Batch: 20; loss: 2.73; acc: 0.83
Batch: 40; loss: 1.99; acc: 0.86
Batch: 60; loss: 4.44; acc: 0.8
Batch: 80; loss: 2.85; acc: 0.81
Batch: 100; loss: 3.47; acc: 0.81
Batch: 120; loss: 2.27; acc: 0.84
Batch: 140; loss: 3.84; acc: 0.83
Batch: 160; loss: 3.61; acc: 0.75
Batch: 180; loss: 4.13; acc: 0.84
Batch: 200; loss: 1.65; acc: 0.81
Batch: 220; loss: 4.53; acc: 0.72
Batch: 240; loss: 3.66; acc: 0.77
Batch: 260; loss: 2.96; acc: 0.73
Batch: 280; loss: 2.9; acc: 0.84
Batch: 300; loss: 5.44; acc: 0.75
Batch: 320; loss: 2.51; acc: 0.78
Batch: 340; loss: 3.85; acc: 0.8
Batch: 360; loss: 3.29; acc: 0.78
Batch: 380; loss: 5.3; acc: 0.72
Batch: 400; loss: 4.17; acc: 0.78
Batch: 420; loss: 3.4; acc: 0.75
Batch: 440; loss: 2.97; acc: 0.83
Batch: 460; loss: 2.66; acc: 0.81
Batch: 480; loss: 3.1; acc: 0.77
Batch: 500; loss: 2.43; acc: 0.78
Batch: 520; loss: 3.38; acc: 0.77
Batch: 540; loss: 1.73; acc: 0.8
Batch: 560; loss: 5.72; acc: 0.64
Batch: 580; loss: 2.79; acc: 0.84
Batch: 600; loss: 3.17; acc: 0.81
Batch: 620; loss: 3.71; acc: 0.72
Train Epoch over. train_loss: 3.35; train_accuracy: 0.79 

Batch: 0; loss: 4.64; acc: 0.78
Batch: 20; loss: 6.41; acc: 0.69
Batch: 40; loss: 3.76; acc: 0.84
Batch: 60; loss: 3.12; acc: 0.89
Batch: 80; loss: 3.87; acc: 0.78
Batch: 100; loss: 4.09; acc: 0.72
Batch: 120; loss: 3.78; acc: 0.84
Batch: 140; loss: 4.94; acc: 0.72
Val Epoch over. val_loss: 3.3514701575989934; val_accuracy: 0.7940883757961783 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 3.63; acc: 0.8
Batch: 20; loss: 8.53; acc: 0.64
Batch: 40; loss: 3.03; acc: 0.75
Batch: 60; loss: 3.7; acc: 0.73
Batch: 80; loss: 3.97; acc: 0.81
Batch: 100; loss: 1.95; acc: 0.84
Batch: 120; loss: 3.27; acc: 0.81
Batch: 140; loss: 3.09; acc: 0.81
Batch: 160; loss: 3.22; acc: 0.81
Batch: 180; loss: 3.01; acc: 0.77
Batch: 200; loss: 5.73; acc: 0.69
Batch: 220; loss: 2.7; acc: 0.81
Batch: 240; loss: 2.33; acc: 0.81
Batch: 260; loss: 1.69; acc: 0.94
Batch: 280; loss: 3.18; acc: 0.75
Batch: 300; loss: 2.32; acc: 0.81
Batch: 320; loss: 3.01; acc: 0.81
Batch: 340; loss: 2.31; acc: 0.84
Batch: 360; loss: 2.92; acc: 0.83
Batch: 380; loss: 1.36; acc: 0.83
Batch: 400; loss: 4.17; acc: 0.77
Batch: 420; loss: 1.13; acc: 0.88
Batch: 440; loss: 2.08; acc: 0.89
Batch: 460; loss: 2.43; acc: 0.89
Batch: 480; loss: 3.68; acc: 0.8
Batch: 500; loss: 3.26; acc: 0.83
Batch: 520; loss: 4.09; acc: 0.78
Batch: 540; loss: 2.72; acc: 0.8
Batch: 560; loss: 3.02; acc: 0.78
Batch: 580; loss: 4.01; acc: 0.78
Batch: 600; loss: 2.63; acc: 0.81
Batch: 620; loss: 3.99; acc: 0.75
Train Epoch over. train_loss: 3.2; train_accuracy: 0.8 

Batch: 0; loss: 3.25; acc: 0.81
Batch: 20; loss: 5.58; acc: 0.66
Batch: 40; loss: 3.29; acc: 0.88
Batch: 60; loss: 4.28; acc: 0.78
Batch: 80; loss: 2.38; acc: 0.89
Batch: 100; loss: 3.24; acc: 0.78
Batch: 120; loss: 2.54; acc: 0.8
Batch: 140; loss: 7.85; acc: 0.66
Val Epoch over. val_loss: 3.268301294108105; val_accuracy: 0.7944864649681529 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 1.65; acc: 0.86
Batch: 20; loss: 4.41; acc: 0.77
Batch: 40; loss: 1.56; acc: 0.89
Batch: 60; loss: 2.42; acc: 0.83
Batch: 80; loss: 2.13; acc: 0.81
Batch: 100; loss: 1.98; acc: 0.84
Batch: 120; loss: 2.85; acc: 0.84
Batch: 140; loss: 2.91; acc: 0.88
Batch: 160; loss: 1.76; acc: 0.83
Batch: 180; loss: 1.16; acc: 0.94
Batch: 200; loss: 4.14; acc: 0.78
Batch: 220; loss: 3.26; acc: 0.75
Batch: 240; loss: 2.0; acc: 0.83
Batch: 260; loss: 4.23; acc: 0.77
Batch: 280; loss: 2.5; acc: 0.81
Batch: 300; loss: 2.96; acc: 0.81
Batch: 320; loss: 3.78; acc: 0.77
Batch: 340; loss: 2.66; acc: 0.84
Batch: 360; loss: 1.61; acc: 0.88
Batch: 380; loss: 2.31; acc: 0.8
Batch: 400; loss: 2.96; acc: 0.77
Batch: 420; loss: 2.39; acc: 0.8
Batch: 440; loss: 3.54; acc: 0.75
Batch: 460; loss: 1.6; acc: 0.86
Batch: 480; loss: 3.79; acc: 0.75
Batch: 500; loss: 3.54; acc: 0.83
Batch: 520; loss: 3.64; acc: 0.73
Batch: 540; loss: 3.22; acc: 0.75
Batch: 560; loss: 2.79; acc: 0.83
Batch: 580; loss: 3.05; acc: 0.83
Batch: 600; loss: 2.57; acc: 0.83
Batch: 620; loss: 2.93; acc: 0.83
Train Epoch over. train_loss: 3.09; train_accuracy: 0.8 

Batch: 0; loss: 2.79; acc: 0.84
Batch: 20; loss: 5.26; acc: 0.69
Batch: 40; loss: 2.41; acc: 0.84
Batch: 60; loss: 3.86; acc: 0.83
Batch: 80; loss: 3.46; acc: 0.81
Batch: 100; loss: 2.83; acc: 0.77
Batch: 120; loss: 1.95; acc: 0.89
Batch: 140; loss: 7.08; acc: 0.7
Val Epoch over. val_loss: 3.1247582012301036; val_accuracy: 0.8089171974522293 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 2.92; acc: 0.81
Batch: 20; loss: 1.87; acc: 0.84
Batch: 40; loss: 3.86; acc: 0.78
Batch: 60; loss: 5.46; acc: 0.75
Batch: 80; loss: 1.78; acc: 0.81
Batch: 100; loss: 2.72; acc: 0.8
Batch: 120; loss: 3.2; acc: 0.83
Batch: 140; loss: 3.89; acc: 0.81
Batch: 160; loss: 2.08; acc: 0.83
Batch: 180; loss: 3.83; acc: 0.73
Batch: 200; loss: 4.75; acc: 0.64
Batch: 220; loss: 3.23; acc: 0.81
Batch: 240; loss: 1.26; acc: 0.84
Batch: 260; loss: 5.42; acc: 0.72
Batch: 280; loss: 1.96; acc: 0.88
Batch: 300; loss: 1.44; acc: 0.83
Batch: 320; loss: 4.53; acc: 0.75
Batch: 340; loss: 1.67; acc: 0.81
Batch: 360; loss: 2.99; acc: 0.81
Batch: 380; loss: 1.34; acc: 0.89
Batch: 400; loss: 2.13; acc: 0.83
Batch: 420; loss: 4.4; acc: 0.8
Batch: 440; loss: 2.5; acc: 0.86
Batch: 460; loss: 1.7; acc: 0.83
Batch: 480; loss: 3.11; acc: 0.8
Batch: 500; loss: 3.22; acc: 0.83
Batch: 520; loss: 3.76; acc: 0.78
Batch: 540; loss: 4.76; acc: 0.81
Batch: 560; loss: 4.78; acc: 0.77
Batch: 580; loss: 2.41; acc: 0.75
Batch: 600; loss: 5.18; acc: 0.73
Batch: 620; loss: 1.64; acc: 0.84
Train Epoch over. train_loss: 3.06; train_accuracy: 0.81 

Batch: 0; loss: 2.49; acc: 0.89
Batch: 20; loss: 3.97; acc: 0.73
Batch: 40; loss: 3.52; acc: 0.84
Batch: 60; loss: 4.38; acc: 0.81
Batch: 80; loss: 3.51; acc: 0.77
Batch: 100; loss: 3.55; acc: 0.73
Batch: 120; loss: 2.92; acc: 0.81
Batch: 140; loss: 5.93; acc: 0.7
Val Epoch over. val_loss: 2.9848598200044814; val_accuracy: 0.806031050955414 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 2.53; acc: 0.84
Batch: 20; loss: 1.69; acc: 0.88
Batch: 40; loss: 2.48; acc: 0.83
Batch: 60; loss: 2.04; acc: 0.84
Batch: 80; loss: 2.73; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.91
Batch: 120; loss: 1.73; acc: 0.91
Batch: 140; loss: 2.96; acc: 0.86
Batch: 160; loss: 3.97; acc: 0.75
Batch: 180; loss: 1.53; acc: 0.86
Batch: 200; loss: 1.98; acc: 0.8
Batch: 220; loss: 5.29; acc: 0.78
Batch: 240; loss: 1.38; acc: 0.91
Batch: 260; loss: 2.92; acc: 0.77
Batch: 280; loss: 3.52; acc: 0.81
Batch: 300; loss: 2.01; acc: 0.81
Batch: 320; loss: 3.72; acc: 0.81
Batch: 340; loss: 5.64; acc: 0.73
Batch: 360; loss: 2.54; acc: 0.86
Batch: 380; loss: 2.37; acc: 0.84
Batch: 400; loss: 4.7; acc: 0.8
Batch: 420; loss: 2.79; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.88
Batch: 460; loss: 3.21; acc: 0.81
Batch: 480; loss: 4.48; acc: 0.8
Batch: 500; loss: 3.08; acc: 0.84
Batch: 520; loss: 2.98; acc: 0.86
Batch: 540; loss: 2.76; acc: 0.84
Batch: 560; loss: 2.72; acc: 0.78
Batch: 580; loss: 3.26; acc: 0.78
Batch: 600; loss: 3.45; acc: 0.73
Batch: 620; loss: 2.87; acc: 0.78
Train Epoch over. train_loss: 3.0; train_accuracy: 0.81 

Batch: 0; loss: 3.62; acc: 0.8
Batch: 20; loss: 6.47; acc: 0.67
Batch: 40; loss: 1.83; acc: 0.84
Batch: 60; loss: 4.06; acc: 0.77
Batch: 80; loss: 4.85; acc: 0.77
Batch: 100; loss: 4.37; acc: 0.72
Batch: 120; loss: 5.11; acc: 0.8
Batch: 140; loss: 8.09; acc: 0.64
Val Epoch over. val_loss: 3.750594301208569; val_accuracy: 0.7788614649681529 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 1.8; acc: 0.88
Batch: 20; loss: 3.22; acc: 0.77
Batch: 40; loss: 3.56; acc: 0.77
Batch: 60; loss: 1.67; acc: 0.84
Batch: 80; loss: 5.88; acc: 0.67
Batch: 100; loss: 1.59; acc: 0.84
Batch: 120; loss: 3.13; acc: 0.81
Batch: 140; loss: 1.7; acc: 0.88
Batch: 160; loss: 4.1; acc: 0.73
Batch: 180; loss: 2.74; acc: 0.84
Batch: 200; loss: 1.2; acc: 0.91
Batch: 220; loss: 2.03; acc: 0.89
Batch: 240; loss: 2.46; acc: 0.75
Batch: 260; loss: 3.61; acc: 0.81
Batch: 280; loss: 3.44; acc: 0.8
Batch: 300; loss: 3.21; acc: 0.77
Batch: 320; loss: 4.68; acc: 0.77
Batch: 340; loss: 2.1; acc: 0.83
Batch: 360; loss: 3.68; acc: 0.86
Batch: 380; loss: 4.65; acc: 0.78
Batch: 400; loss: 1.32; acc: 0.88
Batch: 420; loss: 1.22; acc: 0.91
Batch: 440; loss: 4.78; acc: 0.8
Batch: 460; loss: 3.49; acc: 0.73
Batch: 480; loss: 3.11; acc: 0.83
Batch: 500; loss: 2.68; acc: 0.84
Batch: 520; loss: 2.91; acc: 0.81
Batch: 540; loss: 2.5; acc: 0.83
Batch: 560; loss: 3.25; acc: 0.77
Batch: 580; loss: 2.69; acc: 0.77
Batch: 600; loss: 2.33; acc: 0.86
Batch: 620; loss: 2.38; acc: 0.83
Train Epoch over. train_loss: 2.88; train_accuracy: 0.81 

Batch: 0; loss: 2.28; acc: 0.8
Batch: 20; loss: 6.43; acc: 0.67
Batch: 40; loss: 2.69; acc: 0.8
Batch: 60; loss: 3.52; acc: 0.77
Batch: 80; loss: 2.89; acc: 0.75
Batch: 100; loss: 3.02; acc: 0.8
Batch: 120; loss: 3.85; acc: 0.83
Batch: 140; loss: 6.13; acc: 0.67
Val Epoch over. val_loss: 3.281489024686206; val_accuracy: 0.7916003184713376 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 3.36; acc: 0.78
Batch: 20; loss: 3.57; acc: 0.8
Batch: 40; loss: 2.32; acc: 0.86
Batch: 60; loss: 3.11; acc: 0.83
Batch: 80; loss: 3.42; acc: 0.89
Batch: 100; loss: 2.85; acc: 0.8
Batch: 120; loss: 2.57; acc: 0.78
Batch: 140; loss: 2.82; acc: 0.8
Batch: 160; loss: 2.78; acc: 0.78
Batch: 180; loss: 2.06; acc: 0.78
Batch: 200; loss: 2.53; acc: 0.89
Batch: 220; loss: 3.76; acc: 0.8
Batch: 240; loss: 1.78; acc: 0.8
Batch: 260; loss: 3.42; acc: 0.84
Batch: 280; loss: 1.82; acc: 0.8
Batch: 300; loss: 4.09; acc: 0.78
Batch: 320; loss: 4.57; acc: 0.78
Batch: 340; loss: 2.99; acc: 0.84
Batch: 360; loss: 2.84; acc: 0.81
Batch: 380; loss: 2.91; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.86
Batch: 420; loss: 1.76; acc: 0.81
Batch: 440; loss: 1.13; acc: 0.89
Batch: 460; loss: 2.34; acc: 0.86
Batch: 480; loss: 2.5; acc: 0.83
Batch: 500; loss: 1.2; acc: 0.88
Batch: 520; loss: 2.82; acc: 0.81
Batch: 540; loss: 1.27; acc: 0.81
Batch: 560; loss: 4.61; acc: 0.75
Batch: 580; loss: 1.76; acc: 0.86
Batch: 600; loss: 2.73; acc: 0.8
Batch: 620; loss: 3.35; acc: 0.75
Train Epoch over. train_loss: 2.86; train_accuracy: 0.81 

Batch: 0; loss: 3.0; acc: 0.78
Batch: 20; loss: 6.28; acc: 0.7
Batch: 40; loss: 3.69; acc: 0.81
Batch: 60; loss: 3.33; acc: 0.83
Batch: 80; loss: 2.92; acc: 0.8
Batch: 100; loss: 4.39; acc: 0.77
Batch: 120; loss: 3.11; acc: 0.81
Batch: 140; loss: 8.5; acc: 0.61
Val Epoch over. val_loss: 3.551629646948189; val_accuracy: 0.7752786624203821 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 2.97; acc: 0.77
Batch: 20; loss: 1.09; acc: 0.91
Batch: 40; loss: 4.05; acc: 0.8
Batch: 60; loss: 2.11; acc: 0.88
Batch: 80; loss: 3.41; acc: 0.81
Batch: 100; loss: 1.47; acc: 0.84
Batch: 120; loss: 3.29; acc: 0.78
Batch: 140; loss: 1.3; acc: 0.91
Batch: 160; loss: 2.45; acc: 0.84
Batch: 180; loss: 3.46; acc: 0.86
Batch: 200; loss: 4.66; acc: 0.73
Batch: 220; loss: 3.09; acc: 0.81
Batch: 240; loss: 1.24; acc: 0.88
Batch: 260; loss: 1.26; acc: 0.92
Batch: 280; loss: 3.33; acc: 0.91
Batch: 300; loss: 1.91; acc: 0.84
Batch: 320; loss: 2.59; acc: 0.8
Batch: 340; loss: 2.19; acc: 0.81
Batch: 360; loss: 4.47; acc: 0.83
Batch: 380; loss: 5.42; acc: 0.81
Batch: 400; loss: 3.84; acc: 0.75
Batch: 420; loss: 5.12; acc: 0.75
Batch: 440; loss: 3.27; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.83
Batch: 480; loss: 2.44; acc: 0.8
Batch: 500; loss: 3.5; acc: 0.75
Batch: 520; loss: 3.39; acc: 0.78
Batch: 540; loss: 5.03; acc: 0.77
Batch: 560; loss: 2.92; acc: 0.8
Batch: 580; loss: 0.93; acc: 0.91
Batch: 600; loss: 1.68; acc: 0.8
Batch: 620; loss: 4.97; acc: 0.73
Train Epoch over. train_loss: 2.89; train_accuracy: 0.81 

Batch: 0; loss: 3.35; acc: 0.83
Batch: 20; loss: 5.17; acc: 0.73
Batch: 40; loss: 2.32; acc: 0.88
Batch: 60; loss: 4.24; acc: 0.73
Batch: 80; loss: 3.91; acc: 0.75
Batch: 100; loss: 4.1; acc: 0.7
Batch: 120; loss: 2.65; acc: 0.83
Batch: 140; loss: 7.78; acc: 0.73
Val Epoch over. val_loss: 3.3337048290261797; val_accuracy: 0.7866242038216561 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 2.48; acc: 0.75
Batch: 20; loss: 2.09; acc: 0.81
Batch: 40; loss: 2.45; acc: 0.88
Batch: 60; loss: 3.88; acc: 0.77
Batch: 80; loss: 1.93; acc: 0.89
Batch: 100; loss: 1.9; acc: 0.83
Batch: 120; loss: 4.06; acc: 0.75
Batch: 140; loss: 2.95; acc: 0.83
Batch: 160; loss: 2.07; acc: 0.81
Batch: 180; loss: 2.85; acc: 0.77
Batch: 200; loss: 2.93; acc: 0.8
Batch: 220; loss: 3.17; acc: 0.83
Batch: 240; loss: 2.74; acc: 0.83
Batch: 260; loss: 0.95; acc: 0.88
Batch: 280; loss: 3.26; acc: 0.8
Batch: 300; loss: 4.89; acc: 0.78
Batch: 320; loss: 0.97; acc: 0.88
Batch: 340; loss: 3.94; acc: 0.78
Batch: 360; loss: 3.32; acc: 0.72
Batch: 380; loss: 1.39; acc: 0.84
Batch: 400; loss: 4.56; acc: 0.73
Batch: 420; loss: 2.91; acc: 0.84
Batch: 440; loss: 1.86; acc: 0.88
Batch: 460; loss: 4.22; acc: 0.72
Batch: 480; loss: 4.6; acc: 0.75
Batch: 500; loss: 3.52; acc: 0.78
Batch: 520; loss: 2.21; acc: 0.81
Batch: 540; loss: 2.99; acc: 0.86
Batch: 560; loss: 1.22; acc: 0.84
Batch: 580; loss: 3.06; acc: 0.81
Batch: 600; loss: 1.97; acc: 0.81
Batch: 620; loss: 3.85; acc: 0.75
Train Epoch over. train_loss: 2.82; train_accuracy: 0.81 

Batch: 0; loss: 1.3; acc: 0.88
Batch: 20; loss: 5.65; acc: 0.72
Batch: 40; loss: 1.95; acc: 0.86
Batch: 60; loss: 3.96; acc: 0.8
Batch: 80; loss: 3.59; acc: 0.77
Batch: 100; loss: 3.9; acc: 0.73
Batch: 120; loss: 3.11; acc: 0.88
Batch: 140; loss: 6.05; acc: 0.72
Val Epoch over. val_loss: 3.280314149750266; val_accuracy: 0.7916003184713376 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.96; acc: 0.78
Batch: 20; loss: 1.37; acc: 0.83
Batch: 40; loss: 2.42; acc: 0.8
Batch: 60; loss: 0.86; acc: 0.89
Batch: 80; loss: 1.22; acc: 0.89
Batch: 100; loss: 1.8; acc: 0.81
Batch: 120; loss: 1.38; acc: 0.89
Batch: 140; loss: 0.83; acc: 0.88
Batch: 160; loss: 1.96; acc: 0.83
Batch: 180; loss: 3.51; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.92
Batch: 220; loss: 3.16; acc: 0.86
Batch: 240; loss: 3.69; acc: 0.81
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 2.89; acc: 0.81
Batch: 300; loss: 1.48; acc: 0.92
Batch: 320; loss: 3.1; acc: 0.83
Batch: 340; loss: 1.94; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.92
Batch: 380; loss: 1.23; acc: 0.88
Batch: 400; loss: 1.68; acc: 0.84
Batch: 420; loss: 1.45; acc: 0.92
Batch: 440; loss: 2.13; acc: 0.83
Batch: 460; loss: 2.78; acc: 0.8
Batch: 480; loss: 1.14; acc: 0.92
Batch: 500; loss: 2.87; acc: 0.84
Batch: 520; loss: 1.7; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 1.88; acc: 0.86
Batch: 580; loss: 1.46; acc: 0.91
Batch: 600; loss: 1.28; acc: 0.88
Batch: 620; loss: 2.26; acc: 0.84
Train Epoch over. train_loss: 1.81; train_accuracy: 0.85 

Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 3.83; acc: 0.81
Batch: 40; loss: 1.92; acc: 0.84
Batch: 60; loss: 2.48; acc: 0.86
Batch: 80; loss: 1.07; acc: 0.86
Batch: 100; loss: 2.51; acc: 0.81
Batch: 120; loss: 2.24; acc: 0.84
Batch: 140; loss: 6.11; acc: 0.72
Val Epoch over. val_loss: 2.1634796621503343; val_accuracy: 0.8310111464968153 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.2; acc: 0.83
Batch: 20; loss: 1.99; acc: 0.83
Batch: 40; loss: 0.82; acc: 0.95
Batch: 60; loss: 2.1; acc: 0.84
Batch: 80; loss: 2.26; acc: 0.78
Batch: 100; loss: 1.0; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 1.52; acc: 0.81
Batch: 180; loss: 1.26; acc: 0.84
Batch: 200; loss: 1.36; acc: 0.89
Batch: 220; loss: 1.04; acc: 0.89
Batch: 240; loss: 2.51; acc: 0.8
Batch: 260; loss: 0.84; acc: 0.89
Batch: 280; loss: 1.79; acc: 0.86
Batch: 300; loss: 2.41; acc: 0.78
Batch: 320; loss: 2.43; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.78; acc: 0.89
Batch: 380; loss: 2.49; acc: 0.83
Batch: 400; loss: 1.61; acc: 0.86
Batch: 420; loss: 1.57; acc: 0.84
Batch: 440; loss: 1.39; acc: 0.88
Batch: 460; loss: 0.81; acc: 0.91
Batch: 480; loss: 2.61; acc: 0.83
Batch: 500; loss: 1.84; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.94
Batch: 540; loss: 0.6; acc: 0.94
Batch: 560; loss: 1.86; acc: 0.84
Batch: 580; loss: 2.0; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 1.21; acc: 0.86
Train Epoch over. train_loss: 1.56; train_accuracy: 0.86 

Batch: 0; loss: 0.73; acc: 0.89
Batch: 20; loss: 3.79; acc: 0.8
Batch: 40; loss: 2.09; acc: 0.86
Batch: 60; loss: 2.52; acc: 0.86
Batch: 80; loss: 1.2; acc: 0.83
Batch: 100; loss: 2.36; acc: 0.8
Batch: 120; loss: 2.17; acc: 0.84
Batch: 140; loss: 6.15; acc: 0.72
Val Epoch over. val_loss: 2.0803338484779283; val_accuracy: 0.8360867834394905 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.9; acc: 0.83
Batch: 20; loss: 1.16; acc: 0.88
Batch: 40; loss: 2.06; acc: 0.88
Batch: 60; loss: 2.12; acc: 0.84
Batch: 80; loss: 1.68; acc: 0.89
Batch: 100; loss: 2.08; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.89
Batch: 140; loss: 1.98; acc: 0.84
Batch: 160; loss: 2.3; acc: 0.89
Batch: 180; loss: 1.93; acc: 0.88
Batch: 200; loss: 1.24; acc: 0.88
Batch: 220; loss: 1.96; acc: 0.78
Batch: 240; loss: 1.07; acc: 0.84
Batch: 260; loss: 2.29; acc: 0.83
Batch: 280; loss: 1.59; acc: 0.84
Batch: 300; loss: 2.8; acc: 0.81
Batch: 320; loss: 2.05; acc: 0.86
Batch: 340; loss: 1.38; acc: 0.84
Batch: 360; loss: 1.66; acc: 0.86
Batch: 380; loss: 0.93; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.91
Batch: 420; loss: 1.38; acc: 0.84
Batch: 440; loss: 0.77; acc: 0.94
Batch: 460; loss: 1.54; acc: 0.92
Batch: 480; loss: 2.26; acc: 0.83
Batch: 500; loss: 1.15; acc: 0.92
Batch: 520; loss: 2.0; acc: 0.8
Batch: 540; loss: 1.59; acc: 0.81
Batch: 560; loss: 1.75; acc: 0.89
Batch: 580; loss: 0.86; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 1.48; train_accuracy: 0.86 

Batch: 0; loss: 0.79; acc: 0.89
Batch: 20; loss: 3.52; acc: 0.84
Batch: 40; loss: 1.73; acc: 0.88
Batch: 60; loss: 2.35; acc: 0.83
Batch: 80; loss: 1.13; acc: 0.86
Batch: 100; loss: 2.36; acc: 0.8
Batch: 120; loss: 2.43; acc: 0.84
Batch: 140; loss: 5.86; acc: 0.72
Val Epoch over. val_loss: 1.9969870282016742; val_accuracy: 0.8371815286624203 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.26; acc: 0.77
Batch: 20; loss: 1.89; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.89
Batch: 60; loss: 1.1; acc: 0.88
Batch: 80; loss: 0.95; acc: 0.92
Batch: 100; loss: 2.42; acc: 0.84
Batch: 120; loss: 1.04; acc: 0.88
Batch: 140; loss: 2.23; acc: 0.88
Batch: 160; loss: 1.0; acc: 0.92
Batch: 180; loss: 1.86; acc: 0.86
Batch: 200; loss: 1.38; acc: 0.88
Batch: 220; loss: 1.39; acc: 0.89
Batch: 240; loss: 1.62; acc: 0.89
Batch: 260; loss: 2.41; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.92
Batch: 300; loss: 1.66; acc: 0.84
Batch: 320; loss: 0.85; acc: 0.89
Batch: 340; loss: 1.27; acc: 0.86
Batch: 360; loss: 2.3; acc: 0.88
Batch: 380; loss: 1.22; acc: 0.89
Batch: 400; loss: 0.88; acc: 0.89
Batch: 420; loss: 1.25; acc: 0.84
Batch: 440; loss: 1.93; acc: 0.81
Batch: 460; loss: 0.98; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.88
Batch: 500; loss: 0.96; acc: 0.91
Batch: 520; loss: 2.13; acc: 0.83
Batch: 540; loss: 1.43; acc: 0.81
Batch: 560; loss: 1.18; acc: 0.91
Batch: 580; loss: 1.49; acc: 0.92
Batch: 600; loss: 2.53; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.89
Train Epoch over. train_loss: 1.45; train_accuracy: 0.86 

Batch: 0; loss: 0.82; acc: 0.88
Batch: 20; loss: 3.29; acc: 0.81
Batch: 40; loss: 1.86; acc: 0.86
Batch: 60; loss: 2.35; acc: 0.81
Batch: 80; loss: 0.98; acc: 0.86
Batch: 100; loss: 2.43; acc: 0.81
Batch: 120; loss: 2.24; acc: 0.84
Batch: 140; loss: 5.67; acc: 0.72
Val Epoch over. val_loss: 1.9707642448176244; val_accuracy: 0.8393710191082803 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.12; acc: 0.73
Batch: 20; loss: 2.69; acc: 0.83
Batch: 40; loss: 1.5; acc: 0.88
Batch: 60; loss: 1.64; acc: 0.83
Batch: 80; loss: 2.09; acc: 0.88
Batch: 100; loss: 0.95; acc: 0.91
Batch: 120; loss: 1.47; acc: 0.86
Batch: 140; loss: 2.53; acc: 0.8
Batch: 160; loss: 1.31; acc: 0.89
Batch: 180; loss: 2.71; acc: 0.77
Batch: 200; loss: 1.32; acc: 0.81
Batch: 220; loss: 2.22; acc: 0.86
Batch: 240; loss: 1.24; acc: 0.88
Batch: 260; loss: 2.53; acc: 0.8
Batch: 280; loss: 0.58; acc: 0.92
Batch: 300; loss: 3.03; acc: 0.8
Batch: 320; loss: 2.69; acc: 0.73
Batch: 340; loss: 1.93; acc: 0.86
Batch: 360; loss: 1.15; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 1.81; acc: 0.8
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 1.85; acc: 0.89
Batch: 460; loss: 1.81; acc: 0.91
Batch: 480; loss: 1.95; acc: 0.83
Batch: 500; loss: 1.75; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.86
Batch: 540; loss: 1.63; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.89
Batch: 580; loss: 1.51; acc: 0.81
Batch: 600; loss: 1.65; acc: 0.86
Batch: 620; loss: 2.29; acc: 0.75
Train Epoch over. train_loss: 1.43; train_accuracy: 0.86 

Batch: 0; loss: 0.84; acc: 0.91
Batch: 20; loss: 3.22; acc: 0.8
Batch: 40; loss: 1.78; acc: 0.88
Batch: 60; loss: 2.25; acc: 0.84
Batch: 80; loss: 1.02; acc: 0.89
Batch: 100; loss: 2.33; acc: 0.8
Batch: 120; loss: 2.49; acc: 0.83
Batch: 140; loss: 5.72; acc: 0.73
Val Epoch over. val_loss: 1.962379341482357; val_accuracy: 0.838077229299363 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 1.62; acc: 0.89
Batch: 40; loss: 1.6; acc: 0.84
Batch: 60; loss: 1.44; acc: 0.91
Batch: 80; loss: 0.67; acc: 0.94
Batch: 100; loss: 2.69; acc: 0.73
Batch: 120; loss: 0.75; acc: 0.88
Batch: 140; loss: 1.3; acc: 0.88
Batch: 160; loss: 1.05; acc: 0.86
Batch: 180; loss: 1.84; acc: 0.83
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 1.53; acc: 0.86
Batch: 240; loss: 1.14; acc: 0.81
Batch: 260; loss: 3.32; acc: 0.84
Batch: 280; loss: 1.77; acc: 0.83
Batch: 300; loss: 2.14; acc: 0.83
Batch: 320; loss: 1.14; acc: 0.88
Batch: 340; loss: 1.15; acc: 0.86
Batch: 360; loss: 1.53; acc: 0.89
Batch: 380; loss: 3.83; acc: 0.83
Batch: 400; loss: 2.07; acc: 0.91
Batch: 420; loss: 2.18; acc: 0.83
Batch: 440; loss: 0.9; acc: 0.86
Batch: 460; loss: 1.46; acc: 0.83
Batch: 480; loss: 1.35; acc: 0.84
Batch: 500; loss: 0.88; acc: 0.91
Batch: 520; loss: 2.65; acc: 0.81
Batch: 540; loss: 2.87; acc: 0.83
Batch: 560; loss: 0.73; acc: 0.88
Batch: 580; loss: 1.95; acc: 0.78
Batch: 600; loss: 1.37; acc: 0.89
Batch: 620; loss: 1.3; acc: 0.89
Train Epoch over. train_loss: 1.41; train_accuracy: 0.86 

Batch: 0; loss: 0.9; acc: 0.86
Batch: 20; loss: 3.11; acc: 0.81
Batch: 40; loss: 2.15; acc: 0.83
Batch: 60; loss: 2.41; acc: 0.83
Batch: 80; loss: 1.04; acc: 0.88
Batch: 100; loss: 2.53; acc: 0.8
Batch: 120; loss: 2.23; acc: 0.83
Batch: 140; loss: 5.7; acc: 0.72
Val Epoch over. val_loss: 1.9586165224670604; val_accuracy: 0.8317078025477707 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.84; acc: 0.83
Batch: 20; loss: 1.57; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.92; acc: 0.94
Batch: 80; loss: 3.05; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.91
Batch: 140; loss: 1.92; acc: 0.89
Batch: 160; loss: 1.16; acc: 0.88
Batch: 180; loss: 1.64; acc: 0.83
Batch: 200; loss: 1.24; acc: 0.91
Batch: 220; loss: 1.4; acc: 0.84
Batch: 240; loss: 2.38; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 1.0; acc: 0.91
Batch: 300; loss: 1.44; acc: 0.94
Batch: 320; loss: 1.01; acc: 0.86
Batch: 340; loss: 3.0; acc: 0.81
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 1.77; acc: 0.84
Batch: 400; loss: 1.46; acc: 0.88
Batch: 420; loss: 0.92; acc: 0.88
Batch: 440; loss: 0.81; acc: 0.88
Batch: 460; loss: 2.2; acc: 0.86
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 1.36; acc: 0.84
Batch: 520; loss: 1.8; acc: 0.83
Batch: 540; loss: 2.34; acc: 0.77
Batch: 560; loss: 2.64; acc: 0.8
Batch: 580; loss: 1.58; acc: 0.84
Batch: 600; loss: 1.03; acc: 0.89
Batch: 620; loss: 3.14; acc: 0.83
Train Epoch over. train_loss: 1.4; train_accuracy: 0.86 

Batch: 0; loss: 0.88; acc: 0.84
Batch: 20; loss: 3.26; acc: 0.78
Batch: 40; loss: 1.95; acc: 0.86
Batch: 60; loss: 2.29; acc: 0.84
Batch: 80; loss: 1.23; acc: 0.86
Batch: 100; loss: 2.48; acc: 0.8
Batch: 120; loss: 1.98; acc: 0.83
Batch: 140; loss: 5.58; acc: 0.72
Val Epoch over. val_loss: 1.925575658963744; val_accuracy: 0.8339968152866242 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.21; acc: 0.81
Batch: 20; loss: 2.29; acc: 0.86
Batch: 40; loss: 1.32; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.89
Batch: 80; loss: 2.04; acc: 0.86
Batch: 100; loss: 1.41; acc: 0.83
Batch: 120; loss: 1.55; acc: 0.86
Batch: 140; loss: 2.74; acc: 0.77
Batch: 160; loss: 1.7; acc: 0.86
Batch: 180; loss: 1.64; acc: 0.86
Batch: 200; loss: 1.1; acc: 0.92
Batch: 220; loss: 1.42; acc: 0.88
Batch: 240; loss: 1.63; acc: 0.89
Batch: 260; loss: 2.55; acc: 0.84
Batch: 280; loss: 1.02; acc: 0.86
Batch: 300; loss: 1.09; acc: 0.88
Batch: 320; loss: 2.45; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.92
Batch: 400; loss: 1.06; acc: 0.92
Batch: 420; loss: 0.97; acc: 0.86
Batch: 440; loss: 1.06; acc: 0.89
Batch: 460; loss: 0.79; acc: 0.91
Batch: 480; loss: 1.69; acc: 0.81
Batch: 500; loss: 1.38; acc: 0.89
Batch: 520; loss: 1.56; acc: 0.91
Batch: 540; loss: 1.16; acc: 0.84
Batch: 560; loss: 1.1; acc: 0.89
Batch: 580; loss: 2.77; acc: 0.88
Batch: 600; loss: 0.95; acc: 0.89
Batch: 620; loss: 0.99; acc: 0.89
Train Epoch over. train_loss: 1.39; train_accuracy: 0.86 

Batch: 0; loss: 0.94; acc: 0.86
Batch: 20; loss: 2.87; acc: 0.78
Batch: 40; loss: 1.93; acc: 0.84
Batch: 60; loss: 2.24; acc: 0.84
Batch: 80; loss: 1.36; acc: 0.86
Batch: 100; loss: 2.67; acc: 0.8
Batch: 120; loss: 2.19; acc: 0.83
Batch: 140; loss: 5.8; acc: 0.72
Val Epoch over. val_loss: 1.9278291505613145; val_accuracy: 0.8349920382165605 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.68; acc: 0.94
Batch: 20; loss: 2.03; acc: 0.84
Batch: 40; loss: 1.37; acc: 0.84
Batch: 60; loss: 2.22; acc: 0.84
Batch: 80; loss: 2.11; acc: 0.83
Batch: 100; loss: 1.24; acc: 0.88
Batch: 120; loss: 1.91; acc: 0.83
Batch: 140; loss: 1.01; acc: 0.91
Batch: 160; loss: 1.24; acc: 0.88
Batch: 180; loss: 0.97; acc: 0.86
Batch: 200; loss: 1.5; acc: 0.84
Batch: 220; loss: 0.85; acc: 0.89
Batch: 240; loss: 1.16; acc: 0.86
Batch: 260; loss: 0.99; acc: 0.84
Batch: 280; loss: 2.79; acc: 0.83
Batch: 300; loss: 0.87; acc: 0.88
Batch: 320; loss: 1.84; acc: 0.81
Batch: 340; loss: 3.02; acc: 0.77
Batch: 360; loss: 2.05; acc: 0.88
Batch: 380; loss: 1.52; acc: 0.78
Batch: 400; loss: 2.73; acc: 0.86
Batch: 420; loss: 2.18; acc: 0.81
Batch: 440; loss: 1.91; acc: 0.89
Batch: 460; loss: 1.35; acc: 0.86
Batch: 480; loss: 2.21; acc: 0.91
Batch: 500; loss: 2.28; acc: 0.81
Batch: 520; loss: 1.33; acc: 0.88
Batch: 540; loss: 1.15; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.78; acc: 0.89
Batch: 620; loss: 1.84; acc: 0.83
Train Epoch over. train_loss: 1.38; train_accuracy: 0.87 

Batch: 0; loss: 0.87; acc: 0.84
Batch: 20; loss: 2.86; acc: 0.81
Batch: 40; loss: 1.92; acc: 0.86
Batch: 60; loss: 2.17; acc: 0.86
Batch: 80; loss: 1.19; acc: 0.86
Batch: 100; loss: 2.58; acc: 0.8
Batch: 120; loss: 2.22; acc: 0.83
Batch: 140; loss: 5.72; acc: 0.73
Val Epoch over. val_loss: 1.9340275765224626; val_accuracy: 0.8332006369426752 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.89; acc: 0.8
Batch: 20; loss: 1.06; acc: 0.88
Batch: 40; loss: 1.52; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.92
Batch: 80; loss: 2.01; acc: 0.89
Batch: 100; loss: 1.08; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 1.31; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.95
Batch: 180; loss: 0.65; acc: 0.92
Batch: 200; loss: 1.3; acc: 0.92
Batch: 220; loss: 2.34; acc: 0.81
Batch: 240; loss: 1.86; acc: 0.84
Batch: 260; loss: 1.03; acc: 0.88
Batch: 280; loss: 1.09; acc: 0.83
Batch: 300; loss: 1.36; acc: 0.88
Batch: 320; loss: 1.37; acc: 0.78
Batch: 340; loss: 1.48; acc: 0.81
Batch: 360; loss: 1.96; acc: 0.84
Batch: 380; loss: 1.9; acc: 0.88
Batch: 400; loss: 0.74; acc: 0.89
Batch: 420; loss: 2.79; acc: 0.78
Batch: 440; loss: 1.96; acc: 0.84
Batch: 460; loss: 1.77; acc: 0.88
Batch: 480; loss: 1.7; acc: 0.83
Batch: 500; loss: 2.81; acc: 0.78
Batch: 520; loss: 1.99; acc: 0.81
Batch: 540; loss: 2.06; acc: 0.81
Batch: 560; loss: 2.07; acc: 0.84
Batch: 580; loss: 2.0; acc: 0.89
Batch: 600; loss: 1.0; acc: 0.88
Batch: 620; loss: 1.4; acc: 0.88
Train Epoch over. train_loss: 1.37; train_accuracy: 0.87 

Batch: 0; loss: 0.87; acc: 0.88
Batch: 20; loss: 3.12; acc: 0.8
Batch: 40; loss: 1.91; acc: 0.84
Batch: 60; loss: 2.48; acc: 0.8
Batch: 80; loss: 1.29; acc: 0.84
Batch: 100; loss: 2.36; acc: 0.83
Batch: 120; loss: 2.28; acc: 0.83
Batch: 140; loss: 5.81; acc: 0.7
Val Epoch over. val_loss: 1.923082423058285; val_accuracy: 0.8371815286624203 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.48; acc: 0.81
Batch: 20; loss: 2.52; acc: 0.83
Batch: 40; loss: 1.48; acc: 0.84
Batch: 60; loss: 1.42; acc: 0.86
Batch: 80; loss: 1.05; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.94
Batch: 120; loss: 1.3; acc: 0.81
Batch: 140; loss: 1.24; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.92
Batch: 180; loss: 1.38; acc: 0.88
Batch: 200; loss: 1.28; acc: 0.84
Batch: 220; loss: 1.13; acc: 0.92
Batch: 240; loss: 2.19; acc: 0.83
Batch: 260; loss: 1.24; acc: 0.86
Batch: 280; loss: 2.51; acc: 0.8
Batch: 300; loss: 1.67; acc: 0.86
Batch: 320; loss: 1.46; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.92
Batch: 360; loss: 1.31; acc: 0.92
Batch: 380; loss: 1.44; acc: 0.92
Batch: 400; loss: 2.38; acc: 0.78
Batch: 420; loss: 2.47; acc: 0.81
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 1.04; acc: 0.92
Batch: 480; loss: 1.26; acc: 0.88
Batch: 500; loss: 1.21; acc: 0.78
Batch: 520; loss: 0.93; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 1.68; acc: 0.83
Batch: 580; loss: 1.17; acc: 0.91
Batch: 600; loss: 0.67; acc: 0.91
Batch: 620; loss: 1.08; acc: 0.88
Train Epoch over. train_loss: 1.28; train_accuracy: 0.87 

Batch: 0; loss: 0.86; acc: 0.88
Batch: 20; loss: 3.05; acc: 0.8
Batch: 40; loss: 1.92; acc: 0.84
Batch: 60; loss: 2.33; acc: 0.83
Batch: 80; loss: 1.26; acc: 0.88
Batch: 100; loss: 2.41; acc: 0.81
Batch: 120; loss: 2.3; acc: 0.83
Batch: 140; loss: 5.78; acc: 0.7
Val Epoch over. val_loss: 1.9000306820413868; val_accuracy: 0.836484872611465 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.47; acc: 0.86
Batch: 20; loss: 1.14; acc: 0.84
Batch: 40; loss: 1.03; acc: 0.89
Batch: 60; loss: 1.08; acc: 0.88
Batch: 80; loss: 1.77; acc: 0.88
Batch: 100; loss: 1.12; acc: 0.91
Batch: 120; loss: 2.01; acc: 0.83
Batch: 140; loss: 2.09; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.8; acc: 0.88
Batch: 200; loss: 1.11; acc: 0.88
Batch: 220; loss: 0.88; acc: 0.92
Batch: 240; loss: 0.75; acc: 0.91
Batch: 260; loss: 2.08; acc: 0.89
Batch: 280; loss: 1.24; acc: 0.88
Batch: 300; loss: 1.12; acc: 0.91
Batch: 320; loss: 0.78; acc: 0.86
Batch: 340; loss: 1.75; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.88
Batch: 380; loss: 1.9; acc: 0.84
Batch: 400; loss: 1.58; acc: 0.78
Batch: 420; loss: 1.17; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 1.61; acc: 0.89
Batch: 480; loss: 0.7; acc: 0.89
Batch: 500; loss: 2.7; acc: 0.86
Batch: 520; loss: 1.71; acc: 0.78
Batch: 540; loss: 1.96; acc: 0.88
Batch: 560; loss: 2.41; acc: 0.84
Batch: 580; loss: 1.31; acc: 0.88
Batch: 600; loss: 0.84; acc: 0.91
Batch: 620; loss: 2.13; acc: 0.81
Train Epoch over. train_loss: 1.27; train_accuracy: 0.87 

Batch: 0; loss: 0.88; acc: 0.88
Batch: 20; loss: 2.99; acc: 0.8
Batch: 40; loss: 1.94; acc: 0.84
Batch: 60; loss: 2.24; acc: 0.83
Batch: 80; loss: 1.26; acc: 0.86
Batch: 100; loss: 2.46; acc: 0.83
Batch: 120; loss: 2.21; acc: 0.83
Batch: 140; loss: 5.74; acc: 0.7
Val Epoch over. val_loss: 1.89199445486828; val_accuracy: 0.8362858280254777 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.29; acc: 0.88
Batch: 20; loss: 1.12; acc: 0.86
Batch: 40; loss: 0.89; acc: 0.92
Batch: 60; loss: 1.04; acc: 0.83
Batch: 80; loss: 1.56; acc: 0.8
Batch: 100; loss: 2.08; acc: 0.84
Batch: 120; loss: 2.4; acc: 0.84
Batch: 140; loss: 2.91; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.91
Batch: 180; loss: 1.14; acc: 0.84
Batch: 200; loss: 0.81; acc: 0.91
Batch: 220; loss: 1.06; acc: 0.88
Batch: 240; loss: 1.71; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.94
Batch: 280; loss: 1.18; acc: 0.89
Batch: 300; loss: 1.44; acc: 0.86
Batch: 320; loss: 0.93; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 1.73; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.88
Batch: 400; loss: 0.86; acc: 0.91
Batch: 420; loss: 1.18; acc: 0.88
Batch: 440; loss: 0.88; acc: 0.91
Batch: 460; loss: 1.13; acc: 0.88
Batch: 480; loss: 1.0; acc: 0.86
Batch: 500; loss: 1.33; acc: 0.89
Batch: 520; loss: 1.04; acc: 0.91
Batch: 540; loss: 1.35; acc: 0.77
Batch: 560; loss: 1.8; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.91
Batch: 600; loss: 1.82; acc: 0.86
Batch: 620; loss: 1.67; acc: 0.86
Train Epoch over. train_loss: 1.27; train_accuracy: 0.87 

Batch: 0; loss: 0.89; acc: 0.86
Batch: 20; loss: 3.05; acc: 0.8
Batch: 40; loss: 1.91; acc: 0.84
Batch: 60; loss: 2.26; acc: 0.8
Batch: 80; loss: 1.25; acc: 0.86
Batch: 100; loss: 2.46; acc: 0.83
Batch: 120; loss: 2.26; acc: 0.83
Batch: 140; loss: 5.72; acc: 0.7
Val Epoch over. val_loss: 1.889594368684064; val_accuracy: 0.8362858280254777 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 1.19; acc: 0.83
Batch: 40; loss: 1.65; acc: 0.8
Batch: 60; loss: 0.51; acc: 0.94
Batch: 80; loss: 0.96; acc: 0.91
Batch: 100; loss: 1.27; acc: 0.88
Batch: 120; loss: 1.84; acc: 0.81
Batch: 140; loss: 1.34; acc: 0.83
Batch: 160; loss: 0.78; acc: 0.94
Batch: 180; loss: 0.94; acc: 0.89
Batch: 200; loss: 1.23; acc: 0.88
Batch: 220; loss: 0.82; acc: 0.86
Batch: 240; loss: 1.88; acc: 0.78
Batch: 260; loss: 1.12; acc: 0.92
Batch: 280; loss: 2.07; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.91
Batch: 320; loss: 1.89; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.91
Batch: 380; loss: 0.72; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 1.08; acc: 0.83
Batch: 440; loss: 1.07; acc: 0.88
Batch: 460; loss: 2.09; acc: 0.81
Batch: 480; loss: 2.32; acc: 0.8
Batch: 500; loss: 0.84; acc: 0.89
Batch: 520; loss: 0.89; acc: 0.88
Batch: 540; loss: 0.89; acc: 0.92
Batch: 560; loss: 1.37; acc: 0.88
Batch: 580; loss: 1.45; acc: 0.83
Batch: 600; loss: 1.58; acc: 0.83
Batch: 620; loss: 1.18; acc: 0.92
Train Epoch over. train_loss: 1.27; train_accuracy: 0.87 

Batch: 0; loss: 0.91; acc: 0.88
Batch: 20; loss: 3.03; acc: 0.8
Batch: 40; loss: 1.93; acc: 0.84
Batch: 60; loss: 2.25; acc: 0.8
Batch: 80; loss: 1.24; acc: 0.86
Batch: 100; loss: 2.49; acc: 0.83
Batch: 120; loss: 2.24; acc: 0.83
Batch: 140; loss: 5.69; acc: 0.7
Val Epoch over. val_loss: 1.8874555071637888; val_accuracy: 0.8359872611464968 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.84; acc: 0.84
Batch: 20; loss: 1.12; acc: 0.88
Batch: 40; loss: 1.54; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.94
Batch: 80; loss: 1.93; acc: 0.81
Batch: 100; loss: 1.31; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.86
Batch: 140; loss: 1.5; acc: 0.86
Batch: 160; loss: 0.76; acc: 0.92
Batch: 180; loss: 1.09; acc: 0.86
Batch: 200; loss: 1.32; acc: 0.89
Batch: 220; loss: 1.24; acc: 0.8
Batch: 240; loss: 0.99; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 1.64; acc: 0.88
Batch: 300; loss: 1.36; acc: 0.83
Batch: 320; loss: 0.75; acc: 0.84
Batch: 340; loss: 2.8; acc: 0.8
Batch: 360; loss: 1.88; acc: 0.91
Batch: 380; loss: 0.88; acc: 0.88
Batch: 400; loss: 0.96; acc: 0.91
Batch: 420; loss: 0.92; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 1.82; acc: 0.86
Batch: 480; loss: 2.21; acc: 0.83
Batch: 500; loss: 1.81; acc: 0.81
Batch: 520; loss: 1.35; acc: 0.91
Batch: 540; loss: 1.75; acc: 0.81
Batch: 560; loss: 1.98; acc: 0.86
Batch: 580; loss: 1.08; acc: 0.89
Batch: 600; loss: 1.44; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 1.26; train_accuracy: 0.87 

Batch: 0; loss: 0.91; acc: 0.88
Batch: 20; loss: 3.03; acc: 0.8
Batch: 40; loss: 1.96; acc: 0.83
Batch: 60; loss: 2.22; acc: 0.81
Batch: 80; loss: 1.26; acc: 0.88
Batch: 100; loss: 2.42; acc: 0.83
Batch: 120; loss: 2.23; acc: 0.83
Batch: 140; loss: 5.73; acc: 0.7
Val Epoch over. val_loss: 1.8842047906605301; val_accuracy: 0.8368829617834395 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.77; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.91
Batch: 40; loss: 0.75; acc: 0.91
Batch: 60; loss: 0.92; acc: 0.89
Batch: 80; loss: 1.57; acc: 0.78
Batch: 100; loss: 1.26; acc: 0.86
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 1.32; acc: 0.86
Batch: 160; loss: 1.47; acc: 0.83
Batch: 180; loss: 1.42; acc: 0.91
Batch: 200; loss: 0.81; acc: 0.89
Batch: 220; loss: 1.01; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 1.26; acc: 0.84
Batch: 280; loss: 2.18; acc: 0.83
Batch: 300; loss: 1.11; acc: 0.89
Batch: 320; loss: 1.48; acc: 0.78
Batch: 340; loss: 1.57; acc: 0.89
Batch: 360; loss: 1.27; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 1.21; acc: 0.81
Batch: 420; loss: 1.26; acc: 0.88
Batch: 440; loss: 0.77; acc: 0.92
Batch: 460; loss: 1.27; acc: 0.83
Batch: 480; loss: 0.72; acc: 0.89
Batch: 500; loss: 1.34; acc: 0.88
Batch: 520; loss: 1.16; acc: 0.91
Batch: 540; loss: 0.71; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.95
Batch: 580; loss: 1.17; acc: 0.83
Batch: 600; loss: 1.53; acc: 0.91
Batch: 620; loss: 1.27; acc: 0.83
Train Epoch over. train_loss: 1.26; train_accuracy: 0.87 

Batch: 0; loss: 0.93; acc: 0.88
Batch: 20; loss: 3.02; acc: 0.8
Batch: 40; loss: 1.93; acc: 0.83
Batch: 60; loss: 2.24; acc: 0.8
Batch: 80; loss: 1.27; acc: 0.86
Batch: 100; loss: 2.51; acc: 0.83
Batch: 120; loss: 2.27; acc: 0.83
Batch: 140; loss: 5.68; acc: 0.7
Val Epoch over. val_loss: 1.8874641689145641; val_accuracy: 0.8377786624203821 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.52; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 1.35; acc: 0.89
Batch: 60; loss: 1.54; acc: 0.83
Batch: 80; loss: 1.95; acc: 0.91
Batch: 100; loss: 2.11; acc: 0.83
Batch: 120; loss: 1.19; acc: 0.88
Batch: 140; loss: 1.57; acc: 0.88
Batch: 160; loss: 1.35; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 2.08; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.84; acc: 0.88
Batch: 280; loss: 1.15; acc: 0.86
Batch: 300; loss: 1.78; acc: 0.89
Batch: 320; loss: 1.28; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.84
Batch: 360; loss: 1.94; acc: 0.81
Batch: 380; loss: 2.46; acc: 0.86
Batch: 400; loss: 2.24; acc: 0.86
Batch: 420; loss: 0.88; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.95
Batch: 460; loss: 1.26; acc: 0.89
Batch: 480; loss: 1.73; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.92
Batch: 560; loss: 0.93; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.73; acc: 0.88
Train Epoch over. train_loss: 1.26; train_accuracy: 0.87 

Batch: 0; loss: 0.93; acc: 0.86
Batch: 20; loss: 2.98; acc: 0.8
Batch: 40; loss: 1.93; acc: 0.83
Batch: 60; loss: 2.2; acc: 0.81
Batch: 80; loss: 1.25; acc: 0.88
Batch: 100; loss: 2.51; acc: 0.83
Batch: 120; loss: 2.31; acc: 0.83
Batch: 140; loss: 5.67; acc: 0.7
Val Epoch over. val_loss: 1.8865263424101908; val_accuracy: 0.8369824840764332 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.51; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.95
Batch: 40; loss: 1.1; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.94
Batch: 80; loss: 0.66; acc: 0.94
Batch: 100; loss: 0.9; acc: 0.84
Batch: 120; loss: 2.12; acc: 0.86
Batch: 140; loss: 2.86; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.91
Batch: 180; loss: 1.62; acc: 0.91
Batch: 200; loss: 2.05; acc: 0.83
Batch: 220; loss: 2.0; acc: 0.81
Batch: 240; loss: 1.6; acc: 0.86
Batch: 260; loss: 2.2; acc: 0.83
Batch: 280; loss: 1.31; acc: 0.83
Batch: 300; loss: 1.38; acc: 0.88
Batch: 320; loss: 2.8; acc: 0.89
Batch: 340; loss: 0.89; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.85; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.91
Batch: 420; loss: 2.01; acc: 0.83
Batch: 440; loss: 0.7; acc: 0.91
Batch: 460; loss: 1.54; acc: 0.84
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 1.49; acc: 0.86
Batch: 520; loss: 1.63; acc: 0.84
Batch: 540; loss: 1.22; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 1.05; acc: 0.86
Batch: 600; loss: 1.78; acc: 0.84
Batch: 620; loss: 1.34; acc: 0.84
Train Epoch over. train_loss: 1.26; train_accuracy: 0.87 

Batch: 0; loss: 0.95; acc: 0.86
Batch: 20; loss: 2.98; acc: 0.8
Batch: 40; loss: 1.96; acc: 0.83
Batch: 60; loss: 2.22; acc: 0.8
Batch: 80; loss: 1.3; acc: 0.84
Batch: 100; loss: 2.53; acc: 0.83
Batch: 120; loss: 2.22; acc: 0.83
Batch: 140; loss: 5.68; acc: 0.7
Val Epoch over. val_loss: 1.8845163149059199; val_accuracy: 0.8366839171974523 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.82; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.89
Batch: 40; loss: 0.74; acc: 0.91
Batch: 60; loss: 0.98; acc: 0.86
Batch: 80; loss: 2.09; acc: 0.84
Batch: 100; loss: 1.19; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.86
Batch: 140; loss: 1.93; acc: 0.81
Batch: 160; loss: 1.19; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.94
Batch: 200; loss: 1.38; acc: 0.84
Batch: 220; loss: 0.82; acc: 0.88
Batch: 240; loss: 2.8; acc: 0.84
Batch: 260; loss: 3.08; acc: 0.84
Batch: 280; loss: 1.87; acc: 0.88
Batch: 300; loss: 1.1; acc: 0.89
Batch: 320; loss: 1.92; acc: 0.83
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 1.38; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.91; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 1.09; acc: 0.86
Batch: 520; loss: 1.03; acc: 0.86
Batch: 540; loss: 1.26; acc: 0.89
Batch: 560; loss: 2.38; acc: 0.84
Batch: 580; loss: 1.32; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.92
Batch: 620; loss: 1.16; acc: 0.88
Train Epoch over. train_loss: 1.26; train_accuracy: 0.87 

Batch: 0; loss: 0.94; acc: 0.86
Batch: 20; loss: 3.03; acc: 0.8
Batch: 40; loss: 1.96; acc: 0.83
Batch: 60; loss: 2.23; acc: 0.8
Batch: 80; loss: 1.29; acc: 0.86
Batch: 100; loss: 2.48; acc: 0.81
Batch: 120; loss: 2.24; acc: 0.83
Batch: 140; loss: 5.69; acc: 0.72
Val Epoch over. val_loss: 1.8848914223112119; val_accuracy: 0.8370820063694268 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.98; acc: 0.89
Batch: 20; loss: 1.27; acc: 0.78
Batch: 40; loss: 0.89; acc: 0.94
Batch: 60; loss: 0.94; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 1.13; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.86
Batch: 140; loss: 1.16; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.95
Batch: 180; loss: 0.79; acc: 0.92
Batch: 200; loss: 1.12; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 1.15; acc: 0.86
Batch: 260; loss: 1.31; acc: 0.89
Batch: 280; loss: 1.49; acc: 0.83
Batch: 300; loss: 0.95; acc: 0.91
Batch: 320; loss: 1.1; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 1.41; acc: 0.88
Batch: 380; loss: 1.06; acc: 0.94
Batch: 400; loss: 1.34; acc: 0.86
Batch: 420; loss: 1.29; acc: 0.84
Batch: 440; loss: 2.36; acc: 0.73
Batch: 460; loss: 1.21; acc: 0.89
Batch: 480; loss: 1.26; acc: 0.89
Batch: 500; loss: 1.64; acc: 0.8
Batch: 520; loss: 1.25; acc: 0.83
Batch: 540; loss: 1.18; acc: 0.81
Batch: 560; loss: 1.48; acc: 0.89
Batch: 580; loss: 1.57; acc: 0.88
Batch: 600; loss: 0.84; acc: 0.86
Batch: 620; loss: 1.72; acc: 0.84
Train Epoch over. train_loss: 1.26; train_accuracy: 0.87 

Batch: 0; loss: 0.94; acc: 0.86
Batch: 20; loss: 2.97; acc: 0.8
Batch: 40; loss: 1.94; acc: 0.83
Batch: 60; loss: 2.19; acc: 0.8
Batch: 80; loss: 1.28; acc: 0.88
Batch: 100; loss: 2.52; acc: 0.83
Batch: 120; loss: 2.3; acc: 0.83
Batch: 140; loss: 5.69; acc: 0.7
Val Epoch over. val_loss: 1.8852863030828488; val_accuracy: 0.8361863057324841 

plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_1000_lr_0.1_seed_1_epochs_30_batchsize_64
plots/subspace_training/MLP/2020-01-10 12:59:17/d_dim_XXXXX_lr_0.1_seed_1_epochs_30_batchsize_64
