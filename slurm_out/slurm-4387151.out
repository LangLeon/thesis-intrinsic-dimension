model : reg_lenet_3
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-20 16:50:48
nonzero elements in E: 449900
elements in E: 449900
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.33; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.32; acc: 0.05
Batch: 80; loss: 2.31; acc: 0.14
Batch: 100; loss: 2.32; acc: 0.02
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.32; acc: 0.08
Batch: 160; loss: 2.32; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.12
Batch: 220; loss: 2.32; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.31; acc: 0.09
Batch: 300; loss: 2.32; acc: 0.05
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.31; acc: 0.06
Batch: 400; loss: 2.29; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.31; acc: 0.05
Batch: 460; loss: 2.31; acc: 0.09
Batch: 480; loss: 2.29; acc: 0.03
Batch: 500; loss: 2.31; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.09
Batch: 540; loss: 2.3; acc: 0.03
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.29; acc: 0.12
Batch: 600; loss: 2.31; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.31; acc: 0.06
Batch: 680; loss: 2.29; acc: 0.19
Batch: 700; loss: 2.31; acc: 0.09
Batch: 720; loss: 2.29; acc: 0.12
Batch: 740; loss: 2.29; acc: 0.19
Batch: 760; loss: 2.29; acc: 0.12
Batch: 780; loss: 2.29; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.23
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.2
Batch: 120; loss: 2.29; acc: 0.2
Batch: 140; loss: 2.3; acc: 0.16
Val Epoch over. val_loss: 2.299354803789953; val_accuracy: 0.13405652866242038 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.2
Batch: 60; loss: 2.31; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.16
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.2
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.14
Batch: 380; loss: 2.29; acc: 0.11
Batch: 400; loss: 2.29; acc: 0.16
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.29; acc: 0.16
Batch: 460; loss: 2.28; acc: 0.11
Batch: 480; loss: 2.29; acc: 0.05
Batch: 500; loss: 2.29; acc: 0.09
Batch: 520; loss: 2.31; acc: 0.08
Batch: 540; loss: 2.29; acc: 0.17
Batch: 560; loss: 2.31; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.28; acc: 0.11
Batch: 620; loss: 2.29; acc: 0.12
Batch: 640; loss: 2.31; acc: 0.06
Batch: 660; loss: 2.28; acc: 0.09
Batch: 680; loss: 2.3; acc: 0.08
Batch: 700; loss: 2.3; acc: 0.12
Batch: 720; loss: 2.3; acc: 0.11
Batch: 740; loss: 2.28; acc: 0.11
Batch: 760; loss: 2.29; acc: 0.06
Batch: 780; loss: 2.31; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.12
Val Epoch over. val_loss: 2.294872110816324; val_accuracy: 0.10380175159235669 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.08
Batch: 60; loss: 2.28; acc: 0.19
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.16
Batch: 140; loss: 2.29; acc: 0.17
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.29; acc: 0.17
Batch: 220; loss: 2.3; acc: 0.06
Batch: 240; loss: 2.29; acc: 0.08
Batch: 260; loss: 2.28; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.19
Batch: 300; loss: 2.29; acc: 0.14
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.14
Batch: 400; loss: 2.28; acc: 0.14
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.29; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.14
Batch: 480; loss: 2.28; acc: 0.14
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.11
Batch: 540; loss: 2.29; acc: 0.16
Batch: 560; loss: 2.3; acc: 0.16
Batch: 580; loss: 2.28; acc: 0.17
Batch: 600; loss: 2.28; acc: 0.16
Batch: 620; loss: 2.29; acc: 0.09
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.16
Batch: 680; loss: 2.28; acc: 0.17
Batch: 700; loss: 2.3; acc: 0.16
Batch: 720; loss: 2.3; acc: 0.06
Batch: 740; loss: 2.28; acc: 0.14
Batch: 760; loss: 2.28; acc: 0.16
Batch: 780; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.29; train_accuracy: 0.12 

Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.29; acc: 0.17
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.28; acc: 0.27
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.12
Val Epoch over. val_loss: 2.29099056523317; val_accuracy: 0.13654458598726116 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.17
Batch: 20; loss: 2.32; acc: 0.09
Batch: 40; loss: 2.28; acc: 0.17
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.27; acc: 0.17
Batch: 200; loss: 2.29; acc: 0.09
Batch: 220; loss: 2.29; acc: 0.16
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.28; acc: 0.17
Batch: 340; loss: 2.29; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.29; acc: 0.19
Batch: 400; loss: 2.31; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.11
Batch: 440; loss: 2.28; acc: 0.19
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.03
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.29; acc: 0.12
Batch: 560; loss: 2.29; acc: 0.14
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.29; acc: 0.2
Batch: 620; loss: 2.27; acc: 0.17
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.3; acc: 0.17
Batch: 680; loss: 2.3; acc: 0.08
Batch: 700; loss: 2.26; acc: 0.19
Batch: 720; loss: 2.3; acc: 0.14
Batch: 740; loss: 2.3; acc: 0.14
Batch: 760; loss: 2.31; acc: 0.08
Batch: 780; loss: 2.29; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.14 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.29; acc: 0.17
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.23
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.19
Val Epoch over. val_loss: 2.2881630742625827; val_accuracy: 0.1445063694267516 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.27
Batch: 20; loss: 2.28; acc: 0.19
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.31; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.16
Batch: 140; loss: 2.28; acc: 0.22
Batch: 160; loss: 2.28; acc: 0.16
Batch: 180; loss: 2.29; acc: 0.2
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.16
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.28; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.19
Batch: 320; loss: 2.27; acc: 0.19
Batch: 340; loss: 2.29; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.16
Batch: 400; loss: 2.29; acc: 0.22
Batch: 420; loss: 2.27; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.14
Batch: 460; loss: 2.28; acc: 0.16
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.28; acc: 0.09
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.28; acc: 0.19
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.27; acc: 0.19
Batch: 600; loss: 2.29; acc: 0.12
Batch: 620; loss: 2.28; acc: 0.23
Batch: 640; loss: 2.28; acc: 0.16
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.29; acc: 0.09
Batch: 700; loss: 2.28; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.28; acc: 0.14
Batch: 760; loss: 2.28; acc: 0.2
Batch: 780; loss: 2.26; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.14 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.19
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.17
Batch: 80; loss: 2.26; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.09
Batch: 140; loss: 2.29; acc: 0.22
Val Epoch over. val_loss: 2.2847632936611295; val_accuracy: 0.1493829617834395 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.17
Batch: 40; loss: 2.28; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.19
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.28; acc: 0.16
Batch: 180; loss: 2.27; acc: 0.16
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.27; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.14
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.27; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.3; acc: 0.17
Batch: 460; loss: 2.31; acc: 0.08
Batch: 480; loss: 2.27; acc: 0.2
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.31; acc: 0.11
Batch: 540; loss: 2.29; acc: 0.16
Batch: 560; loss: 2.32; acc: 0.12
Batch: 580; loss: 2.28; acc: 0.2
Batch: 600; loss: 2.27; acc: 0.2
Batch: 620; loss: 2.26; acc: 0.19
Batch: 640; loss: 2.29; acc: 0.11
Batch: 660; loss: 2.29; acc: 0.19
Batch: 680; loss: 2.27; acc: 0.16
Batch: 700; loss: 2.26; acc: 0.2
Batch: 720; loss: 2.29; acc: 0.12
Batch: 740; loss: 2.29; acc: 0.19
Batch: 760; loss: 2.28; acc: 0.16
Batch: 780; loss: 2.29; acc: 0.17
Train Epoch over. train_loss: 2.28; train_accuracy: 0.15 

Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.19
Batch: 40; loss: 2.28; acc: 0.19
Batch: 60; loss: 2.27; acc: 0.19
Batch: 80; loss: 2.26; acc: 0.16
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.19
Val Epoch over. val_loss: 2.2813583452990103; val_accuracy: 0.16819267515923567 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.27; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.23
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.17
Batch: 160; loss: 2.29; acc: 0.19
Batch: 180; loss: 2.27; acc: 0.19
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.26; acc: 0.2
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.31; acc: 0.08
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.14
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.28; acc: 0.12
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.27; acc: 0.17
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.28; acc: 0.17
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.28; acc: 0.19
Batch: 480; loss: 2.27; acc: 0.22
Batch: 500; loss: 2.28; acc: 0.19
Batch: 520; loss: 2.25; acc: 0.25
Batch: 540; loss: 2.31; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.22
Batch: 580; loss: 2.28; acc: 0.12
Batch: 600; loss: 2.28; acc: 0.22
Batch: 620; loss: 2.27; acc: 0.19
Batch: 640; loss: 2.28; acc: 0.17
Batch: 660; loss: 2.27; acc: 0.16
Batch: 680; loss: 2.3; acc: 0.09
Batch: 700; loss: 2.27; acc: 0.28
Batch: 720; loss: 2.24; acc: 0.25
Batch: 740; loss: 2.26; acc: 0.2
Batch: 760; loss: 2.28; acc: 0.2
Batch: 780; loss: 2.26; acc: 0.17
Train Epoch over. train_loss: 2.28; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.19
Batch: 40; loss: 2.27; acc: 0.22
Batch: 60; loss: 2.26; acc: 0.2
Batch: 80; loss: 2.25; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.22
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.29; acc: 0.17
Val Epoch over. val_loss: 2.278002755657123; val_accuracy: 0.18789808917197454 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.22
Batch: 20; loss: 2.28; acc: 0.23
Batch: 40; loss: 2.26; acc: 0.25
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.28; acc: 0.17
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.31; acc: 0.14
Batch: 140; loss: 2.28; acc: 0.17
Batch: 160; loss: 2.29; acc: 0.2
Batch: 180; loss: 2.25; acc: 0.28
Batch: 200; loss: 2.28; acc: 0.14
Batch: 220; loss: 2.27; acc: 0.16
Batch: 240; loss: 2.27; acc: 0.23
Batch: 260; loss: 2.27; acc: 0.22
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.29; acc: 0.14
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.29; acc: 0.17
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.27; acc: 0.12
Batch: 400; loss: 2.26; acc: 0.14
Batch: 420; loss: 2.3; acc: 0.22
Batch: 440; loss: 2.31; acc: 0.14
Batch: 460; loss: 2.27; acc: 0.2
Batch: 480; loss: 2.29; acc: 0.19
Batch: 500; loss: 2.28; acc: 0.25
Batch: 520; loss: 2.29; acc: 0.16
Batch: 540; loss: 2.27; acc: 0.25
Batch: 560; loss: 2.25; acc: 0.17
Batch: 580; loss: 2.3; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.17
Batch: 620; loss: 2.27; acc: 0.19
Batch: 640; loss: 2.29; acc: 0.14
Batch: 660; loss: 2.32; acc: 0.19
Batch: 680; loss: 2.26; acc: 0.16
Batch: 700; loss: 2.29; acc: 0.23
Batch: 720; loss: 2.29; acc: 0.2
Batch: 740; loss: 2.27; acc: 0.14
Batch: 760; loss: 2.27; acc: 0.22
Batch: 780; loss: 2.28; acc: 0.19
Train Epoch over. train_loss: 2.28; train_accuracy: 0.19 

Batch: 0; loss: 2.26; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.22
Batch: 40; loss: 2.27; acc: 0.2
Batch: 60; loss: 2.26; acc: 0.17
Batch: 80; loss: 2.25; acc: 0.2
Batch: 100; loss: 2.29; acc: 0.22
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.16
Val Epoch over. val_loss: 2.274575821153677; val_accuracy: 0.1936703821656051 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.19
Batch: 40; loss: 2.29; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.24; acc: 0.28
Batch: 100; loss: 2.31; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.28
Batch: 140; loss: 2.27; acc: 0.23
Batch: 160; loss: 2.28; acc: 0.16
Batch: 180; loss: 2.26; acc: 0.23
Batch: 200; loss: 2.27; acc: 0.22
Batch: 220; loss: 2.26; acc: 0.25
Batch: 240; loss: 2.3; acc: 0.19
Batch: 260; loss: 2.26; acc: 0.2
Batch: 280; loss: 2.26; acc: 0.23
Batch: 300; loss: 2.27; acc: 0.12
Batch: 320; loss: 2.28; acc: 0.14
Batch: 340; loss: 2.28; acc: 0.2
Batch: 360; loss: 2.27; acc: 0.2
Batch: 380; loss: 2.3; acc: 0.17
Batch: 400; loss: 2.27; acc: 0.17
Batch: 420; loss: 2.26; acc: 0.14
Batch: 440; loss: 2.31; acc: 0.17
Batch: 460; loss: 2.26; acc: 0.2
Batch: 480; loss: 2.31; acc: 0.08
Batch: 500; loss: 2.28; acc: 0.27
Batch: 520; loss: 2.28; acc: 0.17
Batch: 540; loss: 2.31; acc: 0.12
Batch: 560; loss: 2.26; acc: 0.23
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.23
Batch: 620; loss: 2.25; acc: 0.2
Batch: 640; loss: 2.29; acc: 0.2
Batch: 660; loss: 2.31; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.2
Batch: 700; loss: 2.27; acc: 0.14
Batch: 720; loss: 2.26; acc: 0.12
Batch: 740; loss: 2.25; acc: 0.19
Batch: 760; loss: 2.31; acc: 0.14
Batch: 780; loss: 2.26; acc: 0.22
Train Epoch over. train_loss: 2.27; train_accuracy: 0.19 

Batch: 0; loss: 2.26; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.22
Batch: 40; loss: 2.26; acc: 0.17
Batch: 60; loss: 2.25; acc: 0.17
Batch: 80; loss: 2.25; acc: 0.19
Batch: 100; loss: 2.28; acc: 0.19
Batch: 120; loss: 2.25; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.19
Val Epoch over. val_loss: 2.268953528373864; val_accuracy: 0.17993630573248406 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.27
Batch: 40; loss: 2.26; acc: 0.25
Batch: 60; loss: 2.27; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.17
Batch: 100; loss: 2.28; acc: 0.11
Batch: 120; loss: 2.25; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.25; acc: 0.2
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.27; acc: 0.28
Batch: 220; loss: 2.25; acc: 0.25
Batch: 240; loss: 2.28; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.16
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.27; acc: 0.14
Batch: 320; loss: 2.27; acc: 0.17
Batch: 340; loss: 2.27; acc: 0.19
Batch: 360; loss: 2.25; acc: 0.22
Batch: 380; loss: 2.26; acc: 0.16
Batch: 400; loss: 2.25; acc: 0.23
Batch: 420; loss: 2.27; acc: 0.12
Batch: 440; loss: 2.23; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.23; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.14
Batch: 540; loss: 2.26; acc: 0.17
Batch: 560; loss: 2.23; acc: 0.31
Batch: 580; loss: 2.26; acc: 0.12
Batch: 600; loss: 2.27; acc: 0.16
Batch: 620; loss: 2.26; acc: 0.16
Batch: 640; loss: 2.28; acc: 0.16
Batch: 660; loss: 2.29; acc: 0.2
Batch: 680; loss: 2.21; acc: 0.2
Batch: 700; loss: 2.25; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.12
Batch: 740; loss: 2.22; acc: 0.2
Batch: 760; loss: 2.26; acc: 0.16
Batch: 780; loss: 2.28; acc: 0.08
Train Epoch over. train_loss: 2.26; train_accuracy: 0.17 

Batch: 0; loss: 2.24; acc: 0.16
Batch: 20; loss: 2.24; acc: 0.25
Batch: 40; loss: 2.24; acc: 0.12
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.24; acc: 0.16
Batch: 100; loss: 2.25; acc: 0.16
Batch: 120; loss: 2.23; acc: 0.22
Batch: 140; loss: 2.26; acc: 0.17
Val Epoch over. val_loss: 2.258751747714486; val_accuracy: 0.15216958598726116 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.27; acc: 0.11
Batch: 20; loss: 2.25; acc: 0.2
Batch: 40; loss: 2.24; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.16
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.24; acc: 0.19
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.27; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.26; acc: 0.17
Batch: 220; loss: 2.3; acc: 0.06
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.25; acc: 0.14
Batch: 280; loss: 2.2; acc: 0.17
Batch: 300; loss: 2.26; acc: 0.16
Batch: 320; loss: 2.2; acc: 0.25
Batch: 340; loss: 2.25; acc: 0.16
Batch: 360; loss: 2.26; acc: 0.14
Batch: 380; loss: 2.24; acc: 0.14
Batch: 400; loss: 2.26; acc: 0.09
Batch: 420; loss: 2.24; acc: 0.17
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.28; acc: 0.11
Batch: 480; loss: 2.25; acc: 0.16
Batch: 500; loss: 2.23; acc: 0.19
Batch: 520; loss: 2.22; acc: 0.25
Batch: 540; loss: 2.26; acc: 0.09
Batch: 560; loss: 2.24; acc: 0.14
Batch: 580; loss: 2.24; acc: 0.14
Batch: 600; loss: 2.28; acc: 0.09
Batch: 620; loss: 2.27; acc: 0.19
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.25; acc: 0.16
Batch: 680; loss: 2.22; acc: 0.19
Batch: 700; loss: 2.24; acc: 0.17
Batch: 720; loss: 2.25; acc: 0.17
Batch: 740; loss: 2.22; acc: 0.19
Batch: 760; loss: 2.26; acc: 0.16
Batch: 780; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.26; train_accuracy: 0.15 

Batch: 0; loss: 2.24; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.23
Batch: 40; loss: 2.23; acc: 0.12
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.17
Batch: 120; loss: 2.21; acc: 0.28
Batch: 140; loss: 2.26; acc: 0.12
Val Epoch over. val_loss: 2.254220989859028; val_accuracy: 0.14042595541401273 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.22; acc: 0.22
Batch: 40; loss: 2.25; acc: 0.14
Batch: 60; loss: 2.23; acc: 0.09
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.25; acc: 0.2
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.23; acc: 0.12
Batch: 160; loss: 2.26; acc: 0.11
Batch: 180; loss: 2.28; acc: 0.11
Batch: 200; loss: 2.24; acc: 0.17
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.25; acc: 0.14
Batch: 260; loss: 2.27; acc: 0.14
Batch: 280; loss: 2.28; acc: 0.08
Batch: 300; loss: 2.25; acc: 0.12
Batch: 320; loss: 2.23; acc: 0.17
Batch: 340; loss: 2.28; acc: 0.11
Batch: 360; loss: 2.29; acc: 0.12
Batch: 380; loss: 2.28; acc: 0.11
Batch: 400; loss: 2.23; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.16
Batch: 460; loss: 2.25; acc: 0.08
Batch: 480; loss: 2.28; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.05
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.22; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.2; acc: 0.14
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.24; acc: 0.12
Batch: 660; loss: 2.19; acc: 0.22
Batch: 680; loss: 2.23; acc: 0.16
Batch: 700; loss: 2.29; acc: 0.09
Batch: 720; loss: 2.24; acc: 0.2
Batch: 740; loss: 2.26; acc: 0.08
Batch: 760; loss: 2.2; acc: 0.2
Batch: 780; loss: 2.25; acc: 0.16
Train Epoch over. train_loss: 2.25; train_accuracy: 0.14 

Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.21; acc: 0.2
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.23; acc: 0.11
Batch: 100; loss: 2.23; acc: 0.16
Batch: 120; loss: 2.2; acc: 0.28
Batch: 140; loss: 2.25; acc: 0.11
Val Epoch over. val_loss: 2.2503029890121167; val_accuracy: 0.13306130573248406 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.25; acc: 0.06
Batch: 20; loss: 2.25; acc: 0.12
Batch: 40; loss: 2.26; acc: 0.14
Batch: 60; loss: 2.26; acc: 0.09
Batch: 80; loss: 2.23; acc: 0.16
Batch: 100; loss: 2.22; acc: 0.19
Batch: 120; loss: 2.24; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.12
Batch: 160; loss: 2.32; acc: 0.12
Batch: 180; loss: 2.28; acc: 0.14
Batch: 200; loss: 2.27; acc: 0.12
Batch: 220; loss: 2.27; acc: 0.11
Batch: 240; loss: 2.27; acc: 0.06
Batch: 260; loss: 2.24; acc: 0.16
Batch: 280; loss: 2.24; acc: 0.19
Batch: 300; loss: 2.26; acc: 0.14
Batch: 320; loss: 2.24; acc: 0.12
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.25; acc: 0.08
Batch: 380; loss: 2.24; acc: 0.2
Batch: 400; loss: 2.23; acc: 0.11
Batch: 420; loss: 2.22; acc: 0.22
Batch: 440; loss: 2.24; acc: 0.17
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.32; acc: 0.09
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.25; acc: 0.16
Batch: 540; loss: 2.23; acc: 0.16
Batch: 560; loss: 2.19; acc: 0.2
Batch: 580; loss: 2.19; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.09
Batch: 620; loss: 2.24; acc: 0.14
Batch: 640; loss: 2.26; acc: 0.09
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.23; acc: 0.19
Batch: 700; loss: 2.22; acc: 0.19
Batch: 720; loss: 2.24; acc: 0.11
Batch: 740; loss: 2.22; acc: 0.16
Batch: 760; loss: 2.25; acc: 0.11
Batch: 780; loss: 2.24; acc: 0.08
Train Epoch over. train_loss: 2.25; train_accuracy: 0.13 

Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.19; acc: 0.2
Batch: 40; loss: 2.22; acc: 0.14
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.23; acc: 0.09
Batch: 100; loss: 2.22; acc: 0.16
Batch: 120; loss: 2.2; acc: 0.22
Batch: 140; loss: 2.25; acc: 0.12
Val Epoch over. val_loss: 2.2469372734142716; val_accuracy: 0.12868232484076433 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.25; acc: 0.11
Batch: 40; loss: 2.25; acc: 0.09
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.28; acc: 0.09
Batch: 100; loss: 2.23; acc: 0.11
Batch: 120; loss: 2.24; acc: 0.09
Batch: 140; loss: 2.24; acc: 0.19
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.11
Batch: 200; loss: 2.15; acc: 0.23
Batch: 220; loss: 2.33; acc: 0.0
Batch: 240; loss: 2.25; acc: 0.16
Batch: 260; loss: 2.23; acc: 0.19
Batch: 280; loss: 2.25; acc: 0.08
Batch: 300; loss: 2.27; acc: 0.06
Batch: 320; loss: 2.26; acc: 0.12
Batch: 340; loss: 2.28; acc: 0.11
Batch: 360; loss: 2.24; acc: 0.14
Batch: 380; loss: 2.2; acc: 0.2
Batch: 400; loss: 2.22; acc: 0.17
Batch: 420; loss: 2.29; acc: 0.08
Batch: 440; loss: 2.32; acc: 0.06
Batch: 460; loss: 2.27; acc: 0.06
Batch: 480; loss: 2.23; acc: 0.12
Batch: 500; loss: 2.23; acc: 0.22
Batch: 520; loss: 2.25; acc: 0.12
Batch: 540; loss: 2.25; acc: 0.11
Batch: 560; loss: 2.25; acc: 0.11
Batch: 580; loss: 2.32; acc: 0.08
Batch: 600; loss: 2.26; acc: 0.08
Batch: 620; loss: 2.26; acc: 0.08
Batch: 640; loss: 2.25; acc: 0.16
Batch: 660; loss: 2.21; acc: 0.17
Batch: 680; loss: 2.2; acc: 0.14
Batch: 700; loss: 2.23; acc: 0.09
Batch: 720; loss: 2.22; acc: 0.17
Batch: 740; loss: 2.28; acc: 0.06
Batch: 760; loss: 2.21; acc: 0.16
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.13 

Batch: 0; loss: 2.22; acc: 0.16
Batch: 20; loss: 2.18; acc: 0.16
Batch: 40; loss: 2.21; acc: 0.14
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.23; acc: 0.09
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.25; acc: 0.12
Val Epoch over. val_loss: 2.2437469154406506; val_accuracy: 0.1272890127388535 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.27; acc: 0.11
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.11
Batch: 100; loss: 2.22; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.11
Batch: 140; loss: 2.18; acc: 0.16
Batch: 160; loss: 2.21; acc: 0.06
Batch: 180; loss: 2.28; acc: 0.08
Batch: 200; loss: 2.2; acc: 0.09
Batch: 220; loss: 2.25; acc: 0.11
Batch: 240; loss: 2.27; acc: 0.12
Batch: 260; loss: 2.24; acc: 0.14
Batch: 280; loss: 2.2; acc: 0.14
Batch: 300; loss: 2.24; acc: 0.16
Batch: 320; loss: 2.25; acc: 0.12
Batch: 340; loss: 2.23; acc: 0.17
Batch: 360; loss: 2.21; acc: 0.16
Batch: 380; loss: 2.26; acc: 0.08
Batch: 400; loss: 2.25; acc: 0.14
Batch: 420; loss: 2.26; acc: 0.17
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.09
Batch: 500; loss: 2.17; acc: 0.22
Batch: 520; loss: 2.23; acc: 0.19
Batch: 540; loss: 2.27; acc: 0.09
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.26; acc: 0.12
Batch: 600; loss: 2.21; acc: 0.11
Batch: 620; loss: 2.23; acc: 0.09
Batch: 640; loss: 2.27; acc: 0.08
Batch: 660; loss: 2.25; acc: 0.12
Batch: 680; loss: 2.21; acc: 0.19
Batch: 700; loss: 2.24; acc: 0.11
Batch: 720; loss: 2.25; acc: 0.14
Batch: 740; loss: 2.25; acc: 0.16
Batch: 760; loss: 2.23; acc: 0.09
Batch: 780; loss: 2.17; acc: 0.14
Train Epoch over. train_loss: 2.24; train_accuracy: 0.13 

Batch: 0; loss: 2.22; acc: 0.17
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.21; acc: 0.17
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.22; acc: 0.09
Batch: 100; loss: 2.2; acc: 0.19
Batch: 120; loss: 2.18; acc: 0.19
Batch: 140; loss: 2.24; acc: 0.12
Val Epoch over. val_loss: 2.23990915231644; val_accuracy: 0.12778662420382167 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.11
Batch: 20; loss: 2.23; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.08
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.25; acc: 0.16
Batch: 120; loss: 2.24; acc: 0.12
Batch: 140; loss: 2.23; acc: 0.11
Batch: 160; loss: 2.22; acc: 0.12
Batch: 180; loss: 2.19; acc: 0.09
Batch: 200; loss: 2.27; acc: 0.05
Batch: 220; loss: 2.21; acc: 0.12
Batch: 240; loss: 2.24; acc: 0.09
Batch: 260; loss: 2.2; acc: 0.16
Batch: 280; loss: 2.2; acc: 0.12
Batch: 300; loss: 2.22; acc: 0.17
Batch: 320; loss: 2.21; acc: 0.11
Batch: 340; loss: 2.21; acc: 0.22
Batch: 360; loss: 2.26; acc: 0.09
Batch: 380; loss: 2.23; acc: 0.09
Batch: 400; loss: 2.25; acc: 0.11
Batch: 420; loss: 2.28; acc: 0.12
Batch: 440; loss: 2.32; acc: 0.06
Batch: 460; loss: 2.24; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.19; acc: 0.14
Batch: 540; loss: 2.18; acc: 0.19
Batch: 560; loss: 2.34; acc: 0.08
Batch: 580; loss: 2.31; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.21; acc: 0.11
Batch: 640; loss: 2.23; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.08
Batch: 680; loss: 2.16; acc: 0.17
Batch: 700; loss: 2.21; acc: 0.19
Batch: 720; loss: 2.21; acc: 0.11
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.24; acc: 0.17
Batch: 780; loss: 2.26; acc: 0.16
Train Epoch over. train_loss: 2.24; train_accuracy: 0.13 

Batch: 0; loss: 2.21; acc: 0.2
Batch: 20; loss: 2.15; acc: 0.19
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.21; acc: 0.09
Batch: 100; loss: 2.19; acc: 0.19
Batch: 120; loss: 2.17; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.12
Val Epoch over. val_loss: 2.234801656880956; val_accuracy: 0.1275875796178344 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.21; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.14
Batch: 40; loss: 2.2; acc: 0.12
Batch: 60; loss: 2.18; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.08
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.19; acc: 0.2
Batch: 140; loss: 2.22; acc: 0.11
Batch: 160; loss: 2.21; acc: 0.17
Batch: 180; loss: 2.25; acc: 0.09
Batch: 200; loss: 2.18; acc: 0.14
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 2.27; acc: 0.11
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.23; acc: 0.09
Batch: 300; loss: 2.12; acc: 0.23
Batch: 320; loss: 2.27; acc: 0.09
Batch: 340; loss: 2.19; acc: 0.2
Batch: 360; loss: 2.27; acc: 0.11
Batch: 380; loss: 2.25; acc: 0.11
Batch: 400; loss: 2.29; acc: 0.14
Batch: 420; loss: 2.14; acc: 0.19
Batch: 440; loss: 2.25; acc: 0.2
Batch: 460; loss: 2.22; acc: 0.16
Batch: 480; loss: 2.24; acc: 0.14
Batch: 500; loss: 2.23; acc: 0.08
Batch: 520; loss: 2.35; acc: 0.08
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.22; acc: 0.11
Batch: 580; loss: 2.2; acc: 0.14
Batch: 600; loss: 2.2; acc: 0.17
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.33; acc: 0.05
Batch: 660; loss: 2.21; acc: 0.12
Batch: 680; loss: 2.27; acc: 0.11
Batch: 700; loss: 2.22; acc: 0.09
Batch: 720; loss: 2.17; acc: 0.17
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.24; acc: 0.17
Batch: 780; loss: 2.11; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.13 

Batch: 0; loss: 2.21; acc: 0.22
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.21; acc: 0.09
Batch: 100; loss: 2.18; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.12
Val Epoch over. val_loss: 2.2292349156300735; val_accuracy: 0.12997611464968153 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.25; acc: 0.08
Batch: 40; loss: 2.17; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.05
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.19; acc: 0.16
Batch: 140; loss: 2.24; acc: 0.08
Batch: 160; loss: 2.22; acc: 0.11
Batch: 180; loss: 2.22; acc: 0.14
Batch: 200; loss: 2.32; acc: 0.08
Batch: 220; loss: 2.25; acc: 0.11
Batch: 240; loss: 2.27; acc: 0.09
Batch: 260; loss: 2.23; acc: 0.14
Batch: 280; loss: 2.17; acc: 0.16
Batch: 300; loss: 2.27; acc: 0.14
Batch: 320; loss: 2.18; acc: 0.14
Batch: 340; loss: 2.26; acc: 0.12
Batch: 360; loss: 2.25; acc: 0.14
Batch: 380; loss: 2.23; acc: 0.16
Batch: 400; loss: 2.31; acc: 0.11
Batch: 420; loss: 2.23; acc: 0.11
Batch: 440; loss: 2.24; acc: 0.19
Batch: 460; loss: 2.25; acc: 0.14
Batch: 480; loss: 2.23; acc: 0.11
Batch: 500; loss: 2.13; acc: 0.19
Batch: 520; loss: 2.21; acc: 0.12
Batch: 540; loss: 2.19; acc: 0.19
Batch: 560; loss: 2.16; acc: 0.17
Batch: 580; loss: 2.23; acc: 0.14
Batch: 600; loss: 2.25; acc: 0.17
Batch: 620; loss: 2.16; acc: 0.2
Batch: 640; loss: 2.2; acc: 0.19
Batch: 660; loss: 2.2; acc: 0.19
Batch: 680; loss: 2.26; acc: 0.05
Batch: 700; loss: 2.2; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.17
Batch: 740; loss: 2.24; acc: 0.16
Batch: 760; loss: 2.15; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.19
Train Epoch over. train_loss: 2.23; train_accuracy: 0.14 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.2
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.12
Val Epoch over. val_loss: 2.2257188399126577; val_accuracy: 0.13793789808917198 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.19
Batch: 20; loss: 2.28; acc: 0.12
Batch: 40; loss: 2.21; acc: 0.17
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.12
Batch: 120; loss: 2.22; acc: 0.06
Batch: 140; loss: 2.21; acc: 0.16
Batch: 160; loss: 2.24; acc: 0.06
Batch: 180; loss: 2.22; acc: 0.08
Batch: 200; loss: 2.24; acc: 0.08
Batch: 220; loss: 2.16; acc: 0.11
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.09
Batch: 280; loss: 2.22; acc: 0.17
Batch: 300; loss: 2.19; acc: 0.2
Batch: 320; loss: 2.26; acc: 0.14
Batch: 340; loss: 2.24; acc: 0.12
Batch: 360; loss: 2.21; acc: 0.16
Batch: 380; loss: 2.19; acc: 0.16
Batch: 400; loss: 2.21; acc: 0.11
Batch: 420; loss: 2.16; acc: 0.17
Batch: 440; loss: 2.3; acc: 0.06
Batch: 460; loss: 2.24; acc: 0.08
Batch: 480; loss: 2.25; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.11
Batch: 520; loss: 2.17; acc: 0.19
Batch: 540; loss: 2.18; acc: 0.14
Batch: 560; loss: 2.26; acc: 0.14
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.29; acc: 0.09
Batch: 620; loss: 2.21; acc: 0.16
Batch: 640; loss: 2.27; acc: 0.12
Batch: 660; loss: 2.17; acc: 0.19
Batch: 680; loss: 2.28; acc: 0.08
Batch: 700; loss: 2.3; acc: 0.08
Batch: 720; loss: 2.26; acc: 0.09
Batch: 740; loss: 2.2; acc: 0.16
Batch: 760; loss: 2.23; acc: 0.17
Batch: 780; loss: 2.19; acc: 0.16
Train Epoch over. train_loss: 2.23; train_accuracy: 0.14 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.2
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.21; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.11
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.224355387839542; val_accuracy: 0.14470541401273884 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.21; acc: 0.19
Batch: 40; loss: 2.07; acc: 0.23
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.15; acc: 0.16
Batch: 120; loss: 2.26; acc: 0.16
Batch: 140; loss: 2.27; acc: 0.11
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.21; acc: 0.11
Batch: 200; loss: 2.25; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.2
Batch: 260; loss: 2.21; acc: 0.11
Batch: 280; loss: 2.21; acc: 0.11
Batch: 300; loss: 2.16; acc: 0.22
Batch: 320; loss: 2.21; acc: 0.12
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.17; acc: 0.17
Batch: 380; loss: 2.15; acc: 0.25
Batch: 400; loss: 2.21; acc: 0.12
Batch: 420; loss: 2.11; acc: 0.27
Batch: 440; loss: 2.2; acc: 0.14
Batch: 460; loss: 2.28; acc: 0.11
Batch: 480; loss: 2.28; acc: 0.06
Batch: 500; loss: 2.3; acc: 0.17
Batch: 520; loss: 2.19; acc: 0.17
Batch: 540; loss: 2.21; acc: 0.14
Batch: 560; loss: 2.24; acc: 0.11
Batch: 580; loss: 2.24; acc: 0.16
Batch: 600; loss: 2.15; acc: 0.19
Batch: 620; loss: 2.2; acc: 0.14
Batch: 640; loss: 2.24; acc: 0.11
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.22; acc: 0.12
Batch: 700; loss: 2.2; acc: 0.09
Batch: 720; loss: 2.2; acc: 0.09
Batch: 740; loss: 2.28; acc: 0.14
Batch: 760; loss: 2.16; acc: 0.17
Batch: 780; loss: 2.23; acc: 0.14
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.23
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.12
Val Epoch over. val_loss: 2.2238048945262934; val_accuracy: 0.14659633757961785 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.2; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.31; acc: 0.11
Batch: 80; loss: 2.22; acc: 0.16
Batch: 100; loss: 2.26; acc: 0.12
Batch: 120; loss: 2.22; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.17
Batch: 160; loss: 2.27; acc: 0.12
Batch: 180; loss: 2.22; acc: 0.16
Batch: 200; loss: 2.26; acc: 0.12
Batch: 220; loss: 2.11; acc: 0.23
Batch: 240; loss: 2.25; acc: 0.11
Batch: 260; loss: 2.27; acc: 0.11
Batch: 280; loss: 2.18; acc: 0.22
Batch: 300; loss: 2.26; acc: 0.09
Batch: 320; loss: 2.17; acc: 0.14
Batch: 340; loss: 2.24; acc: 0.09
Batch: 360; loss: 2.19; acc: 0.2
Batch: 380; loss: 2.19; acc: 0.19
Batch: 400; loss: 2.27; acc: 0.11
Batch: 420; loss: 2.16; acc: 0.2
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.12; acc: 0.27
Batch: 480; loss: 2.2; acc: 0.11
Batch: 500; loss: 2.22; acc: 0.12
Batch: 520; loss: 2.26; acc: 0.09
Batch: 540; loss: 2.25; acc: 0.14
Batch: 560; loss: 2.28; acc: 0.09
Batch: 580; loss: 2.25; acc: 0.11
Batch: 600; loss: 2.17; acc: 0.22
Batch: 620; loss: 2.26; acc: 0.11
Batch: 640; loss: 2.32; acc: 0.06
Batch: 660; loss: 2.15; acc: 0.22
Batch: 680; loss: 2.06; acc: 0.27
Batch: 700; loss: 2.33; acc: 0.09
Batch: 720; loss: 2.23; acc: 0.17
Batch: 740; loss: 2.29; acc: 0.09
Batch: 760; loss: 2.21; acc: 0.16
Batch: 780; loss: 2.25; acc: 0.14
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.12
Val Epoch over. val_loss: 2.2237183986955387; val_accuracy: 0.14769108280254778 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.25; acc: 0.14
Batch: 20; loss: 2.24; acc: 0.11
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.2; acc: 0.2
Batch: 80; loss: 2.22; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.12
Batch: 120; loss: 2.33; acc: 0.08
Batch: 140; loss: 2.22; acc: 0.11
Batch: 160; loss: 2.22; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.12
Batch: 200; loss: 2.21; acc: 0.14
Batch: 220; loss: 2.2; acc: 0.12
Batch: 240; loss: 2.18; acc: 0.19
Batch: 260; loss: 2.22; acc: 0.12
Batch: 280; loss: 2.25; acc: 0.08
Batch: 300; loss: 2.19; acc: 0.22
Batch: 320; loss: 2.21; acc: 0.19
Batch: 340; loss: 2.24; acc: 0.2
Batch: 360; loss: 2.32; acc: 0.09
Batch: 380; loss: 2.16; acc: 0.2
Batch: 400; loss: 2.2; acc: 0.06
Batch: 420; loss: 2.23; acc: 0.16
Batch: 440; loss: 2.23; acc: 0.23
Batch: 460; loss: 2.33; acc: 0.06
Batch: 480; loss: 2.31; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.29; acc: 0.14
Batch: 540; loss: 2.19; acc: 0.14
Batch: 560; loss: 2.25; acc: 0.12
Batch: 580; loss: 2.27; acc: 0.12
Batch: 600; loss: 2.24; acc: 0.12
Batch: 620; loss: 2.28; acc: 0.09
Batch: 640; loss: 2.25; acc: 0.09
Batch: 660; loss: 2.39; acc: 0.08
Batch: 680; loss: 2.24; acc: 0.11
Batch: 700; loss: 2.24; acc: 0.09
Batch: 720; loss: 2.23; acc: 0.11
Batch: 740; loss: 2.16; acc: 0.11
Batch: 760; loss: 2.21; acc: 0.17
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223673410476393; val_accuracy: 0.14898487261146498 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.28; acc: 0.17
Batch: 20; loss: 2.28; acc: 0.08
Batch: 40; loss: 2.26; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.27; acc: 0.11
Batch: 100; loss: 2.17; acc: 0.12
Batch: 120; loss: 2.21; acc: 0.17
Batch: 140; loss: 2.22; acc: 0.17
Batch: 160; loss: 2.25; acc: 0.08
Batch: 180; loss: 2.36; acc: 0.09
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.34; acc: 0.08
Batch: 240; loss: 2.18; acc: 0.17
Batch: 260; loss: 2.24; acc: 0.12
Batch: 280; loss: 2.17; acc: 0.25
Batch: 300; loss: 2.2; acc: 0.12
Batch: 320; loss: 2.17; acc: 0.19
Batch: 340; loss: 2.24; acc: 0.11
Batch: 360; loss: 2.17; acc: 0.2
Batch: 380; loss: 2.24; acc: 0.11
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.17; acc: 0.17
Batch: 440; loss: 2.25; acc: 0.17
Batch: 460; loss: 2.18; acc: 0.16
Batch: 480; loss: 2.22; acc: 0.12
Batch: 500; loss: 2.19; acc: 0.17
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.15; acc: 0.25
Batch: 560; loss: 2.23; acc: 0.17
Batch: 580; loss: 2.18; acc: 0.22
Batch: 600; loss: 2.24; acc: 0.22
Batch: 620; loss: 2.19; acc: 0.17
Batch: 640; loss: 2.23; acc: 0.16
Batch: 660; loss: 2.25; acc: 0.09
Batch: 680; loss: 2.2; acc: 0.19
Batch: 700; loss: 2.25; acc: 0.09
Batch: 720; loss: 2.26; acc: 0.11
Batch: 740; loss: 2.14; acc: 0.16
Batch: 760; loss: 2.28; acc: 0.12
Batch: 780; loss: 2.18; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223677440813393; val_accuracy: 0.1494824840764331 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.31; acc: 0.12
Batch: 20; loss: 2.25; acc: 0.19
Batch: 40; loss: 2.17; acc: 0.27
Batch: 60; loss: 2.18; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.11
Batch: 100; loss: 2.21; acc: 0.11
Batch: 120; loss: 2.13; acc: 0.19
Batch: 140; loss: 2.22; acc: 0.14
Batch: 160; loss: 2.24; acc: 0.19
Batch: 180; loss: 2.2; acc: 0.22
Batch: 200; loss: 2.19; acc: 0.14
Batch: 220; loss: 2.27; acc: 0.09
Batch: 240; loss: 2.24; acc: 0.14
Batch: 260; loss: 2.23; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.16; acc: 0.16
Batch: 320; loss: 2.21; acc: 0.17
Batch: 340; loss: 2.27; acc: 0.08
Batch: 360; loss: 2.22; acc: 0.14
Batch: 380; loss: 2.24; acc: 0.12
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.25; acc: 0.17
Batch: 440; loss: 2.21; acc: 0.14
Batch: 460; loss: 2.13; acc: 0.22
Batch: 480; loss: 2.24; acc: 0.14
Batch: 500; loss: 2.17; acc: 0.2
Batch: 520; loss: 2.16; acc: 0.2
Batch: 540; loss: 2.21; acc: 0.22
Batch: 560; loss: 2.15; acc: 0.17
Batch: 580; loss: 2.18; acc: 0.17
Batch: 600; loss: 2.19; acc: 0.17
Batch: 620; loss: 2.27; acc: 0.11
Batch: 640; loss: 2.21; acc: 0.11
Batch: 660; loss: 2.2; acc: 0.09
Batch: 680; loss: 2.22; acc: 0.14
Batch: 700; loss: 2.31; acc: 0.06
Batch: 720; loss: 2.21; acc: 0.14
Batch: 740; loss: 2.26; acc: 0.09
Batch: 760; loss: 2.14; acc: 0.23
Batch: 780; loss: 2.32; acc: 0.08
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223664131893474; val_accuracy: 0.1500796178343949 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 2.26; acc: 0.14
Batch: 60; loss: 2.23; acc: 0.12
Batch: 80; loss: 2.19; acc: 0.17
Batch: 100; loss: 2.24; acc: 0.17
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.15; acc: 0.22
Batch: 160; loss: 2.25; acc: 0.11
Batch: 180; loss: 2.21; acc: 0.14
Batch: 200; loss: 2.26; acc: 0.12
Batch: 220; loss: 2.22; acc: 0.12
Batch: 240; loss: 2.12; acc: 0.23
Batch: 260; loss: 2.22; acc: 0.19
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.32; acc: 0.08
Batch: 320; loss: 2.22; acc: 0.22
Batch: 340; loss: 2.21; acc: 0.16
Batch: 360; loss: 2.15; acc: 0.17
Batch: 380; loss: 2.22; acc: 0.17
Batch: 400; loss: 2.14; acc: 0.22
Batch: 420; loss: 2.26; acc: 0.19
Batch: 440; loss: 2.22; acc: 0.16
Batch: 460; loss: 2.24; acc: 0.09
Batch: 480; loss: 2.18; acc: 0.25
Batch: 500; loss: 2.34; acc: 0.09
Batch: 520; loss: 2.27; acc: 0.14
Batch: 540; loss: 2.21; acc: 0.17
Batch: 560; loss: 2.27; acc: 0.11
Batch: 580; loss: 2.09; acc: 0.25
Batch: 600; loss: 2.32; acc: 0.08
Batch: 620; loss: 2.16; acc: 0.19
Batch: 640; loss: 2.2; acc: 0.17
Batch: 660; loss: 2.27; acc: 0.11
Batch: 680; loss: 2.12; acc: 0.27
Batch: 700; loss: 2.16; acc: 0.11
Batch: 720; loss: 2.12; acc: 0.22
Batch: 740; loss: 2.26; acc: 0.11
Batch: 760; loss: 2.16; acc: 0.22
Batch: 780; loss: 2.12; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2236474425929367; val_accuracy: 0.15027866242038215 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.14; acc: 0.2
Batch: 20; loss: 2.2; acc: 0.17
Batch: 40; loss: 2.29; acc: 0.06
Batch: 60; loss: 2.08; acc: 0.23
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.2; acc: 0.16
Batch: 120; loss: 2.18; acc: 0.19
Batch: 140; loss: 2.25; acc: 0.12
Batch: 160; loss: 2.23; acc: 0.11
Batch: 180; loss: 2.27; acc: 0.06
Batch: 200; loss: 2.33; acc: 0.11
Batch: 220; loss: 2.17; acc: 0.19
Batch: 240; loss: 2.2; acc: 0.14
Batch: 260; loss: 2.23; acc: 0.12
Batch: 280; loss: 2.12; acc: 0.23
Batch: 300; loss: 2.17; acc: 0.19
Batch: 320; loss: 2.23; acc: 0.19
Batch: 340; loss: 2.16; acc: 0.17
Batch: 360; loss: 2.29; acc: 0.08
Batch: 380; loss: 2.17; acc: 0.17
Batch: 400; loss: 2.15; acc: 0.2
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.23; acc: 0.17
Batch: 460; loss: 2.34; acc: 0.05
Batch: 480; loss: 2.23; acc: 0.17
Batch: 500; loss: 2.23; acc: 0.17
Batch: 520; loss: 2.31; acc: 0.06
Batch: 540; loss: 2.27; acc: 0.14
Batch: 560; loss: 2.26; acc: 0.14
Batch: 580; loss: 2.23; acc: 0.11
Batch: 600; loss: 2.22; acc: 0.16
Batch: 620; loss: 2.2; acc: 0.17
Batch: 640; loss: 2.2; acc: 0.19
Batch: 660; loss: 2.22; acc: 0.14
Batch: 680; loss: 2.26; acc: 0.08
Batch: 700; loss: 2.2; acc: 0.2
Batch: 720; loss: 2.22; acc: 0.14
Batch: 740; loss: 2.2; acc: 0.14
Batch: 760; loss: 2.18; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.14
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2236510127972644; val_accuracy: 0.15027866242038215 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.14
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.17; acc: 0.23
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.17
Batch: 140; loss: 2.21; acc: 0.17
Batch: 160; loss: 2.16; acc: 0.19
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.13; acc: 0.28
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.24; acc: 0.09
Batch: 260; loss: 2.2; acc: 0.12
Batch: 280; loss: 2.23; acc: 0.14
Batch: 300; loss: 2.13; acc: 0.23
Batch: 320; loss: 2.32; acc: 0.08
Batch: 340; loss: 2.21; acc: 0.16
Batch: 360; loss: 2.19; acc: 0.19
Batch: 380; loss: 2.16; acc: 0.23
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.32; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.08
Batch: 460; loss: 2.2; acc: 0.14
Batch: 480; loss: 2.2; acc: 0.16
Batch: 500; loss: 2.28; acc: 0.14
Batch: 520; loss: 2.17; acc: 0.14
Batch: 540; loss: 2.17; acc: 0.12
Batch: 560; loss: 2.26; acc: 0.08
Batch: 580; loss: 2.28; acc: 0.11
Batch: 600; loss: 2.18; acc: 0.22
Batch: 620; loss: 2.17; acc: 0.17
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.09
Batch: 680; loss: 2.34; acc: 0.16
Batch: 700; loss: 2.22; acc: 0.19
Batch: 720; loss: 2.26; acc: 0.11
Batch: 740; loss: 2.23; acc: 0.19
Batch: 760; loss: 2.18; acc: 0.2
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2236765311781768; val_accuracy: 0.1503781847133758 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.18; acc: 0.19
Batch: 80; loss: 2.19; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.21; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.14
Batch: 180; loss: 2.18; acc: 0.2
Batch: 200; loss: 2.2; acc: 0.17
Batch: 220; loss: 2.25; acc: 0.12
Batch: 240; loss: 2.15; acc: 0.2
Batch: 260; loss: 2.21; acc: 0.14
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.23; acc: 0.11
Batch: 320; loss: 2.22; acc: 0.17
Batch: 340; loss: 2.32; acc: 0.03
Batch: 360; loss: 2.23; acc: 0.14
Batch: 380; loss: 2.26; acc: 0.16
Batch: 400; loss: 2.19; acc: 0.19
Batch: 420; loss: 2.14; acc: 0.19
Batch: 440; loss: 2.25; acc: 0.08
Batch: 460; loss: 2.15; acc: 0.12
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.2; acc: 0.16
Batch: 540; loss: 2.3; acc: 0.11
Batch: 560; loss: 2.24; acc: 0.16
Batch: 580; loss: 2.15; acc: 0.19
Batch: 600; loss: 2.31; acc: 0.06
Batch: 620; loss: 2.24; acc: 0.09
Batch: 640; loss: 2.34; acc: 0.06
Batch: 660; loss: 2.25; acc: 0.09
Batch: 680; loss: 2.19; acc: 0.11
Batch: 700; loss: 2.31; acc: 0.02
Batch: 720; loss: 2.24; acc: 0.16
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.12; acc: 0.23
Batch: 780; loss: 2.23; acc: 0.16
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2236665236722133; val_accuracy: 0.15087579617834396 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.23; acc: 0.12
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.11
Batch: 100; loss: 2.25; acc: 0.11
Batch: 120; loss: 2.13; acc: 0.25
Batch: 140; loss: 2.26; acc: 0.11
Batch: 160; loss: 2.17; acc: 0.2
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.25; acc: 0.22
Batch: 240; loss: 2.21; acc: 0.19
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.18; acc: 0.2
Batch: 300; loss: 2.24; acc: 0.17
Batch: 320; loss: 2.31; acc: 0.16
Batch: 340; loss: 2.27; acc: 0.08
Batch: 360; loss: 2.17; acc: 0.19
Batch: 380; loss: 2.22; acc: 0.14
Batch: 400; loss: 2.21; acc: 0.11
Batch: 420; loss: 2.28; acc: 0.17
Batch: 440; loss: 2.2; acc: 0.14
Batch: 460; loss: 2.27; acc: 0.12
Batch: 480; loss: 2.38; acc: 0.09
Batch: 500; loss: 2.13; acc: 0.23
Batch: 520; loss: 2.15; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.11
Batch: 560; loss: 2.23; acc: 0.09
Batch: 580; loss: 2.2; acc: 0.14
Batch: 600; loss: 2.22; acc: 0.09
Batch: 620; loss: 2.15; acc: 0.2
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.19; acc: 0.14
Batch: 700; loss: 2.2; acc: 0.12
Batch: 720; loss: 2.26; acc: 0.14
Batch: 740; loss: 2.25; acc: 0.14
Batch: 760; loss: 2.21; acc: 0.17
Batch: 780; loss: 2.23; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223733780490365; val_accuracy: 0.15087579617834396 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.27; acc: 0.08
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.14; acc: 0.25
Batch: 60; loss: 2.22; acc: 0.06
Batch: 80; loss: 2.21; acc: 0.16
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 2.22; acc: 0.19
Batch: 140; loss: 2.16; acc: 0.16
Batch: 160; loss: 2.23; acc: 0.2
Batch: 180; loss: 2.25; acc: 0.09
Batch: 200; loss: 2.28; acc: 0.12
Batch: 220; loss: 2.24; acc: 0.14
Batch: 240; loss: 2.2; acc: 0.17
Batch: 260; loss: 2.19; acc: 0.11
Batch: 280; loss: 2.24; acc: 0.16
Batch: 300; loss: 2.26; acc: 0.12
Batch: 320; loss: 2.25; acc: 0.16
Batch: 340; loss: 2.26; acc: 0.17
Batch: 360; loss: 2.21; acc: 0.14
Batch: 380; loss: 2.26; acc: 0.12
Batch: 400; loss: 2.17; acc: 0.16
Batch: 420; loss: 2.21; acc: 0.12
Batch: 440; loss: 2.19; acc: 0.2
Batch: 460; loss: 2.2; acc: 0.12
Batch: 480; loss: 2.18; acc: 0.2
Batch: 500; loss: 2.1; acc: 0.3
Batch: 520; loss: 2.28; acc: 0.08
Batch: 540; loss: 2.21; acc: 0.14
Batch: 560; loss: 2.21; acc: 0.16
Batch: 580; loss: 2.21; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.2
Batch: 620; loss: 2.24; acc: 0.14
Batch: 640; loss: 2.2; acc: 0.14
Batch: 660; loss: 2.16; acc: 0.17
Batch: 680; loss: 2.25; acc: 0.17
Batch: 700; loss: 2.14; acc: 0.19
Batch: 720; loss: 2.24; acc: 0.17
Batch: 740; loss: 2.16; acc: 0.19
Batch: 760; loss: 2.24; acc: 0.08
Batch: 780; loss: 2.16; acc: 0.22
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223802820132796; val_accuracy: 0.1519705414012739 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.09; acc: 0.23
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.24; acc: 0.14
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.2; acc: 0.19
Batch: 100; loss: 2.2; acc: 0.14
Batch: 120; loss: 2.17; acc: 0.2
Batch: 140; loss: 2.18; acc: 0.2
Batch: 160; loss: 2.22; acc: 0.09
Batch: 180; loss: 2.16; acc: 0.25
Batch: 200; loss: 2.23; acc: 0.17
Batch: 220; loss: 2.21; acc: 0.2
Batch: 240; loss: 2.31; acc: 0.11
Batch: 260; loss: 2.22; acc: 0.19
Batch: 280; loss: 2.23; acc: 0.17
Batch: 300; loss: 2.29; acc: 0.17
Batch: 320; loss: 2.23; acc: 0.17
Batch: 340; loss: 2.18; acc: 0.2
Batch: 360; loss: 2.22; acc: 0.14
Batch: 380; loss: 2.19; acc: 0.19
Batch: 400; loss: 2.27; acc: 0.12
Batch: 420; loss: 2.24; acc: 0.08
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.17; acc: 0.2
Batch: 520; loss: 2.19; acc: 0.17
Batch: 540; loss: 2.2; acc: 0.11
Batch: 560; loss: 2.22; acc: 0.16
Batch: 580; loss: 2.15; acc: 0.22
Batch: 600; loss: 2.26; acc: 0.17
Batch: 620; loss: 2.27; acc: 0.14
Batch: 640; loss: 2.31; acc: 0.11
Batch: 660; loss: 2.18; acc: 0.16
Batch: 680; loss: 2.24; acc: 0.19
Batch: 700; loss: 2.21; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.14
Batch: 740; loss: 2.28; acc: 0.12
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.19; acc: 0.12
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223776032210915; val_accuracy: 0.15187101910828024 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.19; acc: 0.12
Batch: 40; loss: 2.22; acc: 0.12
Batch: 60; loss: 2.33; acc: 0.09
Batch: 80; loss: 2.25; acc: 0.19
Batch: 100; loss: 2.23; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.31; acc: 0.09
Batch: 160; loss: 2.29; acc: 0.06
Batch: 180; loss: 2.24; acc: 0.12
Batch: 200; loss: 2.31; acc: 0.12
Batch: 220; loss: 2.31; acc: 0.06
Batch: 240; loss: 2.22; acc: 0.23
Batch: 260; loss: 2.16; acc: 0.16
Batch: 280; loss: 2.24; acc: 0.17
Batch: 300; loss: 2.23; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.12
Batch: 340; loss: 2.26; acc: 0.09
Batch: 360; loss: 2.24; acc: 0.16
Batch: 380; loss: 2.17; acc: 0.12
Batch: 400; loss: 2.22; acc: 0.14
Batch: 420; loss: 2.2; acc: 0.23
Batch: 440; loss: 2.15; acc: 0.19
Batch: 460; loss: 2.27; acc: 0.11
Batch: 480; loss: 2.24; acc: 0.08
Batch: 500; loss: 2.18; acc: 0.17
Batch: 520; loss: 2.13; acc: 0.2
Batch: 540; loss: 2.2; acc: 0.17
Batch: 560; loss: 2.21; acc: 0.19
Batch: 580; loss: 2.21; acc: 0.16
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.25; acc: 0.08
Batch: 640; loss: 2.21; acc: 0.17
Batch: 660; loss: 2.21; acc: 0.17
Batch: 680; loss: 2.14; acc: 0.23
Batch: 700; loss: 2.18; acc: 0.11
Batch: 720; loss: 2.3; acc: 0.14
Batch: 740; loss: 2.11; acc: 0.19
Batch: 760; loss: 2.31; acc: 0.08
Batch: 780; loss: 2.14; acc: 0.17
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237503285620623; val_accuracy: 0.151671974522293 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.16
Batch: 20; loss: 2.25; acc: 0.14
Batch: 40; loss: 2.22; acc: 0.19
Batch: 60; loss: 2.13; acc: 0.27
Batch: 80; loss: 2.22; acc: 0.12
Batch: 100; loss: 2.19; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.17; acc: 0.17
Batch: 160; loss: 2.27; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.12
Batch: 200; loss: 2.12; acc: 0.28
Batch: 220; loss: 2.13; acc: 0.27
Batch: 240; loss: 2.28; acc: 0.09
Batch: 260; loss: 2.16; acc: 0.17
Batch: 280; loss: 2.24; acc: 0.19
Batch: 300; loss: 2.17; acc: 0.16
Batch: 320; loss: 2.12; acc: 0.19
Batch: 340; loss: 2.32; acc: 0.08
Batch: 360; loss: 2.26; acc: 0.14
Batch: 380; loss: 2.26; acc: 0.11
Batch: 400; loss: 2.32; acc: 0.12
Batch: 420; loss: 2.18; acc: 0.22
Batch: 440; loss: 2.25; acc: 0.11
Batch: 460; loss: 2.14; acc: 0.19
Batch: 480; loss: 2.21; acc: 0.14
Batch: 500; loss: 2.19; acc: 0.17
Batch: 520; loss: 2.16; acc: 0.19
Batch: 540; loss: 2.23; acc: 0.17
Batch: 560; loss: 2.23; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.06
Batch: 600; loss: 2.18; acc: 0.11
Batch: 620; loss: 2.24; acc: 0.11
Batch: 640; loss: 2.31; acc: 0.08
Batch: 660; loss: 2.15; acc: 0.16
Batch: 680; loss: 2.25; acc: 0.09
Batch: 700; loss: 2.28; acc: 0.11
Batch: 720; loss: 2.26; acc: 0.11
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.19; acc: 0.16
Batch: 780; loss: 2.22; acc: 0.16
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223749107615963; val_accuracy: 0.151671974522293 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.2; acc: 0.17
Batch: 20; loss: 2.19; acc: 0.16
Batch: 40; loss: 2.22; acc: 0.17
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.31; acc: 0.06
Batch: 100; loss: 2.23; acc: 0.14
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.21; acc: 0.16
Batch: 160; loss: 2.19; acc: 0.19
Batch: 180; loss: 2.17; acc: 0.12
Batch: 200; loss: 2.15; acc: 0.2
Batch: 220; loss: 2.23; acc: 0.23
Batch: 240; loss: 2.21; acc: 0.2
Batch: 260; loss: 2.24; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.32; acc: 0.09
Batch: 320; loss: 2.2; acc: 0.12
Batch: 340; loss: 2.19; acc: 0.14
Batch: 360; loss: 2.18; acc: 0.19
Batch: 380; loss: 2.2; acc: 0.27
Batch: 400; loss: 2.18; acc: 0.17
Batch: 420; loss: 2.26; acc: 0.22
Batch: 440; loss: 2.29; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.16
Batch: 480; loss: 2.22; acc: 0.14
Batch: 500; loss: 2.2; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.16
Batch: 540; loss: 2.31; acc: 0.09
Batch: 560; loss: 2.27; acc: 0.09
Batch: 580; loss: 2.18; acc: 0.17
Batch: 600; loss: 2.19; acc: 0.17
Batch: 620; loss: 2.18; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.16
Batch: 660; loss: 2.18; acc: 0.16
Batch: 680; loss: 2.22; acc: 0.16
Batch: 700; loss: 2.15; acc: 0.2
Batch: 720; loss: 2.22; acc: 0.14
Batch: 740; loss: 2.24; acc: 0.16
Batch: 760; loss: 2.27; acc: 0.12
Batch: 780; loss: 2.18; acc: 0.14
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237726867578593; val_accuracy: 0.1520700636942675 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.21; acc: 0.16
Batch: 40; loss: 2.23; acc: 0.11
Batch: 60; loss: 2.27; acc: 0.16
Batch: 80; loss: 2.19; acc: 0.22
Batch: 100; loss: 2.2; acc: 0.14
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.16; acc: 0.12
Batch: 180; loss: 2.21; acc: 0.17
Batch: 200; loss: 2.15; acc: 0.12
Batch: 220; loss: 2.21; acc: 0.16
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.24; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.12
Batch: 300; loss: 2.31; acc: 0.09
Batch: 320; loss: 2.25; acc: 0.12
Batch: 340; loss: 2.14; acc: 0.22
Batch: 360; loss: 2.23; acc: 0.16
Batch: 380; loss: 2.25; acc: 0.14
Batch: 400; loss: 2.26; acc: 0.12
Batch: 420; loss: 2.19; acc: 0.2
Batch: 440; loss: 2.32; acc: 0.14
Batch: 460; loss: 2.21; acc: 0.23
Batch: 480; loss: 2.21; acc: 0.16
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.27; acc: 0.06
Batch: 540; loss: 2.1; acc: 0.2
Batch: 560; loss: 2.28; acc: 0.12
Batch: 580; loss: 2.16; acc: 0.12
Batch: 600; loss: 2.27; acc: 0.12
Batch: 620; loss: 2.14; acc: 0.17
Batch: 640; loss: 2.22; acc: 0.16
Batch: 660; loss: 2.23; acc: 0.2
Batch: 680; loss: 2.22; acc: 0.12
Batch: 700; loss: 2.19; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.23
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.16; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237742493866355; val_accuracy: 0.1520700636942675 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.12; acc: 0.25
Batch: 20; loss: 2.16; acc: 0.2
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.19
Batch: 80; loss: 2.25; acc: 0.16
Batch: 100; loss: 2.2; acc: 0.14
Batch: 120; loss: 2.1; acc: 0.19
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.18; acc: 0.17
Batch: 180; loss: 2.27; acc: 0.11
Batch: 200; loss: 2.24; acc: 0.17
Batch: 220; loss: 2.19; acc: 0.23
Batch: 240; loss: 2.13; acc: 0.2
Batch: 260; loss: 2.19; acc: 0.23
Batch: 280; loss: 2.17; acc: 0.17
Batch: 300; loss: 2.21; acc: 0.14
Batch: 320; loss: 2.32; acc: 0.08
Batch: 340; loss: 2.21; acc: 0.12
Batch: 360; loss: 2.22; acc: 0.09
Batch: 380; loss: 2.18; acc: 0.17
Batch: 400; loss: 2.12; acc: 0.2
Batch: 420; loss: 2.15; acc: 0.25
Batch: 440; loss: 2.19; acc: 0.2
Batch: 460; loss: 2.21; acc: 0.17
Batch: 480; loss: 2.16; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.16
Batch: 520; loss: 2.19; acc: 0.19
Batch: 540; loss: 2.13; acc: 0.17
Batch: 560; loss: 2.23; acc: 0.12
Batch: 580; loss: 2.17; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.12
Batch: 620; loss: 2.25; acc: 0.14
Batch: 640; loss: 2.23; acc: 0.14
Batch: 660; loss: 2.35; acc: 0.08
Batch: 680; loss: 2.24; acc: 0.16
Batch: 700; loss: 2.16; acc: 0.12
Batch: 720; loss: 2.24; acc: 0.14
Batch: 740; loss: 2.29; acc: 0.09
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.17; acc: 0.25
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223790937168583; val_accuracy: 0.15216958598726116 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.16; acc: 0.17
Batch: 20; loss: 2.31; acc: 0.05
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.14
Batch: 80; loss: 2.16; acc: 0.16
Batch: 100; loss: 2.28; acc: 0.11
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.22; acc: 0.14
Batch: 180; loss: 2.21; acc: 0.12
Batch: 200; loss: 2.19; acc: 0.19
Batch: 220; loss: 2.22; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.09
Batch: 260; loss: 2.25; acc: 0.11
Batch: 280; loss: 2.22; acc: 0.17
Batch: 300; loss: 2.24; acc: 0.14
Batch: 320; loss: 2.34; acc: 0.08
Batch: 340; loss: 2.12; acc: 0.25
Batch: 360; loss: 2.24; acc: 0.2
Batch: 380; loss: 2.27; acc: 0.11
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.25; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.14
Batch: 460; loss: 2.23; acc: 0.11
Batch: 480; loss: 2.25; acc: 0.14
Batch: 500; loss: 2.2; acc: 0.19
Batch: 520; loss: 2.22; acc: 0.12
Batch: 540; loss: 2.21; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.09
Batch: 580; loss: 2.25; acc: 0.12
Batch: 600; loss: 2.32; acc: 0.08
Batch: 620; loss: 2.22; acc: 0.14
Batch: 640; loss: 2.29; acc: 0.08
Batch: 660; loss: 2.15; acc: 0.17
Batch: 680; loss: 2.18; acc: 0.2
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.22; acc: 0.12
Batch: 740; loss: 2.25; acc: 0.09
Batch: 760; loss: 2.22; acc: 0.16
Batch: 780; loss: 2.31; acc: 0.08
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237911786243414; val_accuracy: 0.1520700636942675 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.33; acc: 0.12
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.28; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.16; acc: 0.25
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.19; acc: 0.17
Batch: 160; loss: 2.13; acc: 0.22
Batch: 180; loss: 2.16; acc: 0.2
Batch: 200; loss: 2.2; acc: 0.16
Batch: 220; loss: 2.18; acc: 0.17
Batch: 240; loss: 2.27; acc: 0.11
Batch: 260; loss: 2.25; acc: 0.09
Batch: 280; loss: 2.18; acc: 0.2
Batch: 300; loss: 2.15; acc: 0.2
Batch: 320; loss: 2.27; acc: 0.16
Batch: 340; loss: 2.26; acc: 0.17
Batch: 360; loss: 2.24; acc: 0.14
Batch: 380; loss: 2.14; acc: 0.23
Batch: 400; loss: 2.21; acc: 0.16
Batch: 420; loss: 2.23; acc: 0.16
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.28; acc: 0.09
Batch: 480; loss: 2.17; acc: 0.17
Batch: 500; loss: 2.16; acc: 0.16
Batch: 520; loss: 2.23; acc: 0.14
Batch: 540; loss: 2.2; acc: 0.19
Batch: 560; loss: 2.23; acc: 0.14
Batch: 580; loss: 2.3; acc: 0.12
Batch: 600; loss: 2.23; acc: 0.12
Batch: 620; loss: 2.28; acc: 0.11
Batch: 640; loss: 2.14; acc: 0.17
Batch: 660; loss: 2.28; acc: 0.14
Batch: 680; loss: 2.24; acc: 0.19
Batch: 700; loss: 2.16; acc: 0.17
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.21; acc: 0.12
Batch: 760; loss: 2.21; acc: 0.17
Batch: 780; loss: 2.25; acc: 0.11
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237780124518522; val_accuracy: 0.1520700636942675 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.12
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.15; acc: 0.22
Batch: 80; loss: 2.21; acc: 0.16
Batch: 100; loss: 2.24; acc: 0.14
Batch: 120; loss: 2.35; acc: 0.09
Batch: 140; loss: 2.21; acc: 0.16
Batch: 160; loss: 2.33; acc: 0.14
Batch: 180; loss: 2.22; acc: 0.16
Batch: 200; loss: 2.21; acc: 0.17
Batch: 220; loss: 2.21; acc: 0.16
Batch: 240; loss: 2.16; acc: 0.2
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.21; acc: 0.23
Batch: 300; loss: 2.19; acc: 0.22
Batch: 320; loss: 2.26; acc: 0.16
Batch: 340; loss: 2.22; acc: 0.22
Batch: 360; loss: 2.27; acc: 0.09
Batch: 380; loss: 2.21; acc: 0.12
Batch: 400; loss: 2.24; acc: 0.14
Batch: 420; loss: 2.16; acc: 0.2
Batch: 440; loss: 2.19; acc: 0.19
Batch: 460; loss: 2.19; acc: 0.17
Batch: 480; loss: 2.17; acc: 0.17
Batch: 500; loss: 2.21; acc: 0.14
Batch: 520; loss: 2.23; acc: 0.11
Batch: 540; loss: 2.29; acc: 0.17
Batch: 560; loss: 2.05; acc: 0.28
Batch: 580; loss: 2.22; acc: 0.17
Batch: 600; loss: 2.21; acc: 0.14
Batch: 620; loss: 2.11; acc: 0.2
Batch: 640; loss: 2.16; acc: 0.23
Batch: 660; loss: 2.23; acc: 0.17
Batch: 680; loss: 2.17; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.19
Batch: 720; loss: 2.28; acc: 0.14
Batch: 740; loss: 2.26; acc: 0.12
Batch: 760; loss: 2.22; acc: 0.12
Batch: 780; loss: 2.22; acc: 0.14
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223790745826284; val_accuracy: 0.1519705414012739 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.22; acc: 0.12
Batch: 20; loss: 2.28; acc: 0.09
Batch: 40; loss: 2.19; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.18; acc: 0.16
Batch: 100; loss: 2.23; acc: 0.11
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.2; acc: 0.12
Batch: 160; loss: 2.15; acc: 0.14
Batch: 180; loss: 2.27; acc: 0.12
Batch: 200; loss: 2.24; acc: 0.16
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.26; acc: 0.11
Batch: 260; loss: 2.23; acc: 0.16
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.24; acc: 0.19
Batch: 320; loss: 2.23; acc: 0.12
Batch: 340; loss: 2.22; acc: 0.12
Batch: 360; loss: 2.22; acc: 0.11
Batch: 380; loss: 2.17; acc: 0.17
Batch: 400; loss: 2.21; acc: 0.17
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.13; acc: 0.19
Batch: 460; loss: 2.26; acc: 0.11
Batch: 480; loss: 2.33; acc: 0.05
Batch: 500; loss: 2.24; acc: 0.09
Batch: 520; loss: 2.24; acc: 0.16
Batch: 540; loss: 2.28; acc: 0.08
Batch: 560; loss: 2.28; acc: 0.16
Batch: 580; loss: 2.18; acc: 0.16
Batch: 600; loss: 2.11; acc: 0.23
Batch: 620; loss: 2.26; acc: 0.16
Batch: 640; loss: 2.29; acc: 0.06
Batch: 660; loss: 2.32; acc: 0.03
Batch: 680; loss: 2.2; acc: 0.2
Batch: 700; loss: 2.14; acc: 0.27
Batch: 720; loss: 2.31; acc: 0.03
Batch: 740; loss: 2.23; acc: 0.16
Batch: 760; loss: 2.25; acc: 0.09
Batch: 780; loss: 2.14; acc: 0.23
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237955855715805; val_accuracy: 0.1520700636942675 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.11; acc: 0.23
Batch: 40; loss: 2.21; acc: 0.17
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.18; acc: 0.16
Batch: 100; loss: 2.21; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.26; acc: 0.14
Batch: 160; loss: 2.2; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.12
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.27; acc: 0.14
Batch: 240; loss: 2.1; acc: 0.19
Batch: 260; loss: 2.23; acc: 0.08
Batch: 280; loss: 2.23; acc: 0.14
Batch: 300; loss: 2.22; acc: 0.14
Batch: 320; loss: 2.22; acc: 0.19
Batch: 340; loss: 2.15; acc: 0.19
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.22; acc: 0.17
Batch: 400; loss: 2.24; acc: 0.16
Batch: 420; loss: 2.16; acc: 0.17
Batch: 440; loss: 2.27; acc: 0.09
Batch: 460; loss: 2.23; acc: 0.17
Batch: 480; loss: 2.15; acc: 0.2
Batch: 500; loss: 2.22; acc: 0.14
Batch: 520; loss: 2.24; acc: 0.09
Batch: 540; loss: 2.15; acc: 0.2
Batch: 560; loss: 2.17; acc: 0.2
Batch: 580; loss: 2.19; acc: 0.17
Batch: 600; loss: 2.19; acc: 0.12
Batch: 620; loss: 2.23; acc: 0.17
Batch: 640; loss: 2.1; acc: 0.22
Batch: 660; loss: 2.21; acc: 0.16
Batch: 680; loss: 2.25; acc: 0.11
Batch: 700; loss: 2.23; acc: 0.16
Batch: 720; loss: 2.32; acc: 0.06
Batch: 740; loss: 2.2; acc: 0.17
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.22; acc: 0.16
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237964010542366; val_accuracy: 0.15216958598726116 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.16
Batch: 40; loss: 2.35; acc: 0.06
Batch: 60; loss: 2.12; acc: 0.23
Batch: 80; loss: 2.26; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.14
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.08
Batch: 160; loss: 2.2; acc: 0.16
Batch: 180; loss: 2.13; acc: 0.22
Batch: 200; loss: 2.13; acc: 0.17
Batch: 220; loss: 2.2; acc: 0.12
Batch: 240; loss: 2.25; acc: 0.16
Batch: 260; loss: 2.23; acc: 0.12
Batch: 280; loss: 2.17; acc: 0.2
Batch: 300; loss: 2.29; acc: 0.17
Batch: 320; loss: 2.2; acc: 0.17
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.24; acc: 0.06
Batch: 380; loss: 2.19; acc: 0.17
Batch: 400; loss: 2.21; acc: 0.16
Batch: 420; loss: 2.22; acc: 0.14
Batch: 440; loss: 2.17; acc: 0.17
Batch: 460; loss: 2.29; acc: 0.14
Batch: 480; loss: 2.22; acc: 0.16
Batch: 500; loss: 2.31; acc: 0.05
Batch: 520; loss: 2.18; acc: 0.17
Batch: 540; loss: 2.18; acc: 0.2
Batch: 560; loss: 2.14; acc: 0.23
Batch: 580; loss: 2.22; acc: 0.14
Batch: 600; loss: 2.22; acc: 0.08
Batch: 620; loss: 2.21; acc: 0.11
Batch: 640; loss: 2.21; acc: 0.16
Batch: 660; loss: 2.22; acc: 0.22
Batch: 680; loss: 2.11; acc: 0.28
Batch: 700; loss: 2.32; acc: 0.11
Batch: 720; loss: 2.21; acc: 0.23
Batch: 740; loss: 2.24; acc: 0.17
Batch: 760; loss: 2.24; acc: 0.12
Batch: 780; loss: 2.13; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223797022157414; val_accuracy: 0.15216958598726116 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.28; acc: 0.11
Batch: 20; loss: 2.22; acc: 0.14
Batch: 40; loss: 2.2; acc: 0.2
Batch: 60; loss: 2.27; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.26; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.24; acc: 0.19
Batch: 160; loss: 2.19; acc: 0.2
Batch: 180; loss: 2.27; acc: 0.12
Batch: 200; loss: 2.32; acc: 0.11
Batch: 220; loss: 2.21; acc: 0.17
Batch: 240; loss: 2.24; acc: 0.17
Batch: 260; loss: 2.09; acc: 0.25
Batch: 280; loss: 2.21; acc: 0.17
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.28; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.27; acc: 0.09
Batch: 380; loss: 2.19; acc: 0.17
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.33; acc: 0.09
Batch: 440; loss: 2.23; acc: 0.08
Batch: 460; loss: 2.18; acc: 0.19
Batch: 480; loss: 2.28; acc: 0.08
Batch: 500; loss: 2.18; acc: 0.14
Batch: 520; loss: 2.27; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.34; acc: 0.09
Batch: 600; loss: 2.32; acc: 0.11
Batch: 620; loss: 2.2; acc: 0.22
Batch: 640; loss: 2.16; acc: 0.17
Batch: 660; loss: 2.31; acc: 0.12
Batch: 680; loss: 2.21; acc: 0.14
Batch: 700; loss: 2.23; acc: 0.19
Batch: 720; loss: 2.22; acc: 0.16
Batch: 740; loss: 2.23; acc: 0.14
Batch: 760; loss: 2.2; acc: 0.19
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237948323511016; val_accuracy: 0.1520700636942675 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.24; acc: 0.16
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 2.14; acc: 0.23
Batch: 60; loss: 2.25; acc: 0.09
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.12
Batch: 120; loss: 2.21; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.12
Batch: 160; loss: 2.23; acc: 0.16
Batch: 180; loss: 2.22; acc: 0.17
Batch: 200; loss: 2.13; acc: 0.2
Batch: 220; loss: 2.21; acc: 0.17
Batch: 240; loss: 2.17; acc: 0.19
Batch: 260; loss: 2.1; acc: 0.27
Batch: 280; loss: 2.22; acc: 0.08
Batch: 300; loss: 2.25; acc: 0.19
Batch: 320; loss: 2.27; acc: 0.09
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.12
Batch: 380; loss: 2.23; acc: 0.17
Batch: 400; loss: 2.19; acc: 0.22
Batch: 420; loss: 2.35; acc: 0.08
Batch: 440; loss: 2.28; acc: 0.11
Batch: 460; loss: 2.16; acc: 0.23
Batch: 480; loss: 2.19; acc: 0.14
Batch: 500; loss: 2.2; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.16
Batch: 540; loss: 2.2; acc: 0.19
Batch: 560; loss: 2.19; acc: 0.14
Batch: 580; loss: 2.23; acc: 0.14
Batch: 600; loss: 2.21; acc: 0.16
Batch: 620; loss: 2.11; acc: 0.27
Batch: 640; loss: 2.17; acc: 0.19
Batch: 660; loss: 2.19; acc: 0.14
Batch: 680; loss: 2.27; acc: 0.12
Batch: 700; loss: 2.2; acc: 0.11
Batch: 720; loss: 2.2; acc: 0.17
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.23; acc: 0.09
Batch: 780; loss: 2.22; acc: 0.16
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223796783738835; val_accuracy: 0.15216958598726116 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.27; acc: 0.17
Batch: 20; loss: 2.18; acc: 0.16
Batch: 40; loss: 2.15; acc: 0.22
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.16; acc: 0.19
Batch: 100; loss: 2.24; acc: 0.14
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.22; acc: 0.11
Batch: 160; loss: 2.18; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.14
Batch: 200; loss: 2.25; acc: 0.14
Batch: 220; loss: 2.26; acc: 0.14
Batch: 240; loss: 2.25; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.12; acc: 0.25
Batch: 300; loss: 2.23; acc: 0.19
Batch: 320; loss: 2.24; acc: 0.12
Batch: 340; loss: 2.26; acc: 0.12
Batch: 360; loss: 2.27; acc: 0.14
Batch: 380; loss: 2.28; acc: 0.09
Batch: 400; loss: 2.27; acc: 0.11
Batch: 420; loss: 2.11; acc: 0.17
Batch: 440; loss: 2.13; acc: 0.19
Batch: 460; loss: 2.27; acc: 0.14
Batch: 480; loss: 2.24; acc: 0.14
Batch: 500; loss: 2.28; acc: 0.09
Batch: 520; loss: 2.21; acc: 0.16
Batch: 540; loss: 2.27; acc: 0.09
Batch: 560; loss: 2.16; acc: 0.17
Batch: 580; loss: 2.29; acc: 0.14
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.24; acc: 0.17
Batch: 640; loss: 2.16; acc: 0.27
Batch: 660; loss: 2.29; acc: 0.09
Batch: 680; loss: 2.25; acc: 0.12
Batch: 700; loss: 2.22; acc: 0.17
Batch: 720; loss: 2.18; acc: 0.16
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.28; acc: 0.14
Batch: 780; loss: 2.25; acc: 0.12
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237964967253863; val_accuracy: 0.1520700636942675 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.17
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.26; acc: 0.09
Batch: 100; loss: 2.19; acc: 0.16
Batch: 120; loss: 2.25; acc: 0.16
Batch: 140; loss: 2.13; acc: 0.22
Batch: 160; loss: 2.14; acc: 0.2
Batch: 180; loss: 2.25; acc: 0.17
Batch: 200; loss: 2.21; acc: 0.2
Batch: 220; loss: 2.27; acc: 0.11
Batch: 240; loss: 2.27; acc: 0.12
Batch: 260; loss: 2.2; acc: 0.22
Batch: 280; loss: 2.32; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.16
Batch: 320; loss: 2.25; acc: 0.14
Batch: 340; loss: 2.27; acc: 0.12
Batch: 360; loss: 2.23; acc: 0.12
Batch: 380; loss: 2.22; acc: 0.17
Batch: 400; loss: 2.26; acc: 0.14
Batch: 420; loss: 2.2; acc: 0.12
Batch: 440; loss: 2.26; acc: 0.11
Batch: 460; loss: 2.14; acc: 0.27
Batch: 480; loss: 2.25; acc: 0.12
Batch: 500; loss: 2.18; acc: 0.23
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.28; acc: 0.14
Batch: 560; loss: 2.17; acc: 0.22
Batch: 580; loss: 2.13; acc: 0.2
Batch: 600; loss: 2.26; acc: 0.09
Batch: 620; loss: 2.19; acc: 0.12
Batch: 640; loss: 2.15; acc: 0.16
Batch: 660; loss: 2.3; acc: 0.09
Batch: 680; loss: 2.2; acc: 0.14
Batch: 700; loss: 2.14; acc: 0.22
Batch: 720; loss: 2.24; acc: 0.11
Batch: 740; loss: 2.15; acc: 0.23
Batch: 760; loss: 2.15; acc: 0.22
Batch: 780; loss: 2.2; acc: 0.19
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237974321766263; val_accuracy: 0.15216958598726116 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.2; acc: 0.14
Batch: 20; loss: 2.23; acc: 0.2
Batch: 40; loss: 2.19; acc: 0.16
Batch: 60; loss: 2.32; acc: 0.09
Batch: 80; loss: 2.18; acc: 0.16
Batch: 100; loss: 2.18; acc: 0.19
Batch: 120; loss: 2.25; acc: 0.09
Batch: 140; loss: 2.2; acc: 0.14
Batch: 160; loss: 2.16; acc: 0.22
Batch: 180; loss: 2.18; acc: 0.23
Batch: 200; loss: 2.17; acc: 0.2
Batch: 220; loss: 2.3; acc: 0.16
Batch: 240; loss: 2.16; acc: 0.22
Batch: 260; loss: 2.2; acc: 0.14
Batch: 280; loss: 2.22; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.12
Batch: 320; loss: 2.27; acc: 0.11
Batch: 340; loss: 2.11; acc: 0.27
Batch: 360; loss: 2.24; acc: 0.11
Batch: 380; loss: 2.18; acc: 0.16
Batch: 400; loss: 2.23; acc: 0.22
Batch: 420; loss: 2.21; acc: 0.19
Batch: 440; loss: 2.27; acc: 0.11
Batch: 460; loss: 2.17; acc: 0.2
Batch: 480; loss: 2.22; acc: 0.14
Batch: 500; loss: 2.24; acc: 0.09
Batch: 520; loss: 2.23; acc: 0.19
Batch: 540; loss: 2.28; acc: 0.09
Batch: 560; loss: 2.22; acc: 0.11
Batch: 580; loss: 2.33; acc: 0.05
Batch: 600; loss: 2.22; acc: 0.2
Batch: 620; loss: 2.29; acc: 0.09
Batch: 640; loss: 2.24; acc: 0.09
Batch: 660; loss: 2.24; acc: 0.17
Batch: 680; loss: 2.17; acc: 0.19
Batch: 700; loss: 2.14; acc: 0.23
Batch: 720; loss: 2.18; acc: 0.17
Batch: 740; loss: 2.21; acc: 0.09
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2237971694606125; val_accuracy: 0.1520700636942675 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.28; acc: 0.06
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.12; acc: 0.27
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.26; acc: 0.12
Batch: 100; loss: 2.25; acc: 0.08
Batch: 120; loss: 2.18; acc: 0.16
Batch: 140; loss: 2.21; acc: 0.14
Batch: 160; loss: 2.15; acc: 0.2
Batch: 180; loss: 2.19; acc: 0.22
Batch: 200; loss: 2.22; acc: 0.16
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.33; acc: 0.08
Batch: 260; loss: 2.3; acc: 0.05
Batch: 280; loss: 2.25; acc: 0.08
Batch: 300; loss: 2.24; acc: 0.16
Batch: 320; loss: 2.24; acc: 0.14
Batch: 340; loss: 2.27; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.25; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.17; acc: 0.2
Batch: 440; loss: 2.38; acc: 0.06
Batch: 460; loss: 2.2; acc: 0.17
Batch: 480; loss: 2.22; acc: 0.17
Batch: 500; loss: 2.26; acc: 0.12
Batch: 520; loss: 2.17; acc: 0.23
Batch: 540; loss: 2.24; acc: 0.19
Batch: 560; loss: 2.22; acc: 0.14
Batch: 580; loss: 2.15; acc: 0.17
Batch: 600; loss: 2.22; acc: 0.16
Batch: 620; loss: 2.25; acc: 0.09
Batch: 640; loss: 2.26; acc: 0.14
Batch: 660; loss: 2.18; acc: 0.19
Batch: 680; loss: 2.21; acc: 0.19
Batch: 700; loss: 2.21; acc: 0.19
Batch: 720; loss: 2.24; acc: 0.12
Batch: 740; loss: 2.2; acc: 0.12
Batch: 760; loss: 2.19; acc: 0.16
Batch: 780; loss: 2.18; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223800361536111; val_accuracy: 0.1520700636942675 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.19; acc: 0.2
Batch: 40; loss: 2.21; acc: 0.09
Batch: 60; loss: 2.23; acc: 0.17
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.26; acc: 0.11
Batch: 140; loss: 2.23; acc: 0.12
Batch: 160; loss: 2.29; acc: 0.08
Batch: 180; loss: 2.17; acc: 0.2
Batch: 200; loss: 2.25; acc: 0.12
Batch: 220; loss: 2.23; acc: 0.17
Batch: 240; loss: 2.21; acc: 0.17
Batch: 260; loss: 2.17; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.11
Batch: 300; loss: 2.31; acc: 0.14
Batch: 320; loss: 2.08; acc: 0.25
Batch: 340; loss: 2.28; acc: 0.11
Batch: 360; loss: 2.24; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.16
Batch: 400; loss: 2.27; acc: 0.11
Batch: 420; loss: 2.18; acc: 0.17
Batch: 440; loss: 2.16; acc: 0.2
Batch: 460; loss: 2.25; acc: 0.12
Batch: 480; loss: 2.17; acc: 0.22
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.14; acc: 0.17
Batch: 540; loss: 2.18; acc: 0.11
Batch: 560; loss: 2.16; acc: 0.2
Batch: 580; loss: 2.24; acc: 0.12
Batch: 600; loss: 2.21; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.16; acc: 0.23
Batch: 660; loss: 2.23; acc: 0.14
Batch: 680; loss: 2.28; acc: 0.11
Batch: 700; loss: 2.27; acc: 0.11
Batch: 720; loss: 2.25; acc: 0.17
Batch: 740; loss: 2.18; acc: 0.19
Batch: 760; loss: 2.22; acc: 0.14
Batch: 780; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.2238014306232428; val_accuracy: 0.1520700636942675 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.24; acc: 0.12
Batch: 20; loss: 2.21; acc: 0.2
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.21; acc: 0.16
Batch: 100; loss: 2.14; acc: 0.17
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.13; acc: 0.16
Batch: 160; loss: 2.25; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.08
Batch: 200; loss: 2.19; acc: 0.12
Batch: 220; loss: 2.26; acc: 0.16
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.2; acc: 0.2
Batch: 280; loss: 2.2; acc: 0.17
Batch: 300; loss: 2.19; acc: 0.22
Batch: 320; loss: 2.27; acc: 0.09
Batch: 340; loss: 2.2; acc: 0.12
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.21; acc: 0.17
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.21; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.08
Batch: 460; loss: 2.24; acc: 0.12
Batch: 480; loss: 2.18; acc: 0.14
Batch: 500; loss: 2.28; acc: 0.11
Batch: 520; loss: 2.28; acc: 0.17
Batch: 540; loss: 2.24; acc: 0.12
Batch: 560; loss: 2.2; acc: 0.12
Batch: 580; loss: 2.21; acc: 0.14
Batch: 600; loss: 2.18; acc: 0.14
Batch: 620; loss: 2.13; acc: 0.14
Batch: 640; loss: 2.32; acc: 0.06
Batch: 660; loss: 2.32; acc: 0.12
Batch: 680; loss: 2.25; acc: 0.11
Batch: 700; loss: 2.19; acc: 0.16
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.19; acc: 0.22
Batch: 760; loss: 2.2; acc: 0.11
Batch: 780; loss: 2.15; acc: 0.27
Train Epoch over. train_loss: 2.23; train_accuracy: 0.15 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.12; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Val Epoch over. val_loss: 2.223800988713647; val_accuracy: 0.1520700636942675 

plots/subspace_training/reg_lenet_3/2020-01-20 16:50:48/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 1124750
elements in E: 1124750
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.11
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.32; acc: 0.11
Batch: 60; loss: 2.31; acc: 0.11
Batch: 80; loss: 2.32; acc: 0.08
Batch: 100; loss: 2.32; acc: 0.11
Batch: 120; loss: 2.31; acc: 0.09
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.05
Batch: 240; loss: 2.31; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.14
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.06
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.31; acc: 0.05
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.12
Batch: 520; loss: 2.29; acc: 0.08
Batch: 540; loss: 2.31; acc: 0.11
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.31; acc: 0.05
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.3; acc: 0.11
Batch: 660; loss: 2.29; acc: 0.12
Batch: 680; loss: 2.3; acc: 0.09
Batch: 700; loss: 2.31; acc: 0.08
Batch: 720; loss: 2.3; acc: 0.12
Batch: 740; loss: 2.3; acc: 0.06
Batch: 760; loss: 2.27; acc: 0.12
Batch: 780; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.2963506568009686; val_accuracy: 0.09643710191082802 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.31; acc: 0.06
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.29; acc: 0.09
Batch: 240; loss: 2.29; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.29; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.14
Batch: 380; loss: 2.3; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.06
Batch: 420; loss: 2.27; acc: 0.25
Batch: 440; loss: 2.3; acc: 0.08
Batch: 460; loss: 2.29; acc: 0.14
Batch: 480; loss: 2.29; acc: 0.19
Batch: 500; loss: 2.29; acc: 0.12
Batch: 520; loss: 2.29; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.19
Batch: 560; loss: 2.29; acc: 0.2
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.28; acc: 0.2
Batch: 640; loss: 2.29; acc: 0.16
Batch: 660; loss: 2.28; acc: 0.22
Batch: 680; loss: 2.29; acc: 0.17
Batch: 700; loss: 2.3; acc: 0.17
Batch: 720; loss: 2.3; acc: 0.09
Batch: 740; loss: 2.29; acc: 0.17
Batch: 760; loss: 2.28; acc: 0.2
Batch: 780; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.29; train_accuracy: 0.14 

Batch: 0; loss: 2.29; acc: 0.17
Batch: 20; loss: 2.28; acc: 0.25
Batch: 40; loss: 2.28; acc: 0.2
Batch: 60; loss: 2.28; acc: 0.2
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.27; acc: 0.28
Batch: 120; loss: 2.27; acc: 0.25
Batch: 140; loss: 2.29; acc: 0.17
Val Epoch over. val_loss: 2.2875116691467867; val_accuracy: 0.1702826433121019 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.29; acc: 0.19
Batch: 40; loss: 2.28; acc: 0.17
Batch: 60; loss: 2.29; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.09
Batch: 160; loss: 2.27; acc: 0.2
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.27; acc: 0.17
Batch: 220; loss: 2.27; acc: 0.23
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.28; acc: 0.17
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.06
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.27; acc: 0.19
Batch: 440; loss: 2.27; acc: 0.09
Batch: 460; loss: 2.26; acc: 0.2
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.28; acc: 0.12
Batch: 520; loss: 2.3; acc: 0.11
Batch: 540; loss: 2.28; acc: 0.08
Batch: 560; loss: 2.27; acc: 0.23
Batch: 580; loss: 2.26; acc: 0.23
Batch: 600; loss: 2.26; acc: 0.23
Batch: 620; loss: 2.29; acc: 0.09
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.26; acc: 0.22
Batch: 700; loss: 2.26; acc: 0.23
Batch: 720; loss: 2.27; acc: 0.19
Batch: 740; loss: 2.27; acc: 0.27
Batch: 760; loss: 2.27; acc: 0.23
Batch: 780; loss: 2.25; acc: 0.23
Train Epoch over. train_loss: 2.28; train_accuracy: 0.16 

Batch: 0; loss: 2.27; acc: 0.23
Batch: 20; loss: 2.26; acc: 0.23
Batch: 40; loss: 2.25; acc: 0.27
Batch: 60; loss: 2.25; acc: 0.28
Batch: 80; loss: 2.25; acc: 0.27
Batch: 100; loss: 2.25; acc: 0.22
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.28; acc: 0.16
Val Epoch over. val_loss: 2.264733291735315; val_accuracy: 0.20630971337579618 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.23
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.16
Batch: 60; loss: 2.27; acc: 0.2
Batch: 80; loss: 2.27; acc: 0.2
Batch: 100; loss: 2.26; acc: 0.2
Batch: 120; loss: 2.24; acc: 0.22
Batch: 140; loss: 2.26; acc: 0.2
Batch: 160; loss: 2.26; acc: 0.25
Batch: 180; loss: 2.27; acc: 0.12
Batch: 200; loss: 2.24; acc: 0.3
Batch: 220; loss: 2.25; acc: 0.12
Batch: 240; loss: 2.26; acc: 0.17
Batch: 260; loss: 2.24; acc: 0.19
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.26; acc: 0.14
Batch: 320; loss: 2.24; acc: 0.12
Batch: 340; loss: 2.24; acc: 0.14
Batch: 360; loss: 2.27; acc: 0.08
Batch: 380; loss: 2.23; acc: 0.09
Batch: 400; loss: 2.22; acc: 0.19
Batch: 420; loss: 2.24; acc: 0.08
Batch: 440; loss: 2.26; acc: 0.11
Batch: 460; loss: 2.2; acc: 0.2
Batch: 480; loss: 2.19; acc: 0.17
Batch: 500; loss: 2.24; acc: 0.09
Batch: 520; loss: 2.26; acc: 0.11
Batch: 540; loss: 2.22; acc: 0.14
Batch: 560; loss: 2.26; acc: 0.09
Batch: 580; loss: 2.18; acc: 0.2
Batch: 600; loss: 2.21; acc: 0.17
Batch: 620; loss: 2.2; acc: 0.12
Batch: 640; loss: 2.23; acc: 0.11
Batch: 660; loss: 2.21; acc: 0.16
Batch: 680; loss: 2.24; acc: 0.11
Batch: 700; loss: 2.24; acc: 0.19
Batch: 720; loss: 2.21; acc: 0.19
Batch: 740; loss: 2.22; acc: 0.19
Batch: 760; loss: 2.25; acc: 0.03
Batch: 780; loss: 2.26; acc: 0.09
Train Epoch over. train_loss: 2.24; train_accuracy: 0.16 

Batch: 0; loss: 2.23; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.08
Batch: 40; loss: 2.19; acc: 0.16
Batch: 60; loss: 2.2; acc: 0.11
Batch: 80; loss: 2.14; acc: 0.16
Batch: 100; loss: 2.22; acc: 0.14
Batch: 120; loss: 2.22; acc: 0.08
Batch: 140; loss: 2.26; acc: 0.06
Val Epoch over. val_loss: 2.214788327551192; val_accuracy: 0.12997611464968153 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.12
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.06
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.14; acc: 0.2
Batch: 100; loss: 2.25; acc: 0.11
Batch: 120; loss: 2.25; acc: 0.09
Batch: 140; loss: 2.21; acc: 0.12
Batch: 160; loss: 2.17; acc: 0.2
Batch: 180; loss: 2.19; acc: 0.11
Batch: 200; loss: 2.2; acc: 0.12
Batch: 220; loss: 2.14; acc: 0.17
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.17; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.23; acc: 0.22
Batch: 320; loss: 2.19; acc: 0.17
Batch: 340; loss: 2.2; acc: 0.12
Batch: 360; loss: 2.19; acc: 0.17
Batch: 380; loss: 2.14; acc: 0.2
Batch: 400; loss: 2.06; acc: 0.27
Batch: 420; loss: 2.21; acc: 0.16
Batch: 440; loss: 2.09; acc: 0.3
Batch: 460; loss: 2.08; acc: 0.28
Batch: 480; loss: 2.1; acc: 0.2
Batch: 500; loss: 2.17; acc: 0.23
Batch: 520; loss: 2.09; acc: 0.34
Batch: 540; loss: 2.21; acc: 0.22
Batch: 560; loss: 2.27; acc: 0.22
Batch: 580; loss: 2.18; acc: 0.25
Batch: 600; loss: 2.14; acc: 0.28
Batch: 620; loss: 2.1; acc: 0.36
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 1.97; acc: 0.42
Batch: 680; loss: 2.05; acc: 0.28
Batch: 700; loss: 2.05; acc: 0.23
Batch: 720; loss: 2.02; acc: 0.25
Batch: 740; loss: 2.16; acc: 0.23
Batch: 760; loss: 2.21; acc: 0.27
Batch: 780; loss: 1.93; acc: 0.33
Train Epoch over. train_loss: 2.14; train_accuracy: 0.2 

Batch: 0; loss: 2.03; acc: 0.31
Batch: 20; loss: 2.15; acc: 0.27
Batch: 40; loss: 1.97; acc: 0.25
Batch: 60; loss: 1.94; acc: 0.25
Batch: 80; loss: 1.91; acc: 0.31
Batch: 100; loss: 2.04; acc: 0.23
Batch: 120; loss: 1.94; acc: 0.28
Batch: 140; loss: 2.02; acc: 0.27
Val Epoch over. val_loss: 2.0204992795446115; val_accuracy: 0.26552547770700635 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.01; acc: 0.31
Batch: 20; loss: 1.8; acc: 0.36
Batch: 40; loss: 2.0; acc: 0.3
Batch: 60; loss: 1.99; acc: 0.25
Batch: 80; loss: 1.86; acc: 0.36
Batch: 100; loss: 1.9; acc: 0.31
Batch: 120; loss: 2.04; acc: 0.28
Batch: 140; loss: 1.84; acc: 0.36
Batch: 160; loss: 1.85; acc: 0.34
Batch: 180; loss: 1.87; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.27
Batch: 220; loss: 1.83; acc: 0.34
Batch: 240; loss: 1.77; acc: 0.39
Batch: 260; loss: 2.03; acc: 0.2
Batch: 280; loss: 1.88; acc: 0.3
Batch: 300; loss: 1.97; acc: 0.25
Batch: 320; loss: 1.69; acc: 0.39
Batch: 340; loss: 1.95; acc: 0.22
Batch: 360; loss: 1.88; acc: 0.41
Batch: 380; loss: 1.79; acc: 0.34
Batch: 400; loss: 1.82; acc: 0.36
Batch: 420; loss: 1.89; acc: 0.33
Batch: 440; loss: 1.87; acc: 0.3
Batch: 460; loss: 1.78; acc: 0.38
Batch: 480; loss: 1.81; acc: 0.28
Batch: 500; loss: 1.76; acc: 0.33
Batch: 520; loss: 1.81; acc: 0.33
Batch: 540; loss: 1.64; acc: 0.34
Batch: 560; loss: 1.77; acc: 0.42
Batch: 580; loss: 1.8; acc: 0.38
Batch: 600; loss: 1.82; acc: 0.34
Batch: 620; loss: 1.8; acc: 0.38
Batch: 640; loss: 1.82; acc: 0.33
Batch: 660; loss: 1.98; acc: 0.33
Batch: 680; loss: 1.74; acc: 0.38
Batch: 700; loss: 1.65; acc: 0.45
Batch: 720; loss: 1.81; acc: 0.28
Batch: 740; loss: 1.76; acc: 0.39
Batch: 760; loss: 1.82; acc: 0.38
Batch: 780; loss: 1.82; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.34 

Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 1.51; acc: 0.38
Batch: 60; loss: 1.6; acc: 0.42
Batch: 80; loss: 1.56; acc: 0.44
Batch: 100; loss: 1.69; acc: 0.41
Batch: 120; loss: 1.68; acc: 0.39
Batch: 140; loss: 1.66; acc: 0.38
Val Epoch over. val_loss: 1.717062905335882; val_accuracy: 0.3821656050955414 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.73; acc: 0.36
Batch: 40; loss: 1.99; acc: 0.31
Batch: 60; loss: 1.84; acc: 0.31
Batch: 80; loss: 1.74; acc: 0.39
Batch: 100; loss: 1.8; acc: 0.39
Batch: 120; loss: 1.94; acc: 0.28
Batch: 140; loss: 1.61; acc: 0.44
Batch: 160; loss: 1.75; acc: 0.34
Batch: 180; loss: 1.76; acc: 0.31
Batch: 200; loss: 1.66; acc: 0.47
Batch: 220; loss: 1.73; acc: 0.3
Batch: 240; loss: 1.73; acc: 0.39
Batch: 260; loss: 1.67; acc: 0.48
Batch: 280; loss: 1.7; acc: 0.34
Batch: 300; loss: 1.58; acc: 0.48
Batch: 320; loss: 1.65; acc: 0.41
Batch: 340; loss: 1.54; acc: 0.5
Batch: 360; loss: 1.51; acc: 0.5
Batch: 380; loss: 1.48; acc: 0.44
Batch: 400; loss: 1.82; acc: 0.34
Batch: 420; loss: 1.82; acc: 0.27
Batch: 440; loss: 1.83; acc: 0.33
Batch: 460; loss: 1.71; acc: 0.45
Batch: 480; loss: 1.73; acc: 0.36
Batch: 500; loss: 1.78; acc: 0.41
Batch: 520; loss: 1.64; acc: 0.38
Batch: 540; loss: 1.8; acc: 0.31
Batch: 560; loss: 1.83; acc: 0.31
Batch: 580; loss: 1.81; acc: 0.31
Batch: 600; loss: 1.73; acc: 0.36
Batch: 620; loss: 1.91; acc: 0.33
Batch: 640; loss: 1.79; acc: 0.45
Batch: 660; loss: 1.71; acc: 0.41
Batch: 680; loss: 1.61; acc: 0.42
Batch: 700; loss: 1.62; acc: 0.44
Batch: 720; loss: 1.96; acc: 0.28
Batch: 740; loss: 1.74; acc: 0.33
Batch: 760; loss: 1.83; acc: 0.31
Batch: 780; loss: 1.66; acc: 0.41
Train Epoch over. train_loss: 1.72; train_accuracy: 0.38 

Batch: 0; loss: 1.72; acc: 0.41
Batch: 20; loss: 1.89; acc: 0.38
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.56; acc: 0.41
Batch: 80; loss: 1.4; acc: 0.47
Batch: 100; loss: 1.74; acc: 0.41
Batch: 120; loss: 1.74; acc: 0.36
Batch: 140; loss: 1.44; acc: 0.5
Val Epoch over. val_loss: 1.6729958664839435; val_accuracy: 0.40605095541401276 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.3
Batch: 20; loss: 1.71; acc: 0.36
Batch: 40; loss: 1.65; acc: 0.39
Batch: 60; loss: 1.54; acc: 0.48
Batch: 80; loss: 1.63; acc: 0.45
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 1.5; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.53
Batch: 160; loss: 1.65; acc: 0.34
Batch: 180; loss: 1.82; acc: 0.39
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.48; acc: 0.53
Batch: 240; loss: 1.82; acc: 0.33
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.34
Batch: 300; loss: 1.76; acc: 0.42
Batch: 320; loss: 1.63; acc: 0.34
Batch: 340; loss: 1.76; acc: 0.39
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.74; acc: 0.38
Batch: 400; loss: 1.65; acc: 0.42
Batch: 420; loss: 1.69; acc: 0.47
Batch: 440; loss: 1.89; acc: 0.27
Batch: 460; loss: 1.52; acc: 0.48
Batch: 480; loss: 1.45; acc: 0.52
Batch: 500; loss: 1.67; acc: 0.38
Batch: 520; loss: 1.71; acc: 0.42
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.39; acc: 0.52
Batch: 600; loss: 1.55; acc: 0.52
Batch: 620; loss: 1.82; acc: 0.33
Batch: 640; loss: 1.69; acc: 0.45
Batch: 660; loss: 1.68; acc: 0.41
Batch: 680; loss: 1.67; acc: 0.3
Batch: 700; loss: 1.62; acc: 0.45
Batch: 720; loss: 1.5; acc: 0.44
Batch: 740; loss: 1.74; acc: 0.3
Batch: 760; loss: 1.62; acc: 0.45
Batch: 780; loss: 1.5; acc: 0.52
Train Epoch over. train_loss: 1.69; train_accuracy: 0.4 

Batch: 0; loss: 1.71; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.37; acc: 0.55
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.39; acc: 0.53
Batch: 100; loss: 1.7; acc: 0.44
Batch: 120; loss: 1.76; acc: 0.38
Batch: 140; loss: 1.38; acc: 0.56
Val Epoch over. val_loss: 1.6532031540658063; val_accuracy: 0.4189888535031847 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.71; acc: 0.38
Batch: 40; loss: 1.56; acc: 0.45
Batch: 60; loss: 1.64; acc: 0.36
Batch: 80; loss: 1.59; acc: 0.41
Batch: 100; loss: 1.47; acc: 0.42
Batch: 120; loss: 1.83; acc: 0.33
Batch: 140; loss: 1.59; acc: 0.41
Batch: 160; loss: 1.49; acc: 0.5
Batch: 180; loss: 1.69; acc: 0.45
Batch: 200; loss: 1.63; acc: 0.45
Batch: 220; loss: 1.73; acc: 0.5
Batch: 240; loss: 1.49; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.45
Batch: 280; loss: 1.69; acc: 0.38
Batch: 300; loss: 1.47; acc: 0.45
Batch: 320; loss: 1.66; acc: 0.42
Batch: 340; loss: 1.48; acc: 0.45
Batch: 360; loss: 1.54; acc: 0.45
Batch: 380; loss: 1.52; acc: 0.56
Batch: 400; loss: 1.7; acc: 0.47
Batch: 420; loss: 1.87; acc: 0.39
Batch: 440; loss: 1.9; acc: 0.33
Batch: 460; loss: 1.72; acc: 0.45
Batch: 480; loss: 1.55; acc: 0.48
Batch: 500; loss: 1.46; acc: 0.48
Batch: 520; loss: 1.74; acc: 0.42
Batch: 540; loss: 1.63; acc: 0.52
Batch: 560; loss: 1.77; acc: 0.38
Batch: 580; loss: 1.71; acc: 0.44
Batch: 600; loss: 1.78; acc: 0.39
Batch: 620; loss: 1.46; acc: 0.53
Batch: 640; loss: 1.58; acc: 0.45
Batch: 660; loss: 2.03; acc: 0.36
Batch: 680; loss: 1.57; acc: 0.45
Batch: 700; loss: 1.89; acc: 0.28
Batch: 720; loss: 1.87; acc: 0.31
Batch: 740; loss: 1.82; acc: 0.34
Batch: 760; loss: 1.93; acc: 0.34
Batch: 780; loss: 1.71; acc: 0.45
Train Epoch over. train_loss: 1.67; train_accuracy: 0.42 

Batch: 0; loss: 1.65; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.36; acc: 0.47
Batch: 60; loss: 1.5; acc: 0.45
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.74; acc: 0.38
Batch: 140; loss: 1.38; acc: 0.53
Val Epoch over. val_loss: 1.620882038857527; val_accuracy: 0.439390923566879 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.36
Batch: 20; loss: 1.55; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.34
Batch: 60; loss: 1.62; acc: 0.38
Batch: 80; loss: 1.59; acc: 0.44
Batch: 100; loss: 1.64; acc: 0.45
Batch: 120; loss: 1.75; acc: 0.36
Batch: 140; loss: 1.6; acc: 0.44
Batch: 160; loss: 1.87; acc: 0.44
Batch: 180; loss: 1.8; acc: 0.39
Batch: 200; loss: 1.82; acc: 0.38
Batch: 220; loss: 1.56; acc: 0.38
Batch: 240; loss: 1.64; acc: 0.33
Batch: 260; loss: 1.59; acc: 0.52
Batch: 280; loss: 1.63; acc: 0.44
Batch: 300; loss: 1.95; acc: 0.38
Batch: 320; loss: 1.52; acc: 0.44
Batch: 340; loss: 1.73; acc: 0.42
Batch: 360; loss: 1.58; acc: 0.47
Batch: 380; loss: 1.88; acc: 0.33
Batch: 400; loss: 1.92; acc: 0.33
Batch: 420; loss: 1.7; acc: 0.42
Batch: 440; loss: 1.56; acc: 0.5
Batch: 460; loss: 1.52; acc: 0.44
Batch: 480; loss: 1.5; acc: 0.5
Batch: 500; loss: 1.51; acc: 0.45
Batch: 520; loss: 1.36; acc: 0.5
Batch: 540; loss: 1.7; acc: 0.42
Batch: 560; loss: 1.74; acc: 0.44
Batch: 580; loss: 1.54; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.36
Batch: 620; loss: 1.62; acc: 0.42
Batch: 640; loss: 1.76; acc: 0.44
Batch: 660; loss: 1.42; acc: 0.45
Batch: 680; loss: 1.59; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.48
Batch: 720; loss: 1.51; acc: 0.48
Batch: 740; loss: 1.72; acc: 0.39
Batch: 760; loss: 1.6; acc: 0.44
Batch: 780; loss: 1.67; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.66; acc: 0.36
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.51; acc: 0.44
Batch: 80; loss: 1.45; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.42
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.41; acc: 0.55
Val Epoch over. val_loss: 1.6191846132278442; val_accuracy: 0.43869426751592355 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.7; acc: 0.41
Batch: 20; loss: 1.61; acc: 0.44
Batch: 40; loss: 1.68; acc: 0.44
Batch: 60; loss: 1.66; acc: 0.44
Batch: 80; loss: 1.54; acc: 0.38
Batch: 100; loss: 1.54; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.67; acc: 0.44
Batch: 180; loss: 1.7; acc: 0.44
Batch: 200; loss: 1.46; acc: 0.5
Batch: 220; loss: 1.78; acc: 0.38
Batch: 240; loss: 1.58; acc: 0.44
Batch: 260; loss: 1.86; acc: 0.3
Batch: 280; loss: 1.47; acc: 0.48
Batch: 300; loss: 1.5; acc: 0.42
Batch: 320; loss: 1.49; acc: 0.45
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.84; acc: 0.36
Batch: 380; loss: 1.63; acc: 0.42
Batch: 400; loss: 1.51; acc: 0.42
Batch: 420; loss: 1.51; acc: 0.47
Batch: 440; loss: 1.73; acc: 0.41
Batch: 460; loss: 1.66; acc: 0.44
Batch: 480; loss: 1.56; acc: 0.44
Batch: 500; loss: 1.72; acc: 0.42
Batch: 520; loss: 1.39; acc: 0.53
Batch: 540; loss: 1.7; acc: 0.39
Batch: 560; loss: 1.66; acc: 0.45
Batch: 580; loss: 1.6; acc: 0.45
Batch: 600; loss: 1.59; acc: 0.5
Batch: 620; loss: 1.77; acc: 0.45
Batch: 640; loss: 1.63; acc: 0.38
Batch: 660; loss: 1.53; acc: 0.44
Batch: 680; loss: 1.78; acc: 0.38
Batch: 700; loss: 1.89; acc: 0.39
Batch: 720; loss: 1.55; acc: 0.41
Batch: 740; loss: 1.58; acc: 0.47
Batch: 760; loss: 1.81; acc: 0.34
Batch: 780; loss: 1.71; acc: 0.36
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.33
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.58; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.41; acc: 0.48
Val Epoch over. val_loss: 1.6182061798253637; val_accuracy: 0.4411823248407643 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.7; acc: 0.39
Batch: 20; loss: 1.68; acc: 0.34
Batch: 40; loss: 1.78; acc: 0.38
Batch: 60; loss: 1.73; acc: 0.34
Batch: 80; loss: 1.51; acc: 0.47
Batch: 100; loss: 1.77; acc: 0.38
Batch: 120; loss: 1.62; acc: 0.44
Batch: 140; loss: 1.65; acc: 0.42
Batch: 160; loss: 1.64; acc: 0.38
Batch: 180; loss: 1.75; acc: 0.42
Batch: 200; loss: 1.62; acc: 0.44
Batch: 220; loss: 1.53; acc: 0.53
Batch: 240; loss: 1.64; acc: 0.44
Batch: 260; loss: 1.4; acc: 0.61
Batch: 280; loss: 1.65; acc: 0.41
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.54; acc: 0.41
Batch: 340; loss: 1.8; acc: 0.36
Batch: 360; loss: 1.63; acc: 0.36
Batch: 380; loss: 1.66; acc: 0.42
Batch: 400; loss: 1.5; acc: 0.47
Batch: 420; loss: 1.61; acc: 0.38
Batch: 440; loss: 1.4; acc: 0.5
Batch: 460; loss: 1.56; acc: 0.5
Batch: 480; loss: 1.59; acc: 0.38
Batch: 500; loss: 1.77; acc: 0.36
Batch: 520; loss: 1.61; acc: 0.5
Batch: 540; loss: 1.57; acc: 0.39
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.74; acc: 0.41
Batch: 600; loss: 1.62; acc: 0.42
Batch: 620; loss: 1.76; acc: 0.5
Batch: 640; loss: 1.99; acc: 0.33
Batch: 660; loss: 1.96; acc: 0.3
Batch: 680; loss: 1.63; acc: 0.38
Batch: 700; loss: 1.73; acc: 0.45
Batch: 720; loss: 1.55; acc: 0.39
Batch: 740; loss: 1.82; acc: 0.41
Batch: 760; loss: 1.63; acc: 0.42
Batch: 780; loss: 1.78; acc: 0.41
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.39; acc: 0.5
Batch: 60; loss: 1.51; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.41; acc: 0.5
Val Epoch over. val_loss: 1.6182531733421763; val_accuracy: 0.43839570063694266 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.62; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.45
Batch: 40; loss: 1.58; acc: 0.3
Batch: 60; loss: 1.65; acc: 0.41
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.67; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.39
Batch: 140; loss: 1.85; acc: 0.41
Batch: 160; loss: 1.64; acc: 0.42
Batch: 180; loss: 1.75; acc: 0.41
Batch: 200; loss: 1.65; acc: 0.47
Batch: 220; loss: 1.46; acc: 0.56
Batch: 240; loss: 1.76; acc: 0.33
Batch: 260; loss: 1.83; acc: 0.36
Batch: 280; loss: 1.71; acc: 0.39
Batch: 300; loss: 1.38; acc: 0.52
Batch: 320; loss: 1.65; acc: 0.36
Batch: 340; loss: 1.55; acc: 0.52
Batch: 360; loss: 1.77; acc: 0.42
Batch: 380; loss: 1.78; acc: 0.34
Batch: 400; loss: 1.57; acc: 0.44
Batch: 420; loss: 1.67; acc: 0.41
Batch: 440; loss: 1.49; acc: 0.5
Batch: 460; loss: 1.73; acc: 0.42
Batch: 480; loss: 1.65; acc: 0.38
Batch: 500; loss: 1.64; acc: 0.44
Batch: 520; loss: 1.83; acc: 0.39
Batch: 540; loss: 1.59; acc: 0.38
Batch: 560; loss: 1.8; acc: 0.36
Batch: 580; loss: 1.68; acc: 0.36
Batch: 600; loss: 1.75; acc: 0.45
Batch: 620; loss: 1.91; acc: 0.33
Batch: 640; loss: 1.44; acc: 0.48
Batch: 660; loss: 1.52; acc: 0.44
Batch: 680; loss: 1.86; acc: 0.39
Batch: 700; loss: 1.54; acc: 0.45
Batch: 720; loss: 1.79; acc: 0.39
Batch: 740; loss: 1.65; acc: 0.41
Batch: 760; loss: 1.89; acc: 0.33
Batch: 780; loss: 1.92; acc: 0.23
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.87; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.41; acc: 0.5
Val Epoch over. val_loss: 1.6176706727143306; val_accuracy: 0.44018710191082805 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.74; acc: 0.34
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.86; acc: 0.41
Batch: 80; loss: 1.61; acc: 0.44
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.59; acc: 0.44
Batch: 140; loss: 1.74; acc: 0.39
Batch: 160; loss: 1.62; acc: 0.47
Batch: 180; loss: 1.33; acc: 0.56
Batch: 200; loss: 1.7; acc: 0.45
Batch: 220; loss: 1.69; acc: 0.34
Batch: 240; loss: 1.63; acc: 0.42
Batch: 260; loss: 1.73; acc: 0.38
Batch: 280; loss: 1.57; acc: 0.38
Batch: 300; loss: 1.74; acc: 0.39
Batch: 320; loss: 1.76; acc: 0.39
Batch: 340; loss: 1.68; acc: 0.45
Batch: 360; loss: 1.67; acc: 0.48
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.56; acc: 0.41
Batch: 420; loss: 1.82; acc: 0.33
Batch: 440; loss: 1.62; acc: 0.41
Batch: 460; loss: 1.41; acc: 0.55
Batch: 480; loss: 1.52; acc: 0.5
Batch: 500; loss: 1.6; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.42
Batch: 540; loss: 1.58; acc: 0.45
Batch: 560; loss: 1.48; acc: 0.41
Batch: 580; loss: 1.58; acc: 0.5
Batch: 600; loss: 1.81; acc: 0.25
Batch: 620; loss: 1.57; acc: 0.47
Batch: 640; loss: 1.8; acc: 0.38
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.46; acc: 0.47
Batch: 700; loss: 1.57; acc: 0.45
Batch: 720; loss: 1.88; acc: 0.34
Batch: 740; loss: 1.83; acc: 0.31
Batch: 760; loss: 1.83; acc: 0.36
Batch: 780; loss: 1.56; acc: 0.42
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.49; acc: 0.44
Batch: 80; loss: 1.43; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.41; acc: 0.5
Val Epoch over. val_loss: 1.617793176584183; val_accuracy: 0.43899283439490444 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 1.67; acc: 0.34
Batch: 40; loss: 1.76; acc: 0.36
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.8; acc: 0.38
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.64; acc: 0.48
Batch: 140; loss: 1.76; acc: 0.41
Batch: 160; loss: 1.53; acc: 0.44
Batch: 180; loss: 1.61; acc: 0.41
Batch: 200; loss: 1.7; acc: 0.38
Batch: 220; loss: 1.72; acc: 0.45
Batch: 240; loss: 1.69; acc: 0.45
Batch: 260; loss: 1.59; acc: 0.5
Batch: 280; loss: 1.78; acc: 0.38
Batch: 300; loss: 1.91; acc: 0.38
Batch: 320; loss: 1.66; acc: 0.44
Batch: 340; loss: 1.66; acc: 0.44
Batch: 360; loss: 1.39; acc: 0.52
Batch: 380; loss: 1.79; acc: 0.41
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.54; acc: 0.44
Batch: 440; loss: 1.6; acc: 0.41
Batch: 460; loss: 1.88; acc: 0.3
Batch: 480; loss: 1.43; acc: 0.58
Batch: 500; loss: 1.55; acc: 0.45
Batch: 520; loss: 1.65; acc: 0.41
Batch: 540; loss: 1.86; acc: 0.34
Batch: 560; loss: 1.74; acc: 0.36
Batch: 580; loss: 1.85; acc: 0.39
Batch: 600; loss: 1.73; acc: 0.27
Batch: 620; loss: 1.73; acc: 0.44
Batch: 640; loss: 1.63; acc: 0.52
Batch: 660; loss: 1.75; acc: 0.39
Batch: 680; loss: 1.69; acc: 0.44
Batch: 700; loss: 1.74; acc: 0.39
Batch: 720; loss: 1.51; acc: 0.47
Batch: 740; loss: 1.62; acc: 0.45
Batch: 760; loss: 1.58; acc: 0.55
Batch: 780; loss: 1.6; acc: 0.42
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.42
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.42
Batch: 140; loss: 1.42; acc: 0.48
Val Epoch over. val_loss: 1.6176504253581832; val_accuracy: 0.43829617834394907 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.83; acc: 0.42
Batch: 40; loss: 1.53; acc: 0.41
Batch: 60; loss: 1.58; acc: 0.41
Batch: 80; loss: 1.63; acc: 0.42
Batch: 100; loss: 1.61; acc: 0.47
Batch: 120; loss: 1.7; acc: 0.47
Batch: 140; loss: 1.59; acc: 0.42
Batch: 160; loss: 1.74; acc: 0.42
Batch: 180; loss: 1.8; acc: 0.28
Batch: 200; loss: 1.48; acc: 0.45
Batch: 220; loss: 1.77; acc: 0.36
Batch: 240; loss: 1.78; acc: 0.39
Batch: 260; loss: 1.47; acc: 0.44
Batch: 280; loss: 1.43; acc: 0.52
Batch: 300; loss: 1.74; acc: 0.38
Batch: 320; loss: 1.61; acc: 0.48
Batch: 340; loss: 1.68; acc: 0.44
Batch: 360; loss: 1.68; acc: 0.41
Batch: 380; loss: 1.67; acc: 0.38
Batch: 400; loss: 1.58; acc: 0.47
Batch: 420; loss: 1.67; acc: 0.38
Batch: 440; loss: 1.77; acc: 0.38
Batch: 460; loss: 1.69; acc: 0.44
Batch: 480; loss: 1.75; acc: 0.39
Batch: 500; loss: 1.42; acc: 0.5
Batch: 520; loss: 1.57; acc: 0.42
Batch: 540; loss: 1.42; acc: 0.55
Batch: 560; loss: 1.67; acc: 0.47
Batch: 580; loss: 1.76; acc: 0.31
Batch: 600; loss: 1.66; acc: 0.39
Batch: 620; loss: 1.82; acc: 0.36
Batch: 640; loss: 1.58; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.45
Batch: 680; loss: 1.86; acc: 0.33
Batch: 700; loss: 1.64; acc: 0.34
Batch: 720; loss: 1.72; acc: 0.33
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.71; acc: 0.3
Batch: 780; loss: 1.61; acc: 0.39
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.87; acc: 0.31
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.45
Batch: 80; loss: 1.45; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.42
Batch: 140; loss: 1.41; acc: 0.5
Val Epoch over. val_loss: 1.6181564437355964; val_accuracy: 0.4384952229299363 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.72; acc: 0.38
Batch: 20; loss: 2.04; acc: 0.3
Batch: 40; loss: 1.57; acc: 0.42
Batch: 60; loss: 1.7; acc: 0.39
Batch: 80; loss: 1.59; acc: 0.44
Batch: 100; loss: 1.68; acc: 0.41
Batch: 120; loss: 1.69; acc: 0.41
Batch: 140; loss: 1.65; acc: 0.47
Batch: 160; loss: 1.29; acc: 0.55
Batch: 180; loss: 1.84; acc: 0.41
Batch: 200; loss: 1.57; acc: 0.52
Batch: 220; loss: 1.6; acc: 0.44
Batch: 240; loss: 1.71; acc: 0.33
Batch: 260; loss: 1.49; acc: 0.47
Batch: 280; loss: 1.58; acc: 0.47
Batch: 300; loss: 1.64; acc: 0.33
Batch: 320; loss: 1.88; acc: 0.3
Batch: 340; loss: 1.56; acc: 0.47
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 1.58; acc: 0.34
Batch: 400; loss: 1.57; acc: 0.47
Batch: 420; loss: 1.71; acc: 0.44
Batch: 440; loss: 1.89; acc: 0.25
Batch: 460; loss: 1.6; acc: 0.38
Batch: 480; loss: 1.35; acc: 0.5
Batch: 500; loss: 1.6; acc: 0.44
Batch: 520; loss: 1.72; acc: 0.48
Batch: 540; loss: 1.51; acc: 0.47
Batch: 560; loss: 1.45; acc: 0.41
Batch: 580; loss: 1.5; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.47
Batch: 620; loss: 1.52; acc: 0.44
Batch: 640; loss: 1.62; acc: 0.44
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.59; acc: 0.47
Batch: 700; loss: 1.76; acc: 0.39
Batch: 720; loss: 1.47; acc: 0.42
Batch: 740; loss: 1.76; acc: 0.3
Batch: 760; loss: 1.64; acc: 0.39
Batch: 780; loss: 1.62; acc: 0.45
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.39; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6192621774734206; val_accuracy: 0.43580812101910826 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.51; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.38
Batch: 40; loss: 1.52; acc: 0.45
Batch: 60; loss: 1.58; acc: 0.45
Batch: 80; loss: 1.57; acc: 0.44
Batch: 100; loss: 1.33; acc: 0.53
Batch: 120; loss: 1.37; acc: 0.44
Batch: 140; loss: 1.79; acc: 0.36
Batch: 160; loss: 1.66; acc: 0.39
Batch: 180; loss: 1.62; acc: 0.5
Batch: 200; loss: 1.68; acc: 0.5
Batch: 220; loss: 1.81; acc: 0.38
Batch: 240; loss: 1.74; acc: 0.41
Batch: 260; loss: 1.71; acc: 0.44
Batch: 280; loss: 1.77; acc: 0.34
Batch: 300; loss: 1.59; acc: 0.39
Batch: 320; loss: 1.72; acc: 0.41
Batch: 340; loss: 1.7; acc: 0.44
Batch: 360; loss: 1.84; acc: 0.31
Batch: 380; loss: 1.7; acc: 0.39
Batch: 400; loss: 1.48; acc: 0.52
Batch: 420; loss: 1.64; acc: 0.38
Batch: 440; loss: 1.84; acc: 0.34
Batch: 460; loss: 1.88; acc: 0.41
Batch: 480; loss: 1.65; acc: 0.42
Batch: 500; loss: 1.71; acc: 0.41
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.59; acc: 0.41
Batch: 560; loss: 1.65; acc: 0.38
Batch: 580; loss: 1.69; acc: 0.48
Batch: 600; loss: 1.5; acc: 0.5
Batch: 620; loss: 1.76; acc: 0.34
Batch: 640; loss: 1.51; acc: 0.48
Batch: 660; loss: 1.84; acc: 0.33
Batch: 680; loss: 1.4; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.41
Batch: 720; loss: 1.6; acc: 0.47
Batch: 740; loss: 1.5; acc: 0.5
Batch: 760; loss: 1.78; acc: 0.39
Batch: 780; loss: 1.57; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.69; acc: 0.34
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6196116376075016; val_accuracy: 0.43531050955414013 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.45; acc: 0.55
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.58; acc: 0.48
Batch: 60; loss: 1.65; acc: 0.39
Batch: 80; loss: 1.66; acc: 0.39
Batch: 100; loss: 1.64; acc: 0.39
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 1.93; acc: 0.36
Batch: 160; loss: 1.68; acc: 0.45
Batch: 180; loss: 1.56; acc: 0.41
Batch: 200; loss: 1.66; acc: 0.39
Batch: 220; loss: 1.82; acc: 0.41
Batch: 240; loss: 1.77; acc: 0.42
Batch: 260; loss: 1.45; acc: 0.48
Batch: 280; loss: 1.7; acc: 0.45
Batch: 300; loss: 1.76; acc: 0.41
Batch: 320; loss: 1.57; acc: 0.38
Batch: 340; loss: 1.82; acc: 0.3
Batch: 360; loss: 1.6; acc: 0.48
Batch: 380; loss: 1.74; acc: 0.33
Batch: 400; loss: 1.57; acc: 0.39
Batch: 420; loss: 1.54; acc: 0.45
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.6; acc: 0.39
Batch: 480; loss: 1.45; acc: 0.45
Batch: 500; loss: 1.5; acc: 0.45
Batch: 520; loss: 1.6; acc: 0.48
Batch: 540; loss: 1.53; acc: 0.48
Batch: 560; loss: 1.69; acc: 0.44
Batch: 580; loss: 1.71; acc: 0.42
Batch: 600; loss: 1.92; acc: 0.34
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.76; acc: 0.34
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.49; acc: 0.5
Batch: 700; loss: 1.51; acc: 0.48
Batch: 720; loss: 1.61; acc: 0.39
Batch: 740; loss: 1.6; acc: 0.44
Batch: 760; loss: 1.67; acc: 0.41
Batch: 780; loss: 1.76; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.47
Val Epoch over. val_loss: 1.6183793423282113; val_accuracy: 0.4384952229299363 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.7; acc: 0.41
Batch: 20; loss: 1.59; acc: 0.41
Batch: 40; loss: 1.64; acc: 0.44
Batch: 60; loss: 1.76; acc: 0.36
Batch: 80; loss: 1.84; acc: 0.31
Batch: 100; loss: 1.62; acc: 0.41
Batch: 120; loss: 1.81; acc: 0.38
Batch: 140; loss: 1.63; acc: 0.41
Batch: 160; loss: 1.51; acc: 0.47
Batch: 180; loss: 1.43; acc: 0.48
Batch: 200; loss: 1.62; acc: 0.41
Batch: 220; loss: 1.68; acc: 0.34
Batch: 240; loss: 1.68; acc: 0.36
Batch: 260; loss: 1.42; acc: 0.45
Batch: 280; loss: 1.51; acc: 0.5
Batch: 300; loss: 1.8; acc: 0.3
Batch: 320; loss: 1.83; acc: 0.34
Batch: 340; loss: 1.48; acc: 0.48
Batch: 360; loss: 1.77; acc: 0.44
Batch: 380; loss: 1.64; acc: 0.44
Batch: 400; loss: 1.78; acc: 0.36
Batch: 420; loss: 1.59; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.5
Batch: 460; loss: 1.91; acc: 0.38
Batch: 480; loss: 1.69; acc: 0.33
Batch: 500; loss: 1.54; acc: 0.42
Batch: 520; loss: 1.58; acc: 0.47
Batch: 540; loss: 1.72; acc: 0.41
Batch: 560; loss: 1.77; acc: 0.39
Batch: 580; loss: 1.73; acc: 0.38
Batch: 600; loss: 1.45; acc: 0.48
Batch: 620; loss: 1.82; acc: 0.39
Batch: 640; loss: 1.88; acc: 0.41
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.64; acc: 0.47
Batch: 700; loss: 1.99; acc: 0.36
Batch: 720; loss: 1.88; acc: 0.39
Batch: 740; loss: 1.8; acc: 0.34
Batch: 760; loss: 1.42; acc: 0.45
Batch: 780; loss: 1.84; acc: 0.42
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.42
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.47
Val Epoch over. val_loss: 1.617683823700923; val_accuracy: 0.4380971337579618 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.66; acc: 0.38
Batch: 20; loss: 1.78; acc: 0.38
Batch: 40; loss: 1.71; acc: 0.42
Batch: 60; loss: 1.66; acc: 0.36
Batch: 80; loss: 1.68; acc: 0.45
Batch: 100; loss: 1.5; acc: 0.47
Batch: 120; loss: 1.56; acc: 0.52
Batch: 140; loss: 1.87; acc: 0.34
Batch: 160; loss: 1.52; acc: 0.41
Batch: 180; loss: 1.72; acc: 0.5
Batch: 200; loss: 1.36; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.5; acc: 0.47
Batch: 260; loss: 1.54; acc: 0.47
Batch: 280; loss: 1.78; acc: 0.28
Batch: 300; loss: 1.67; acc: 0.38
Batch: 320; loss: 1.69; acc: 0.31
Batch: 340; loss: 1.63; acc: 0.41
Batch: 360; loss: 1.47; acc: 0.45
Batch: 380; loss: 1.53; acc: 0.44
Batch: 400; loss: 1.48; acc: 0.45
Batch: 420; loss: 1.61; acc: 0.47
Batch: 440; loss: 1.68; acc: 0.41
Batch: 460; loss: 1.67; acc: 0.45
Batch: 480; loss: 1.44; acc: 0.48
Batch: 500; loss: 1.66; acc: 0.44
Batch: 520; loss: 1.57; acc: 0.39
Batch: 540; loss: 1.46; acc: 0.5
Batch: 560; loss: 1.63; acc: 0.45
Batch: 580; loss: 1.54; acc: 0.39
Batch: 600; loss: 1.71; acc: 0.44
Batch: 620; loss: 1.46; acc: 0.45
Batch: 640; loss: 1.66; acc: 0.42
Batch: 660; loss: 1.54; acc: 0.44
Batch: 680; loss: 1.72; acc: 0.42
Batch: 700; loss: 1.83; acc: 0.33
Batch: 720; loss: 1.51; acc: 0.44
Batch: 740; loss: 1.84; acc: 0.44
Batch: 760; loss: 1.64; acc: 0.39
Batch: 780; loss: 1.68; acc: 0.39
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.41
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6178174436472024; val_accuracy: 0.43839570063694266 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.44; acc: 0.52
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.64; acc: 0.38
Batch: 60; loss: 1.71; acc: 0.34
Batch: 80; loss: 1.64; acc: 0.39
Batch: 100; loss: 1.72; acc: 0.36
Batch: 120; loss: 1.66; acc: 0.36
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.5; acc: 0.5
Batch: 180; loss: 1.95; acc: 0.34
Batch: 200; loss: 1.75; acc: 0.36
Batch: 220; loss: 1.54; acc: 0.44
Batch: 240; loss: 1.72; acc: 0.41
Batch: 260; loss: 1.79; acc: 0.31
Batch: 280; loss: 1.81; acc: 0.31
Batch: 300; loss: 1.53; acc: 0.5
Batch: 320; loss: 1.59; acc: 0.45
Batch: 340; loss: 1.75; acc: 0.36
Batch: 360; loss: 1.87; acc: 0.36
Batch: 380; loss: 1.97; acc: 0.28
Batch: 400; loss: 1.81; acc: 0.36
Batch: 420; loss: 1.45; acc: 0.47
Batch: 440; loss: 1.79; acc: 0.38
Batch: 460; loss: 1.71; acc: 0.41
Batch: 480; loss: 1.83; acc: 0.27
Batch: 500; loss: 1.52; acc: 0.39
Batch: 520; loss: 1.57; acc: 0.41
Batch: 540; loss: 1.78; acc: 0.33
Batch: 560; loss: 1.57; acc: 0.39
Batch: 580; loss: 1.79; acc: 0.39
Batch: 600; loss: 1.72; acc: 0.36
Batch: 620; loss: 1.59; acc: 0.41
Batch: 640; loss: 1.76; acc: 0.34
Batch: 660; loss: 1.75; acc: 0.39
Batch: 680; loss: 1.46; acc: 0.55
Batch: 700; loss: 1.63; acc: 0.44
Batch: 720; loss: 1.52; acc: 0.52
Batch: 740; loss: 1.58; acc: 0.45
Batch: 760; loss: 1.93; acc: 0.33
Batch: 780; loss: 1.78; acc: 0.41
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.33
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.42
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175655916238287; val_accuracy: 0.43929140127388533 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.72; acc: 0.41
Batch: 20; loss: 1.51; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.41
Batch: 60; loss: 1.8; acc: 0.41
Batch: 80; loss: 1.57; acc: 0.45
Batch: 100; loss: 1.56; acc: 0.45
Batch: 120; loss: 1.56; acc: 0.44
Batch: 140; loss: 1.7; acc: 0.44
Batch: 160; loss: 1.49; acc: 0.5
Batch: 180; loss: 1.58; acc: 0.48
Batch: 200; loss: 1.62; acc: 0.42
Batch: 220; loss: 1.45; acc: 0.42
Batch: 240; loss: 1.66; acc: 0.38
Batch: 260; loss: 1.56; acc: 0.42
Batch: 280; loss: 1.54; acc: 0.45
Batch: 300; loss: 1.62; acc: 0.42
Batch: 320; loss: 1.64; acc: 0.48
Batch: 340; loss: 1.74; acc: 0.39
Batch: 360; loss: 1.77; acc: 0.38
Batch: 380; loss: 1.7; acc: 0.44
Batch: 400; loss: 1.6; acc: 0.42
Batch: 420; loss: 1.41; acc: 0.5
Batch: 440; loss: 1.74; acc: 0.39
Batch: 460; loss: 1.61; acc: 0.42
Batch: 480; loss: 1.78; acc: 0.34
Batch: 500; loss: 1.74; acc: 0.44
Batch: 520; loss: 1.69; acc: 0.34
Batch: 540; loss: 1.69; acc: 0.33
Batch: 560; loss: 1.68; acc: 0.41
Batch: 580; loss: 1.7; acc: 0.31
Batch: 600; loss: 1.64; acc: 0.44
Batch: 620; loss: 1.65; acc: 0.36
Batch: 640; loss: 1.5; acc: 0.48
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.47; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.44
Batch: 720; loss: 1.48; acc: 0.44
Batch: 740; loss: 1.65; acc: 0.53
Batch: 760; loss: 1.8; acc: 0.31
Batch: 780; loss: 1.69; acc: 0.38
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6172676876092413; val_accuracy: 0.43889331210191085 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.71; acc: 0.39
Batch: 20; loss: 1.81; acc: 0.31
Batch: 40; loss: 1.95; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.34
Batch: 80; loss: 1.76; acc: 0.39
Batch: 100; loss: 1.92; acc: 0.28
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.53; acc: 0.52
Batch: 160; loss: 1.67; acc: 0.34
Batch: 180; loss: 1.63; acc: 0.44
Batch: 200; loss: 1.34; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.45
Batch: 240; loss: 1.63; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.39
Batch: 280; loss: 1.68; acc: 0.44
Batch: 300; loss: 1.88; acc: 0.31
Batch: 320; loss: 1.78; acc: 0.42
Batch: 340; loss: 1.42; acc: 0.52
Batch: 360; loss: 1.76; acc: 0.39
Batch: 380; loss: 1.7; acc: 0.42
Batch: 400; loss: 1.72; acc: 0.41
Batch: 420; loss: 1.69; acc: 0.44
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.67; acc: 0.39
Batch: 480; loss: 1.66; acc: 0.39
Batch: 500; loss: 1.56; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.41
Batch: 540; loss: 1.57; acc: 0.44
Batch: 560; loss: 1.67; acc: 0.41
Batch: 580; loss: 1.8; acc: 0.34
Batch: 600; loss: 1.61; acc: 0.47
Batch: 620; loss: 1.87; acc: 0.36
Batch: 640; loss: 1.55; acc: 0.44
Batch: 660; loss: 1.67; acc: 0.39
Batch: 680; loss: 1.73; acc: 0.41
Batch: 700; loss: 1.81; acc: 0.33
Batch: 720; loss: 1.63; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.47
Batch: 760; loss: 1.53; acc: 0.48
Batch: 780; loss: 1.48; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.617766139613595; val_accuracy: 0.4377985668789809 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.57; acc: 0.44
Batch: 20; loss: 1.85; acc: 0.31
Batch: 40; loss: 1.42; acc: 0.55
Batch: 60; loss: 1.56; acc: 0.42
Batch: 80; loss: 1.64; acc: 0.38
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.62; acc: 0.44
Batch: 160; loss: 1.58; acc: 0.47
Batch: 180; loss: 1.68; acc: 0.48
Batch: 200; loss: 1.8; acc: 0.34
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.8; acc: 0.39
Batch: 260; loss: 1.61; acc: 0.31
Batch: 280; loss: 1.74; acc: 0.39
Batch: 300; loss: 1.59; acc: 0.47
Batch: 320; loss: 1.59; acc: 0.48
Batch: 340; loss: 1.55; acc: 0.47
Batch: 360; loss: 1.68; acc: 0.38
Batch: 380; loss: 1.83; acc: 0.47
Batch: 400; loss: 1.73; acc: 0.42
Batch: 420; loss: 1.75; acc: 0.44
Batch: 440; loss: 1.74; acc: 0.34
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.39
Batch: 500; loss: 1.65; acc: 0.34
Batch: 520; loss: 1.49; acc: 0.48
Batch: 540; loss: 1.58; acc: 0.47
Batch: 560; loss: 1.86; acc: 0.38
Batch: 580; loss: 1.62; acc: 0.42
Batch: 600; loss: 1.72; acc: 0.34
Batch: 620; loss: 1.57; acc: 0.47
Batch: 640; loss: 1.55; acc: 0.45
Batch: 660; loss: 1.59; acc: 0.39
Batch: 680; loss: 1.62; acc: 0.45
Batch: 700; loss: 1.57; acc: 0.41
Batch: 720; loss: 1.53; acc: 0.44
Batch: 740; loss: 1.7; acc: 0.45
Batch: 760; loss: 1.55; acc: 0.55
Batch: 780; loss: 1.72; acc: 0.36
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.45
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.42
Batch: 140; loss: 1.41; acc: 0.5
Val Epoch over. val_loss: 1.6174288099738443; val_accuracy: 0.43998805732484075 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.7; acc: 0.41
Batch: 20; loss: 1.58; acc: 0.39
Batch: 40; loss: 1.57; acc: 0.47
Batch: 60; loss: 1.72; acc: 0.36
Batch: 80; loss: 1.46; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.53
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.61; acc: 0.44
Batch: 160; loss: 1.64; acc: 0.39
Batch: 180; loss: 1.56; acc: 0.41
Batch: 200; loss: 1.7; acc: 0.39
Batch: 220; loss: 1.78; acc: 0.47
Batch: 240; loss: 1.61; acc: 0.41
Batch: 260; loss: 1.75; acc: 0.41
Batch: 280; loss: 1.72; acc: 0.33
Batch: 300; loss: 1.55; acc: 0.47
Batch: 320; loss: 1.62; acc: 0.52
Batch: 340; loss: 1.54; acc: 0.47
Batch: 360; loss: 1.52; acc: 0.45
Batch: 380; loss: 1.66; acc: 0.45
Batch: 400; loss: 1.71; acc: 0.42
Batch: 420; loss: 1.52; acc: 0.44
Batch: 440; loss: 1.81; acc: 0.41
Batch: 460; loss: 1.77; acc: 0.34
Batch: 480; loss: 1.55; acc: 0.42
Batch: 500; loss: 1.62; acc: 0.38
Batch: 520; loss: 1.79; acc: 0.34
Batch: 540; loss: 1.82; acc: 0.36
Batch: 560; loss: 1.89; acc: 0.34
Batch: 580; loss: 1.8; acc: 0.38
Batch: 600; loss: 1.96; acc: 0.31
Batch: 620; loss: 1.92; acc: 0.41
Batch: 640; loss: 1.54; acc: 0.38
Batch: 660; loss: 1.7; acc: 0.38
Batch: 680; loss: 1.75; acc: 0.44
Batch: 700; loss: 1.59; acc: 0.42
Batch: 720; loss: 1.76; acc: 0.38
Batch: 740; loss: 1.57; acc: 0.44
Batch: 760; loss: 1.47; acc: 0.42
Batch: 780; loss: 1.39; acc: 0.48
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.618057487876552; val_accuracy: 0.4368033439490446 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.73; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.42
Batch: 40; loss: 1.66; acc: 0.41
Batch: 60; loss: 1.68; acc: 0.38
Batch: 80; loss: 1.53; acc: 0.47
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.39
Batch: 140; loss: 1.6; acc: 0.41
Batch: 160; loss: 1.59; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.39
Batch: 200; loss: 1.91; acc: 0.33
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.63; acc: 0.48
Batch: 260; loss: 1.53; acc: 0.44
Batch: 280; loss: 1.55; acc: 0.5
Batch: 300; loss: 1.73; acc: 0.31
Batch: 320; loss: 1.8; acc: 0.38
Batch: 340; loss: 1.58; acc: 0.52
Batch: 360; loss: 1.7; acc: 0.41
Batch: 380; loss: 1.64; acc: 0.45
Batch: 400; loss: 1.83; acc: 0.42
Batch: 420; loss: 1.72; acc: 0.39
Batch: 440; loss: 1.55; acc: 0.45
Batch: 460; loss: 1.6; acc: 0.41
Batch: 480; loss: 1.69; acc: 0.5
Batch: 500; loss: 1.47; acc: 0.56
Batch: 520; loss: 1.74; acc: 0.36
Batch: 540; loss: 1.56; acc: 0.47
Batch: 560; loss: 1.78; acc: 0.38
Batch: 580; loss: 1.54; acc: 0.5
Batch: 600; loss: 1.75; acc: 0.41
Batch: 620; loss: 1.68; acc: 0.45
Batch: 640; loss: 1.54; acc: 0.52
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.81; acc: 0.39
Batch: 700; loss: 1.67; acc: 0.41
Batch: 720; loss: 1.75; acc: 0.36
Batch: 740; loss: 1.64; acc: 0.41
Batch: 760; loss: 1.77; acc: 0.42
Batch: 780; loss: 1.7; acc: 0.41
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.37; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.42
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.617514658885397; val_accuracy: 0.4390923566878981 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.96; acc: 0.25
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.61; acc: 0.45
Batch: 80; loss: 1.48; acc: 0.58
Batch: 100; loss: 1.79; acc: 0.34
Batch: 120; loss: 1.73; acc: 0.42
Batch: 140; loss: 1.89; acc: 0.38
Batch: 160; loss: 1.61; acc: 0.45
Batch: 180; loss: 1.79; acc: 0.34
Batch: 200; loss: 1.81; acc: 0.39
Batch: 220; loss: 1.61; acc: 0.39
Batch: 240; loss: 1.56; acc: 0.45
Batch: 260; loss: 1.82; acc: 0.41
Batch: 280; loss: 1.63; acc: 0.44
Batch: 300; loss: 1.51; acc: 0.52
Batch: 320; loss: 1.55; acc: 0.47
Batch: 340; loss: 1.55; acc: 0.44
Batch: 360; loss: 1.57; acc: 0.45
Batch: 380; loss: 1.47; acc: 0.47
Batch: 400; loss: 1.74; acc: 0.38
Batch: 420; loss: 1.73; acc: 0.44
Batch: 440; loss: 1.8; acc: 0.38
Batch: 460; loss: 1.67; acc: 0.34
Batch: 480; loss: 1.76; acc: 0.34
Batch: 500; loss: 1.56; acc: 0.48
Batch: 520; loss: 1.56; acc: 0.47
Batch: 540; loss: 1.74; acc: 0.36
Batch: 560; loss: 1.8; acc: 0.39
Batch: 580; loss: 1.68; acc: 0.33
Batch: 600; loss: 1.8; acc: 0.41
Batch: 620; loss: 1.51; acc: 0.47
Batch: 640; loss: 1.61; acc: 0.38
Batch: 660; loss: 1.68; acc: 0.45
Batch: 680; loss: 1.5; acc: 0.55
Batch: 700; loss: 1.83; acc: 0.41
Batch: 720; loss: 1.54; acc: 0.5
Batch: 740; loss: 1.93; acc: 0.34
Batch: 760; loss: 1.49; acc: 0.53
Batch: 780; loss: 1.77; acc: 0.38
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6177098227154678; val_accuracy: 0.4384952229299363 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.74; acc: 0.38
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 1.61; acc: 0.42
Batch: 60; loss: 1.77; acc: 0.44
Batch: 80; loss: 1.52; acc: 0.5
Batch: 100; loss: 1.68; acc: 0.44
Batch: 120; loss: 1.52; acc: 0.44
Batch: 140; loss: 1.66; acc: 0.42
Batch: 160; loss: 1.68; acc: 0.39
Batch: 180; loss: 1.58; acc: 0.41
Batch: 200; loss: 1.57; acc: 0.44
Batch: 220; loss: 1.63; acc: 0.38
Batch: 240; loss: 1.6; acc: 0.41
Batch: 260; loss: 1.54; acc: 0.47
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.57; acc: 0.44
Batch: 320; loss: 1.44; acc: 0.52
Batch: 340; loss: 1.76; acc: 0.44
Batch: 360; loss: 1.85; acc: 0.39
Batch: 380; loss: 1.74; acc: 0.39
Batch: 400; loss: 1.77; acc: 0.36
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.73; acc: 0.33
Batch: 460; loss: 1.65; acc: 0.39
Batch: 480; loss: 1.58; acc: 0.44
Batch: 500; loss: 1.67; acc: 0.39
Batch: 520; loss: 1.49; acc: 0.48
Batch: 540; loss: 1.66; acc: 0.44
Batch: 560; loss: 1.71; acc: 0.41
Batch: 580; loss: 1.53; acc: 0.45
Batch: 600; loss: 1.67; acc: 0.52
Batch: 620; loss: 1.67; acc: 0.39
Batch: 640; loss: 1.68; acc: 0.44
Batch: 660; loss: 1.56; acc: 0.42
Batch: 680; loss: 1.61; acc: 0.42
Batch: 700; loss: 1.68; acc: 0.41
Batch: 720; loss: 1.63; acc: 0.48
Batch: 740; loss: 1.53; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.74; acc: 0.38
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.37; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.6; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.48
Val Epoch over. val_loss: 1.6183227649919547; val_accuracy: 0.4365047770700637 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.63; acc: 0.47
Batch: 40; loss: 1.41; acc: 0.61
Batch: 60; loss: 1.96; acc: 0.38
Batch: 80; loss: 1.51; acc: 0.45
Batch: 100; loss: 1.67; acc: 0.39
Batch: 120; loss: 1.59; acc: 0.48
Batch: 140; loss: 1.58; acc: 0.47
Batch: 160; loss: 1.49; acc: 0.5
Batch: 180; loss: 1.73; acc: 0.34
Batch: 200; loss: 1.52; acc: 0.38
Batch: 220; loss: 1.55; acc: 0.48
Batch: 240; loss: 1.7; acc: 0.44
Batch: 260; loss: 1.62; acc: 0.41
Batch: 280; loss: 1.64; acc: 0.44
Batch: 300; loss: 1.56; acc: 0.45
Batch: 320; loss: 1.54; acc: 0.48
Batch: 340; loss: 1.6; acc: 0.47
Batch: 360; loss: 1.66; acc: 0.39
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.81; acc: 0.39
Batch: 460; loss: 1.86; acc: 0.39
Batch: 480; loss: 1.56; acc: 0.5
Batch: 500; loss: 1.78; acc: 0.38
Batch: 520; loss: 1.79; acc: 0.38
Batch: 540; loss: 1.74; acc: 0.41
Batch: 560; loss: 1.57; acc: 0.47
Batch: 580; loss: 1.43; acc: 0.48
Batch: 600; loss: 1.73; acc: 0.38
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.6; acc: 0.39
Batch: 660; loss: 1.6; acc: 0.53
Batch: 680; loss: 1.8; acc: 0.36
Batch: 700; loss: 1.68; acc: 0.39
Batch: 720; loss: 1.76; acc: 0.45
Batch: 740; loss: 1.72; acc: 0.38
Batch: 760; loss: 1.4; acc: 0.52
Batch: 780; loss: 1.63; acc: 0.42
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.617400650006191; val_accuracy: 0.43829617834394907 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.81; acc: 0.33
Batch: 20; loss: 1.66; acc: 0.33
Batch: 40; loss: 1.91; acc: 0.45
Batch: 60; loss: 1.67; acc: 0.36
Batch: 80; loss: 1.55; acc: 0.42
Batch: 100; loss: 1.82; acc: 0.33
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 1.43; acc: 0.45
Batch: 160; loss: 1.71; acc: 0.42
Batch: 180; loss: 1.51; acc: 0.48
Batch: 200; loss: 1.75; acc: 0.33
Batch: 220; loss: 1.58; acc: 0.41
Batch: 240; loss: 1.81; acc: 0.41
Batch: 260; loss: 1.71; acc: 0.38
Batch: 280; loss: 1.77; acc: 0.34
Batch: 300; loss: 1.76; acc: 0.38
Batch: 320; loss: 1.6; acc: 0.44
Batch: 340; loss: 1.53; acc: 0.45
Batch: 360; loss: 1.8; acc: 0.33
Batch: 380; loss: 1.62; acc: 0.38
Batch: 400; loss: 1.68; acc: 0.44
Batch: 420; loss: 1.82; acc: 0.36
Batch: 440; loss: 1.63; acc: 0.44
Batch: 460; loss: 1.58; acc: 0.39
Batch: 480; loss: 1.56; acc: 0.52
Batch: 500; loss: 1.71; acc: 0.45
Batch: 520; loss: 1.77; acc: 0.36
Batch: 540; loss: 1.47; acc: 0.44
Batch: 560; loss: 1.66; acc: 0.41
Batch: 580; loss: 1.72; acc: 0.41
Batch: 600; loss: 1.9; acc: 0.42
Batch: 620; loss: 2.0; acc: 0.36
Batch: 640; loss: 1.69; acc: 0.42
Batch: 660; loss: 1.86; acc: 0.31
Batch: 680; loss: 1.61; acc: 0.56
Batch: 700; loss: 1.84; acc: 0.31
Batch: 720; loss: 1.47; acc: 0.52
Batch: 740; loss: 1.45; acc: 0.5
Batch: 760; loss: 1.5; acc: 0.52
Batch: 780; loss: 1.66; acc: 0.45
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174632181787187; val_accuracy: 0.43789808917197454 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.68; acc: 0.41
Batch: 20; loss: 1.56; acc: 0.45
Batch: 40; loss: 1.67; acc: 0.41
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.59; acc: 0.44
Batch: 100; loss: 1.73; acc: 0.36
Batch: 120; loss: 1.63; acc: 0.42
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.81; acc: 0.33
Batch: 180; loss: 1.61; acc: 0.45
Batch: 200; loss: 1.76; acc: 0.39
Batch: 220; loss: 1.52; acc: 0.44
Batch: 240; loss: 1.77; acc: 0.31
Batch: 260; loss: 1.82; acc: 0.36
Batch: 280; loss: 1.79; acc: 0.44
Batch: 300; loss: 1.65; acc: 0.41
Batch: 320; loss: 1.7; acc: 0.39
Batch: 340; loss: 1.81; acc: 0.36
Batch: 360; loss: 1.47; acc: 0.45
Batch: 380; loss: 1.94; acc: 0.25
Batch: 400; loss: 1.68; acc: 0.42
Batch: 420; loss: 1.55; acc: 0.45
Batch: 440; loss: 1.78; acc: 0.34
Batch: 460; loss: 1.82; acc: 0.36
Batch: 480; loss: 1.69; acc: 0.38
Batch: 500; loss: 1.74; acc: 0.3
Batch: 520; loss: 1.71; acc: 0.39
Batch: 540; loss: 1.64; acc: 0.42
Batch: 560; loss: 1.68; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.33
Batch: 600; loss: 1.35; acc: 0.58
Batch: 620; loss: 1.51; acc: 0.39
Batch: 640; loss: 1.52; acc: 0.48
Batch: 660; loss: 1.61; acc: 0.45
Batch: 680; loss: 1.73; acc: 0.34
Batch: 700; loss: 1.68; acc: 0.42
Batch: 720; loss: 1.9; acc: 0.28
Batch: 740; loss: 1.61; acc: 0.47
Batch: 760; loss: 1.62; acc: 0.45
Batch: 780; loss: 1.56; acc: 0.41
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174878777971693; val_accuracy: 0.43889331210191085 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.75; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.34; acc: 0.59
Batch: 60; loss: 1.75; acc: 0.41
Batch: 80; loss: 1.66; acc: 0.42
Batch: 100; loss: 1.52; acc: 0.45
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.44
Batch: 160; loss: 1.69; acc: 0.36
Batch: 180; loss: 1.41; acc: 0.5
Batch: 200; loss: 1.67; acc: 0.41
Batch: 220; loss: 1.67; acc: 0.36
Batch: 240; loss: 1.48; acc: 0.44
Batch: 260; loss: 1.77; acc: 0.41
Batch: 280; loss: 1.72; acc: 0.52
Batch: 300; loss: 1.63; acc: 0.45
Batch: 320; loss: 1.76; acc: 0.38
Batch: 340; loss: 1.68; acc: 0.52
Batch: 360; loss: 1.76; acc: 0.44
Batch: 380; loss: 1.69; acc: 0.44
Batch: 400; loss: 1.74; acc: 0.41
Batch: 420; loss: 1.79; acc: 0.36
Batch: 440; loss: 1.57; acc: 0.44
Batch: 460; loss: 1.78; acc: 0.44
Batch: 480; loss: 1.56; acc: 0.42
Batch: 500; loss: 1.79; acc: 0.3
Batch: 520; loss: 1.8; acc: 0.36
Batch: 540; loss: 1.48; acc: 0.48
Batch: 560; loss: 1.64; acc: 0.42
Batch: 580; loss: 1.78; acc: 0.42
Batch: 600; loss: 1.6; acc: 0.38
Batch: 620; loss: 1.44; acc: 0.5
Batch: 640; loss: 1.47; acc: 0.5
Batch: 660; loss: 1.89; acc: 0.34
Batch: 680; loss: 1.72; acc: 0.36
Batch: 700; loss: 1.28; acc: 0.61
Batch: 720; loss: 1.57; acc: 0.42
Batch: 740; loss: 1.56; acc: 0.5
Batch: 760; loss: 1.63; acc: 0.38
Batch: 780; loss: 1.75; acc: 0.33
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174238851875256; val_accuracy: 0.43829617834394907 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.83; acc: 0.33
Batch: 20; loss: 1.74; acc: 0.41
Batch: 40; loss: 1.7; acc: 0.39
Batch: 60; loss: 1.67; acc: 0.42
Batch: 80; loss: 1.54; acc: 0.45
Batch: 100; loss: 1.7; acc: 0.38
Batch: 120; loss: 1.84; acc: 0.34
Batch: 140; loss: 1.61; acc: 0.44
Batch: 160; loss: 1.7; acc: 0.38
Batch: 180; loss: 1.68; acc: 0.39
Batch: 200; loss: 1.87; acc: 0.38
Batch: 220; loss: 1.63; acc: 0.41
Batch: 240; loss: 1.55; acc: 0.44
Batch: 260; loss: 1.52; acc: 0.41
Batch: 280; loss: 1.46; acc: 0.41
Batch: 300; loss: 1.79; acc: 0.38
Batch: 320; loss: 1.46; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.45
Batch: 360; loss: 1.66; acc: 0.41
Batch: 380; loss: 1.65; acc: 0.44
Batch: 400; loss: 1.63; acc: 0.47
Batch: 420; loss: 1.53; acc: 0.47
Batch: 440; loss: 1.62; acc: 0.48
Batch: 460; loss: 1.62; acc: 0.34
Batch: 480; loss: 1.67; acc: 0.45
Batch: 500; loss: 1.61; acc: 0.45
Batch: 520; loss: 1.7; acc: 0.41
Batch: 540; loss: 1.83; acc: 0.36
Batch: 560; loss: 1.66; acc: 0.44
Batch: 580; loss: 1.52; acc: 0.45
Batch: 600; loss: 1.3; acc: 0.58
Batch: 620; loss: 1.52; acc: 0.47
Batch: 640; loss: 1.68; acc: 0.38
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.52; acc: 0.48
Batch: 700; loss: 1.78; acc: 0.39
Batch: 720; loss: 1.72; acc: 0.36
Batch: 740; loss: 1.73; acc: 0.42
Batch: 760; loss: 1.83; acc: 0.36
Batch: 780; loss: 1.66; acc: 0.38
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.48
Val Epoch over. val_loss: 1.6174815635012973; val_accuracy: 0.43789808917197454 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.63; acc: 0.36
Batch: 20; loss: 1.62; acc: 0.42
Batch: 40; loss: 1.6; acc: 0.47
Batch: 60; loss: 1.85; acc: 0.44
Batch: 80; loss: 1.9; acc: 0.38
Batch: 100; loss: 1.74; acc: 0.41
Batch: 120; loss: 1.79; acc: 0.39
Batch: 140; loss: 1.56; acc: 0.34
Batch: 160; loss: 1.86; acc: 0.34
Batch: 180; loss: 1.98; acc: 0.38
Batch: 200; loss: 1.61; acc: 0.41
Batch: 220; loss: 1.72; acc: 0.42
Batch: 240; loss: 1.86; acc: 0.42
Batch: 260; loss: 1.65; acc: 0.44
Batch: 280; loss: 1.75; acc: 0.44
Batch: 300; loss: 1.65; acc: 0.41
Batch: 320; loss: 1.81; acc: 0.34
Batch: 340; loss: 1.53; acc: 0.5
Batch: 360; loss: 1.66; acc: 0.41
Batch: 380; loss: 1.33; acc: 0.55
Batch: 400; loss: 1.48; acc: 0.45
Batch: 420; loss: 1.64; acc: 0.38
Batch: 440; loss: 1.84; acc: 0.42
Batch: 460; loss: 1.68; acc: 0.42
Batch: 480; loss: 1.66; acc: 0.39
Batch: 500; loss: 1.82; acc: 0.3
Batch: 520; loss: 1.65; acc: 0.45
Batch: 540; loss: 1.8; acc: 0.31
Batch: 560; loss: 1.5; acc: 0.53
Batch: 580; loss: 1.51; acc: 0.5
Batch: 600; loss: 1.81; acc: 0.39
Batch: 620; loss: 1.84; acc: 0.38
Batch: 640; loss: 1.68; acc: 0.44
Batch: 660; loss: 1.75; acc: 0.36
Batch: 680; loss: 1.61; acc: 0.45
Batch: 700; loss: 1.7; acc: 0.39
Batch: 720; loss: 1.64; acc: 0.42
Batch: 740; loss: 1.58; acc: 0.42
Batch: 760; loss: 2.01; acc: 0.36
Batch: 780; loss: 1.85; acc: 0.33
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.48
Val Epoch over. val_loss: 1.617689210897798; val_accuracy: 0.4381966560509554 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.87; acc: 0.34
Batch: 40; loss: 1.75; acc: 0.41
Batch: 60; loss: 1.46; acc: 0.48
Batch: 80; loss: 1.54; acc: 0.48
Batch: 100; loss: 1.89; acc: 0.3
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.75; acc: 0.33
Batch: 160; loss: 1.43; acc: 0.5
Batch: 180; loss: 1.76; acc: 0.3
Batch: 200; loss: 1.34; acc: 0.52
Batch: 220; loss: 1.75; acc: 0.39
Batch: 240; loss: 1.62; acc: 0.44
Batch: 260; loss: 1.55; acc: 0.42
Batch: 280; loss: 1.87; acc: 0.31
Batch: 300; loss: 1.81; acc: 0.31
Batch: 320; loss: 1.62; acc: 0.47
Batch: 340; loss: 1.74; acc: 0.38
Batch: 360; loss: 1.86; acc: 0.33
Batch: 380; loss: 1.62; acc: 0.45
Batch: 400; loss: 1.57; acc: 0.47
Batch: 420; loss: 1.48; acc: 0.41
Batch: 440; loss: 1.47; acc: 0.47
Batch: 460; loss: 1.29; acc: 0.61
Batch: 480; loss: 1.65; acc: 0.39
Batch: 500; loss: 1.56; acc: 0.52
Batch: 520; loss: 1.73; acc: 0.34
Batch: 540; loss: 1.5; acc: 0.44
Batch: 560; loss: 1.56; acc: 0.44
Batch: 580; loss: 1.52; acc: 0.52
Batch: 600; loss: 1.7; acc: 0.45
Batch: 620; loss: 1.65; acc: 0.41
Batch: 640; loss: 1.74; acc: 0.42
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.53; acc: 0.5
Batch: 700; loss: 1.77; acc: 0.44
Batch: 720; loss: 1.69; acc: 0.41
Batch: 740; loss: 1.63; acc: 0.41
Batch: 760; loss: 1.6; acc: 0.44
Batch: 780; loss: 1.58; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175155685206128; val_accuracy: 0.43829617834394907 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.54; acc: 0.44
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.77; acc: 0.39
Batch: 60; loss: 1.56; acc: 0.45
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.58; acc: 0.47
Batch: 120; loss: 1.72; acc: 0.34
Batch: 140; loss: 1.34; acc: 0.52
Batch: 160; loss: 1.4; acc: 0.58
Batch: 180; loss: 1.63; acc: 0.44
Batch: 200; loss: 1.76; acc: 0.38
Batch: 220; loss: 1.73; acc: 0.33
Batch: 240; loss: 1.65; acc: 0.44
Batch: 260; loss: 1.69; acc: 0.33
Batch: 280; loss: 1.71; acc: 0.38
Batch: 300; loss: 1.52; acc: 0.45
Batch: 320; loss: 1.65; acc: 0.41
Batch: 340; loss: 1.47; acc: 0.45
Batch: 360; loss: 1.85; acc: 0.34
Batch: 380; loss: 1.68; acc: 0.39
Batch: 400; loss: 1.69; acc: 0.39
Batch: 420; loss: 1.78; acc: 0.41
Batch: 440; loss: 1.71; acc: 0.39
Batch: 460; loss: 1.66; acc: 0.44
Batch: 480; loss: 1.8; acc: 0.34
Batch: 500; loss: 1.56; acc: 0.53
Batch: 520; loss: 1.76; acc: 0.39
Batch: 540; loss: 1.7; acc: 0.42
Batch: 560; loss: 1.73; acc: 0.31
Batch: 580; loss: 1.74; acc: 0.47
Batch: 600; loss: 1.63; acc: 0.42
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.8; acc: 0.38
Batch: 660; loss: 1.66; acc: 0.42
Batch: 680; loss: 1.81; acc: 0.39
Batch: 700; loss: 1.62; acc: 0.42
Batch: 720; loss: 1.83; acc: 0.33
Batch: 740; loss: 1.77; acc: 0.42
Batch: 760; loss: 1.7; acc: 0.38
Batch: 780; loss: 1.72; acc: 0.41
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175921411271308; val_accuracy: 0.4375 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.49; acc: 0.38
Batch: 20; loss: 1.47; acc: 0.5
Batch: 40; loss: 1.67; acc: 0.39
Batch: 60; loss: 1.58; acc: 0.39
Batch: 80; loss: 1.69; acc: 0.34
Batch: 100; loss: 1.77; acc: 0.39
Batch: 120; loss: 1.88; acc: 0.38
Batch: 140; loss: 1.7; acc: 0.36
Batch: 160; loss: 1.61; acc: 0.41
Batch: 180; loss: 1.62; acc: 0.47
Batch: 200; loss: 1.71; acc: 0.45
Batch: 220; loss: 1.43; acc: 0.48
Batch: 240; loss: 1.71; acc: 0.41
Batch: 260; loss: 1.81; acc: 0.38
Batch: 280; loss: 1.31; acc: 0.64
Batch: 300; loss: 1.64; acc: 0.42
Batch: 320; loss: 1.55; acc: 0.45
Batch: 340; loss: 1.65; acc: 0.42
Batch: 360; loss: 1.61; acc: 0.44
Batch: 380; loss: 1.93; acc: 0.28
Batch: 400; loss: 1.53; acc: 0.47
Batch: 420; loss: 1.65; acc: 0.44
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.73; acc: 0.34
Batch: 480; loss: 1.89; acc: 0.38
Batch: 500; loss: 1.6; acc: 0.42
Batch: 520; loss: 1.54; acc: 0.5
Batch: 540; loss: 1.76; acc: 0.47
Batch: 560; loss: 1.33; acc: 0.55
Batch: 580; loss: 1.67; acc: 0.44
Batch: 600; loss: 1.76; acc: 0.39
Batch: 620; loss: 1.48; acc: 0.52
Batch: 640; loss: 1.71; acc: 0.45
Batch: 660; loss: 1.77; acc: 0.42
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.69; acc: 0.38
Batch: 720; loss: 1.71; acc: 0.42
Batch: 740; loss: 1.6; acc: 0.48
Batch: 760; loss: 1.71; acc: 0.39
Batch: 780; loss: 1.88; acc: 0.33
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175193353823036; val_accuracy: 0.43889331210191085 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.56; acc: 0.47
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.76; acc: 0.42
Batch: 60; loss: 1.61; acc: 0.47
Batch: 80; loss: 1.78; acc: 0.31
Batch: 100; loss: 1.76; acc: 0.44
Batch: 120; loss: 1.54; acc: 0.47
Batch: 140; loss: 1.53; acc: 0.48
Batch: 160; loss: 1.69; acc: 0.38
Batch: 180; loss: 1.89; acc: 0.31
Batch: 200; loss: 1.77; acc: 0.44
Batch: 220; loss: 1.76; acc: 0.42
Batch: 240; loss: 1.57; acc: 0.41
Batch: 260; loss: 1.68; acc: 0.44
Batch: 280; loss: 1.63; acc: 0.47
Batch: 300; loss: 1.51; acc: 0.48
Batch: 320; loss: 1.72; acc: 0.5
Batch: 340; loss: 1.7; acc: 0.42
Batch: 360; loss: 1.81; acc: 0.41
Batch: 380; loss: 1.72; acc: 0.42
Batch: 400; loss: 1.8; acc: 0.33
Batch: 420; loss: 1.62; acc: 0.41
Batch: 440; loss: 1.68; acc: 0.44
Batch: 460; loss: 1.55; acc: 0.41
Batch: 480; loss: 1.61; acc: 0.47
Batch: 500; loss: 1.59; acc: 0.47
Batch: 520; loss: 1.51; acc: 0.47
Batch: 540; loss: 1.77; acc: 0.36
Batch: 560; loss: 1.67; acc: 0.47
Batch: 580; loss: 1.61; acc: 0.47
Batch: 600; loss: 1.65; acc: 0.39
Batch: 620; loss: 1.51; acc: 0.47
Batch: 640; loss: 1.75; acc: 0.38
Batch: 660; loss: 1.78; acc: 0.28
Batch: 680; loss: 1.4; acc: 0.55
Batch: 700; loss: 1.53; acc: 0.47
Batch: 720; loss: 1.7; acc: 0.38
Batch: 740; loss: 1.54; acc: 0.47
Batch: 760; loss: 1.66; acc: 0.38
Batch: 780; loss: 1.6; acc: 0.38
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175988130508714; val_accuracy: 0.4375 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.87; acc: 0.39
Batch: 20; loss: 1.7; acc: 0.38
Batch: 40; loss: 1.72; acc: 0.36
Batch: 60; loss: 1.68; acc: 0.45
Batch: 80; loss: 1.71; acc: 0.39
Batch: 100; loss: 1.52; acc: 0.47
Batch: 120; loss: 1.58; acc: 0.44
Batch: 140; loss: 1.78; acc: 0.38
Batch: 160; loss: 1.48; acc: 0.47
Batch: 180; loss: 1.81; acc: 0.31
Batch: 200; loss: 1.79; acc: 0.36
Batch: 220; loss: 1.64; acc: 0.44
Batch: 240; loss: 1.71; acc: 0.44
Batch: 260; loss: 1.6; acc: 0.56
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.58; acc: 0.47
Batch: 320; loss: 1.66; acc: 0.33
Batch: 340; loss: 1.52; acc: 0.5
Batch: 360; loss: 1.86; acc: 0.36
Batch: 380; loss: 1.65; acc: 0.42
Batch: 400; loss: 1.67; acc: 0.41
Batch: 420; loss: 1.64; acc: 0.39
Batch: 440; loss: 1.76; acc: 0.36
Batch: 460; loss: 1.73; acc: 0.39
Batch: 480; loss: 1.65; acc: 0.39
Batch: 500; loss: 1.85; acc: 0.34
Batch: 520; loss: 1.64; acc: 0.41
Batch: 540; loss: 1.7; acc: 0.44
Batch: 560; loss: 1.65; acc: 0.45
Batch: 580; loss: 1.58; acc: 0.39
Batch: 600; loss: 1.65; acc: 0.42
Batch: 620; loss: 1.54; acc: 0.47
Batch: 640; loss: 1.73; acc: 0.39
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.72; acc: 0.41
Batch: 700; loss: 1.68; acc: 0.44
Batch: 720; loss: 1.66; acc: 0.44
Batch: 740; loss: 1.7; acc: 0.44
Batch: 760; loss: 1.76; acc: 0.38
Batch: 780; loss: 1.85; acc: 0.36
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175110552720964; val_accuracy: 0.43730095541401276 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.67; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.38
Batch: 40; loss: 1.66; acc: 0.47
Batch: 60; loss: 1.56; acc: 0.45
Batch: 80; loss: 1.51; acc: 0.41
Batch: 100; loss: 1.56; acc: 0.38
Batch: 120; loss: 1.73; acc: 0.38
Batch: 140; loss: 1.7; acc: 0.36
Batch: 160; loss: 1.65; acc: 0.41
Batch: 180; loss: 1.53; acc: 0.39
Batch: 200; loss: 1.62; acc: 0.47
Batch: 220; loss: 1.62; acc: 0.44
Batch: 240; loss: 1.52; acc: 0.48
Batch: 260; loss: 1.65; acc: 0.44
Batch: 280; loss: 1.69; acc: 0.39
Batch: 300; loss: 1.67; acc: 0.44
Batch: 320; loss: 1.7; acc: 0.39
Batch: 340; loss: 1.66; acc: 0.38
Batch: 360; loss: 1.63; acc: 0.5
Batch: 380; loss: 1.67; acc: 0.41
Batch: 400; loss: 1.74; acc: 0.34
Batch: 420; loss: 1.65; acc: 0.47
Batch: 440; loss: 1.79; acc: 0.33
Batch: 460; loss: 1.81; acc: 0.41
Batch: 480; loss: 1.84; acc: 0.34
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.66; acc: 0.44
Batch: 540; loss: 1.78; acc: 0.39
Batch: 560; loss: 1.5; acc: 0.48
Batch: 580; loss: 1.72; acc: 0.36
Batch: 600; loss: 1.68; acc: 0.42
Batch: 620; loss: 1.65; acc: 0.38
Batch: 640; loss: 1.75; acc: 0.44
Batch: 660; loss: 1.46; acc: 0.55
Batch: 680; loss: 1.6; acc: 0.41
Batch: 700; loss: 1.82; acc: 0.34
Batch: 720; loss: 1.87; acc: 0.33
Batch: 740; loss: 1.55; acc: 0.44
Batch: 760; loss: 1.59; acc: 0.47
Batch: 780; loss: 1.73; acc: 0.34
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175238372413976; val_accuracy: 0.43710191082802546 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.8; acc: 0.33
Batch: 40; loss: 1.69; acc: 0.44
Batch: 60; loss: 1.64; acc: 0.39
Batch: 80; loss: 1.71; acc: 0.42
Batch: 100; loss: 1.64; acc: 0.42
Batch: 120; loss: 1.9; acc: 0.39
Batch: 140; loss: 1.53; acc: 0.44
Batch: 160; loss: 1.61; acc: 0.38
Batch: 180; loss: 1.78; acc: 0.39
Batch: 200; loss: 1.69; acc: 0.41
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.88; acc: 0.31
Batch: 260; loss: 1.45; acc: 0.47
Batch: 280; loss: 1.74; acc: 0.41
Batch: 300; loss: 1.6; acc: 0.38
Batch: 320; loss: 1.77; acc: 0.36
Batch: 340; loss: 1.6; acc: 0.5
Batch: 360; loss: 1.67; acc: 0.52
Batch: 380; loss: 1.49; acc: 0.42
Batch: 400; loss: 1.7; acc: 0.38
Batch: 420; loss: 1.78; acc: 0.47
Batch: 440; loss: 1.49; acc: 0.47
Batch: 460; loss: 1.47; acc: 0.41
Batch: 480; loss: 1.58; acc: 0.42
Batch: 500; loss: 1.71; acc: 0.45
Batch: 520; loss: 1.62; acc: 0.44
Batch: 540; loss: 1.61; acc: 0.45
Batch: 560; loss: 1.72; acc: 0.42
Batch: 580; loss: 1.61; acc: 0.45
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.5; acc: 0.5
Batch: 640; loss: 1.85; acc: 0.34
Batch: 660; loss: 1.63; acc: 0.42
Batch: 680; loss: 1.51; acc: 0.58
Batch: 700; loss: 1.63; acc: 0.48
Batch: 720; loss: 1.48; acc: 0.44
Batch: 740; loss: 1.56; acc: 0.5
Batch: 760; loss: 1.67; acc: 0.41
Batch: 780; loss: 1.7; acc: 0.33
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175369806350417; val_accuracy: 0.43740047770700635 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.48; acc: 0.42
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.69; acc: 0.39
Batch: 60; loss: 1.68; acc: 0.44
Batch: 80; loss: 1.74; acc: 0.39
Batch: 100; loss: 1.65; acc: 0.45
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.79; acc: 0.34
Batch: 160; loss: 1.63; acc: 0.42
Batch: 180; loss: 1.72; acc: 0.41
Batch: 200; loss: 1.67; acc: 0.42
Batch: 220; loss: 1.89; acc: 0.3
Batch: 240; loss: 1.68; acc: 0.36
Batch: 260; loss: 1.83; acc: 0.36
Batch: 280; loss: 1.47; acc: 0.52
Batch: 300; loss: 1.52; acc: 0.56
Batch: 320; loss: 1.92; acc: 0.33
Batch: 340; loss: 1.68; acc: 0.39
Batch: 360; loss: 1.43; acc: 0.52
Batch: 380; loss: 1.63; acc: 0.39
Batch: 400; loss: 1.78; acc: 0.38
Batch: 420; loss: 1.8; acc: 0.34
Batch: 440; loss: 1.53; acc: 0.42
Batch: 460; loss: 1.74; acc: 0.34
Batch: 480; loss: 1.59; acc: 0.55
Batch: 500; loss: 1.74; acc: 0.36
Batch: 520; loss: 1.45; acc: 0.53
Batch: 540; loss: 1.73; acc: 0.41
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.6; acc: 0.44
Batch: 600; loss: 1.71; acc: 0.34
Batch: 620; loss: 1.56; acc: 0.52
Batch: 640; loss: 1.47; acc: 0.44
Batch: 660; loss: 1.6; acc: 0.45
Batch: 680; loss: 1.73; acc: 0.41
Batch: 700; loss: 1.72; acc: 0.39
Batch: 720; loss: 1.69; acc: 0.36
Batch: 740; loss: 1.66; acc: 0.41
Batch: 760; loss: 1.73; acc: 0.41
Batch: 780; loss: 1.75; acc: 0.34
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174937425904972; val_accuracy: 0.43799761146496813 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.72; acc: 0.31
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.69; acc: 0.45
Batch: 60; loss: 1.71; acc: 0.38
Batch: 80; loss: 1.61; acc: 0.45
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.84; acc: 0.31
Batch: 140; loss: 1.59; acc: 0.45
Batch: 160; loss: 1.51; acc: 0.48
Batch: 180; loss: 1.43; acc: 0.47
Batch: 200; loss: 1.69; acc: 0.42
Batch: 220; loss: 1.61; acc: 0.44
Batch: 240; loss: 1.63; acc: 0.47
Batch: 260; loss: 1.54; acc: 0.47
Batch: 280; loss: 1.7; acc: 0.47
Batch: 300; loss: 1.73; acc: 0.34
Batch: 320; loss: 1.65; acc: 0.44
Batch: 340; loss: 1.46; acc: 0.55
Batch: 360; loss: 1.6; acc: 0.39
Batch: 380; loss: 1.65; acc: 0.47
Batch: 400; loss: 1.48; acc: 0.48
Batch: 420; loss: 1.39; acc: 0.44
Batch: 440; loss: 1.58; acc: 0.45
Batch: 460; loss: 1.77; acc: 0.39
Batch: 480; loss: 1.53; acc: 0.47
Batch: 500; loss: 1.72; acc: 0.39
Batch: 520; loss: 1.66; acc: 0.36
Batch: 540; loss: 1.42; acc: 0.53
Batch: 560; loss: 1.6; acc: 0.44
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.66; acc: 0.44
Batch: 620; loss: 1.63; acc: 0.44
Batch: 640; loss: 1.64; acc: 0.47
Batch: 660; loss: 1.38; acc: 0.47
Batch: 680; loss: 1.89; acc: 0.39
Batch: 700; loss: 1.9; acc: 0.39
Batch: 720; loss: 1.99; acc: 0.27
Batch: 740; loss: 1.8; acc: 0.36
Batch: 760; loss: 1.68; acc: 0.31
Batch: 780; loss: 1.49; acc: 0.52
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174333566313337; val_accuracy: 0.4380971337579618 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.65; acc: 0.39
Batch: 40; loss: 1.6; acc: 0.42
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.37; acc: 0.52
Batch: 100; loss: 1.73; acc: 0.39
Batch: 120; loss: 1.5; acc: 0.52
Batch: 140; loss: 1.58; acc: 0.48
Batch: 160; loss: 1.41; acc: 0.53
Batch: 180; loss: 1.73; acc: 0.45
Batch: 200; loss: 1.65; acc: 0.42
Batch: 220; loss: 1.72; acc: 0.42
Batch: 240; loss: 1.61; acc: 0.41
Batch: 260; loss: 1.5; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.44
Batch: 300; loss: 1.79; acc: 0.45
Batch: 320; loss: 1.54; acc: 0.44
Batch: 340; loss: 1.54; acc: 0.45
Batch: 360; loss: 1.71; acc: 0.44
Batch: 380; loss: 1.72; acc: 0.3
Batch: 400; loss: 1.61; acc: 0.47
Batch: 420; loss: 1.78; acc: 0.38
Batch: 440; loss: 1.47; acc: 0.56
Batch: 460; loss: 1.51; acc: 0.47
Batch: 480; loss: 1.51; acc: 0.52
Batch: 500; loss: 1.63; acc: 0.34
Batch: 520; loss: 1.67; acc: 0.42
Batch: 540; loss: 1.77; acc: 0.41
Batch: 560; loss: 1.57; acc: 0.48
Batch: 580; loss: 1.76; acc: 0.44
Batch: 600; loss: 1.6; acc: 0.34
Batch: 620; loss: 1.8; acc: 0.38
Batch: 640; loss: 1.77; acc: 0.41
Batch: 660; loss: 1.73; acc: 0.34
Batch: 680; loss: 1.7; acc: 0.36
Batch: 700; loss: 1.43; acc: 0.55
Batch: 720; loss: 1.7; acc: 0.41
Batch: 740; loss: 1.53; acc: 0.44
Batch: 760; loss: 1.78; acc: 0.34
Batch: 780; loss: 1.28; acc: 0.55
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174572903639193; val_accuracy: 0.43829617834394907 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.5; acc: 0.48
Batch: 20; loss: 1.77; acc: 0.38
Batch: 40; loss: 1.56; acc: 0.45
Batch: 60; loss: 1.74; acc: 0.41
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.64; acc: 0.42
Batch: 120; loss: 1.52; acc: 0.45
Batch: 140; loss: 1.56; acc: 0.44
Batch: 160; loss: 1.78; acc: 0.42
Batch: 180; loss: 1.66; acc: 0.42
Batch: 200; loss: 1.96; acc: 0.36
Batch: 220; loss: 1.7; acc: 0.39
Batch: 240; loss: 1.66; acc: 0.39
Batch: 260; loss: 1.57; acc: 0.39
Batch: 280; loss: 1.6; acc: 0.39
Batch: 300; loss: 1.6; acc: 0.39
Batch: 320; loss: 1.52; acc: 0.56
Batch: 340; loss: 1.5; acc: 0.48
Batch: 360; loss: 1.6; acc: 0.45
Batch: 380; loss: 1.7; acc: 0.34
Batch: 400; loss: 1.68; acc: 0.38
Batch: 420; loss: 1.86; acc: 0.36
Batch: 440; loss: 1.67; acc: 0.44
Batch: 460; loss: 1.96; acc: 0.38
Batch: 480; loss: 1.53; acc: 0.53
Batch: 500; loss: 1.78; acc: 0.5
Batch: 520; loss: 1.43; acc: 0.48
Batch: 540; loss: 1.79; acc: 0.31
Batch: 560; loss: 1.93; acc: 0.34
Batch: 580; loss: 1.71; acc: 0.41
Batch: 600; loss: 1.44; acc: 0.42
Batch: 620; loss: 1.69; acc: 0.36
Batch: 640; loss: 1.75; acc: 0.36
Batch: 660; loss: 1.67; acc: 0.42
Batch: 680; loss: 1.67; acc: 0.41
Batch: 700; loss: 1.78; acc: 0.34
Batch: 720; loss: 1.77; acc: 0.33
Batch: 740; loss: 1.62; acc: 0.47
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.7; acc: 0.39
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175212184335017; val_accuracy: 0.43769904458598724 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 1.6; acc: 0.47
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.83; acc: 0.39
Batch: 100; loss: 1.72; acc: 0.36
Batch: 120; loss: 1.56; acc: 0.45
Batch: 140; loss: 1.62; acc: 0.44
Batch: 160; loss: 1.79; acc: 0.36
Batch: 180; loss: 1.9; acc: 0.3
Batch: 200; loss: 1.51; acc: 0.52
Batch: 220; loss: 1.5; acc: 0.5
Batch: 240; loss: 1.45; acc: 0.59
Batch: 260; loss: 1.59; acc: 0.47
Batch: 280; loss: 1.6; acc: 0.5
Batch: 300; loss: 1.7; acc: 0.38
Batch: 320; loss: 1.85; acc: 0.41
Batch: 340; loss: 1.75; acc: 0.42
Batch: 360; loss: 1.62; acc: 0.44
Batch: 380; loss: 1.67; acc: 0.38
Batch: 400; loss: 1.47; acc: 0.5
Batch: 420; loss: 1.71; acc: 0.36
Batch: 440; loss: 1.77; acc: 0.36
Batch: 460; loss: 1.65; acc: 0.44
Batch: 480; loss: 1.75; acc: 0.41
Batch: 500; loss: 1.35; acc: 0.56
Batch: 520; loss: 1.72; acc: 0.42
Batch: 540; loss: 1.72; acc: 0.36
Batch: 560; loss: 1.82; acc: 0.41
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.56; acc: 0.48
Batch: 620; loss: 1.67; acc: 0.47
Batch: 640; loss: 1.46; acc: 0.44
Batch: 660; loss: 1.75; acc: 0.36
Batch: 680; loss: 1.83; acc: 0.38
Batch: 700; loss: 1.62; acc: 0.45
Batch: 720; loss: 1.54; acc: 0.47
Batch: 740; loss: 1.51; acc: 0.44
Batch: 760; loss: 1.45; acc: 0.47
Batch: 780; loss: 1.51; acc: 0.45
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175051411246038; val_accuracy: 0.43759952229299365 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.59; acc: 0.44
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.75; acc: 0.34
Batch: 60; loss: 1.56; acc: 0.48
Batch: 80; loss: 1.79; acc: 0.41
Batch: 100; loss: 1.69; acc: 0.39
Batch: 120; loss: 1.67; acc: 0.44
Batch: 140; loss: 1.75; acc: 0.42
Batch: 160; loss: 1.68; acc: 0.44
Batch: 180; loss: 1.81; acc: 0.31
Batch: 200; loss: 1.61; acc: 0.45
Batch: 220; loss: 1.77; acc: 0.39
Batch: 240; loss: 1.62; acc: 0.42
Batch: 260; loss: 1.73; acc: 0.41
Batch: 280; loss: 1.82; acc: 0.34
Batch: 300; loss: 1.98; acc: 0.31
Batch: 320; loss: 1.63; acc: 0.45
Batch: 340; loss: 1.67; acc: 0.45
Batch: 360; loss: 1.44; acc: 0.42
Batch: 380; loss: 1.69; acc: 0.38
Batch: 400; loss: 1.56; acc: 0.5
Batch: 420; loss: 1.78; acc: 0.36
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.68; acc: 0.39
Batch: 480; loss: 1.57; acc: 0.42
Batch: 500; loss: 1.81; acc: 0.38
Batch: 520; loss: 1.73; acc: 0.38
Batch: 540; loss: 1.49; acc: 0.41
Batch: 560; loss: 1.43; acc: 0.5
Batch: 580; loss: 1.51; acc: 0.45
Batch: 600; loss: 1.73; acc: 0.47
Batch: 620; loss: 1.57; acc: 0.45
Batch: 640; loss: 1.57; acc: 0.41
Batch: 660; loss: 1.61; acc: 0.42
Batch: 680; loss: 1.45; acc: 0.52
Batch: 700; loss: 1.83; acc: 0.34
Batch: 720; loss: 1.78; acc: 0.36
Batch: 740; loss: 1.78; acc: 0.42
Batch: 760; loss: 1.74; acc: 0.42
Batch: 780; loss: 1.59; acc: 0.41
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6175011085097197; val_accuracy: 0.4377985668789809 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.39
Batch: 40; loss: 1.91; acc: 0.33
Batch: 60; loss: 1.47; acc: 0.61
Batch: 80; loss: 1.54; acc: 0.48
Batch: 100; loss: 1.47; acc: 0.42
Batch: 120; loss: 1.65; acc: 0.39
Batch: 140; loss: 1.9; acc: 0.34
Batch: 160; loss: 1.82; acc: 0.39
Batch: 180; loss: 1.52; acc: 0.48
Batch: 200; loss: 1.69; acc: 0.42
Batch: 220; loss: 1.42; acc: 0.52
Batch: 240; loss: 1.66; acc: 0.53
Batch: 260; loss: 1.77; acc: 0.33
Batch: 280; loss: 1.79; acc: 0.36
Batch: 300; loss: 1.6; acc: 0.39
Batch: 320; loss: 1.74; acc: 0.31
Batch: 340; loss: 1.56; acc: 0.52
Batch: 360; loss: 1.74; acc: 0.41
Batch: 380; loss: 1.6; acc: 0.45
Batch: 400; loss: 1.73; acc: 0.33
Batch: 420; loss: 1.89; acc: 0.34
Batch: 440; loss: 1.6; acc: 0.42
Batch: 460; loss: 1.57; acc: 0.48
Batch: 480; loss: 1.59; acc: 0.44
Batch: 500; loss: 1.66; acc: 0.38
Batch: 520; loss: 1.63; acc: 0.42
Batch: 540; loss: 1.5; acc: 0.47
Batch: 560; loss: 1.77; acc: 0.34
Batch: 580; loss: 1.72; acc: 0.45
Batch: 600; loss: 1.64; acc: 0.42
Batch: 620; loss: 1.7; acc: 0.34
Batch: 640; loss: 1.84; acc: 0.39
Batch: 660; loss: 1.56; acc: 0.45
Batch: 680; loss: 1.79; acc: 0.31
Batch: 700; loss: 1.43; acc: 0.45
Batch: 720; loss: 1.82; acc: 0.34
Batch: 740; loss: 1.67; acc: 0.39
Batch: 760; loss: 1.6; acc: 0.45
Batch: 780; loss: 1.49; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.67; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.6174937653693424; val_accuracy: 0.4380971337579618 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.63; acc: 0.33
Batch: 20; loss: 1.62; acc: 0.41
Batch: 40; loss: 1.49; acc: 0.45
Batch: 60; loss: 1.44; acc: 0.5
Batch: 80; loss: 1.64; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.44
Batch: 120; loss: 1.48; acc: 0.52
Batch: 140; loss: 1.71; acc: 0.44
Batch: 160; loss: 1.78; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.47
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.61; acc: 0.44
Batch: 240; loss: 1.52; acc: 0.45
Batch: 260; loss: 1.53; acc: 0.48
Batch: 280; loss: 1.66; acc: 0.44
Batch: 300; loss: 1.53; acc: 0.52
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.56; acc: 0.47
Batch: 360; loss: 1.49; acc: 0.53
Batch: 380; loss: 1.73; acc: 0.34
Batch: 400; loss: 1.43; acc: 0.45
Batch: 420; loss: 1.66; acc: 0.38
Batch: 440; loss: 1.63; acc: 0.47
Batch: 460; loss: 1.64; acc: 0.47
Batch: 480; loss: 1.7; acc: 0.36
Batch: 500; loss: 1.79; acc: 0.31
Batch: 520; loss: 1.76; acc: 0.41
Batch: 540; loss: 1.69; acc: 0.38
Batch: 560; loss: 1.6; acc: 0.42
Batch: 580; loss: 1.56; acc: 0.52
Batch: 600; loss: 1.63; acc: 0.41
Batch: 620; loss: 1.46; acc: 0.42
Batch: 640; loss: 1.56; acc: 0.48
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.5; acc: 0.5
Batch: 700; loss: 1.53; acc: 0.47
Batch: 720; loss: 1.47; acc: 0.45
Batch: 740; loss: 1.73; acc: 0.39
Batch: 760; loss: 1.59; acc: 0.44
Batch: 780; loss: 1.97; acc: 0.36
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.68; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.5
Val Epoch over. val_loss: 1.61755874582157; val_accuracy: 0.43700238853503187 

plots/subspace_training/reg_lenet_3/2020-01-20 16:50:48/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 2249500
elements in E: 2249500
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.32; acc: 0.03
Batch: 80; loss: 2.31; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.09
Batch: 140; loss: 2.29; acc: 0.17
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.29; acc: 0.2
Batch: 220; loss: 2.31; acc: 0.03
Batch: 240; loss: 2.31; acc: 0.06
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.14
Batch: 300; loss: 2.29; acc: 0.25
Batch: 320; loss: 2.3; acc: 0.05
Batch: 340; loss: 2.3; acc: 0.17
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.16
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.29; acc: 0.17
Batch: 460; loss: 2.3; acc: 0.08
Batch: 480; loss: 2.29; acc: 0.14
Batch: 500; loss: 2.29; acc: 0.11
Batch: 520; loss: 2.29; acc: 0.09
Batch: 540; loss: 2.29; acc: 0.22
Batch: 560; loss: 2.29; acc: 0.14
Batch: 580; loss: 2.3; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.19
Batch: 620; loss: 2.29; acc: 0.14
Batch: 640; loss: 2.31; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.29; acc: 0.17
Batch: 700; loss: 2.29; acc: 0.08
Batch: 720; loss: 2.28; acc: 0.17
Batch: 740; loss: 2.29; acc: 0.14
Batch: 760; loss: 2.29; acc: 0.19
Batch: 780; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.14 

Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.29; acc: 0.22
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.19
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.2
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.29; acc: 0.17
Val Epoch over. val_loss: 2.290354655806426; val_accuracy: 0.14709394904458598 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.19
Batch: 60; loss: 2.29; acc: 0.19
Batch: 80; loss: 2.29; acc: 0.16
Batch: 100; loss: 2.29; acc: 0.2
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.31; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.29; acc: 0.17
Batch: 200; loss: 2.29; acc: 0.16
Batch: 220; loss: 2.29; acc: 0.14
Batch: 240; loss: 2.28; acc: 0.22
Batch: 260; loss: 2.29; acc: 0.16
Batch: 280; loss: 2.29; acc: 0.16
Batch: 300; loss: 2.29; acc: 0.19
Batch: 320; loss: 2.29; acc: 0.22
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.28; acc: 0.27
Batch: 400; loss: 2.28; acc: 0.23
Batch: 420; loss: 2.29; acc: 0.2
Batch: 440; loss: 2.28; acc: 0.17
Batch: 460; loss: 2.29; acc: 0.12
Batch: 480; loss: 2.29; acc: 0.16
Batch: 500; loss: 2.29; acc: 0.16
Batch: 520; loss: 2.29; acc: 0.17
Batch: 540; loss: 2.28; acc: 0.19
Batch: 560; loss: 2.26; acc: 0.31
Batch: 580; loss: 2.27; acc: 0.27
Batch: 600; loss: 2.28; acc: 0.19
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.27; acc: 0.2
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.27; acc: 0.27
Batch: 700; loss: 2.28; acc: 0.31
Batch: 720; loss: 2.28; acc: 0.2
Batch: 740; loss: 2.28; acc: 0.23
Batch: 760; loss: 2.27; acc: 0.28
Batch: 780; loss: 2.26; acc: 0.28
Train Epoch over. train_loss: 2.28; train_accuracy: 0.19 

Batch: 0; loss: 2.27; acc: 0.31
Batch: 20; loss: 2.28; acc: 0.25
Batch: 40; loss: 2.27; acc: 0.25
Batch: 60; loss: 2.26; acc: 0.34
Batch: 80; loss: 2.26; acc: 0.38
Batch: 100; loss: 2.28; acc: 0.27
Batch: 120; loss: 2.27; acc: 0.25
Batch: 140; loss: 2.27; acc: 0.25
Val Epoch over. val_loss: 2.2700060233948336; val_accuracy: 0.2649283439490446 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.17
Batch: 20; loss: 2.26; acc: 0.33
Batch: 40; loss: 2.27; acc: 0.23
Batch: 60; loss: 2.28; acc: 0.25
Batch: 80; loss: 2.26; acc: 0.23
Batch: 100; loss: 2.27; acc: 0.2
Batch: 120; loss: 2.23; acc: 0.44
Batch: 140; loss: 2.26; acc: 0.36
Batch: 160; loss: 2.26; acc: 0.25
Batch: 180; loss: 2.24; acc: 0.38
Batch: 200; loss: 2.24; acc: 0.27
Batch: 220; loss: 2.24; acc: 0.28
Batch: 240; loss: 2.27; acc: 0.25
Batch: 260; loss: 2.27; acc: 0.28
Batch: 280; loss: 2.27; acc: 0.22
Batch: 300; loss: 2.26; acc: 0.28
Batch: 320; loss: 2.25; acc: 0.31
Batch: 340; loss: 2.25; acc: 0.25
Batch: 360; loss: 2.29; acc: 0.12
Batch: 380; loss: 2.22; acc: 0.34
Batch: 400; loss: 2.24; acc: 0.28
Batch: 420; loss: 2.25; acc: 0.2
Batch: 440; loss: 2.23; acc: 0.31
Batch: 460; loss: 2.23; acc: 0.31
Batch: 480; loss: 2.24; acc: 0.25
Batch: 500; loss: 2.26; acc: 0.25
Batch: 520; loss: 2.24; acc: 0.27
Batch: 540; loss: 2.22; acc: 0.2
Batch: 560; loss: 2.23; acc: 0.17
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.21; acc: 0.2
Batch: 620; loss: 2.2; acc: 0.28
Batch: 640; loss: 2.23; acc: 0.22
Batch: 660; loss: 2.2; acc: 0.23
Batch: 680; loss: 2.2; acc: 0.27
Batch: 700; loss: 2.19; acc: 0.31
Batch: 720; loss: 2.22; acc: 0.17
Batch: 740; loss: 2.2; acc: 0.22
Batch: 760; loss: 2.25; acc: 0.16
Batch: 780; loss: 2.18; acc: 0.25
Train Epoch over. train_loss: 2.24; train_accuracy: 0.26 

Batch: 0; loss: 2.18; acc: 0.27
Batch: 20; loss: 2.23; acc: 0.25
Batch: 40; loss: 2.16; acc: 0.27
Batch: 60; loss: 2.13; acc: 0.31
Batch: 80; loss: 2.11; acc: 0.38
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.2; acc: 0.31
Batch: 140; loss: 2.15; acc: 0.25
Val Epoch over. val_loss: 2.172181460508116; val_accuracy: 0.2867237261146497 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.19; acc: 0.36
Batch: 20; loss: 2.21; acc: 0.19
Batch: 40; loss: 2.13; acc: 0.36
Batch: 60; loss: 2.13; acc: 0.41
Batch: 80; loss: 2.15; acc: 0.27
Batch: 100; loss: 2.13; acc: 0.2
Batch: 120; loss: 2.1; acc: 0.38
Batch: 140; loss: 2.09; acc: 0.3
Batch: 160; loss: 2.07; acc: 0.39
Batch: 180; loss: 2.1; acc: 0.31
Batch: 200; loss: 2.05; acc: 0.36
Batch: 220; loss: 2.08; acc: 0.23
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.01; acc: 0.27
Batch: 280; loss: 1.84; acc: 0.5
Batch: 300; loss: 1.89; acc: 0.38
Batch: 320; loss: 1.93; acc: 0.33
Batch: 340; loss: 1.71; acc: 0.47
Batch: 360; loss: 1.94; acc: 0.33
Batch: 380; loss: 1.77; acc: 0.38
Batch: 400; loss: 1.86; acc: 0.33
Batch: 420; loss: 1.65; acc: 0.47
Batch: 440; loss: 1.69; acc: 0.39
Batch: 460; loss: 1.59; acc: 0.44
Batch: 480; loss: 1.65; acc: 0.45
Batch: 500; loss: 1.41; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.52
Batch: 540; loss: 1.4; acc: 0.52
Batch: 560; loss: 1.51; acc: 0.55
Batch: 580; loss: 1.46; acc: 0.58
Batch: 600; loss: 1.43; acc: 0.55
Batch: 620; loss: 1.38; acc: 0.61
Batch: 640; loss: 1.51; acc: 0.52
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.67; acc: 0.53
Batch: 720; loss: 1.19; acc: 0.73
Batch: 740; loss: 1.43; acc: 0.58
Batch: 760; loss: 1.27; acc: 0.62
Batch: 780; loss: 1.26; acc: 0.58
Train Epoch over. train_loss: 1.75; train_accuracy: 0.42 

Batch: 0; loss: 1.43; acc: 0.56
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 1.15; acc: 0.58
Batch: 80; loss: 1.07; acc: 0.69
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.23; acc: 0.58
Val Epoch over. val_loss: 1.3468705719443643; val_accuracy: 0.5720541401273885 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.52
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 1.55; acc: 0.55
Batch: 60; loss: 1.34; acc: 0.53
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 1.34; acc: 0.56
Batch: 160; loss: 1.35; acc: 0.61
Batch: 180; loss: 1.54; acc: 0.53
Batch: 200; loss: 1.22; acc: 0.58
Batch: 220; loss: 1.24; acc: 0.64
Batch: 240; loss: 1.54; acc: 0.56
Batch: 260; loss: 1.24; acc: 0.58
Batch: 280; loss: 1.23; acc: 0.53
Batch: 300; loss: 1.29; acc: 0.58
Batch: 320; loss: 1.14; acc: 0.61
Batch: 340; loss: 1.37; acc: 0.52
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.28; acc: 0.59
Batch: 400; loss: 1.4; acc: 0.56
Batch: 420; loss: 1.29; acc: 0.5
Batch: 440; loss: 1.09; acc: 0.61
Batch: 460; loss: 1.22; acc: 0.59
Batch: 480; loss: 1.31; acc: 0.55
Batch: 500; loss: 1.17; acc: 0.66
Batch: 520; loss: 1.36; acc: 0.59
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.53
Batch: 580; loss: 1.21; acc: 0.52
Batch: 600; loss: 1.08; acc: 0.62
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 1.16; acc: 0.55
Batch: 660; loss: 1.03; acc: 0.59
Batch: 680; loss: 0.98; acc: 0.66
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.56
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 0.96; acc: 0.69
Train Epoch over. train_loss: 1.2; train_accuracy: 0.61 

Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.67
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.59
Batch: 120; loss: 1.01; acc: 0.67
Batch: 140; loss: 0.96; acc: 0.64
Val Epoch over. val_loss: 1.0209197705718362; val_accuracy: 0.6709792993630573 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.55
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 1.01; acc: 0.64
Batch: 80; loss: 0.9; acc: 0.7
Batch: 100; loss: 0.94; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 1.21; acc: 0.61
Batch: 160; loss: 0.82; acc: 0.72
Batch: 180; loss: 1.08; acc: 0.69
Batch: 200; loss: 0.99; acc: 0.67
Batch: 220; loss: 1.41; acc: 0.58
Batch: 240; loss: 0.86; acc: 0.69
Batch: 260; loss: 1.13; acc: 0.61
Batch: 280; loss: 1.13; acc: 0.53
Batch: 300; loss: 1.17; acc: 0.55
Batch: 320; loss: 1.03; acc: 0.62
Batch: 340; loss: 0.92; acc: 0.67
Batch: 360; loss: 1.14; acc: 0.66
Batch: 380; loss: 0.8; acc: 0.73
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 0.87; acc: 0.7
Batch: 440; loss: 0.85; acc: 0.7
Batch: 460; loss: 0.85; acc: 0.73
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 0.84; acc: 0.7
Batch: 540; loss: 1.16; acc: 0.67
Batch: 560; loss: 0.8; acc: 0.75
Batch: 580; loss: 0.95; acc: 0.69
Batch: 600; loss: 1.24; acc: 0.58
Batch: 620; loss: 0.9; acc: 0.7
Batch: 640; loss: 1.05; acc: 0.64
Batch: 660; loss: 1.28; acc: 0.48
Batch: 680; loss: 1.17; acc: 0.58
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 0.9; acc: 0.7
Batch: 740; loss: 1.03; acc: 0.67
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 1.04; train_accuracy: 0.67 

Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.92; acc: 0.62
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 1.08; acc: 0.62
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.88; acc: 0.66
Val Epoch over. val_loss: 0.9834991814983878; val_accuracy: 0.6854100318471338 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.7
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 1.01; acc: 0.64
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.64
Batch: 100; loss: 0.95; acc: 0.72
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.72
Batch: 160; loss: 0.88; acc: 0.8
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 0.93; acc: 0.67
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 0.89; acc: 0.69
Batch: 260; loss: 0.91; acc: 0.78
Batch: 280; loss: 0.92; acc: 0.7
Batch: 300; loss: 0.96; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 1.36; acc: 0.56
Batch: 360; loss: 1.19; acc: 0.61
Batch: 380; loss: 1.06; acc: 0.62
Batch: 400; loss: 0.98; acc: 0.66
Batch: 420; loss: 0.74; acc: 0.73
Batch: 440; loss: 1.03; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.69
Batch: 480; loss: 0.83; acc: 0.75
Batch: 500; loss: 0.88; acc: 0.67
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.92; acc: 0.73
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 1.2; acc: 0.61
Batch: 600; loss: 1.25; acc: 0.55
Batch: 620; loss: 0.92; acc: 0.7
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 1.06; acc: 0.69
Batch: 680; loss: 1.22; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.62
Batch: 720; loss: 1.09; acc: 0.62
Batch: 740; loss: 0.89; acc: 0.69
Batch: 760; loss: 0.94; acc: 0.67
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 1.01; train_accuracy: 0.68 

Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.89; acc: 0.69
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 1.14; acc: 0.61
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.76; acc: 0.73
Val Epoch over. val_loss: 0.9719918064630715; val_accuracy: 0.6865047770700637 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.7
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 1.13; acc: 0.66
Batch: 60; loss: 0.85; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 1.03; acc: 0.67
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 0.78; acc: 0.7
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.15; acc: 0.59
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 0.94; acc: 0.67
Batch: 260; loss: 1.13; acc: 0.58
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 1.21; acc: 0.56
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.12; acc: 0.62
Batch: 380; loss: 0.94; acc: 0.67
Batch: 400; loss: 1.23; acc: 0.62
Batch: 420; loss: 0.82; acc: 0.67
Batch: 440; loss: 1.12; acc: 0.64
Batch: 460; loss: 0.99; acc: 0.69
Batch: 480; loss: 0.89; acc: 0.67
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 0.89; acc: 0.59
Batch: 540; loss: 1.09; acc: 0.62
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.92; acc: 0.66
Batch: 600; loss: 1.56; acc: 0.56
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 1.13; acc: 0.69
Batch: 660; loss: 0.86; acc: 0.69
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.0; acc: 0.67
Batch: 740; loss: 1.09; acc: 0.66
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.8; acc: 0.77
Train Epoch over. train_loss: 1.0; train_accuracy: 0.68 

Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.01; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.95; acc: 0.64
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 1.12; acc: 0.64
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.68; acc: 0.75
Val Epoch over. val_loss: 1.0049163431498656; val_accuracy: 0.6895899681528662 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.15; acc: 0.58
Batch: 80; loss: 0.94; acc: 0.73
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.7
Batch: 160; loss: 0.94; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.66
Batch: 200; loss: 1.29; acc: 0.56
Batch: 220; loss: 1.3; acc: 0.66
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.7
Batch: 280; loss: 0.77; acc: 0.77
Batch: 300; loss: 1.08; acc: 0.59
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 0.94; acc: 0.7
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 1.01; acc: 0.62
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 0.89; acc: 0.77
Batch: 440; loss: 0.9; acc: 0.73
Batch: 460; loss: 0.82; acc: 0.75
Batch: 480; loss: 0.63; acc: 0.77
Batch: 500; loss: 0.89; acc: 0.72
Batch: 520; loss: 1.07; acc: 0.61
Batch: 540; loss: 1.07; acc: 0.73
Batch: 560; loss: 1.02; acc: 0.66
Batch: 580; loss: 0.95; acc: 0.67
Batch: 600; loss: 1.04; acc: 0.58
Batch: 620; loss: 1.14; acc: 0.67
Batch: 640; loss: 1.05; acc: 0.69
Batch: 660; loss: 0.91; acc: 0.72
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 0.83; acc: 0.73
Batch: 720; loss: 1.15; acc: 0.52
Batch: 740; loss: 1.03; acc: 0.72
Batch: 760; loss: 1.06; acc: 0.67
Batch: 780; loss: 1.08; acc: 0.56
Train Epoch over. train_loss: 1.0; train_accuracy: 0.68 

Batch: 0; loss: 1.25; acc: 0.55
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.82; acc: 0.69
Val Epoch over. val_loss: 0.9513880378881078; val_accuracy: 0.6991441082802548 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.61
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.59
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.86; acc: 0.75
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 1.02; acc: 0.64
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.94; acc: 0.75
Batch: 260; loss: 0.88; acc: 0.72
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.08; acc: 0.61
Batch: 320; loss: 0.81; acc: 0.77
Batch: 340; loss: 1.1; acc: 0.67
Batch: 360; loss: 0.98; acc: 0.69
Batch: 380; loss: 1.02; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.7
Batch: 420; loss: 1.21; acc: 0.55
Batch: 440; loss: 0.72; acc: 0.78
Batch: 460; loss: 1.08; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.53
Batch: 500; loss: 0.93; acc: 0.67
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 1.15; acc: 0.61
Batch: 560; loss: 1.25; acc: 0.61
Batch: 580; loss: 1.01; acc: 0.62
Batch: 600; loss: 1.17; acc: 0.62
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 1.08; acc: 0.59
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 1.0; acc: 0.62
Batch: 700; loss: 1.01; acc: 0.64
Batch: 720; loss: 1.05; acc: 0.64
Batch: 740; loss: 1.05; acc: 0.64
Batch: 760; loss: 0.94; acc: 0.72
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.0; train_accuracy: 0.68 

Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.03; acc: 0.64
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.94; acc: 0.66
Batch: 80; loss: 0.66; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.64
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9982215473606328; val_accuracy: 0.676453025477707 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.72
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 1.02; acc: 0.61
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.61
Batch: 120; loss: 1.02; acc: 0.69
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 1.12; acc: 0.69
Batch: 180; loss: 0.84; acc: 0.7
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 1.09; acc: 0.62
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 0.93; acc: 0.7
Batch: 280; loss: 0.88; acc: 0.75
Batch: 300; loss: 0.98; acc: 0.64
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 0.91; acc: 0.66
Batch: 380; loss: 0.86; acc: 0.7
Batch: 400; loss: 1.02; acc: 0.62
Batch: 420; loss: 0.96; acc: 0.69
Batch: 440; loss: 1.05; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.66
Batch: 480; loss: 1.01; acc: 0.66
Batch: 500; loss: 0.93; acc: 0.75
Batch: 520; loss: 0.86; acc: 0.72
Batch: 540; loss: 0.95; acc: 0.7
Batch: 560; loss: 0.91; acc: 0.67
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 1.01; acc: 0.73
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.05; acc: 0.62
Batch: 680; loss: 1.04; acc: 0.7
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.03; acc: 0.67
Batch: 760; loss: 1.23; acc: 0.64
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.17; acc: 0.56
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 0.84; acc: 0.72
Val Epoch over. val_loss: 0.957814344365126; val_accuracy: 0.6938694267515924 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 0.97; acc: 0.64
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 1.11; acc: 0.62
Batch: 160; loss: 0.9; acc: 0.66
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 1.02; acc: 0.66
Batch: 220; loss: 0.89; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.73
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 0.87; acc: 0.72
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 1.07; acc: 0.69
Batch: 360; loss: 0.86; acc: 0.7
Batch: 380; loss: 0.89; acc: 0.69
Batch: 400; loss: 1.2; acc: 0.61
Batch: 420; loss: 1.28; acc: 0.67
Batch: 440; loss: 1.03; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.66
Batch: 480; loss: 1.27; acc: 0.56
Batch: 500; loss: 0.8; acc: 0.75
Batch: 520; loss: 1.1; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.58
Batch: 560; loss: 0.81; acc: 0.72
Batch: 580; loss: 0.85; acc: 0.72
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 1.16; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.64
Batch: 660; loss: 0.7; acc: 0.73
Batch: 680; loss: 1.08; acc: 0.67
Batch: 700; loss: 1.08; acc: 0.69
Batch: 720; loss: 0.92; acc: 0.69
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 0.92; acc: 0.69
Batch: 780; loss: 1.0; acc: 0.69
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.69
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 1.12; acc: 0.64
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.75; acc: 0.67
Val Epoch over. val_loss: 0.9433804473299889; val_accuracy: 0.6955613057324841 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 1.0; acc: 0.66
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 0.9; acc: 0.75
Batch: 160; loss: 1.2; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 1.21; acc: 0.7
Batch: 220; loss: 1.16; acc: 0.66
Batch: 240; loss: 0.95; acc: 0.69
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 0.93; acc: 0.73
Batch: 340; loss: 0.92; acc: 0.73
Batch: 360; loss: 0.97; acc: 0.7
Batch: 380; loss: 1.27; acc: 0.58
Batch: 400; loss: 1.01; acc: 0.64
Batch: 420; loss: 0.97; acc: 0.67
Batch: 440; loss: 1.06; acc: 0.67
Batch: 460; loss: 1.05; acc: 0.64
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 1.02; acc: 0.64
Batch: 520; loss: 0.9; acc: 0.64
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.11; acc: 0.69
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 1.06; acc: 0.61
Batch: 640; loss: 0.89; acc: 0.67
Batch: 660; loss: 0.8; acc: 0.72
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 0.94; acc: 0.7
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 1.11; acc: 0.67
Batch: 780; loss: 0.98; acc: 0.75
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.58
Batch: 20; loss: 1.08; acc: 0.64
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 1.12; acc: 0.62
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.77; acc: 0.72
Val Epoch over. val_loss: 0.9455269013240839; val_accuracy: 0.6941679936305732 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.55
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.69
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.87; acc: 0.66
Batch: 160; loss: 1.08; acc: 0.66
Batch: 180; loss: 0.94; acc: 0.69
Batch: 200; loss: 1.03; acc: 0.62
Batch: 220; loss: 1.21; acc: 0.62
Batch: 240; loss: 0.83; acc: 0.8
Batch: 260; loss: 1.16; acc: 0.55
Batch: 280; loss: 0.79; acc: 0.72
Batch: 300; loss: 0.92; acc: 0.69
Batch: 320; loss: 1.01; acc: 0.69
Batch: 340; loss: 1.15; acc: 0.61
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 0.91; acc: 0.66
Batch: 400; loss: 1.02; acc: 0.59
Batch: 420; loss: 1.01; acc: 0.7
Batch: 440; loss: 0.95; acc: 0.73
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 0.91; acc: 0.7
Batch: 520; loss: 0.91; acc: 0.67
Batch: 540; loss: 0.92; acc: 0.73
Batch: 560; loss: 0.88; acc: 0.69
Batch: 580; loss: 1.17; acc: 0.56
Batch: 600; loss: 1.07; acc: 0.59
Batch: 620; loss: 0.83; acc: 0.77
Batch: 640; loss: 0.87; acc: 0.77
Batch: 660; loss: 0.88; acc: 0.78
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.8; acc: 0.69
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 0.86; acc: 0.7
Batch: 760; loss: 0.9; acc: 0.7
Batch: 780; loss: 0.94; acc: 0.77
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.9; acc: 0.69
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 1.09; acc: 0.64
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.68; acc: 0.72
Val Epoch over. val_loss: 0.9468010851911678; val_accuracy: 0.6967555732484076 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.66
Batch: 40; loss: 0.91; acc: 0.73
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 1.2; acc: 0.59
Batch: 100; loss: 0.86; acc: 0.69
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 1.05; acc: 0.69
Batch: 160; loss: 1.53; acc: 0.52
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 0.94; acc: 0.69
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 0.98; acc: 0.67
Batch: 260; loss: 1.05; acc: 0.66
Batch: 280; loss: 0.98; acc: 0.72
Batch: 300; loss: 1.0; acc: 0.67
Batch: 320; loss: 0.91; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.72
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.12; acc: 0.62
Batch: 420; loss: 0.95; acc: 0.69
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 1.02; acc: 0.73
Batch: 520; loss: 0.97; acc: 0.66
Batch: 540; loss: 1.11; acc: 0.67
Batch: 560; loss: 0.96; acc: 0.61
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.69
Batch: 620; loss: 1.44; acc: 0.59
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 1.15; acc: 0.61
Batch: 680; loss: 1.11; acc: 0.73
Batch: 700; loss: 0.9; acc: 0.75
Batch: 720; loss: 0.92; acc: 0.67
Batch: 740; loss: 1.02; acc: 0.64
Batch: 760; loss: 0.93; acc: 0.72
Batch: 780; loss: 1.09; acc: 0.59
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 0.97; acc: 0.66
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.88; acc: 0.7
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.67
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.73; acc: 0.72
Val Epoch over. val_loss: 0.9484305157782925; val_accuracy: 0.6961584394904459 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.62
Batch: 60; loss: 0.98; acc: 0.69
Batch: 80; loss: 0.89; acc: 0.69
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.69
Batch: 160; loss: 0.98; acc: 0.67
Batch: 180; loss: 0.84; acc: 0.77
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 0.99; acc: 0.67
Batch: 260; loss: 0.77; acc: 0.7
Batch: 280; loss: 0.96; acc: 0.69
Batch: 300; loss: 0.96; acc: 0.72
Batch: 320; loss: 1.03; acc: 0.66
Batch: 340; loss: 1.09; acc: 0.67
Batch: 360; loss: 0.95; acc: 0.64
Batch: 380; loss: 1.02; acc: 0.75
Batch: 400; loss: 0.97; acc: 0.7
Batch: 420; loss: 1.15; acc: 0.55
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.83; acc: 0.72
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 1.09; acc: 0.56
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 1.14; acc: 0.64
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 0.95; acc: 0.64
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 1.14; acc: 0.58
Batch: 640; loss: 1.07; acc: 0.69
Batch: 660; loss: 0.75; acc: 0.73
Batch: 680; loss: 0.8; acc: 0.72
Batch: 700; loss: 1.13; acc: 0.61
Batch: 720; loss: 1.09; acc: 0.66
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 1.1; acc: 0.67
Batch: 780; loss: 0.96; acc: 0.64
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 0.99; acc: 0.67
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 0.72; acc: 0.69
Val Epoch over. val_loss: 0.9402226691792726; val_accuracy: 0.6962579617834395 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.87; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.7
Batch: 80; loss: 1.23; acc: 0.66
Batch: 100; loss: 0.93; acc: 0.67
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.67
Batch: 160; loss: 0.85; acc: 0.7
Batch: 180; loss: 0.88; acc: 0.73
Batch: 200; loss: 1.19; acc: 0.64
Batch: 220; loss: 1.07; acc: 0.56
Batch: 240; loss: 1.25; acc: 0.56
Batch: 260; loss: 0.84; acc: 0.72
Batch: 280; loss: 1.04; acc: 0.61
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 0.9; acc: 0.67
Batch: 340; loss: 0.81; acc: 0.69
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 1.18; acc: 0.64
Batch: 460; loss: 1.09; acc: 0.64
Batch: 480; loss: 1.1; acc: 0.66
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 0.86; acc: 0.7
Batch: 540; loss: 0.97; acc: 0.73
Batch: 560; loss: 1.04; acc: 0.62
Batch: 580; loss: 1.17; acc: 0.58
Batch: 600; loss: 0.91; acc: 0.72
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 1.11; acc: 0.66
Batch: 660; loss: 0.99; acc: 0.67
Batch: 680; loss: 0.88; acc: 0.73
Batch: 700; loss: 0.87; acc: 0.7
Batch: 720; loss: 1.09; acc: 0.61
Batch: 740; loss: 1.06; acc: 0.67
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.97; acc: 0.66
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.98; acc: 0.66
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.72
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.69
Val Epoch over. val_loss: 0.9457018310856667; val_accuracy: 0.6934713375796179 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 1.15; acc: 0.61
Batch: 80; loss: 1.31; acc: 0.59
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 1.03; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.61
Batch: 160; loss: 0.82; acc: 0.67
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.96; acc: 0.64
Batch: 220; loss: 0.85; acc: 0.67
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 0.89; acc: 0.75
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 1.14; acc: 0.55
Batch: 340; loss: 0.91; acc: 0.7
Batch: 360; loss: 1.38; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.62
Batch: 400; loss: 0.73; acc: 0.77
Batch: 420; loss: 0.99; acc: 0.69
Batch: 440; loss: 0.84; acc: 0.7
Batch: 460; loss: 1.2; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.67
Batch: 500; loss: 0.69; acc: 0.75
Batch: 520; loss: 0.9; acc: 0.72
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 1.07; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.61
Batch: 600; loss: 1.07; acc: 0.64
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.85; acc: 0.67
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 1.1; acc: 0.64
Batch: 700; loss: 1.05; acc: 0.69
Batch: 720; loss: 0.97; acc: 0.67
Batch: 740; loss: 1.06; acc: 0.66
Batch: 760; loss: 0.85; acc: 0.72
Batch: 780; loss: 0.99; acc: 0.64
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.22; acc: 0.59
Batch: 20; loss: 0.97; acc: 0.66
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.94; acc: 0.73
Batch: 140; loss: 0.77; acc: 0.69
Val Epoch over. val_loss: 0.9461724154508797; val_accuracy: 0.6932722929936306 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.92; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 1.1; acc: 0.73
Batch: 60; loss: 0.9; acc: 0.66
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 0.89; acc: 0.64
Batch: 160; loss: 1.16; acc: 0.64
Batch: 180; loss: 0.85; acc: 0.73
Batch: 200; loss: 0.88; acc: 0.69
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 0.99; acc: 0.67
Batch: 260; loss: 1.12; acc: 0.67
Batch: 280; loss: 1.0; acc: 0.59
Batch: 300; loss: 0.87; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 0.96; acc: 0.67
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 1.29; acc: 0.56
Batch: 420; loss: 0.82; acc: 0.69
Batch: 440; loss: 0.87; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.66
Batch: 480; loss: 1.07; acc: 0.62
Batch: 500; loss: 1.24; acc: 0.61
Batch: 520; loss: 1.17; acc: 0.66
Batch: 540; loss: 0.62; acc: 0.75
Batch: 560; loss: 0.96; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.66
Batch: 600; loss: 0.74; acc: 0.73
Batch: 620; loss: 1.18; acc: 0.58
Batch: 640; loss: 1.08; acc: 0.58
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.94; acc: 0.69
Batch: 700; loss: 0.91; acc: 0.67
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 1.14; acc: 0.66
Batch: 760; loss: 1.03; acc: 0.64
Batch: 780; loss: 0.92; acc: 0.75
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.22; acc: 0.56
Batch: 20; loss: 0.97; acc: 0.66
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.77; acc: 0.72
Val Epoch over. val_loss: 0.9464271216635491; val_accuracy: 0.6917794585987261 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 1.01; acc: 0.61
Batch: 40; loss: 1.11; acc: 0.59
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.64
Batch: 100; loss: 1.01; acc: 0.66
Batch: 120; loss: 1.03; acc: 0.7
Batch: 140; loss: 1.26; acc: 0.67
Batch: 160; loss: 0.98; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.05; acc: 0.7
Batch: 220; loss: 0.91; acc: 0.67
Batch: 240; loss: 1.0; acc: 0.61
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.8; acc: 0.72
Batch: 300; loss: 0.97; acc: 0.66
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 1.14; acc: 0.64
Batch: 360; loss: 0.77; acc: 0.73
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.84; acc: 0.7
Batch: 420; loss: 0.89; acc: 0.69
Batch: 440; loss: 1.25; acc: 0.59
Batch: 460; loss: 1.03; acc: 0.64
Batch: 480; loss: 0.91; acc: 0.7
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 1.09; acc: 0.77
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 1.04; acc: 0.66
Batch: 660; loss: 0.83; acc: 0.75
Batch: 680; loss: 0.8; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 0.88; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.72
Batch: 780; loss: 1.19; acc: 0.61
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.58
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.71; acc: 0.72
Val Epoch over. val_loss: 0.9453420181562946; val_accuracy: 0.6945660828025477 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.95; acc: 0.66
Batch: 60; loss: 1.3; acc: 0.62
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.64
Batch: 120; loss: 0.85; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.7
Batch: 160; loss: 1.19; acc: 0.66
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 0.96; acc: 0.62
Batch: 260; loss: 0.83; acc: 0.69
Batch: 280; loss: 1.16; acc: 0.56
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 1.06; acc: 0.64
Batch: 340; loss: 0.71; acc: 0.73
Batch: 360; loss: 1.19; acc: 0.67
Batch: 380; loss: 1.14; acc: 0.69
Batch: 400; loss: 0.96; acc: 0.64
Batch: 420; loss: 1.06; acc: 0.66
Batch: 440; loss: 0.88; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.66
Batch: 480; loss: 1.01; acc: 0.64
Batch: 500; loss: 1.06; acc: 0.64
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.61
Batch: 580; loss: 0.91; acc: 0.72
Batch: 600; loss: 1.02; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.58
Batch: 640; loss: 1.08; acc: 0.7
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 1.05; acc: 0.62
Batch: 700; loss: 1.0; acc: 0.61
Batch: 720; loss: 0.99; acc: 0.73
Batch: 740; loss: 1.04; acc: 0.69
Batch: 760; loss: 0.95; acc: 0.73
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.7
Val Epoch over. val_loss: 0.9371203567571701; val_accuracy: 0.694765127388535 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.95; acc: 0.64
Batch: 60; loss: 0.89; acc: 0.67
Batch: 80; loss: 0.88; acc: 0.72
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.66
Batch: 160; loss: 0.99; acc: 0.75
Batch: 180; loss: 0.85; acc: 0.72
Batch: 200; loss: 1.38; acc: 0.56
Batch: 220; loss: 1.02; acc: 0.78
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 1.13; acc: 0.66
Batch: 280; loss: 1.0; acc: 0.67
Batch: 300; loss: 1.01; acc: 0.64
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 1.0; acc: 0.61
Batch: 380; loss: 1.08; acc: 0.62
Batch: 400; loss: 1.2; acc: 0.69
Batch: 420; loss: 1.16; acc: 0.64
Batch: 440; loss: 0.94; acc: 0.67
Batch: 460; loss: 1.19; acc: 0.66
Batch: 480; loss: 0.94; acc: 0.62
Batch: 500; loss: 1.11; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.66
Batch: 540; loss: 0.86; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.59
Batch: 600; loss: 0.95; acc: 0.7
Batch: 620; loss: 1.17; acc: 0.67
Batch: 640; loss: 0.75; acc: 0.73
Batch: 660; loss: 0.73; acc: 0.72
Batch: 680; loss: 0.75; acc: 0.69
Batch: 700; loss: 0.9; acc: 0.66
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 1.14; acc: 0.64
Batch: 780; loss: 0.85; acc: 0.72
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.97; acc: 0.66
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.71; acc: 0.72
Val Epoch over. val_loss: 0.9397548190347708; val_accuracy: 0.6943670382165605 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 1.44; acc: 0.59
Batch: 40; loss: 0.99; acc: 0.66
Batch: 60; loss: 0.94; acc: 0.66
Batch: 80; loss: 1.03; acc: 0.61
Batch: 100; loss: 1.12; acc: 0.59
Batch: 120; loss: 1.04; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.77
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 1.02; acc: 0.67
Batch: 200; loss: 1.0; acc: 0.69
Batch: 220; loss: 0.81; acc: 0.75
Batch: 240; loss: 1.03; acc: 0.66
Batch: 260; loss: 0.96; acc: 0.64
Batch: 280; loss: 0.98; acc: 0.66
Batch: 300; loss: 1.28; acc: 0.62
Batch: 320; loss: 0.93; acc: 0.69
Batch: 340; loss: 1.08; acc: 0.61
Batch: 360; loss: 0.83; acc: 0.7
Batch: 380; loss: 0.95; acc: 0.69
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 0.89; acc: 0.7
Batch: 440; loss: 0.89; acc: 0.67
Batch: 460; loss: 0.99; acc: 0.66
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.69
Batch: 520; loss: 1.03; acc: 0.67
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 0.98; acc: 0.7
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.69
Batch: 620; loss: 1.03; acc: 0.64
Batch: 640; loss: 1.08; acc: 0.62
Batch: 660; loss: 1.15; acc: 0.64
Batch: 680; loss: 0.96; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.31; acc: 0.61
Batch: 760; loss: 0.89; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.75
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 0.95; acc: 0.66
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.69; acc: 0.73
Val Epoch over. val_loss: 0.9386490520777976; val_accuracy: 0.6938694267515924 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 1.12; acc: 0.62
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.67
Batch: 100; loss: 0.95; acc: 0.67
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 0.84; acc: 0.73
Batch: 160; loss: 0.8; acc: 0.72
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.95; acc: 0.67
Batch: 220; loss: 1.0; acc: 0.66
Batch: 240; loss: 1.06; acc: 0.7
Batch: 260; loss: 0.89; acc: 0.64
Batch: 280; loss: 0.97; acc: 0.77
Batch: 300; loss: 1.06; acc: 0.62
Batch: 320; loss: 0.78; acc: 0.72
Batch: 340; loss: 0.99; acc: 0.7
Batch: 360; loss: 0.92; acc: 0.72
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 1.16; acc: 0.61
Batch: 420; loss: 0.73; acc: 0.8
Batch: 440; loss: 0.93; acc: 0.77
Batch: 460; loss: 1.18; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.66
Batch: 520; loss: 0.86; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 0.84; acc: 0.69
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 1.02; acc: 0.58
Batch: 620; loss: 1.12; acc: 0.59
Batch: 640; loss: 0.99; acc: 0.7
Batch: 660; loss: 1.08; acc: 0.62
Batch: 680; loss: 1.27; acc: 0.58
Batch: 700; loss: 0.97; acc: 0.67
Batch: 720; loss: 0.98; acc: 0.75
Batch: 740; loss: 0.8; acc: 0.66
Batch: 760; loss: 1.33; acc: 0.59
Batch: 780; loss: 1.03; acc: 0.66
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.68; acc: 0.72
Val Epoch over. val_loss: 0.9374332054025808; val_accuracy: 0.6933718152866242 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 0.76; acc: 0.73
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 1.05; acc: 0.69
Batch: 160; loss: 1.02; acc: 0.7
Batch: 180; loss: 0.9; acc: 0.67
Batch: 200; loss: 0.91; acc: 0.78
Batch: 220; loss: 1.1; acc: 0.73
Batch: 240; loss: 1.12; acc: 0.58
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 0.84; acc: 0.67
Batch: 300; loss: 1.05; acc: 0.77
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.92; acc: 0.7
Batch: 360; loss: 1.2; acc: 0.67
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 1.31; acc: 0.52
Batch: 420; loss: 1.13; acc: 0.7
Batch: 440; loss: 1.18; acc: 0.61
Batch: 460; loss: 1.07; acc: 0.67
Batch: 480; loss: 1.0; acc: 0.64
Batch: 500; loss: 0.97; acc: 0.69
Batch: 520; loss: 1.3; acc: 0.64
Batch: 540; loss: 1.32; acc: 0.53
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 1.5; acc: 0.56
Batch: 600; loss: 0.95; acc: 0.69
Batch: 620; loss: 0.81; acc: 0.7
Batch: 640; loss: 1.22; acc: 0.55
Batch: 660; loss: 0.84; acc: 0.72
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 1.07; acc: 0.7
Batch: 720; loss: 1.07; acc: 0.69
Batch: 740; loss: 1.05; acc: 0.61
Batch: 760; loss: 1.09; acc: 0.72
Batch: 780; loss: 1.11; acc: 0.64
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.93; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.75
Val Epoch over. val_loss: 0.9373593083612478; val_accuracy: 0.6935708598726115 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.13; acc: 0.59
Batch: 20; loss: 0.77; acc: 0.67
Batch: 40; loss: 0.82; acc: 0.75
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.97; acc: 0.67
Batch: 100; loss: 1.12; acc: 0.58
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 1.0; acc: 0.69
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.92; acc: 0.66
Batch: 200; loss: 0.76; acc: 0.66
Batch: 220; loss: 1.14; acc: 0.66
Batch: 240; loss: 0.86; acc: 0.77
Batch: 260; loss: 0.93; acc: 0.7
Batch: 280; loss: 1.05; acc: 0.66
Batch: 300; loss: 0.74; acc: 0.8
Batch: 320; loss: 0.9; acc: 0.73
Batch: 340; loss: 1.05; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.73
Batch: 380; loss: 0.83; acc: 0.77
Batch: 400; loss: 1.11; acc: 0.69
Batch: 420; loss: 1.1; acc: 0.64
Batch: 440; loss: 1.14; acc: 0.61
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 1.05; acc: 0.64
Batch: 500; loss: 1.23; acc: 0.7
Batch: 520; loss: 1.11; acc: 0.62
Batch: 540; loss: 1.13; acc: 0.61
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 0.81; acc: 0.73
Batch: 600; loss: 0.77; acc: 0.69
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.98; acc: 0.66
Batch: 660; loss: 1.03; acc: 0.7
Batch: 680; loss: 1.41; acc: 0.59
Batch: 700; loss: 0.78; acc: 0.69
Batch: 720; loss: 1.04; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.66
Batch: 760; loss: 1.37; acc: 0.56
Batch: 780; loss: 0.93; acc: 0.67
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.19; acc: 0.55
Batch: 20; loss: 0.97; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.67
Val Epoch over. val_loss: 0.939814211456639; val_accuracy: 0.6935708598726115 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.08; acc: 0.64
Batch: 20; loss: 0.94; acc: 0.64
Batch: 40; loss: 1.0; acc: 0.67
Batch: 60; loss: 1.09; acc: 0.59
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 1.01; acc: 0.67
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.91; acc: 0.72
Batch: 160; loss: 0.88; acc: 0.67
Batch: 180; loss: 1.02; acc: 0.67
Batch: 200; loss: 1.0; acc: 0.64
Batch: 220; loss: 1.08; acc: 0.61
Batch: 240; loss: 0.94; acc: 0.69
Batch: 260; loss: 0.87; acc: 0.66
Batch: 280; loss: 1.06; acc: 0.66
Batch: 300; loss: 0.87; acc: 0.78
Batch: 320; loss: 1.22; acc: 0.62
Batch: 340; loss: 1.0; acc: 0.67
Batch: 360; loss: 1.27; acc: 0.56
Batch: 380; loss: 0.74; acc: 0.72
Batch: 400; loss: 1.07; acc: 0.69
Batch: 420; loss: 0.98; acc: 0.73
Batch: 440; loss: 0.77; acc: 0.81
Batch: 460; loss: 1.07; acc: 0.75
Batch: 480; loss: 0.83; acc: 0.7
Batch: 500; loss: 1.23; acc: 0.58
Batch: 520; loss: 1.04; acc: 0.66
Batch: 540; loss: 0.89; acc: 0.62
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.7
Batch: 600; loss: 0.97; acc: 0.7
Batch: 620; loss: 1.07; acc: 0.67
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.61
Batch: 680; loss: 0.96; acc: 0.73
Batch: 700; loss: 0.97; acc: 0.67
Batch: 720; loss: 0.84; acc: 0.75
Batch: 740; loss: 0.93; acc: 0.67
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.94; acc: 0.67
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.18; acc: 0.56
Batch: 20; loss: 0.98; acc: 0.67
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.71; acc: 0.73
Val Epoch over. val_loss: 0.941140164805066; val_accuracy: 0.6938694267515924 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 1.14; acc: 0.52
Batch: 60; loss: 0.9; acc: 0.66
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 0.98; acc: 0.61
Batch: 200; loss: 1.08; acc: 0.62
Batch: 220; loss: 1.36; acc: 0.61
Batch: 240; loss: 0.99; acc: 0.7
Batch: 260; loss: 1.08; acc: 0.67
Batch: 280; loss: 1.0; acc: 0.67
Batch: 300; loss: 1.02; acc: 0.62
Batch: 320; loss: 1.03; acc: 0.62
Batch: 340; loss: 0.86; acc: 0.72
Batch: 360; loss: 0.78; acc: 0.75
Batch: 380; loss: 0.87; acc: 0.69
Batch: 400; loss: 1.04; acc: 0.67
Batch: 420; loss: 1.01; acc: 0.7
Batch: 440; loss: 1.22; acc: 0.61
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.58
Batch: 500; loss: 1.09; acc: 0.67
Batch: 520; loss: 0.99; acc: 0.67
Batch: 540; loss: 1.0; acc: 0.66
Batch: 560; loss: 0.78; acc: 0.69
Batch: 580; loss: 0.98; acc: 0.69
Batch: 600; loss: 1.06; acc: 0.59
Batch: 620; loss: 1.06; acc: 0.69
Batch: 640; loss: 0.98; acc: 0.7
Batch: 660; loss: 0.89; acc: 0.7
Batch: 680; loss: 0.89; acc: 0.69
Batch: 700; loss: 0.96; acc: 0.64
Batch: 720; loss: 0.96; acc: 0.62
Batch: 740; loss: 1.04; acc: 0.66
Batch: 760; loss: 1.26; acc: 0.62
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 0.93; acc: 0.67
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.69
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.66; acc: 0.77
Val Epoch over. val_loss: 0.9389521185379879; val_accuracy: 0.6948646496815286 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.93; acc: 0.72
Batch: 20; loss: 0.94; acc: 0.7
Batch: 40; loss: 0.96; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 1.13; acc: 0.66
Batch: 160; loss: 0.88; acc: 0.7
Batch: 180; loss: 0.8; acc: 0.75
Batch: 200; loss: 0.82; acc: 0.67
Batch: 220; loss: 0.86; acc: 0.73
Batch: 240; loss: 0.97; acc: 0.73
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 0.9; acc: 0.66
Batch: 300; loss: 0.77; acc: 0.7
Batch: 320; loss: 0.93; acc: 0.7
Batch: 340; loss: 0.75; acc: 0.73
Batch: 360; loss: 1.17; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.58
Batch: 400; loss: 0.97; acc: 0.67
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 0.91; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.72
Batch: 480; loss: 1.34; acc: 0.59
Batch: 500; loss: 0.92; acc: 0.72
Batch: 520; loss: 1.09; acc: 0.7
Batch: 540; loss: 1.06; acc: 0.75
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 1.02; acc: 0.67
Batch: 600; loss: 0.81; acc: 0.72
Batch: 620; loss: 0.75; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 0.89; acc: 0.72
Batch: 680; loss: 0.83; acc: 0.72
Batch: 700; loss: 0.8; acc: 0.72
Batch: 720; loss: 1.12; acc: 0.62
Batch: 740; loss: 0.96; acc: 0.69
Batch: 760; loss: 1.03; acc: 0.62
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.19; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.67
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.71; acc: 0.72
Val Epoch over. val_loss: 0.940591846112233; val_accuracy: 0.6923765923566879 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.88; acc: 0.62
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 0.81; acc: 0.67
Batch: 120; loss: 1.09; acc: 0.67
Batch: 140; loss: 0.86; acc: 0.72
Batch: 160; loss: 0.94; acc: 0.69
Batch: 180; loss: 1.06; acc: 0.64
Batch: 200; loss: 1.0; acc: 0.67
Batch: 220; loss: 1.06; acc: 0.67
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 1.04; acc: 0.67
Batch: 280; loss: 1.07; acc: 0.64
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 1.08; acc: 0.62
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 1.0; acc: 0.69
Batch: 380; loss: 0.94; acc: 0.73
Batch: 400; loss: 0.88; acc: 0.72
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 0.94; acc: 0.66
Batch: 460; loss: 0.97; acc: 0.62
Batch: 480; loss: 0.94; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.64
Batch: 520; loss: 0.99; acc: 0.59
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.58
Batch: 600; loss: 1.22; acc: 0.59
Batch: 620; loss: 0.88; acc: 0.77
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.7
Batch: 700; loss: 1.33; acc: 0.61
Batch: 720; loss: 0.96; acc: 0.62
Batch: 740; loss: 0.9; acc: 0.66
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.66
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.18; acc: 0.56
Batch: 20; loss: 0.97; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.7
Val Epoch over. val_loss: 0.940982500838626; val_accuracy: 0.6925756369426752 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 0.93; acc: 0.64
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.66
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.04; acc: 0.66
Batch: 140; loss: 1.22; acc: 0.69
Batch: 160; loss: 0.94; acc: 0.72
Batch: 180; loss: 0.73; acc: 0.75
Batch: 200; loss: 0.93; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 1.02; acc: 0.67
Batch: 280; loss: 1.25; acc: 0.61
Batch: 300; loss: 0.9; acc: 0.69
Batch: 320; loss: 0.89; acc: 0.69
Batch: 340; loss: 0.9; acc: 0.73
Batch: 360; loss: 1.22; acc: 0.59
Batch: 380; loss: 0.96; acc: 0.69
Batch: 400; loss: 0.92; acc: 0.66
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 0.99; acc: 0.67
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 1.16; acc: 0.59
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 1.19; acc: 0.62
Batch: 540; loss: 0.87; acc: 0.72
Batch: 560; loss: 0.95; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.64
Batch: 600; loss: 0.67; acc: 0.77
Batch: 620; loss: 0.96; acc: 0.69
Batch: 640; loss: 0.78; acc: 0.77
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.24; acc: 0.69
Batch: 700; loss: 0.79; acc: 0.73
Batch: 720; loss: 0.81; acc: 0.7
Batch: 740; loss: 0.87; acc: 0.7
Batch: 760; loss: 0.9; acc: 0.69
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.22; acc: 0.56
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.67
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.66; acc: 0.77
Val Epoch over. val_loss: 0.9367863342260859; val_accuracy: 0.6938694267515924 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 1.14; acc: 0.66
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 1.16; acc: 0.69
Batch: 100; loss: 0.78; acc: 0.75
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.97; acc: 0.61
Batch: 160; loss: 1.12; acc: 0.61
Batch: 180; loss: 0.86; acc: 0.75
Batch: 200; loss: 0.84; acc: 0.78
Batch: 220; loss: 0.92; acc: 0.61
Batch: 240; loss: 0.84; acc: 0.72
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.77
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.91; acc: 0.67
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.99; acc: 0.67
Batch: 420; loss: 0.86; acc: 0.7
Batch: 440; loss: 1.0; acc: 0.7
Batch: 460; loss: 1.01; acc: 0.67
Batch: 480; loss: 0.82; acc: 0.73
Batch: 500; loss: 1.07; acc: 0.66
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 0.99; acc: 0.66
Batch: 560; loss: 1.11; acc: 0.58
Batch: 580; loss: 0.88; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.73
Batch: 620; loss: 1.12; acc: 0.56
Batch: 640; loss: 0.84; acc: 0.8
Batch: 660; loss: 1.03; acc: 0.7
Batch: 680; loss: 0.84; acc: 0.78
Batch: 700; loss: 1.24; acc: 0.64
Batch: 720; loss: 0.89; acc: 0.69
Batch: 740; loss: 1.1; acc: 0.72
Batch: 760; loss: 1.07; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.72
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.19; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.73
Val Epoch over. val_loss: 0.9389380136872553; val_accuracy: 0.6936703821656051 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 1.02; acc: 0.69
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.69
Batch: 100; loss: 0.98; acc: 0.7
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.67
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.66
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 1.0; acc: 0.67
Batch: 240; loss: 0.82; acc: 0.67
Batch: 260; loss: 0.79; acc: 0.73
Batch: 280; loss: 1.32; acc: 0.53
Batch: 300; loss: 0.92; acc: 0.69
Batch: 320; loss: 0.97; acc: 0.75
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.77; acc: 0.77
Batch: 440; loss: 1.06; acc: 0.69
Batch: 460; loss: 0.98; acc: 0.67
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 1.02; acc: 0.64
Batch: 520; loss: 0.87; acc: 0.73
Batch: 540; loss: 1.15; acc: 0.69
Batch: 560; loss: 0.99; acc: 0.67
Batch: 580; loss: 0.98; acc: 0.67
Batch: 600; loss: 1.21; acc: 0.55
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.95; acc: 0.7
Batch: 660; loss: 0.96; acc: 0.72
Batch: 680; loss: 1.19; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 1.32; acc: 0.59
Batch: 780; loss: 0.97; acc: 0.61
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.73
Val Epoch over. val_loss: 0.9391673076304661; val_accuracy: 0.692078025477707 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 1.23; acc: 0.59
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 1.12; acc: 0.62
Batch: 100; loss: 0.92; acc: 0.69
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.97; acc: 0.72
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.95; acc: 0.69
Batch: 200; loss: 0.89; acc: 0.67
Batch: 220; loss: 1.32; acc: 0.62
Batch: 240; loss: 1.01; acc: 0.67
Batch: 260; loss: 0.97; acc: 0.72
Batch: 280; loss: 1.0; acc: 0.77
Batch: 300; loss: 0.94; acc: 0.64
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 0.8; acc: 0.77
Batch: 360; loss: 1.0; acc: 0.69
Batch: 380; loss: 1.36; acc: 0.64
Batch: 400; loss: 0.7; acc: 0.8
Batch: 420; loss: 0.91; acc: 0.72
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 1.01; acc: 0.64
Batch: 480; loss: 0.92; acc: 0.7
Batch: 500; loss: 1.14; acc: 0.62
Batch: 520; loss: 1.08; acc: 0.67
Batch: 540; loss: 1.2; acc: 0.61
Batch: 560; loss: 1.19; acc: 0.61
Batch: 580; loss: 1.22; acc: 0.64
Batch: 600; loss: 0.94; acc: 0.67
Batch: 620; loss: 1.24; acc: 0.64
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 0.97; acc: 0.67
Batch: 680; loss: 0.66; acc: 0.73
Batch: 700; loss: 1.0; acc: 0.7
Batch: 720; loss: 0.94; acc: 0.75
Batch: 740; loss: 0.94; acc: 0.67
Batch: 760; loss: 0.75; acc: 0.78
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.78
Val Epoch over. val_loss: 0.9381183695261646; val_accuracy: 0.6941679936305732 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.1; acc: 0.56
Batch: 20; loss: 1.19; acc: 0.56
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.81; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.56
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 1.04; acc: 0.69
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.93; acc: 0.7
Batch: 200; loss: 0.92; acc: 0.73
Batch: 220; loss: 0.83; acc: 0.7
Batch: 240; loss: 1.02; acc: 0.66
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 0.96; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.64
Batch: 320; loss: 0.94; acc: 0.62
Batch: 340; loss: 0.8; acc: 0.75
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 1.02; acc: 0.64
Batch: 400; loss: 0.95; acc: 0.7
Batch: 420; loss: 1.16; acc: 0.55
Batch: 440; loss: 0.89; acc: 0.66
Batch: 460; loss: 0.92; acc: 0.73
Batch: 480; loss: 1.16; acc: 0.59
Batch: 500; loss: 1.03; acc: 0.64
Batch: 520; loss: 1.18; acc: 0.55
Batch: 540; loss: 1.12; acc: 0.64
Batch: 560; loss: 1.03; acc: 0.62
Batch: 580; loss: 0.86; acc: 0.77
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 1.27; acc: 0.59
Batch: 640; loss: 0.94; acc: 0.64
Batch: 660; loss: 1.14; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.59
Batch: 700; loss: 0.86; acc: 0.67
Batch: 720; loss: 1.04; acc: 0.61
Batch: 740; loss: 1.27; acc: 0.59
Batch: 760; loss: 0.71; acc: 0.77
Batch: 780; loss: 1.07; acc: 0.61
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.67; acc: 0.78
Val Epoch over. val_loss: 0.9371560310861867; val_accuracy: 0.6933718152866242 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.98; acc: 0.62
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.61
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.87; acc: 0.67
Batch: 160; loss: 0.83; acc: 0.7
Batch: 180; loss: 0.87; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.69
Batch: 220; loss: 1.02; acc: 0.7
Batch: 240; loss: 0.89; acc: 0.67
Batch: 260; loss: 0.92; acc: 0.7
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 1.15; acc: 0.59
Batch: 320; loss: 0.86; acc: 0.72
Batch: 340; loss: 1.07; acc: 0.66
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 1.12; acc: 0.59
Batch: 400; loss: 0.75; acc: 0.75
Batch: 420; loss: 0.96; acc: 0.67
Batch: 440; loss: 1.07; acc: 0.58
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 0.86; acc: 0.7
Batch: 500; loss: 0.99; acc: 0.7
Batch: 520; loss: 1.14; acc: 0.61
Batch: 540; loss: 0.93; acc: 0.62
Batch: 560; loss: 1.18; acc: 0.64
Batch: 580; loss: 0.75; acc: 0.73
Batch: 600; loss: 1.03; acc: 0.66
Batch: 620; loss: 0.91; acc: 0.67
Batch: 640; loss: 1.18; acc: 0.69
Batch: 660; loss: 0.93; acc: 0.67
Batch: 680; loss: 0.8; acc: 0.78
Batch: 700; loss: 1.14; acc: 0.61
Batch: 720; loss: 0.91; acc: 0.75
Batch: 740; loss: 0.67; acc: 0.77
Batch: 760; loss: 0.98; acc: 0.7
Batch: 780; loss: 0.86; acc: 0.64
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.75
Val Epoch over. val_loss: 0.9378212174032904; val_accuracy: 0.6944665605095541 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.25; acc: 0.67
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 1.1; acc: 0.69
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.94; acc: 0.7
Batch: 160; loss: 1.03; acc: 0.56
Batch: 180; loss: 1.21; acc: 0.61
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 1.32; acc: 0.55
Batch: 240; loss: 0.9; acc: 0.7
Batch: 260; loss: 0.98; acc: 0.67
Batch: 280; loss: 1.05; acc: 0.73
Batch: 300; loss: 1.04; acc: 0.67
Batch: 320; loss: 1.01; acc: 0.62
Batch: 340; loss: 0.82; acc: 0.75
Batch: 360; loss: 0.97; acc: 0.7
Batch: 380; loss: 0.87; acc: 0.72
Batch: 400; loss: 0.84; acc: 0.77
Batch: 420; loss: 0.95; acc: 0.69
Batch: 440; loss: 1.13; acc: 0.59
Batch: 460; loss: 1.02; acc: 0.73
Batch: 480; loss: 1.02; acc: 0.67
Batch: 500; loss: 0.86; acc: 0.67
Batch: 520; loss: 1.27; acc: 0.62
Batch: 540; loss: 0.94; acc: 0.72
Batch: 560; loss: 1.17; acc: 0.62
Batch: 580; loss: 1.24; acc: 0.64
Batch: 600; loss: 1.04; acc: 0.64
Batch: 620; loss: 1.04; acc: 0.75
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 0.94; acc: 0.67
Batch: 680; loss: 0.93; acc: 0.64
Batch: 700; loss: 0.93; acc: 0.75
Batch: 720; loss: 1.0; acc: 0.62
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 1.01; acc: 0.66
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.72
Val Epoch over. val_loss: 0.9389874627635737; val_accuracy: 0.6946656050955414 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 1.18; acc: 0.59
Batch: 40; loss: 0.78; acc: 0.72
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.94; acc: 0.67
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 0.95; acc: 0.66
Batch: 180; loss: 1.46; acc: 0.59
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.98; acc: 0.69
Batch: 240; loss: 0.66; acc: 0.78
Batch: 260; loss: 0.99; acc: 0.66
Batch: 280; loss: 1.41; acc: 0.53
Batch: 300; loss: 1.18; acc: 0.61
Batch: 320; loss: 1.09; acc: 0.64
Batch: 340; loss: 0.79; acc: 0.69
Batch: 360; loss: 0.9; acc: 0.69
Batch: 380; loss: 0.98; acc: 0.66
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 0.93; acc: 0.7
Batch: 440; loss: 0.89; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.7
Batch: 480; loss: 0.95; acc: 0.72
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.84; acc: 0.72
Batch: 560; loss: 1.09; acc: 0.61
Batch: 580; loss: 0.85; acc: 0.72
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 1.06; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.64
Batch: 660; loss: 0.8; acc: 0.66
Batch: 680; loss: 1.08; acc: 0.64
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 1.06; acc: 0.7
Batch: 740; loss: 0.99; acc: 0.67
Batch: 760; loss: 0.71; acc: 0.72
Batch: 780; loss: 0.86; acc: 0.69
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9371977668658943; val_accuracy: 0.6938694267515924 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 1.33; acc: 0.62
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.66
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.81; acc: 0.7
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 1.24; acc: 0.69
Batch: 220; loss: 0.9; acc: 0.75
Batch: 240; loss: 0.95; acc: 0.69
Batch: 260; loss: 0.83; acc: 0.72
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 1.27; acc: 0.53
Batch: 320; loss: 1.31; acc: 0.59
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 1.06; acc: 0.73
Batch: 380; loss: 1.23; acc: 0.61
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.97; acc: 0.64
Batch: 440; loss: 1.06; acc: 0.64
Batch: 460; loss: 1.03; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 0.93; acc: 0.62
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 1.02; acc: 0.66
Batch: 560; loss: 0.88; acc: 0.67
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.95; acc: 0.69
Batch: 640; loss: 1.01; acc: 0.59
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.69
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.87; acc: 0.75
Batch: 740; loss: 0.94; acc: 0.69
Batch: 760; loss: 1.33; acc: 0.58
Batch: 780; loss: 1.06; acc: 0.64
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.19; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.77
Val Epoch over. val_loss: 0.9379401358829182; val_accuracy: 0.6929737261146497 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 0.85; acc: 0.64
Batch: 40; loss: 0.92; acc: 0.66
Batch: 60; loss: 0.87; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.66
Batch: 100; loss: 1.0; acc: 0.67
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 1.06; acc: 0.67
Batch: 160; loss: 1.31; acc: 0.56
Batch: 180; loss: 1.22; acc: 0.55
Batch: 200; loss: 1.35; acc: 0.56
Batch: 220; loss: 1.12; acc: 0.66
Batch: 240; loss: 0.91; acc: 0.77
Batch: 260; loss: 0.82; acc: 0.7
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 0.85; acc: 0.69
Batch: 320; loss: 0.87; acc: 0.67
Batch: 340; loss: 0.85; acc: 0.7
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 1.12; acc: 0.64
Batch: 400; loss: 1.21; acc: 0.66
Batch: 420; loss: 0.95; acc: 0.67
Batch: 440; loss: 1.02; acc: 0.69
Batch: 460; loss: 0.95; acc: 0.7
Batch: 480; loss: 0.94; acc: 0.69
Batch: 500; loss: 0.95; acc: 0.66
Batch: 520; loss: 0.81; acc: 0.7
Batch: 540; loss: 1.05; acc: 0.64
Batch: 560; loss: 1.07; acc: 0.64
Batch: 580; loss: 0.84; acc: 0.72
Batch: 600; loss: 0.75; acc: 0.66
Batch: 620; loss: 0.85; acc: 0.7
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.84; acc: 0.72
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 1.03; acc: 0.67
Batch: 760; loss: 0.93; acc: 0.75
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 0.9373522021208599; val_accuracy: 0.6941679936305732 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.59
Batch: 40; loss: 0.9; acc: 0.69
Batch: 60; loss: 0.88; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 1.3; acc: 0.58
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 1.11; acc: 0.61
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.78; acc: 0.72
Batch: 200; loss: 0.79; acc: 0.75
Batch: 220; loss: 0.75; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.62
Batch: 260; loss: 0.73; acc: 0.7
Batch: 280; loss: 0.85; acc: 0.67
Batch: 300; loss: 0.97; acc: 0.64
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 1.03; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 0.91; acc: 0.67
Batch: 400; loss: 1.03; acc: 0.66
Batch: 420; loss: 0.75; acc: 0.72
Batch: 440; loss: 1.07; acc: 0.58
Batch: 460; loss: 1.11; acc: 0.69
Batch: 480; loss: 1.21; acc: 0.59
Batch: 500; loss: 1.0; acc: 0.69
Batch: 520; loss: 0.88; acc: 0.72
Batch: 540; loss: 1.1; acc: 0.73
Batch: 560; loss: 1.06; acc: 0.66
Batch: 580; loss: 1.23; acc: 0.64
Batch: 600; loss: 1.14; acc: 0.55
Batch: 620; loss: 0.96; acc: 0.67
Batch: 640; loss: 0.89; acc: 0.75
Batch: 660; loss: 0.86; acc: 0.7
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 0.81; acc: 0.75
Batch: 720; loss: 0.88; acc: 0.69
Batch: 740; loss: 0.89; acc: 0.67
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 0.95; acc: 0.62
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9376373061328936; val_accuracy: 0.6935708598726115 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.04; acc: 0.61
Batch: 20; loss: 0.99; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.78
Batch: 60; loss: 1.01; acc: 0.67
Batch: 80; loss: 0.82; acc: 0.69
Batch: 100; loss: 0.95; acc: 0.62
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.88; acc: 0.69
Batch: 160; loss: 0.85; acc: 0.72
Batch: 180; loss: 0.76; acc: 0.7
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 1.0; acc: 0.66
Batch: 240; loss: 1.09; acc: 0.64
Batch: 260; loss: 0.99; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.64
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 0.74; acc: 0.75
Batch: 340; loss: 1.03; acc: 0.72
Batch: 360; loss: 0.95; acc: 0.69
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.99; acc: 0.62
Batch: 420; loss: 0.76; acc: 0.73
Batch: 440; loss: 1.16; acc: 0.59
Batch: 460; loss: 1.04; acc: 0.66
Batch: 480; loss: 1.17; acc: 0.69
Batch: 500; loss: 0.9; acc: 0.72
Batch: 520; loss: 0.91; acc: 0.66
Batch: 540; loss: 1.1; acc: 0.58
Batch: 560; loss: 0.88; acc: 0.72
Batch: 580; loss: 1.11; acc: 0.66
Batch: 600; loss: 0.95; acc: 0.7
Batch: 620; loss: 0.86; acc: 0.69
Batch: 640; loss: 1.17; acc: 0.59
Batch: 660; loss: 0.96; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.64
Batch: 720; loss: 1.04; acc: 0.61
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 1.02; acc: 0.61
Batch: 780; loss: 0.91; acc: 0.77
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 0.95; acc: 0.67
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9373873811998185; val_accuracy: 0.6929737261146497 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.93; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.67
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.82; acc: 0.72
Batch: 140; loss: 0.75; acc: 0.75
Batch: 160; loss: 0.92; acc: 0.64
Batch: 180; loss: 0.89; acc: 0.67
Batch: 200; loss: 0.91; acc: 0.69
Batch: 220; loss: 0.88; acc: 0.73
Batch: 240; loss: 1.11; acc: 0.64
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.94; acc: 0.75
Batch: 300; loss: 0.91; acc: 0.67
Batch: 320; loss: 1.05; acc: 0.72
Batch: 340; loss: 1.16; acc: 0.72
Batch: 360; loss: 0.7; acc: 0.77
Batch: 380; loss: 0.98; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 0.88; acc: 0.66
Batch: 440; loss: 0.88; acc: 0.7
Batch: 460; loss: 1.04; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.67
Batch: 500; loss: 1.07; acc: 0.64
Batch: 520; loss: 1.03; acc: 0.75
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 1.02; acc: 0.66
Batch: 600; loss: 0.96; acc: 0.72
Batch: 620; loss: 1.1; acc: 0.62
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.94; acc: 0.67
Batch: 700; loss: 0.93; acc: 0.75
Batch: 720; loss: 0.95; acc: 0.69
Batch: 740; loss: 0.97; acc: 0.66
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 1.23; acc: 0.61
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 0.937561338304714; val_accuracy: 0.6924761146496815 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 0.75; acc: 0.75
Batch: 60; loss: 1.45; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.09; acc: 0.62
Batch: 120; loss: 1.26; acc: 0.61
Batch: 140; loss: 0.82; acc: 0.72
Batch: 160; loss: 1.0; acc: 0.7
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.88; acc: 0.64
Batch: 220; loss: 0.78; acc: 0.73
Batch: 240; loss: 0.99; acc: 0.72
Batch: 260; loss: 1.23; acc: 0.58
Batch: 280; loss: 1.05; acc: 0.67
Batch: 300; loss: 0.87; acc: 0.67
Batch: 320; loss: 0.87; acc: 0.7
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 1.13; acc: 0.58
Batch: 380; loss: 1.51; acc: 0.58
Batch: 400; loss: 1.07; acc: 0.64
Batch: 420; loss: 1.08; acc: 0.66
Batch: 440; loss: 0.82; acc: 0.72
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 1.0; acc: 0.64
Batch: 500; loss: 0.91; acc: 0.72
Batch: 520; loss: 1.23; acc: 0.59
Batch: 540; loss: 1.03; acc: 0.7
Batch: 560; loss: 0.92; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.77
Batch: 600; loss: 1.36; acc: 0.53
Batch: 620; loss: 0.81; acc: 0.72
Batch: 640; loss: 0.65; acc: 0.75
Batch: 660; loss: 1.26; acc: 0.5
Batch: 680; loss: 0.85; acc: 0.73
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 0.86; acc: 0.67
Batch: 740; loss: 0.77; acc: 0.75
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.66
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.95; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 0.9371441991845514; val_accuracy: 0.6930732484076433 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.91; acc: 0.61
Batch: 60; loss: 0.93; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 0.9; acc: 0.66
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.91; acc: 0.69
Batch: 160; loss: 1.07; acc: 0.64
Batch: 180; loss: 1.07; acc: 0.61
Batch: 200; loss: 1.03; acc: 0.69
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 1.12; acc: 0.67
Batch: 260; loss: 0.97; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.66
Batch: 300; loss: 0.9; acc: 0.72
Batch: 320; loss: 0.82; acc: 0.7
Batch: 340; loss: 1.21; acc: 0.59
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 1.09; acc: 0.62
Batch: 400; loss: 1.06; acc: 0.73
Batch: 420; loss: 0.95; acc: 0.73
Batch: 440; loss: 0.79; acc: 0.73
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 0.88; acc: 0.73
Batch: 500; loss: 1.38; acc: 0.59
Batch: 520; loss: 0.85; acc: 0.73
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.9; acc: 0.67
Batch: 600; loss: 1.12; acc: 0.59
Batch: 620; loss: 0.89; acc: 0.69
Batch: 640; loss: 0.95; acc: 0.72
Batch: 660; loss: 0.91; acc: 0.7
Batch: 680; loss: 1.17; acc: 0.62
Batch: 700; loss: 1.12; acc: 0.64
Batch: 720; loss: 0.97; acc: 0.7
Batch: 740; loss: 1.13; acc: 0.58
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.07; acc: 0.66
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.66
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.75
Val Epoch over. val_loss: 0.9377348033865546; val_accuracy: 0.6926751592356688 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.67
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 1.12; acc: 0.62
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.61
Batch: 160; loss: 0.75; acc: 0.73
Batch: 180; loss: 1.05; acc: 0.64
Batch: 200; loss: 0.94; acc: 0.72
Batch: 220; loss: 1.01; acc: 0.75
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 1.14; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.72
Batch: 320; loss: 1.07; acc: 0.55
Batch: 340; loss: 0.88; acc: 0.66
Batch: 360; loss: 0.78; acc: 0.7
Batch: 380; loss: 1.0; acc: 0.69
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.66
Batch: 460; loss: 0.74; acc: 0.8
Batch: 480; loss: 0.85; acc: 0.69
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 1.36; acc: 0.59
Batch: 540; loss: 0.92; acc: 0.72
Batch: 560; loss: 0.9; acc: 0.7
Batch: 580; loss: 0.87; acc: 0.67
Batch: 600; loss: 1.06; acc: 0.69
Batch: 620; loss: 0.91; acc: 0.73
Batch: 640; loss: 1.02; acc: 0.67
Batch: 660; loss: 0.99; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.69
Batch: 700; loss: 0.93; acc: 0.62
Batch: 720; loss: 1.25; acc: 0.59
Batch: 740; loss: 1.06; acc: 0.56
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 0.9; acc: 0.77
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 0.937996259730333; val_accuracy: 0.6926751592356688 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 0.93; acc: 0.67
Batch: 40; loss: 1.09; acc: 0.61
Batch: 60; loss: 1.0; acc: 0.59
Batch: 80; loss: 0.93; acc: 0.67
Batch: 100; loss: 0.88; acc: 0.7
Batch: 120; loss: 0.9; acc: 0.66
Batch: 140; loss: 1.06; acc: 0.66
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 1.03; acc: 0.69
Batch: 200; loss: 1.14; acc: 0.59
Batch: 220; loss: 1.04; acc: 0.66
Batch: 240; loss: 1.1; acc: 0.7
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 0.92; acc: 0.7
Batch: 300; loss: 0.88; acc: 0.72
Batch: 320; loss: 0.97; acc: 0.73
Batch: 340; loss: 1.12; acc: 0.69
Batch: 360; loss: 0.87; acc: 0.66
Batch: 380; loss: 0.7; acc: 0.78
Batch: 400; loss: 0.89; acc: 0.73
Batch: 420; loss: 1.0; acc: 0.7
Batch: 440; loss: 1.14; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.78
Batch: 480; loss: 0.93; acc: 0.66
Batch: 500; loss: 0.92; acc: 0.67
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 1.05; acc: 0.64
Batch: 560; loss: 0.96; acc: 0.66
Batch: 580; loss: 0.77; acc: 0.72
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.2; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.59
Batch: 660; loss: 0.99; acc: 0.66
Batch: 680; loss: 1.23; acc: 0.59
Batch: 700; loss: 0.77; acc: 0.73
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 1.18; acc: 0.7
Batch: 760; loss: 0.9; acc: 0.72
Batch: 780; loss: 1.04; acc: 0.73
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9375859470504104; val_accuracy: 0.692078025477707 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.88; acc: 0.64
Batch: 20; loss: 0.86; acc: 0.67
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 0.9; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 1.21; acc: 0.55
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 1.04; acc: 0.62
Batch: 160; loss: 1.11; acc: 0.59
Batch: 180; loss: 1.24; acc: 0.62
Batch: 200; loss: 1.09; acc: 0.64
Batch: 220; loss: 0.93; acc: 0.81
Batch: 240; loss: 0.99; acc: 0.66
Batch: 260; loss: 0.91; acc: 0.72
Batch: 280; loss: 0.94; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.67
Batch: 320; loss: 0.95; acc: 0.67
Batch: 340; loss: 0.95; acc: 0.73
Batch: 360; loss: 0.96; acc: 0.73
Batch: 380; loss: 0.85; acc: 0.72
Batch: 400; loss: 1.02; acc: 0.66
Batch: 420; loss: 0.79; acc: 0.67
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.97; acc: 0.62
Batch: 480; loss: 1.02; acc: 0.72
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 1.03; acc: 0.69
Batch: 540; loss: 0.79; acc: 0.7
Batch: 560; loss: 0.94; acc: 0.73
Batch: 580; loss: 0.86; acc: 0.77
Batch: 600; loss: 0.81; acc: 0.72
Batch: 620; loss: 0.91; acc: 0.64
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.88; acc: 0.75
Batch: 680; loss: 1.32; acc: 0.56
Batch: 700; loss: 1.05; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.61
Batch: 740; loss: 0.82; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.69
Batch: 780; loss: 0.89; acc: 0.72
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.75
Val Epoch over. val_loss: 0.9376702437734907; val_accuracy: 0.6927746815286624 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.02; acc: 0.67
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 1.13; acc: 0.55
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.08; acc: 0.66
Batch: 160; loss: 1.17; acc: 0.56
Batch: 180; loss: 1.04; acc: 0.67
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.0; acc: 0.58
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 1.09; acc: 0.58
Batch: 280; loss: 0.83; acc: 0.69
Batch: 300; loss: 0.91; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.62
Batch: 340; loss: 0.83; acc: 0.7
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 0.84; acc: 0.72
Batch: 420; loss: 0.87; acc: 0.73
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 1.06; acc: 0.59
Batch: 520; loss: 0.74; acc: 0.8
Batch: 540; loss: 1.06; acc: 0.61
Batch: 560; loss: 1.2; acc: 0.58
Batch: 580; loss: 0.83; acc: 0.73
Batch: 600; loss: 1.01; acc: 0.73
Batch: 620; loss: 0.81; acc: 0.7
Batch: 640; loss: 1.06; acc: 0.73
Batch: 660; loss: 0.87; acc: 0.7
Batch: 680; loss: 0.72; acc: 0.7
Batch: 700; loss: 1.08; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 0.98; acc: 0.7
Batch: 760; loss: 1.13; acc: 0.64
Batch: 780; loss: 0.95; acc: 0.72
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 0.96; acc: 0.66
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.75
Val Epoch over. val_loss: 0.9381464799498297; val_accuracy: 0.6918789808917197 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.85; acc: 0.69
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 1.1; acc: 0.58
Batch: 60; loss: 1.16; acc: 0.59
Batch: 80; loss: 1.16; acc: 0.53
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.85; acc: 0.73
Batch: 160; loss: 0.85; acc: 0.7
Batch: 180; loss: 1.08; acc: 0.59
Batch: 200; loss: 1.07; acc: 0.66
Batch: 220; loss: 1.01; acc: 0.7
Batch: 240; loss: 1.0; acc: 0.67
Batch: 260; loss: 0.87; acc: 0.77
Batch: 280; loss: 0.96; acc: 0.69
Batch: 300; loss: 0.98; acc: 0.69
Batch: 320; loss: 1.1; acc: 0.62
Batch: 340; loss: 1.31; acc: 0.69
Batch: 360; loss: 0.89; acc: 0.72
Batch: 380; loss: 0.96; acc: 0.64
Batch: 400; loss: 0.99; acc: 0.64
Batch: 420; loss: 0.77; acc: 0.81
Batch: 440; loss: 0.88; acc: 0.66
Batch: 460; loss: 0.99; acc: 0.69
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 1.15; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 0.9; acc: 0.67
Batch: 560; loss: 0.91; acc: 0.66
Batch: 580; loss: 0.91; acc: 0.72
Batch: 600; loss: 1.1; acc: 0.66
Batch: 620; loss: 0.87; acc: 0.67
Batch: 640; loss: 1.23; acc: 0.64
Batch: 660; loss: 1.03; acc: 0.69
Batch: 680; loss: 0.94; acc: 0.62
Batch: 700; loss: 1.18; acc: 0.62
Batch: 720; loss: 0.98; acc: 0.7
Batch: 740; loss: 1.0; acc: 0.59
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 1.0; acc: 0.7
Train Epoch over. train_loss: 0.98; train_accuracy: 0.68 

Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9378057674617525; val_accuracy: 0.6934713375796179 

plots/subspace_training/reg_lenet_3/2020-01-20 16:50:48/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 3374248
elements in E: 3374250
fraction nonzero: 0.9999994072756909
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.03
Batch: 160; loss: 2.31; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.17
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.2
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.16
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.27; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.19
Batch: 420; loss: 2.27; acc: 0.2
Batch: 440; loss: 2.28; acc: 0.05
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.2
Batch: 500; loss: 2.27; acc: 0.16
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.27; acc: 0.09
Batch: 560; loss: 2.27; acc: 0.17
Batch: 580; loss: 2.27; acc: 0.14
Batch: 600; loss: 2.26; acc: 0.17
Batch: 620; loss: 2.26; acc: 0.16
Batch: 640; loss: 2.26; acc: 0.17
Batch: 660; loss: 2.26; acc: 0.11
Batch: 680; loss: 2.24; acc: 0.16
Batch: 700; loss: 2.23; acc: 0.16
Batch: 720; loss: 2.22; acc: 0.12
Batch: 740; loss: 2.21; acc: 0.22
Batch: 760; loss: 2.22; acc: 0.17
Batch: 780; loss: 2.22; acc: 0.17
Train Epoch over. train_loss: 2.28; train_accuracy: 0.14 

Batch: 0; loss: 2.22; acc: 0.16
Batch: 20; loss: 2.23; acc: 0.12
Batch: 40; loss: 2.19; acc: 0.12
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.19; acc: 0.19
Batch: 100; loss: 2.2; acc: 0.17
Batch: 120; loss: 2.22; acc: 0.09
Batch: 140; loss: 2.2; acc: 0.19
Val Epoch over. val_loss: 2.2049693119753697; val_accuracy: 0.1761544585987261 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.21; acc: 0.16
Batch: 20; loss: 2.2; acc: 0.06
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.2; acc: 0.16
Batch: 100; loss: 2.15; acc: 0.19
Batch: 120; loss: 2.19; acc: 0.16
Batch: 140; loss: 2.15; acc: 0.28
Batch: 160; loss: 2.15; acc: 0.16
Batch: 180; loss: 2.14; acc: 0.27
Batch: 200; loss: 2.1; acc: 0.25
Batch: 220; loss: 2.11; acc: 0.22
Batch: 240; loss: 2.07; acc: 0.27
Batch: 260; loss: 2.09; acc: 0.2
Batch: 280; loss: 2.07; acc: 0.28
Batch: 300; loss: 2.03; acc: 0.2
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.27
Batch: 360; loss: 2.0; acc: 0.27
Batch: 380; loss: 1.84; acc: 0.33
Batch: 400; loss: 1.89; acc: 0.36
Batch: 420; loss: 1.95; acc: 0.25
Batch: 440; loss: 1.74; acc: 0.38
Batch: 460; loss: 1.54; acc: 0.47
Batch: 480; loss: 1.73; acc: 0.33
Batch: 500; loss: 1.6; acc: 0.45
Batch: 520; loss: 1.41; acc: 0.58
Batch: 540; loss: 1.28; acc: 0.58
Batch: 560; loss: 1.3; acc: 0.55
Batch: 580; loss: 1.44; acc: 0.55
Batch: 600; loss: 1.2; acc: 0.67
Batch: 620; loss: 1.31; acc: 0.59
Batch: 640; loss: 0.98; acc: 0.72
Batch: 660; loss: 1.37; acc: 0.62
Batch: 680; loss: 1.1; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.12; acc: 0.66
Batch: 740; loss: 0.98; acc: 0.69
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.3; acc: 0.5
Train Epoch over. train_loss: 1.74; train_accuracy: 0.38 

Batch: 0; loss: 1.42; acc: 0.52
Batch: 20; loss: 1.27; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.64
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.87; acc: 0.69
Batch: 100; loss: 1.1; acc: 0.64
Batch: 120; loss: 1.14; acc: 0.61
Batch: 140; loss: 0.67; acc: 0.73
Val Epoch over. val_loss: 1.0495244279788558; val_accuracy: 0.6520700636942676 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 1.08; acc: 0.66
Batch: 60; loss: 1.1; acc: 0.67
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 0.9; acc: 0.75
Batch: 160; loss: 0.89; acc: 0.72
Batch: 180; loss: 1.05; acc: 0.69
Batch: 200; loss: 1.3; acc: 0.56
Batch: 220; loss: 0.97; acc: 0.72
Batch: 240; loss: 0.87; acc: 0.75
Batch: 260; loss: 0.92; acc: 0.69
Batch: 280; loss: 1.0; acc: 0.62
Batch: 300; loss: 0.98; acc: 0.67
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 1.36; acc: 0.59
Batch: 360; loss: 0.86; acc: 0.75
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 1.11; acc: 0.67
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 1.28; acc: 0.58
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.08; acc: 0.61
Batch: 520; loss: 0.91; acc: 0.73
Batch: 540; loss: 0.86; acc: 0.64
Batch: 560; loss: 0.91; acc: 0.72
Batch: 580; loss: 0.96; acc: 0.66
Batch: 600; loss: 0.96; acc: 0.73
Batch: 620; loss: 1.07; acc: 0.73
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 1.09; acc: 0.67
Batch: 680; loss: 0.89; acc: 0.7
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.97; acc: 0.67
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 0.84; acc: 0.75
Train Epoch over. train_loss: 0.99; train_accuracy: 0.69 

Batch: 0; loss: 1.32; acc: 0.55
Batch: 20; loss: 1.06; acc: 0.62
Batch: 40; loss: 0.71; acc: 0.86
Batch: 60; loss: 1.02; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.78
Batch: 120; loss: 1.16; acc: 0.73
Batch: 140; loss: 0.45; acc: 0.86
Val Epoch over. val_loss: 0.9678979018691239; val_accuracy: 0.6886942675159236 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.72
Batch: 60; loss: 1.01; acc: 0.67
Batch: 80; loss: 0.78; acc: 0.72
Batch: 100; loss: 1.26; acc: 0.69
Batch: 120; loss: 0.98; acc: 0.66
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.83; acc: 0.7
Batch: 240; loss: 0.8; acc: 0.7
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.91; acc: 0.73
Batch: 300; loss: 0.76; acc: 0.77
Batch: 320; loss: 0.99; acc: 0.67
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.73
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 0.93; acc: 0.7
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.8
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.93; acc: 0.73
Batch: 540; loss: 0.62; acc: 0.78
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 0.79; acc: 0.77
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 0.88; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 0.99; acc: 0.59
Batch: 760; loss: 1.05; acc: 0.64
Batch: 780; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 0.76; acc: 0.67
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.87; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.88
Val Epoch over. val_loss: 0.7450442046496519; val_accuracy: 0.7717953821656051 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.88; acc: 0.64
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 1.09; acc: 0.66
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.79; acc: 0.78
Batch: 300; loss: 0.94; acc: 0.69
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.75
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.84; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.62; acc: 0.75
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.84; acc: 0.77
Batch: 540; loss: 0.73; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.66
Batch: 660; loss: 0.71; acc: 0.78
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.69
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.46; acc: 0.8
Val Epoch over. val_loss: 0.65780685320022; val_accuracy: 0.790406050955414 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.64
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.81; acc: 0.73
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.7
Batch: 400; loss: 0.7; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.75
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.66; acc: 0.75
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.73; acc: 0.77
Batch: 620; loss: 1.0; acc: 0.73
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.69; acc: 0.77
Batch: 700; loss: 1.0; acc: 0.69
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.84; acc: 0.64
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.84; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.66
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.6587567175649534; val_accuracy: 0.7972730891719745 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 0.47; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.73
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.94; acc: 0.7
Batch: 320; loss: 0.9; acc: 0.81
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.89; acc: 0.67
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.84; acc: 0.69
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.75; acc: 0.67
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.86
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.75; acc: 0.72
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.88; acc: 0.78
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 1.42; acc: 0.55
Batch: 20; loss: 1.44; acc: 0.55
Batch: 40; loss: 0.81; acc: 0.78
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 0.88; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.69
Batch: 120; loss: 1.79; acc: 0.61
Batch: 140; loss: 0.95; acc: 0.69
Val Epoch over. val_loss: 1.138359611960733; val_accuracy: 0.6596337579617835 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.69
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 1.03; acc: 0.67
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.64; acc: 0.73
Batch: 160; loss: 0.71; acc: 0.81
Batch: 180; loss: 0.75; acc: 0.67
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.64; acc: 0.77
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.69; acc: 0.77
Batch: 280; loss: 0.59; acc: 0.8
Batch: 300; loss: 0.77; acc: 0.77
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.8; acc: 0.73
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.77
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.7; acc: 0.75
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.83; acc: 0.7
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.57; acc: 0.78
Batch: 720; loss: 0.73; acc: 0.75
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.81; acc: 0.72
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 0.73; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 0.42; acc: 0.84
Val Epoch over. val_loss: 0.7582599387806692; val_accuracy: 0.7592555732484076 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.75
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.72
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.78
Batch: 400; loss: 0.67; acc: 0.77
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.98; acc: 0.64
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.72; acc: 0.8
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.85; acc: 0.67
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 1.09; acc: 0.62
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.7441827372001235; val_accuracy: 0.7639331210191083 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.72
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.69; acc: 0.7
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.77
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.81; acc: 0.7
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.77
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.78
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.38; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.57; acc: 0.78
Batch: 720; loss: 0.59; acc: 0.77
Batch: 740; loss: 0.75; acc: 0.67
Batch: 760; loss: 0.84; acc: 0.73
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.67; acc: 0.75
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.88
Val Epoch over. val_loss: 0.5631962776373906; val_accuracy: 0.8199641719745223 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.69
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.63; acc: 0.75
Batch: 160; loss: 0.61; acc: 0.77
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.81
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.77
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.58; acc: 0.8
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.78
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5362818122479567; val_accuracy: 0.8387738853503185 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.45; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.72; acc: 0.77
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.61; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.77
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.67; acc: 0.73
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.5324787307696738; val_accuracy: 0.8373805732484076 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.71; acc: 0.73
Batch: 200; loss: 0.65; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.78
Batch: 280; loss: 0.73; acc: 0.75
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.89; acc: 0.72
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.51; acc: 0.83
Batch: 420; loss: 0.78; acc: 0.73
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.75
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.75
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.78
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.44; acc: 0.88
Val Epoch over. val_loss: 0.5571378897519628; val_accuracy: 0.8275278662420382 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.75
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.86; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.8
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 0.85; acc: 0.7
Batch: 620; loss: 0.53; acc: 0.8
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.9; acc: 0.78
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.88
Val Epoch over. val_loss: 0.5720700395714705; val_accuracy: 0.8256369426751592 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.83
Batch: 180; loss: 0.77; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.73
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.83; acc: 0.73
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.81
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.5533837069561527; val_accuracy: 0.8316082802547771 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.77
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.73
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.92; acc: 0.67
Batch: 680; loss: 0.58; acc: 0.75
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.5431980328385237; val_accuracy: 0.8314092356687898 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.73
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.75
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.75
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5382315457626513; val_accuracy: 0.8347929936305732 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.8
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.75
Batch: 220; loss: 0.69; acc: 0.73
Batch: 240; loss: 0.67; acc: 0.75
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.78; acc: 0.77
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.84; acc: 0.7
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.61; acc: 0.77
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.69; acc: 0.77
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.83; acc: 0.72
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.78
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5484516203023826; val_accuracy: 0.8275278662420382 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.84; acc: 0.7
Batch: 180; loss: 0.78; acc: 0.73
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.78; acc: 0.73
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.65; acc: 0.75
Batch: 380; loss: 0.62; acc: 0.77
Batch: 400; loss: 0.89; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.77
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.8
Batch: 540; loss: 0.86; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.75
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.71; acc: 0.73
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.58; acc: 0.75
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.5448891504365168; val_accuracy: 0.8322054140127388 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.86; acc: 0.72
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.79; acc: 0.78
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.75
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5433415772428938; val_accuracy: 0.8307125796178344 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.77
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.76; acc: 0.73
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.73
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.73
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.61; acc: 0.75
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.5296835704783726; val_accuracy: 0.8354896496815286 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.77
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.72
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.78
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.77
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.75
Batch: 360; loss: 0.72; acc: 0.77
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.74; acc: 0.73
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.75
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.75
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.79; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.81; acc: 0.7
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.5338686111436528; val_accuracy: 0.8362858280254777 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.74; acc: 0.75
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.67; acc: 0.77
Batch: 220; loss: 0.6; acc: 0.78
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.78
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.7
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.78
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.73
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.73
Batch: 540; loss: 0.62; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.7
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.67; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5238789282976442; val_accuracy: 0.8384753184713376 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.0; acc: 0.73
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.59; acc: 0.77
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.89; acc: 0.7
Batch: 340; loss: 0.99; acc: 0.75
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.77
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.85; acc: 0.75
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.77
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.75
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.75
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.5270480165246186; val_accuracy: 0.8401671974522293 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.59; acc: 0.77
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.78
Batch: 320; loss: 0.41; acc: 0.81
Batch: 340; loss: 0.87; acc: 0.67
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.77
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.77
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.78
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.8; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.72
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.6; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.5237773941580657; val_accuracy: 0.8385748407643312 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.7
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.73
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.64; acc: 0.73
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.81; acc: 0.75
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.84; acc: 0.75
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.81; acc: 0.77
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.52; acc: 0.78
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.69; acc: 0.78
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.41; acc: 0.83
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.47; acc: 0.81
Batch: 740; loss: 0.61; acc: 0.78
Batch: 760; loss: 0.97; acc: 0.77
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5291830640120111; val_accuracy: 0.8385748407643312 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.75
Batch: 180; loss: 0.79; acc: 0.7
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.78
Batch: 340; loss: 0.53; acc: 0.8
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.95; acc: 0.7
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 0.64; acc: 0.77
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.77
Batch: 600; loss: 0.83; acc: 0.77
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.83; acc: 0.77
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5254420980716207; val_accuracy: 0.8405652866242038 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.88; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.75
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.77; acc: 0.72
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.77
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.88; acc: 0.7
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.77
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.78
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.8
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.77
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.5277001062396226; val_accuracy: 0.8367834394904459 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.75
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.72
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.82; acc: 0.72
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.94; acc: 0.67
Batch: 400; loss: 0.86; acc: 0.73
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.91; acc: 0.72
Batch: 540; loss: 0.59; acc: 0.8
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.65; acc: 0.73
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.78
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.75
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.5262100010351011; val_accuracy: 0.8367834394904459 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.78; acc: 0.7
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.85; acc: 0.75
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.98; acc: 0.75
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.81; acc: 0.77
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.61; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.74; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.75
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.531404567657003; val_accuracy: 0.8367834394904459 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.82; acc: 0.73
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.75
Batch: 400; loss: 0.88; acc: 0.73
Batch: 420; loss: 0.57; acc: 0.78
Batch: 440; loss: 0.64; acc: 0.77
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.75
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.5226861291630253; val_accuracy: 0.836484872611465 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.76; acc: 0.7
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.78
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.84; acc: 0.75
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.77
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.78
Batch: 420; loss: 0.64; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.78
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.69; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.76; acc: 0.75
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.99; acc: 0.75
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.5252741543920176; val_accuracy: 0.8388734076433121 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.73
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.57; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.72
Batch: 240; loss: 0.43; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 0.68; acc: 0.77
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.79; acc: 0.73
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.9; acc: 0.69
Batch: 740; loss: 0.58; acc: 0.77
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.521569435668599; val_accuracy: 0.8375796178343949 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.56; acc: 0.78
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.75
Batch: 300; loss: 0.71; acc: 0.78
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.98; acc: 0.7
Batch: 420; loss: 0.75; acc: 0.75
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.94; acc: 0.77
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.77
Batch: 720; loss: 0.65; acc: 0.77
Batch: 740; loss: 0.57; acc: 0.75
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5219572426597024; val_accuracy: 0.8367834394904459 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.9; acc: 0.7
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.85; acc: 0.7
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.72
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.86; acc: 0.67
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.73
Batch: 540; loss: 0.86; acc: 0.73
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.77
Batch: 600; loss: 0.8; acc: 0.78
Batch: 620; loss: 0.55; acc: 0.78
Batch: 640; loss: 0.65; acc: 0.78
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.79; acc: 0.75
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.98; acc: 0.7
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.5232337493995193; val_accuracy: 0.8396695859872612 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.72
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.8; acc: 0.73
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.85; acc: 0.81
Batch: 520; loss: 0.57; acc: 0.77
Batch: 540; loss: 0.69; acc: 0.72
Batch: 560; loss: 0.75; acc: 0.7
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.78
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5237564659042723; val_accuracy: 0.8381767515923567 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.7
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.73
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 1.2; acc: 0.67
Batch: 380; loss: 0.47; acc: 0.8
Batch: 400; loss: 0.86; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.77
Batch: 560; loss: 0.67; acc: 0.75
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.75
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.52; acc: 0.8
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.77
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.5223898080883512; val_accuracy: 0.8379777070063694 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.78
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.75
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.73
Batch: 480; loss: 0.69; acc: 0.75
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.75; acc: 0.72
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.77
Batch: 700; loss: 0.62; acc: 0.75
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.77
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5216021304297599; val_accuracy: 0.8409633757961783 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.73
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.88; acc: 0.7
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.77
Batch: 240; loss: 0.63; acc: 0.77
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.57; acc: 0.78
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.77
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.86; acc: 0.69
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.99; acc: 0.72
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.8
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5230967793495033; val_accuracy: 0.8374800955414012 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.79; acc: 0.7
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.81; acc: 0.7
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.73
Batch: 180; loss: 0.61; acc: 0.78
Batch: 200; loss: 0.6; acc: 0.78
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.75
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.75
Batch: 380; loss: 0.6; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.75; acc: 0.75
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.5206508712404093; val_accuracy: 0.8400676751592356 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.7
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.87; acc: 0.75
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.73; acc: 0.77
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.77
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.78
Batch: 640; loss: 0.47; acc: 0.77
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.77
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.5219248898659542; val_accuracy: 0.839171974522293 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.75
Batch: 160; loss: 0.5; acc: 0.81
Batch: 180; loss: 0.87; acc: 0.72
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.78
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.86; acc: 0.73
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.78
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.65; acc: 0.78
Batch: 500; loss: 0.58; acc: 0.77
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.84; acc: 0.75
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.85; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5202415428913323; val_accuracy: 0.8387738853503185 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.77
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.8; acc: 0.75
Batch: 180; loss: 0.49; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.77; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.78
Batch: 340; loss: 0.53; acc: 0.78
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.75
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.77
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.5207071274898614; val_accuracy: 0.839171974522293 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.9; acc: 0.66
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.88; acc: 0.73
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.8
Batch: 360; loss: 0.81; acc: 0.77
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.75
Batch: 420; loss: 0.68; acc: 0.73
Batch: 440; loss: 0.61; acc: 0.78
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.62; acc: 0.75
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.73
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.75
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.75
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.77
Batch: 760; loss: 0.62; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5204107368447978; val_accuracy: 0.8401671974522293 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.64; acc: 0.75
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.82; acc: 0.78
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.78
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.8
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.81; acc: 0.78
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.84; acc: 0.75
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5201269914010528; val_accuracy: 0.839171974522293 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.67; acc: 0.75
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.73
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.58; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.79; acc: 0.75
Batch: 460; loss: 0.79; acc: 0.7
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.7
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.78
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.77
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.5200192552463264; val_accuracy: 0.8379777070063694 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.95; acc: 0.72
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.78
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.75
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.8
Batch: 280; loss: 0.68; acc: 0.78
Batch: 300; loss: 0.62; acc: 0.78
Batch: 320; loss: 0.79; acc: 0.73
Batch: 340; loss: 0.88; acc: 0.72
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.73
Batch: 420; loss: 0.41; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.72
Batch: 500; loss: 0.59; acc: 0.75
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.5197183971002604; val_accuracy: 0.8383757961783439 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.96; acc: 0.69
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.71; acc: 0.73
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.83; acc: 0.78
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.78
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.77
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.75
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.5198454139338937; val_accuracy: 0.8390724522292994 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.78
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.73
Batch: 520; loss: 0.58; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.72
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.63; acc: 0.77
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.77; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.75
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5198581362985502; val_accuracy: 0.8387738853503185 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.73
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.75
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.82; acc: 0.75
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.8
Batch: 300; loss: 0.78; acc: 0.78
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.75
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.75; acc: 0.69
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.72; acc: 0.75
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.77
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5196439994938055; val_accuracy: 0.8385748407643312 

plots/subspace_training/reg_lenet_3/2020-01-20 16:50:48/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 4498998
elements in E: 4499000
fraction nonzero: 0.9999995554567682
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.11
Batch: 40; loss: 2.31; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.31; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.29; acc: 0.16
Batch: 300; loss: 2.28; acc: 0.12
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.28; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.06
Batch: 380; loss: 2.29; acc: 0.11
Batch: 400; loss: 2.29; acc: 0.06
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.05
Batch: 460; loss: 2.28; acc: 0.08
Batch: 480; loss: 2.28; acc: 0.12
Batch: 500; loss: 2.28; acc: 0.09
Batch: 520; loss: 2.27; acc: 0.16
Batch: 540; loss: 2.27; acc: 0.09
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.27; acc: 0.06
Batch: 600; loss: 2.25; acc: 0.19
Batch: 620; loss: 2.26; acc: 0.17
Batch: 640; loss: 2.28; acc: 0.09
Batch: 660; loss: 2.28; acc: 0.11
Batch: 680; loss: 2.26; acc: 0.09
Batch: 700; loss: 2.26; acc: 0.17
Batch: 720; loss: 2.26; acc: 0.2
Batch: 740; loss: 2.25; acc: 0.22
Batch: 760; loss: 2.25; acc: 0.19
Batch: 780; loss: 2.26; acc: 0.23
Train Epoch over. train_loss: 2.29; train_accuracy: 0.12 

Batch: 0; loss: 2.25; acc: 0.28
Batch: 20; loss: 2.25; acc: 0.38
Batch: 40; loss: 2.24; acc: 0.39
Batch: 60; loss: 2.24; acc: 0.38
Batch: 80; loss: 2.23; acc: 0.41
Batch: 100; loss: 2.24; acc: 0.39
Batch: 120; loss: 2.23; acc: 0.47
Batch: 140; loss: 2.25; acc: 0.36
Val Epoch over. val_loss: 2.24871526554132; val_accuracy: 0.32275079617834396 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.23; acc: 0.36
Batch: 20; loss: 2.24; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.42
Batch: 60; loss: 2.23; acc: 0.33
Batch: 80; loss: 2.22; acc: 0.3
Batch: 100; loss: 2.2; acc: 0.38
Batch: 120; loss: 2.2; acc: 0.33
Batch: 140; loss: 2.16; acc: 0.44
Batch: 160; loss: 2.17; acc: 0.41
Batch: 180; loss: 2.12; acc: 0.44
Batch: 200; loss: 2.13; acc: 0.36
Batch: 220; loss: 2.06; acc: 0.38
Batch: 240; loss: 1.96; acc: 0.55
Batch: 260; loss: 1.98; acc: 0.41
Batch: 280; loss: 1.79; acc: 0.56
Batch: 300; loss: 1.67; acc: 0.45
Batch: 320; loss: 1.58; acc: 0.45
Batch: 340; loss: 1.55; acc: 0.41
Batch: 360; loss: 1.4; acc: 0.61
Batch: 380; loss: 1.46; acc: 0.56
Batch: 400; loss: 1.32; acc: 0.52
Batch: 420; loss: 0.88; acc: 0.81
Batch: 440; loss: 1.22; acc: 0.56
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.18; acc: 0.62
Batch: 500; loss: 1.17; acc: 0.59
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 0.83; acc: 0.69
Batch: 560; loss: 1.15; acc: 0.62
Batch: 580; loss: 0.96; acc: 0.66
Batch: 600; loss: 1.14; acc: 0.66
Batch: 620; loss: 1.01; acc: 0.64
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 0.78; acc: 0.73
Batch: 680; loss: 1.09; acc: 0.66
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 1.01; acc: 0.66
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.74; acc: 0.73
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

Batch: 0; loss: 0.93; acc: 0.69
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.48; acc: 0.83
Val Epoch over. val_loss: 0.8195074204426662; val_accuracy: 0.7291003184713376 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.61
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.86; acc: 0.77
Batch: 200; loss: 0.73; acc: 0.73
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 0.71; acc: 0.78
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.93; acc: 0.7
Batch: 300; loss: 0.53; acc: 0.8
Batch: 320; loss: 0.84; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.72
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.64; acc: 0.77
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.86; acc: 0.67
Batch: 460; loss: 0.54; acc: 0.77
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.86; acc: 0.62
Batch: 580; loss: 0.76; acc: 0.72
Batch: 600; loss: 0.78; acc: 0.84
Batch: 620; loss: 0.73; acc: 0.72
Batch: 640; loss: 0.78; acc: 0.75
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.81; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.75
Batch: 760; loss: 0.92; acc: 0.72
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.77 

Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.75
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.6270691364244291; val_accuracy: 0.8027468152866242 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.93; acc: 0.78
Batch: 220; loss: 0.59; acc: 0.77
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.61; acc: 0.77
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.77
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.72; acc: 0.73
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.62; train_accuracy: 0.8 

Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5926584630824958; val_accuracy: 0.823546974522293 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.77
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.55; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.8
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.8
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.78
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.84; acc: 0.72
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.5386378630331368; val_accuracy: 0.8432523885350318 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.77
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.88; acc: 0.78
Batch: 260; loss: 0.65; acc: 0.75
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.7
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.78
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.75
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.78
Val Epoch over. val_loss: 0.7944650680396208; val_accuracy: 0.7648288216560509 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.78
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.78; acc: 0.75
Batch: 580; loss: 0.76; acc: 0.73
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.55; acc: 0.77
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.78
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.5558665505829891; val_accuracy: 0.8332006369426752 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.75; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.86
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.5590971774736028; val_accuracy: 0.8288216560509554 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.75
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.8
Batch: 680; loss: 0.54; acc: 0.78
Batch: 700; loss: 0.8; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.9; acc: 0.66
Batch: 80; loss: 0.45; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.66
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.7486528165780815; val_accuracy: 0.7513933121019108 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.73
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.8
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.72
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.47; acc: 0.78
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.87; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.75
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.88
Val Epoch over. val_loss: 0.7402077666513479; val_accuracy: 0.7633359872611465 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.81
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.82; acc: 0.8
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.43670946929105525; val_accuracy: 0.8728105095541401 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.82; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.78
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4526306442014731; val_accuracy: 0.8668391719745223 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.77
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.7
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.47964828148199495; val_accuracy: 0.8556926751592356 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.81; acc: 0.78
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.32; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.31; acc: 0.88
Val Epoch over. val_loss: 0.6737482148181101; val_accuracy: 0.7866242038216561 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.78
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.81; acc: 0.78
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.92; acc: 0.72
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4991193078695589; val_accuracy: 0.8489251592356688 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.8
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.81; acc: 0.8
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.8
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.72
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.44230772080315145; val_accuracy: 0.865843949044586 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.84; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.77
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.72
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.66; acc: 0.78
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.5066679766413512; val_accuracy: 0.8478304140127388 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.78; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.31; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4394561356514882; val_accuracy: 0.8673367834394905 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.83
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.77
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.78
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.41846524150508224; val_accuracy: 0.876890923566879 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.57; acc: 0.81
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.4239134894814461; val_accuracy: 0.8772890127388535 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.8
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.81; acc: 0.77
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.38; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.73
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.78
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4193717606223313; val_accuracy: 0.8764928343949044 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.78
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.77
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.41114080934577685; val_accuracy: 0.8798765923566879 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.8
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.407946951022953; val_accuracy: 0.8817675159235668 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.29; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.8
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.41012703594128797; val_accuracy: 0.8792794585987261 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.83
Batch: 620; loss: 0.52; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.78
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.59; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.78
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.4115441851555162; val_accuracy: 0.8779856687898089 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.75
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4278702933792096; val_accuracy: 0.8737062101910829 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.8
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.75; acc: 0.84
Batch: 600; loss: 0.6; acc: 0.8
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4156903018047855; val_accuracy: 0.8781847133757962 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.75
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 1.09; acc: 0.8
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.69; acc: 0.75
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.58; acc: 0.73
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.73
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4096725510468908; val_accuracy: 0.8799761146496815 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.88
Batch: 440; loss: 0.82; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.4379633825960433; val_accuracy: 0.865047770700637 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.8; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.41103235712856245; val_accuracy: 0.8777866242038217 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.62; acc: 0.77
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4055740752132835; val_accuracy: 0.8807722929936306 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.8
Batch: 300; loss: 0.76; acc: 0.83
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4039352931983911; val_accuracy: 0.8813694267515924 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.81
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.95; acc: 0.8
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4047825078770613; val_accuracy: 0.8825636942675159 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.40443243570388504; val_accuracy: 0.8820660828025477 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.88; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.8
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.92
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.77
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40502449699268217; val_accuracy: 0.8821656050955414 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.8; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.77
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40410308430718767; val_accuracy: 0.8813694267515924 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.8
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.78
Batch: 380; loss: 0.58; acc: 0.78
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.81
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4025600668825921; val_accuracy: 0.8816679936305732 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.82; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.83; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.95; acc: 0.75
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.40462052542123067; val_accuracy: 0.8805732484076433 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4142281277354356; val_accuracy: 0.8769904458598726 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4042212354719259; val_accuracy: 0.8835589171974523 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.78
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40380441696400854; val_accuracy: 0.8821656050955414 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.8
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.8
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.77
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.79; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4038435794460546; val_accuracy: 0.8823646496815286 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.78
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.97; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4026297004359543; val_accuracy: 0.8831608280254777 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.66; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4024346839090821; val_accuracy: 0.8823646496815286 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.83; acc: 0.8
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.78
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.40322777353654243; val_accuracy: 0.8826632165605095 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40394706649195616; val_accuracy: 0.8829617834394905 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.98
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.78
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.403774159776557; val_accuracy: 0.8815684713375797 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.8
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40539566180698433; val_accuracy: 0.8827627388535032 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40310158390717904; val_accuracy: 0.8820660828025477 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.85; acc: 0.78
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4035048768114132; val_accuracy: 0.882265127388535 

plots/subspace_training/reg_lenet_3/2020-01-20 16:50:48/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 4948898
elements in E: 4948900
fraction nonzero: 0.9999995958697893
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.32; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.12
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.12
Batch: 320; loss: 2.29; acc: 0.17
Batch: 340; loss: 2.27; acc: 0.2
Batch: 360; loss: 2.28; acc: 0.27
Batch: 380; loss: 2.28; acc: 0.27
Batch: 400; loss: 2.28; acc: 0.22
Batch: 420; loss: 2.28; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.3
Batch: 460; loss: 2.28; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.3
Batch: 500; loss: 2.27; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.33
Batch: 540; loss: 2.26; acc: 0.28
Batch: 560; loss: 2.27; acc: 0.27
Batch: 580; loss: 2.26; acc: 0.22
Batch: 600; loss: 2.26; acc: 0.38
Batch: 620; loss: 2.24; acc: 0.41
Batch: 640; loss: 2.25; acc: 0.38
Batch: 660; loss: 2.23; acc: 0.39
Batch: 680; loss: 2.24; acc: 0.39
Batch: 700; loss: 2.25; acc: 0.39
Batch: 720; loss: 2.24; acc: 0.42
Batch: 740; loss: 2.2; acc: 0.44
Batch: 760; loss: 2.2; acc: 0.42
Batch: 780; loss: 2.22; acc: 0.36
Train Epoch over. train_loss: 2.27; train_accuracy: 0.22 

Batch: 0; loss: 2.2; acc: 0.36
Batch: 20; loss: 2.22; acc: 0.27
Batch: 40; loss: 2.17; acc: 0.45
Batch: 60; loss: 2.19; acc: 0.44
Batch: 80; loss: 2.2; acc: 0.34
Batch: 100; loss: 2.21; acc: 0.38
Batch: 120; loss: 2.19; acc: 0.41
Batch: 140; loss: 2.2; acc: 0.36
Val Epoch over. val_loss: 2.2003141436607216; val_accuracy: 0.37908041401273884 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.19; acc: 0.42
Batch: 20; loss: 2.22; acc: 0.3
Batch: 40; loss: 2.16; acc: 0.48
Batch: 60; loss: 2.21; acc: 0.3
Batch: 80; loss: 2.13; acc: 0.45
Batch: 100; loss: 2.14; acc: 0.34
Batch: 120; loss: 2.12; acc: 0.38
Batch: 140; loss: 2.05; acc: 0.48
Batch: 160; loss: 2.06; acc: 0.36
Batch: 180; loss: 1.98; acc: 0.36
Batch: 200; loss: 1.82; acc: 0.45
Batch: 220; loss: 1.68; acc: 0.45
Batch: 240; loss: 1.66; acc: 0.5
Batch: 260; loss: 1.51; acc: 0.5
Batch: 280; loss: 1.33; acc: 0.53
Batch: 300; loss: 1.29; acc: 0.53
Batch: 320; loss: 1.35; acc: 0.55
Batch: 340; loss: 0.96; acc: 0.72
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 0.88; acc: 0.7
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.86; acc: 0.75
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.98; acc: 0.67
Batch: 520; loss: 0.85; acc: 0.73
Batch: 540; loss: 0.87; acc: 0.69
Batch: 560; loss: 0.87; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.72
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 0.86; acc: 0.7
Batch: 640; loss: 0.7; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.93; acc: 0.73
Batch: 720; loss: 0.74; acc: 0.73
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.73
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 1.27; train_accuracy: 0.62 

Batch: 0; loss: 1.01; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.59
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.97; acc: 0.66
Batch: 80; loss: 0.69; acc: 0.73
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.56
Batch: 140; loss: 0.96; acc: 0.75
Val Epoch over. val_loss: 0.9519379873564289; val_accuracy: 0.6820262738853503 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.59
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.73; acc: 0.72
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.73
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.75
Batch: 200; loss: 0.59; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.75
Batch: 240; loss: 0.89; acc: 0.75
Batch: 260; loss: 0.95; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.69; acc: 0.75
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 1.01; acc: 0.64
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.72; acc: 0.67
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.7
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.5467004346050275; val_accuracy: 0.8298168789808917 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.59; acc: 0.78
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.72; acc: 0.77
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.64; acc: 0.7
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.78
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.68; acc: 0.78
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.75
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.78
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.77
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.75
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.54; acc: 0.8
Val Epoch over. val_loss: 0.6121608277035367; val_accuracy: 0.8164808917197452 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.83; acc: 0.78
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.88; acc: 0.77
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.77
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.77
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.61; acc: 0.78
Batch: 640; loss: 0.49; acc: 0.8
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.76; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.72
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.73
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.88
Val Epoch over. val_loss: 0.5328915867076558; val_accuracy: 0.831906847133758 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.78
Batch: 160; loss: 0.48; acc: 0.78
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.78
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.5; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.77
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.77
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.53; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.75
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.76; acc: 0.69
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.51; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.88
Val Epoch over. val_loss: 0.5003434785042599; val_accuracy: 0.8420581210191083 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.73
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.72; acc: 0.73
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 1.04; acc: 0.66
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.95; acc: 0.7
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.5777843388592362; val_accuracy: 0.8179737261146497 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 0.62; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.73
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.8
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.73
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.77
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.56; acc: 0.77
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 1.05; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.56; acc: 0.81
Val Epoch over. val_loss: 0.8550684047732383; val_accuracy: 0.7384554140127388 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.77
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.67; acc: 0.73
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.66
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.78
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.78; acc: 0.73
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.68; acc: 0.75
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.49420489978258775; val_accuracy: 0.841062898089172 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.72; acc: 0.73
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.66
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.4672420162019456; val_accuracy: 0.8540007961783439 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.8
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.84
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.45021256899378104; val_accuracy: 0.8587778662420382 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.78
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.77
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.73; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.8
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.43115021316868485; val_accuracy: 0.863953025477707 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.42; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.75
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.43009667658502126; val_accuracy: 0.8653463375796179 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.63; acc: 0.78
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.77
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.63; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.72
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.78
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.5114818703217111; val_accuracy: 0.8377786624203821 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.78
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.67; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.435359698287241; val_accuracy: 0.8671377388535032 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.73
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.51; acc: 0.8
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.72
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.77
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.66
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4549892075881837; val_accuracy: 0.8551950636942676 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.53; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.8
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.54; acc: 0.78
Batch: 760; loss: 0.55; acc: 0.8
Batch: 780; loss: 0.36; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.4784447988317271; val_accuracy: 0.8525079617834395 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.81; acc: 0.75
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.4639460657052933; val_accuracy: 0.85828025477707 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.77
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.64; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.73
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.43104861263826394; val_accuracy: 0.865047770700637 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.82; acc: 0.7
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.81
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4195963137658538; val_accuracy: 0.8679339171974523 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.78
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.41108194396943804; val_accuracy: 0.8732085987261147 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.77
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.41913454713904935; val_accuracy: 0.8717157643312102 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.58; acc: 0.78
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.8
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.41863759739953244; val_accuracy: 0.868531050955414 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.77
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.83
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.4187558062706783; val_accuracy: 0.8705214968152867 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.38; acc: 0.81
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.85; acc: 0.75
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4250475356628181; val_accuracy: 0.8679339171974523 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.81
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.67
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4240130307568107; val_accuracy: 0.8705214968152867 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.8
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.8
Batch: 220; loss: 0.34; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.68; acc: 0.72
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.7
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4092633884613681; val_accuracy: 0.8736066878980892 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.77
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.78
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.81
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.66; acc: 0.78
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.42047206848669966; val_accuracy: 0.8724124203821656 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.8
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4158142312506961; val_accuracy: 0.8697253184713376 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.69
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.41087028256077673; val_accuracy: 0.8722133757961783 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.77
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.73
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.8
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.65; acc: 0.78
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.408553206521994; val_accuracy: 0.8746019108280255 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.73
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.41330496098395364; val_accuracy: 0.8740047770700637 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.81
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.78
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.40962724525267913; val_accuracy: 0.8761942675159236 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.8
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.40758169475634387; val_accuracy: 0.8762937898089171 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.83
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.81
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.8
Batch: 680; loss: 0.58; acc: 0.78
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.7
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4092497339199303; val_accuracy: 0.8758957006369427 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.55; acc: 0.78
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40797428729807494; val_accuracy: 0.8749004777070064 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.72
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.77
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.8
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.7
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4100711065208077; val_accuracy: 0.873109076433121 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.83; acc: 0.83
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.98
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4114050299024126; val_accuracy: 0.8759952229299363 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.8
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.77
Batch: 660; loss: 0.76; acc: 0.73
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.94; acc: 0.75
Batch: 760; loss: 0.7; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.40894413374032185; val_accuracy: 0.8746019108280255 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.72
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.40862224018497834; val_accuracy: 0.8757961783439491 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.78
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.78
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.77
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.89; acc: 0.8
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4086283469560799; val_accuracy: 0.8738057324840764 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.8; acc: 0.81
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.81
Batch: 780; loss: 0.35; acc: 0.97
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4073638243090575; val_accuracy: 0.8744028662420382 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.78
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.83
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.40807974808345177; val_accuracy: 0.87609474522293 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.78
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.40951375696499637; val_accuracy: 0.8759952229299363 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.81
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.40748829361359784; val_accuracy: 0.8759952229299363 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.78; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.7
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4080128393070713; val_accuracy: 0.876890923566879 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.8
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4072172032894602; val_accuracy: 0.8755971337579618 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4100577807540347; val_accuracy: 0.8728105095541401 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.75
Batch: 260; loss: 0.54; acc: 0.75
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40657739877510984; val_accuracy: 0.8755971337579618 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.81
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.407409355661292; val_accuracy: 0.8743033439490446 

plots/subspace_training/reg_lenet_3/2020-01-20 16:50:48/d_dim_110_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5398798
elements in E: 5398800
fraction nonzero: 0.9999996295473068
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.28; acc: 0.17
Batch: 60; loss: 2.31; acc: 0.09
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.31; acc: 0.16
Batch: 180; loss: 2.28; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.29; acc: 0.09
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.29; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.28; acc: 0.25
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.12
Batch: 380; loss: 2.28; acc: 0.16
Batch: 400; loss: 2.27; acc: 0.38
Batch: 420; loss: 2.27; acc: 0.33
Batch: 440; loss: 2.26; acc: 0.42
Batch: 460; loss: 2.27; acc: 0.38
Batch: 480; loss: 2.25; acc: 0.55
Batch: 500; loss: 2.25; acc: 0.47
Batch: 520; loss: 2.25; acc: 0.39
Batch: 540; loss: 2.23; acc: 0.33
Batch: 560; loss: 2.22; acc: 0.52
Batch: 580; loss: 2.19; acc: 0.33
Batch: 600; loss: 2.19; acc: 0.31
Batch: 620; loss: 2.15; acc: 0.33
Batch: 640; loss: 2.07; acc: 0.48
Batch: 660; loss: 2.01; acc: 0.39
Batch: 680; loss: 1.96; acc: 0.44
Batch: 700; loss: 1.85; acc: 0.39
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.34; acc: 0.61
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.67
Train Epoch over. train_loss: 2.15; train_accuracy: 0.29 

Batch: 0; loss: 1.65; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.55
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.27; acc: 0.66
Batch: 120; loss: 1.48; acc: 0.66
Batch: 140; loss: 1.62; acc: 0.59
Val Epoch over. val_loss: 1.5092667094461478; val_accuracy: 0.5771297770700637 

Traceback (most recent call last):
  File "ddim_vs_acc.py", line 98, in <module>
    main()
  File "ddim_vs_acc.py", line 27, in main
    train_loss, train_accuracy, val_loss, val_accuracy = train_model_once(ARGS)
  File "/home/llang/thesis-intrinsic-dimension/classify_mnist.py", line 55, in train_model_once
    train_loss, train_acc, val_loss, val_acc, subspace_distance = train_epoch(model,train_loader,val_loader,optimizer,criterion,ARGS)
  File "/home/llang/thesis-intrinsic-dimension/train_helpers.py", line 19, in train_epoch
    subspace_distance = optimizer.compute_subspace_distance()
  File "/home/llang/thesis-intrinsic-dimension/optimizers.py", line 130, in compute_subspace_distance
    difference_vector = p_D - diff_D
RuntimeError: CUDA out of memory. Tried to allocate 7.54 GiB (GPU 0; 10.92 GiB total capacity; 42.31 MiB already allocated; 2.83 GiB free; 7.50 GiB cached)
/var/spool/slurm-llnl/slurmd/job4387151/slurm_script: line 26: --print_freq=20: command not found
