Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.05
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.06
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.17
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.3; acc: 0.03
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.05
Batch: 520; loss: 2.3; acc: 0.05
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.12
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.33; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302467819991385; val_accuracy: 0.10121417197452229 

Epoch 2 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.14
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.09
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.05
Batch: 400; loss: 2.3; acc: 0.06
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.05
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.03
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.03
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024565520559905; val_accuracy: 0.10121417197452229 

Epoch 3 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.05
Batch: 40; loss: 2.3; acc: 0.17
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.3; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.17
Batch: 240; loss: 2.3; acc: 0.03
Batch: 260; loss: 2.3; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.17
Batch: 300; loss: 2.3; acc: 0.05
Batch: 320; loss: 2.3; acc: 0.05
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.19
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024455346878927; val_accuracy: 0.10121417197452229 

Epoch 4 start
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.05
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.2
Batch: 160; loss: 2.3; acc: 0.03
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.05
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.09
Batch: 360; loss: 2.3; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.14
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.17
Batch: 620; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024348088890125; val_accuracy: 0.10121417197452229 

Epoch 5 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.05
Batch: 40; loss: 2.3; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.16
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.06
Batch: 380; loss: 2.3; acc: 0.17
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.19
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.17
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302424341250377; val_accuracy: 0.10121417197452229 

Epoch 6 start
Batch: 0; loss: 2.3; acc: 0.03
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.08
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.3; acc: 0.06
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024140953258345; val_accuracy: 0.10121417197452229 

Epoch 7 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.3; acc: 0.03
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.03
Batch: 240; loss: 2.3; acc: 0.14
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024039405166725; val_accuracy: 0.10121417197452229 

Epoch 8 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.0
Batch: 40; loss: 2.3; acc: 0.19
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.05
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.03
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.3; acc: 0.06
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.03
Batch: 380; loss: 2.3; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.05
Batch: 460; loss: 2.3; acc: 0.14
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.12
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.02
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3023938175978933; val_accuracy: 0.11096735668789809 

Epoch 9 start
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.03
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.19
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3023837645342398; val_accuracy: 0.11096735668789809 

Epoch 10 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.3; acc: 0.2
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.2
Batch: 200; loss: 2.3; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.22
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.17
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.14
Batch: 620; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302373816253273; val_accuracy: 0.11096735668789809 

plots/subspace_None_d_dim_None_model_None_lr_None_seed_None_epochs_None_batchsize_None_2019-12-19 01:11:08.670834
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.05
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.06
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.17
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.3; acc: 0.03
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.05
Batch: 520; loss: 2.3; acc: 0.05
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.12
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.33; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302590907758968; val_accuracy: 0.10121417197452229 

Epoch 2 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.14
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.09
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.05
Batch: 400; loss: 2.3; acc: 0.06
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.05
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.03
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.03
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3025464039699286; val_accuracy: 0.10121417197452229 

Epoch 3 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.05
Batch: 40; loss: 2.3; acc: 0.17
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.3; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.17
Batch: 240; loss: 2.3; acc: 0.03
Batch: 260; loss: 2.3; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.17
Batch: 300; loss: 2.3; acc: 0.05
Batch: 320; loss: 2.3; acc: 0.05
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.19
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302502984453918; val_accuracy: 0.10121417197452229 

Epoch 4 start
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.05
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.2
Batch: 160; loss: 2.3; acc: 0.03
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.05
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.09
Batch: 360; loss: 2.3; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.14
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.17
Batch: 620; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024599628084026; val_accuracy: 0.10121417197452229 

Epoch 5 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.05
Batch: 40; loss: 2.3; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.16
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.06
Batch: 380; loss: 2.3; acc: 0.17
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.19
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.17
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3024177262737493; val_accuracy: 0.10121417197452229 

Epoch 6 start
Batch: 0; loss: 2.3; acc: 0.03
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.08
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.3; acc: 0.06
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302376434301874; val_accuracy: 0.10121417197452229 

Epoch 7 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.3; acc: 0.03
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.03
Batch: 240; loss: 2.3; acc: 0.14
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.17
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3023355538678016; val_accuracy: 0.10121417197452229 

Epoch 8 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.0
Batch: 40; loss: 2.3; acc: 0.19
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.05
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.03
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.3; acc: 0.06
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.05
Batch: 380; loss: 2.3; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.19
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.16
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3022946278760386; val_accuracy: 0.11096735668789809 

Epoch 9 start
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.03
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.19
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302253504467618; val_accuracy: 0.11096735668789809 

Epoch 10 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.3; acc: 0.2
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.2
Batch: 200; loss: 2.3; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.22
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.31; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.17
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.14
Batch: 620; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302211623282949; val_accuracy: 0.11096735668789809 

plots/subspace_None_d_dim_None_model_None_lr_None_seed_None_epochs_None_batchsize_None_2019-12-19 01:13:10.991196
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.0
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.3; acc: 0.05
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.3; acc: 0.16
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.09
Batch: 360; loss: 2.3; acc: 0.03
Batch: 380; loss: 2.3; acc: 0.16
Batch: 400; loss: 2.3; acc: 0.03
Batch: 420; loss: 2.3; acc: 0.06
Batch: 440; loss: 2.3; acc: 0.06
Batch: 460; loss: 2.3; acc: 0.14
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.11
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.19
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.33; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.17
Val Epoch over. val_loss: 2.3024889314250583; val_accuracy: 0.09514331210191083 

Epoch 2 start
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.02
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.05
Batch: 540; loss: 2.3; acc: 0.08
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.05
Batch: 620; loss: 2.3; acc: 0.03
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.03
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.17
Val Epoch over. val_loss: 2.3023972723894057; val_accuracy: 0.09514331210191083 

Epoch 3 start
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.3; acc: 0.03
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.3; acc: 0.05
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.3; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.22
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.3; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.05
Batch: 440; loss: 2.3; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.19
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.16
Batch: 600; loss: 2.3; acc: 0.19
Batch: 620; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302310214680471; val_accuracy: 0.11096735668789809 

Epoch 4 start
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.36
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.3; acc: 0.23
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.02
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.05
Batch: 400; loss: 2.3; acc: 0.17
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.3; acc: 0.03
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.05
Batch: 520; loss: 2.3; acc: 0.09
Batch: 540; loss: 2.3; acc: 0.14
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302226649727791; val_accuracy: 0.11096735668789809 

Epoch 5 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.19
Batch: 360; loss: 2.3; acc: 0.17
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.17
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.19
Batch: 540; loss: 2.3; acc: 0.19
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.302145766604478; val_accuracy: 0.11096735668789809 

Epoch 6 start
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.17
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.03
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.2
Batch: 360; loss: 2.3; acc: 0.17
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.3; acc: 0.06
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.16
Batch: 480; loss: 2.3; acc: 0.19
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.05
Batch: 540; loss: 2.31; acc: 0.02
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.22
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3020659966073977; val_accuracy: 0.11096735668789809 

Epoch 7 start
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.22
Batch: 180; loss: 2.3; acc: 0.16
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.16
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.3; acc: 0.16
Batch: 280; loss: 2.3; acc: 0.06
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.12
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.09
Batch: 540; loss: 2.3; acc: 0.16
Batch: 560; loss: 2.3; acc: 0.12
Batch: 580; loss: 2.3; acc: 0.03
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.301985456685352; val_accuracy: 0.11096735668789809 

Epoch 8 start
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.2
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.16
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.14
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.301901729243576; val_accuracy: 0.11096735668789809 

Epoch 9 start
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.03
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.19
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3018105136361093; val_accuracy: 0.11096735668789809 

Epoch 10 start
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.3; acc: 0.2
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.2
Batch: 200; loss: 2.3; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.31; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.22
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.31; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.17
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.14
Batch: 620; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Val Epoch over. val_loss: 2.3017028046261734; val_accuracy: 0.11096735668789809 

plots/subspace_None_d_dim_None_model_None_lr_None_seed_None_epochs_None_batchsize_None_2019-12-19 01:15:15.653769
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.03
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.06
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.03
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.03
Batch: 600; loss: 2.3; acc: 0.14
Batch: 620; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.33; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.03
Batch: 140; loss: 2.3; acc: 0.12
Val Epoch over. val_loss: 2.302413065722034; val_accuracy: 0.0973328025477707 

Epoch 2 start
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.03
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.17
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.3; acc: 0.2
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.12
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.17
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.08
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.09
Batch: 600; loss: 2.3; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.03
Batch: 140; loss: 2.3; acc: 0.12
Val Epoch over. val_loss: 2.3021056545767813; val_accuracy: 0.0973328025477707 

Epoch 3 start
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.02
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.05
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.17
Batch: 460; loss: 2.3; acc: 0.08
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.11
Batch: 540; loss: 2.3; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.05
Batch: 580; loss: 2.3; acc: 0.17
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.3; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.03
Batch: 140; loss: 2.3; acc: 0.12
Val Epoch over. val_loss: 2.3016585514044308; val_accuracy: 0.09743232484076433 

Epoch 4 start
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.06
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.3; acc: 0.05
Batch: 340; loss: 2.3; acc: 0.06
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.08
Batch: 480; loss: 2.3; acc: 0.17
Batch: 500; loss: 2.3; acc: 0.16
Batch: 520; loss: 2.3; acc: 0.11
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.16
Batch: 580; loss: 2.3; acc: 0.2
Batch: 600; loss: 2.3; acc: 0.2
Batch: 620; loss: 2.3; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.19
Batch: 80; loss: 2.3; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.14
Val Epoch over. val_loss: 2.300504207611084; val_accuracy: 0.12639331210191082 

Epoch 5 start
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.3; acc: 0.2
Batch: 120; loss: 2.3; acc: 0.19
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.25
Batch: 180; loss: 2.3; acc: 0.17
Batch: 200; loss: 2.3; acc: 0.2
Batch: 220; loss: 2.3; acc: 0.2
Batch: 240; loss: 2.3; acc: 0.27
Batch: 260; loss: 2.3; acc: 0.22
Batch: 280; loss: 2.3; acc: 0.2
Batch: 300; loss: 2.3; acc: 0.16
Batch: 320; loss: 2.3; acc: 0.19
Batch: 340; loss: 2.3; acc: 0.12
Batch: 360; loss: 2.3; acc: 0.27
Batch: 380; loss: 2.29; acc: 0.25
Batch: 400; loss: 2.29; acc: 0.17
Batch: 420; loss: 2.3; acc: 0.09
Batch: 440; loss: 2.29; acc: 0.16
Batch: 460; loss: 2.3; acc: 0.16
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.22
Batch: 540; loss: 2.3; acc: 0.16
Batch: 560; loss: 2.3; acc: 0.16
Batch: 580; loss: 2.29; acc: 0.14
Batch: 600; loss: 2.29; acc: 0.22
Batch: 620; loss: 2.29; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.17 

Batch: 0; loss: 2.29; acc: 0.2
Batch: 20; loss: 2.29; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.29; acc: 0.28
Batch: 80; loss: 2.29; acc: 0.27
Batch: 100; loss: 2.29; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.17
Batch: 140; loss: 2.29; acc: 0.22
Val Epoch over. val_loss: 2.2892153065675385; val_accuracy: 0.1913813694267516 

Epoch 6 start
Batch: 0; loss: 2.29; acc: 0.27
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.28; acc: 0.22
Batch: 80; loss: 2.28; acc: 0.22
Batch: 100; loss: 2.28; acc: 0.25
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.27; acc: 0.19
Batch: 160; loss: 2.26; acc: 0.2
Batch: 180; loss: 2.23; acc: 0.3
Batch: 200; loss: 2.22; acc: 0.34
Batch: 220; loss: 2.2; acc: 0.33
Batch: 240; loss: 2.12; acc: 0.38
Batch: 260; loss: 2.04; acc: 0.34
Batch: 280; loss: 1.73; acc: 0.72
Batch: 300; loss: 1.22; acc: 0.77
Batch: 320; loss: 1.07; acc: 0.73
Batch: 340; loss: 0.91; acc: 0.69
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.75
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.88; acc: 0.7
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 1.39; train_accuracy: 0.56 

Batch: 0; loss: 0.47; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.75
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.93; acc: 0.69
Val Epoch over. val_loss: 0.5465176133972824; val_accuracy: 0.8277269108280255 

Epoch 7 start
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.7
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 0.5323741492951751; val_accuracy: 0.8360867834394905 

Epoch 8 start
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.91
Batch: 140; loss: 0.76; acc: 0.8
Val Epoch over. val_loss: 0.3859125775326589; val_accuracy: 0.8821656050955414 

Epoch 9 start
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 0.3865608372221327; val_accuracy: 0.8819665605095541 

Epoch 10 start
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.8; acc: 0.8
Val Epoch over. val_loss: 0.3842411650593873; val_accuracy: 0.8846536624203821 

plots/subspace_None_d_dim_None_model_None_lr_None_seed_None_epochs_None_batchsize_None_2019-12-19 01:17:24.962191
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.05
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.02
Batch: 180; loss: 2.3; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.3; acc: 0.2
Batch: 520; loss: 2.3; acc: 0.09
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.2
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.33; train_accuracy: 0.11 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.19
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.2
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.3; acc: 0.19
Batch: 120; loss: 2.3; acc: 0.2
Batch: 140; loss: 2.3; acc: 0.2
Val Epoch over. val_loss: 2.2990657372079837; val_accuracy: 0.15893710191082802 

Epoch 2 start
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.17
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.2
Batch: 100; loss: 2.3; acc: 0.2
Batch: 120; loss: 2.3; acc: 0.17
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.29; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.19
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.28; acc: 0.23
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.27; acc: 0.14
Batch: 320; loss: 2.26; acc: 0.22
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.23; acc: 0.22
Batch: 380; loss: 2.21; acc: 0.22
Batch: 400; loss: 2.17; acc: 0.23
Batch: 420; loss: 2.07; acc: 0.42
Batch: 440; loss: 1.78; acc: 0.58
Batch: 460; loss: 1.37; acc: 0.62
Batch: 480; loss: 0.94; acc: 0.72
Batch: 500; loss: 0.86; acc: 0.78
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.78
Batch: 620; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 0.62; acc: 0.75
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.75
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 0.5892296805503262; val_accuracy: 0.8086186305732485 

Epoch 3 start
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.75
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.73
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.95; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 1.13; acc: 0.69
Val Epoch over. val_loss: 0.5964708904362028; val_accuracy: 0.8194665605095541 

Epoch 4 start
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.66; acc: 0.86
Val Epoch over. val_loss: 0.31605736930279216; val_accuracy: 0.9013734076433121 

Epoch 5 start
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.58; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.61; acc: 0.83
Val Epoch over. val_loss: 0.2653176285753584; val_accuracy: 0.9210788216560509 

Epoch 6 start
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.83
Val Epoch over. val_loss: 0.3464263311711846; val_accuracy: 0.8924164012738853 

Epoch 7 start
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.2696025945293676; val_accuracy: 0.9181926751592356 

Epoch 8 start
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.2665925651409064; val_accuracy: 0.918093152866242 

Epoch 9 start
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.24657771318771277; val_accuracy: 0.9253582802547771 

Epoch 10 start
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.21565053028285883; val_accuracy: 0.9326234076433121 

plots/subspace_None_d_dim_None_model_None_lr_None_seed_None_epochs_None_batchsize_None_2019-12-19 01:19:37.257984
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.06
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.06
Batch: 440; loss: 2.3; acc: 0.06
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.11
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.29; acc: 0.09
Batch: 580; loss: 2.29; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.14
Batch: 620; loss: 2.29; acc: 0.2
Train Epoch over. train_loss: 2.33; train_accuracy: 0.11 

Batch: 0; loss: 2.29; acc: 0.22
Batch: 20; loss: 2.29; acc: 0.23
Batch: 40; loss: 2.29; acc: 0.27
Batch: 60; loss: 2.3; acc: 0.19
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.29; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.17
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.2932535842725428; val_accuracy: 0.19148089171974522 

Epoch 2 start
Batch: 0; loss: 2.29; acc: 0.19
Batch: 20; loss: 2.29; acc: 0.2
Batch: 40; loss: 2.29; acc: 0.22
Batch: 60; loss: 2.29; acc: 0.38
Batch: 80; loss: 2.28; acc: 0.34
Batch: 100; loss: 2.28; acc: 0.31
Batch: 120; loss: 2.27; acc: 0.33
Batch: 140; loss: 2.25; acc: 0.48
Batch: 160; loss: 2.23; acc: 0.42
Batch: 180; loss: 2.21; acc: 0.41
Batch: 200; loss: 2.16; acc: 0.41
Batch: 220; loss: 1.74; acc: 0.55
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 0.99; acc: 0.69
Batch: 280; loss: 0.99; acc: 0.67
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.87; acc: 0.7
Batch: 360; loss: 0.86; acc: 0.69
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.81
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 1.18; train_accuracy: 0.66 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 0.36984974686886857; val_accuracy: 0.8870421974522293 

Epoch 3 start
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.84
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.65; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.17; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.3708264624142343; val_accuracy: 0.8818670382165605 

Epoch 4 start
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.8
Val Epoch over. val_loss: 0.3236316215650291; val_accuracy: 0.897093949044586 

Epoch 5 start
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.52; acc: 0.84
Val Epoch over. val_loss: 0.22749866170298522; val_accuracy: 0.9288415605095541 

Epoch 6 start
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.25161391648517295; val_accuracy: 0.9243630573248408 

Epoch 7 start
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.64; acc: 0.81
Val Epoch over. val_loss: 0.2268098936339093; val_accuracy: 0.9286425159235668 

Epoch 8 start
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.68; acc: 0.83
Val Epoch over. val_loss: 0.22865595820413273; val_accuracy: 0.9286425159235668 

Epoch 9 start
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.86
Val Epoch over. val_loss: 0.19632592804397747; val_accuracy: 0.9382961783439491 

Epoch 10 start
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.68; acc: 0.8
Val Epoch over. val_loss: 0.21688013903464481; val_accuracy: 0.9338176751592356 

plots/subspace_None_d_dim_None_model_None_lr_None_seed_None_epochs_None_batchsize_None_2019-12-19 01:21:52.073480
plots/subspace_True_d_dim_XXXXX_model_lenet_lr_0.1_seed_1_epochs_10_batchsize_64_2019-12-19 01:21:52.378022
