model : reg_lenet_3
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : False
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-22 14:40:21
nonzero elements in E: 2138
elements in E: 449900
fraction nonzero: 0.0047521671482551675
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.16
Batch: 160; loss: 2.32; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.31; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.08
Batch: 520; loss: 2.31; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.16
Batch: 600; loss: 2.3; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.09
Batch: 700; loss: 2.3; acc: 0.06
Batch: 720; loss: 2.3; acc: 0.16
Batch: 740; loss: 2.31; acc: 0.08
Batch: 760; loss: 2.31; acc: 0.06
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.11
Val Epoch over. val_loss: 2.3027772660468036; val_accuracy: 0.09623805732484077 

The current subspace-distance is: 1.991510316656786e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.31; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.3; acc: 0.11
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.31; acc: 0.08
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.2
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.12
Batch: 440; loss: 2.31; acc: 0.02
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.31; acc: 0.08
Batch: 540; loss: 2.32; acc: 0.02
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.31; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.29; acc: 0.12
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.28; acc: 0.2
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.3; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.17
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.11
Val Epoch over. val_loss: 2.300465371198715; val_accuracy: 0.09623805732484077 

The current subspace-distance is: 2.5918182018358493e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.31; acc: 0.03
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.29; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.08
Batch: 240; loss: 2.31; acc: 0.03
Batch: 260; loss: 2.31; acc: 0.05
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.17
Batch: 340; loss: 2.3; acc: 0.12
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.28; acc: 0.12
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.3; acc: 0.08
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.28; acc: 0.16
Batch: 500; loss: 2.3; acc: 0.11
Batch: 520; loss: 2.3; acc: 0.09
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.31; acc: 0.06
Batch: 580; loss: 2.31; acc: 0.08
Batch: 600; loss: 2.31; acc: 0.06
Batch: 620; loss: 2.31; acc: 0.03
Batch: 640; loss: 2.28; acc: 0.16
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.3; acc: 0.16
Batch: 700; loss: 2.31; acc: 0.03
Batch: 720; loss: 2.3; acc: 0.11
Batch: 740; loss: 2.29; acc: 0.05
Batch: 760; loss: 2.3; acc: 0.11
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.2980738020246956; val_accuracy: 0.09623805732484077 

The current subspace-distance is: 3.435816552155302e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.08
Batch: 240; loss: 2.28; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.31; acc: 0.03
Batch: 340; loss: 2.29; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.06
Batch: 380; loss: 2.3; acc: 0.06
Batch: 400; loss: 2.29; acc: 0.06
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.31; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.29; acc: 0.17
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.28; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.3; acc: 0.08
Batch: 660; loss: 2.3; acc: 0.11
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.3; acc: 0.05
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.31; acc: 0.06
Batch: 760; loss: 2.31; acc: 0.03
Batch: 780; loss: 2.3; acc: 0.05
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.296666772502243; val_accuracy: 0.09623805732484077 

The current subspace-distance is: 5.071766736364225e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.08
Batch: 200; loss: 2.29; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.31; acc: 0.06
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.05
Batch: 380; loss: 2.3; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.09
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.29; acc: 0.08
Batch: 540; loss: 2.29; acc: 0.11
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.14
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.28; acc: 0.14
Batch: 640; loss: 2.3; acc: 0.05
Batch: 660; loss: 2.3; acc: 0.16
Batch: 680; loss: 2.3; acc: 0.06
Batch: 700; loss: 2.3; acc: 0.02
Batch: 720; loss: 2.31; acc: 0.08
Batch: 740; loss: 2.29; acc: 0.06
Batch: 760; loss: 2.28; acc: 0.11
Batch: 780; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.29; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.294503031262926; val_accuracy: 0.09643710191082802 

The current subspace-distance is: 6.000147095619468e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.05
Batch: 60; loss: 2.29; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.03
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.16
Batch: 220; loss: 2.29; acc: 0.08
Batch: 240; loss: 2.29; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.16
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.09
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.09
Batch: 440; loss: 2.28; acc: 0.12
Batch: 460; loss: 2.28; acc: 0.16
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.28; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.09
Batch: 540; loss: 2.29; acc: 0.16
Batch: 560; loss: 2.28; acc: 0.16
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.29; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.08
Batch: 640; loss: 2.3; acc: 0.06
Batch: 660; loss: 2.3; acc: 0.03
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.27; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.12
Batch: 740; loss: 2.28; acc: 0.16
Batch: 760; loss: 2.3; acc: 0.12
Batch: 780; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.29; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.17
Batch: 40; loss: 2.28; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.17
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.12
Val Epoch over. val_loss: 2.2896119819325245; val_accuracy: 0.10589171974522293 

The current subspace-distance is: 6.352895070449449e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.19
Batch: 20; loss: 2.28; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.28; acc: 0.16
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.22
Batch: 160; loss: 2.29; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.29; acc: 0.14
Batch: 240; loss: 2.28; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.06
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.28; acc: 0.11
Batch: 440; loss: 2.28; acc: 0.19
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.16
Batch: 500; loss: 2.28; acc: 0.08
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.27; acc: 0.23
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.28; acc: 0.09
Batch: 600; loss: 2.28; acc: 0.17
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.16
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.3; acc: 0.11
Batch: 720; loss: 2.28; acc: 0.12
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.09
Batch: 780; loss: 2.29; acc: 0.12
Train Epoch over. train_loss: 2.29; train_accuracy: 0.12 

Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.23
Batch: 40; loss: 2.27; acc: 0.17
Batch: 60; loss: 2.27; acc: 0.17
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.27; acc: 0.17
Batch: 140; loss: 2.28; acc: 0.14
Val Epoch over. val_loss: 2.28499698790775; val_accuracy: 0.13465366242038215 

The current subspace-distance is: 6.8430522333073895e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.03
Batch: 20; loss: 2.29; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.12
Batch: 80; loss: 2.28; acc: 0.16
Batch: 100; loss: 2.28; acc: 0.09
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.22
Batch: 160; loss: 2.28; acc: 0.16
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.12
Batch: 220; loss: 2.29; acc: 0.23
Batch: 240; loss: 2.28; acc: 0.14
Batch: 260; loss: 2.3; acc: 0.06
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.34
Batch: 320; loss: 2.28; acc: 0.16
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.28; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.29; acc: 0.11
Batch: 440; loss: 2.27; acc: 0.19
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.19
Batch: 500; loss: 2.28; acc: 0.08
Batch: 520; loss: 2.28; acc: 0.17
Batch: 540; loss: 2.28; acc: 0.17
Batch: 560; loss: 2.27; acc: 0.23
Batch: 580; loss: 2.28; acc: 0.19
Batch: 600; loss: 2.28; acc: 0.2
Batch: 620; loss: 2.28; acc: 0.16
Batch: 640; loss: 2.27; acc: 0.2
Batch: 660; loss: 2.27; acc: 0.22
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.28; acc: 0.17
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.27; acc: 0.11
Batch: 760; loss: 2.28; acc: 0.12
Batch: 780; loss: 2.28; acc: 0.17
Train Epoch over. train_loss: 2.28; train_accuracy: 0.16 

Batch: 0; loss: 2.28; acc: 0.19
Batch: 20; loss: 2.28; acc: 0.3
Batch: 40; loss: 2.26; acc: 0.22
Batch: 60; loss: 2.27; acc: 0.27
Batch: 80; loss: 2.27; acc: 0.19
Batch: 100; loss: 2.26; acc: 0.22
Batch: 120; loss: 2.27; acc: 0.27
Batch: 140; loss: 2.27; acc: 0.22
Val Epoch over. val_loss: 2.2777001888129362; val_accuracy: 0.19406847133757962 

The current subspace-distance is: 9.92481545836199e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.19
Batch: 20; loss: 2.27; acc: 0.19
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.34
Batch: 80; loss: 2.29; acc: 0.19
Batch: 100; loss: 2.27; acc: 0.19
Batch: 120; loss: 2.27; acc: 0.27
Batch: 140; loss: 2.27; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.23
Batch: 180; loss: 2.26; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.22
Batch: 220; loss: 2.27; acc: 0.23
Batch: 240; loss: 2.27; acc: 0.23
Batch: 260; loss: 2.28; acc: 0.17
Batch: 280; loss: 2.26; acc: 0.27
Batch: 300; loss: 2.28; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.14
Batch: 340; loss: 2.28; acc: 0.27
Batch: 360; loss: 2.27; acc: 0.22
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.28; acc: 0.19
Batch: 440; loss: 2.28; acc: 0.11
Batch: 460; loss: 2.27; acc: 0.27
Batch: 480; loss: 2.27; acc: 0.25
Batch: 500; loss: 2.28; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.22
Batch: 540; loss: 2.25; acc: 0.3
Batch: 560; loss: 2.27; acc: 0.27
Batch: 580; loss: 2.28; acc: 0.19
Batch: 600; loss: 2.26; acc: 0.25
Batch: 620; loss: 2.26; acc: 0.28
Batch: 640; loss: 2.26; acc: 0.22
Batch: 660; loss: 2.28; acc: 0.23
Batch: 680; loss: 2.27; acc: 0.19
Batch: 700; loss: 2.26; acc: 0.23
Batch: 720; loss: 2.28; acc: 0.2
Batch: 740; loss: 2.25; acc: 0.25
Batch: 760; loss: 2.26; acc: 0.27
Batch: 780; loss: 2.26; acc: 0.22
Train Epoch over. train_loss: 2.27; train_accuracy: 0.21 

Batch: 0; loss: 2.28; acc: 0.11
Batch: 20; loss: 2.28; acc: 0.25
Batch: 40; loss: 2.23; acc: 0.28
Batch: 60; loss: 2.25; acc: 0.25
Batch: 80; loss: 2.25; acc: 0.3
Batch: 100; loss: 2.23; acc: 0.34
Batch: 120; loss: 2.25; acc: 0.27
Batch: 140; loss: 2.26; acc: 0.33
Val Epoch over. val_loss: 2.2635243683104305; val_accuracy: 0.23168789808917198 

The current subspace-distance is: 1.1829173672595061e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.25
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.26; acc: 0.23
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.26; acc: 0.31
Batch: 120; loss: 2.25; acc: 0.22
Batch: 140; loss: 2.24; acc: 0.27
Batch: 160; loss: 2.26; acc: 0.23
Batch: 180; loss: 2.27; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.27
Batch: 220; loss: 2.26; acc: 0.2
Batch: 240; loss: 2.25; acc: 0.22
Batch: 260; loss: 2.26; acc: 0.25
Batch: 280; loss: 2.26; acc: 0.17
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.25; acc: 0.28
Batch: 340; loss: 2.27; acc: 0.17
Batch: 360; loss: 2.25; acc: 0.28
Batch: 380; loss: 2.25; acc: 0.23
Batch: 400; loss: 2.25; acc: 0.22
Batch: 420; loss: 2.19; acc: 0.45
Batch: 440; loss: 2.27; acc: 0.25
Batch: 460; loss: 2.28; acc: 0.14
Batch: 480; loss: 2.25; acc: 0.2
Batch: 500; loss: 2.26; acc: 0.25
Batch: 520; loss: 2.27; acc: 0.08
Batch: 540; loss: 2.27; acc: 0.11
Batch: 560; loss: 2.24; acc: 0.23
Batch: 580; loss: 2.27; acc: 0.12
Batch: 600; loss: 2.26; acc: 0.3
Batch: 620; loss: 2.25; acc: 0.25
Batch: 640; loss: 2.23; acc: 0.23
Batch: 660; loss: 2.27; acc: 0.17
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.26; acc: 0.19
Batch: 720; loss: 2.25; acc: 0.17
Batch: 740; loss: 2.27; acc: 0.12
Batch: 760; loss: 2.25; acc: 0.14
Batch: 780; loss: 2.23; acc: 0.2
Train Epoch over. train_loss: 2.26; train_accuracy: 0.22 

Batch: 0; loss: 2.28; acc: 0.11
Batch: 20; loss: 2.28; acc: 0.22
Batch: 40; loss: 2.21; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.19
Batch: 80; loss: 2.22; acc: 0.23
Batch: 100; loss: 2.19; acc: 0.36
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.3
Val Epoch over. val_loss: 2.2464252717935356; val_accuracy: 0.21666003184713375 

The current subspace-distance is: 1.4101616216066759e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.22; acc: 0.2
Batch: 20; loss: 2.25; acc: 0.19
Batch: 40; loss: 2.22; acc: 0.23
Batch: 60; loss: 2.22; acc: 0.28
Batch: 80; loss: 2.27; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.19
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.27; acc: 0.22
Batch: 160; loss: 2.24; acc: 0.25
Batch: 180; loss: 2.25; acc: 0.27
Batch: 200; loss: 2.25; acc: 0.17
Batch: 220; loss: 2.29; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.2
Batch: 260; loss: 2.25; acc: 0.22
Batch: 280; loss: 2.22; acc: 0.28
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.23; acc: 0.27
Batch: 340; loss: 2.21; acc: 0.27
Batch: 360; loss: 2.23; acc: 0.22
Batch: 380; loss: 2.23; acc: 0.17
Batch: 400; loss: 2.25; acc: 0.23
Batch: 420; loss: 2.25; acc: 0.22
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.26; acc: 0.17
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.29; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.22
Batch: 540; loss: 2.23; acc: 0.16
Batch: 560; loss: 2.27; acc: 0.17
Batch: 580; loss: 2.27; acc: 0.31
Batch: 600; loss: 2.19; acc: 0.25
Batch: 620; loss: 2.25; acc: 0.2
Batch: 640; loss: 2.24; acc: 0.2
Batch: 660; loss: 2.24; acc: 0.19
Batch: 680; loss: 2.24; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.27
Batch: 720; loss: 2.24; acc: 0.17
Batch: 740; loss: 2.22; acc: 0.25
Batch: 760; loss: 2.29; acc: 0.17
Batch: 780; loss: 2.31; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.21 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.19
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.18; acc: 0.33
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.28
Val Epoch over. val_loss: 2.239792021976155; val_accuracy: 0.20879777070063693 

The current subspace-distance is: 1.5186276868917048e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.25; acc: 0.23
Batch: 20; loss: 2.25; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.26; acc: 0.25
Batch: 80; loss: 2.24; acc: 0.19
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.23; acc: 0.25
Batch: 140; loss: 2.2; acc: 0.33
Batch: 160; loss: 2.24; acc: 0.28
Batch: 180; loss: 2.2; acc: 0.23
Batch: 200; loss: 2.26; acc: 0.2
Batch: 220; loss: 2.2; acc: 0.25
Batch: 240; loss: 2.28; acc: 0.17
Batch: 260; loss: 2.24; acc: 0.25
Batch: 280; loss: 2.24; acc: 0.2
Batch: 300; loss: 2.2; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.25
Batch: 340; loss: 2.22; acc: 0.22
Batch: 360; loss: 2.18; acc: 0.28
Batch: 380; loss: 2.28; acc: 0.19
Batch: 400; loss: 2.24; acc: 0.14
Batch: 420; loss: 2.28; acc: 0.12
Batch: 440; loss: 2.2; acc: 0.28
Batch: 460; loss: 2.26; acc: 0.16
Batch: 480; loss: 2.24; acc: 0.16
Batch: 500; loss: 2.24; acc: 0.19
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.26; acc: 0.14
Batch: 560; loss: 2.24; acc: 0.22
Batch: 580; loss: 2.24; acc: 0.19
Batch: 600; loss: 2.24; acc: 0.17
Batch: 620; loss: 2.2; acc: 0.25
Batch: 640; loss: 2.23; acc: 0.22
Batch: 660; loss: 2.27; acc: 0.14
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.23; acc: 0.14
Batch: 720; loss: 2.27; acc: 0.17
Batch: 740; loss: 2.26; acc: 0.27
Batch: 760; loss: 2.26; acc: 0.16
Batch: 780; loss: 2.23; acc: 0.22
Train Epoch over. train_loss: 2.24; train_accuracy: 0.2 

Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.23
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.19
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.16; acc: 0.3
Batch: 120; loss: 2.23; acc: 0.11
Batch: 140; loss: 2.23; acc: 0.22
Val Epoch over. val_loss: 2.2338912502215926; val_accuracy: 0.20660828025477707 

The current subspace-distance is: 1.7370926798321307e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.25; acc: 0.17
Batch: 60; loss: 2.22; acc: 0.2
Batch: 80; loss: 2.23; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.25
Batch: 120; loss: 2.27; acc: 0.17
Batch: 140; loss: 2.25; acc: 0.19
Batch: 160; loss: 2.24; acc: 0.14
Batch: 180; loss: 2.22; acc: 0.25
Batch: 200; loss: 2.24; acc: 0.22
Batch: 220; loss: 2.26; acc: 0.22
Batch: 240; loss: 2.25; acc: 0.22
Batch: 260; loss: 2.21; acc: 0.23
Batch: 280; loss: 2.23; acc: 0.28
Batch: 300; loss: 2.24; acc: 0.2
Batch: 320; loss: 2.26; acc: 0.12
Batch: 340; loss: 2.21; acc: 0.27
Batch: 360; loss: 2.27; acc: 0.12
Batch: 380; loss: 2.25; acc: 0.19
Batch: 400; loss: 2.24; acc: 0.23
Batch: 420; loss: 2.32; acc: 0.16
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.18; acc: 0.22
Batch: 480; loss: 2.27; acc: 0.19
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.14
Batch: 540; loss: 2.21; acc: 0.17
Batch: 560; loss: 2.25; acc: 0.23
Batch: 580; loss: 2.23; acc: 0.2
Batch: 600; loss: 2.28; acc: 0.19
Batch: 620; loss: 2.21; acc: 0.2
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.27; acc: 0.17
Batch: 680; loss: 2.25; acc: 0.19
Batch: 700; loss: 2.22; acc: 0.2
Batch: 720; loss: 2.2; acc: 0.23
Batch: 740; loss: 2.24; acc: 0.12
Batch: 760; loss: 2.19; acc: 0.27
Batch: 780; loss: 2.22; acc: 0.17
Train Epoch over. train_loss: 2.24; train_accuracy: 0.2 

Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.25
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.22
Batch: 100; loss: 2.16; acc: 0.3
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.23; acc: 0.16
Val Epoch over. val_loss: 2.2290249463099583; val_accuracy: 0.20402070063694266 

The current subspace-distance is: 1.8241691577713937e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.25; acc: 0.2
Batch: 20; loss: 2.27; acc: 0.12
Batch: 40; loss: 2.2; acc: 0.28
Batch: 60; loss: 2.24; acc: 0.2
Batch: 80; loss: 2.25; acc: 0.22
Batch: 100; loss: 2.23; acc: 0.27
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.21; acc: 0.22
Batch: 160; loss: 2.2; acc: 0.23
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.24; acc: 0.25
Batch: 220; loss: 2.21; acc: 0.19
Batch: 240; loss: 2.2; acc: 0.27
Batch: 260; loss: 2.18; acc: 0.28
Batch: 280; loss: 2.22; acc: 0.19
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.2; acc: 0.23
Batch: 360; loss: 2.21; acc: 0.25
Batch: 380; loss: 2.26; acc: 0.09
Batch: 400; loss: 2.22; acc: 0.16
Batch: 420; loss: 2.25; acc: 0.22
Batch: 440; loss: 2.27; acc: 0.2
Batch: 460; loss: 2.2; acc: 0.27
Batch: 480; loss: 2.23; acc: 0.16
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.23; acc: 0.23
Batch: 580; loss: 2.24; acc: 0.16
Batch: 600; loss: 2.21; acc: 0.27
Batch: 620; loss: 2.26; acc: 0.17
Batch: 640; loss: 2.23; acc: 0.27
Batch: 660; loss: 2.23; acc: 0.17
Batch: 680; loss: 2.22; acc: 0.16
Batch: 700; loss: 2.25; acc: 0.17
Batch: 720; loss: 2.22; acc: 0.23
Batch: 740; loss: 2.26; acc: 0.16
Batch: 760; loss: 2.26; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.27
Train Epoch over. train_loss: 2.23; train_accuracy: 0.2 

Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.25; acc: 0.23
Batch: 40; loss: 2.19; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.17
Batch: 80; loss: 2.19; acc: 0.2
Batch: 100; loss: 2.15; acc: 0.3
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.16
Val Epoch over. val_loss: 2.2250455595125818; val_accuracy: 0.20113455414012738 

The current subspace-distance is: 1.9547580450307578e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.26; acc: 0.2
Batch: 20; loss: 2.25; acc: 0.19
Batch: 40; loss: 2.24; acc: 0.09
Batch: 60; loss: 2.17; acc: 0.31
Batch: 80; loss: 2.26; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.17
Batch: 160; loss: 2.21; acc: 0.25
Batch: 180; loss: 2.28; acc: 0.14
Batch: 200; loss: 2.26; acc: 0.16
Batch: 220; loss: 2.21; acc: 0.19
Batch: 240; loss: 2.17; acc: 0.31
Batch: 260; loss: 2.24; acc: 0.16
Batch: 280; loss: 2.21; acc: 0.25
Batch: 300; loss: 2.21; acc: 0.25
Batch: 320; loss: 2.2; acc: 0.22
Batch: 340; loss: 2.21; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.25; acc: 0.2
Batch: 440; loss: 2.24; acc: 0.23
Batch: 460; loss: 2.2; acc: 0.14
Batch: 480; loss: 2.31; acc: 0.11
Batch: 500; loss: 2.21; acc: 0.2
Batch: 520; loss: 2.22; acc: 0.25
Batch: 540; loss: 2.22; acc: 0.17
Batch: 560; loss: 2.21; acc: 0.22
Batch: 580; loss: 2.24; acc: 0.17
Batch: 600; loss: 2.24; acc: 0.2
Batch: 620; loss: 2.19; acc: 0.23
Batch: 640; loss: 2.21; acc: 0.2
Batch: 660; loss: 2.18; acc: 0.25
Batch: 680; loss: 2.22; acc: 0.23
Batch: 700; loss: 2.25; acc: 0.25
Batch: 720; loss: 2.27; acc: 0.17
Batch: 740; loss: 2.18; acc: 0.27
Batch: 760; loss: 2.23; acc: 0.19
Batch: 780; loss: 2.14; acc: 0.33
Train Epoch over. train_loss: 2.23; train_accuracy: 0.2 

Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.25; acc: 0.22
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.16
Batch: 80; loss: 2.19; acc: 0.2
Batch: 100; loss: 2.14; acc: 0.25
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.17
Val Epoch over. val_loss: 2.221385542754155; val_accuracy: 0.19735270700636942 

The current subspace-distance is: 2.0213014067849144e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.12
Batch: 20; loss: 2.26; acc: 0.23
Batch: 40; loss: 2.25; acc: 0.14
Batch: 60; loss: 2.23; acc: 0.2
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.23; acc: 0.19
Batch: 120; loss: 2.21; acc: 0.2
Batch: 140; loss: 2.19; acc: 0.31
Batch: 160; loss: 2.27; acc: 0.2
Batch: 180; loss: 2.25; acc: 0.14
Batch: 200; loss: 2.25; acc: 0.23
Batch: 220; loss: 2.23; acc: 0.2
Batch: 240; loss: 2.2; acc: 0.19
Batch: 260; loss: 2.23; acc: 0.14
Batch: 280; loss: 2.22; acc: 0.2
Batch: 300; loss: 2.2; acc: 0.19
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.27
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.32; acc: 0.11
Batch: 400; loss: 2.21; acc: 0.23
Batch: 420; loss: 2.16; acc: 0.31
Batch: 440; loss: 2.24; acc: 0.2
Batch: 460; loss: 2.17; acc: 0.22
Batch: 480; loss: 2.22; acc: 0.2
Batch: 500; loss: 2.23; acc: 0.19
Batch: 520; loss: 2.17; acc: 0.23
Batch: 540; loss: 2.26; acc: 0.17
Batch: 560; loss: 2.17; acc: 0.23
Batch: 580; loss: 2.25; acc: 0.09
Batch: 600; loss: 2.2; acc: 0.23
Batch: 620; loss: 2.24; acc: 0.16
Batch: 640; loss: 2.26; acc: 0.09
Batch: 660; loss: 2.23; acc: 0.12
Batch: 680; loss: 2.14; acc: 0.27
Batch: 700; loss: 2.17; acc: 0.22
Batch: 720; loss: 2.19; acc: 0.2
Batch: 740; loss: 2.22; acc: 0.2
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.35; acc: 0.16
Train Epoch over. train_loss: 2.23; train_accuracy: 0.19 

Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.16
Batch: 80; loss: 2.18; acc: 0.2
Batch: 100; loss: 2.14; acc: 0.25
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.19
Val Epoch over. val_loss: 2.218378141427496; val_accuracy: 0.19546178343949044 

The current subspace-distance is: 2.194454282289371e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.2; acc: 0.19
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.26; acc: 0.14
Batch: 60; loss: 2.27; acc: 0.16
Batch: 80; loss: 2.23; acc: 0.17
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.12
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.12; acc: 0.28
Batch: 220; loss: 2.26; acc: 0.14
Batch: 240; loss: 2.23; acc: 0.11
Batch: 260; loss: 2.17; acc: 0.22
Batch: 280; loss: 2.26; acc: 0.17
Batch: 300; loss: 2.22; acc: 0.2
Batch: 320; loss: 2.21; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.2
Batch: 360; loss: 2.18; acc: 0.28
Batch: 380; loss: 2.23; acc: 0.23
Batch: 400; loss: 2.22; acc: 0.2
Batch: 420; loss: 2.22; acc: 0.19
Batch: 440; loss: 2.17; acc: 0.25
Batch: 460; loss: 2.23; acc: 0.22
Batch: 480; loss: 2.24; acc: 0.2
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.2; acc: 0.17
Batch: 540; loss: 2.26; acc: 0.14
Batch: 560; loss: 2.19; acc: 0.3
Batch: 580; loss: 2.25; acc: 0.2
Batch: 600; loss: 2.25; acc: 0.16
Batch: 620; loss: 2.19; acc: 0.22
Batch: 640; loss: 2.22; acc: 0.2
Batch: 660; loss: 2.23; acc: 0.16
Batch: 680; loss: 2.28; acc: 0.12
Batch: 700; loss: 2.19; acc: 0.23
Batch: 720; loss: 2.25; acc: 0.16
Batch: 740; loss: 2.21; acc: 0.25
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.22; acc: 0.2
Train Epoch over. train_loss: 2.23; train_accuracy: 0.19 

Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.18; acc: 0.19
Batch: 100; loss: 2.14; acc: 0.27
Batch: 120; loss: 2.25; acc: 0.14
Batch: 140; loss: 2.21; acc: 0.16
Val Epoch over. val_loss: 2.2157146854765095; val_accuracy: 0.19197850318471338 

The current subspace-distance is: 2.260955261590425e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.27; acc: 0.17
Batch: 20; loss: 2.24; acc: 0.16
Batch: 40; loss: 2.26; acc: 0.19
Batch: 60; loss: 2.2; acc: 0.16
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.2
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Batch: 160; loss: 2.31; acc: 0.14
Batch: 180; loss: 2.21; acc: 0.17
Batch: 200; loss: 2.29; acc: 0.08
Batch: 220; loss: 2.19; acc: 0.17
Batch: 240; loss: 2.27; acc: 0.12
Batch: 260; loss: 2.22; acc: 0.25
Batch: 280; loss: 2.17; acc: 0.23
Batch: 300; loss: 2.25; acc: 0.11
Batch: 320; loss: 2.16; acc: 0.12
Batch: 340; loss: 2.19; acc: 0.17
Batch: 360; loss: 2.2; acc: 0.2
Batch: 380; loss: 2.27; acc: 0.16
Batch: 400; loss: 2.19; acc: 0.19
Batch: 420; loss: 2.25; acc: 0.23
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.25; acc: 0.16
Batch: 480; loss: 2.24; acc: 0.16
Batch: 500; loss: 2.3; acc: 0.16
Batch: 520; loss: 2.27; acc: 0.09
Batch: 540; loss: 2.28; acc: 0.16
Batch: 560; loss: 2.21; acc: 0.2
Batch: 580; loss: 2.25; acc: 0.16
Batch: 600; loss: 2.21; acc: 0.19
Batch: 620; loss: 2.26; acc: 0.16
Batch: 640; loss: 2.31; acc: 0.11
Batch: 660; loss: 2.23; acc: 0.16
Batch: 680; loss: 2.23; acc: 0.19
Batch: 700; loss: 2.2; acc: 0.19
Batch: 720; loss: 2.25; acc: 0.14
Batch: 740; loss: 2.26; acc: 0.09
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.21; acc: 0.22
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.23
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.13; acc: 0.27
Batch: 120; loss: 2.25; acc: 0.16
Batch: 140; loss: 2.21; acc: 0.16
Val Epoch over. val_loss: 2.2136525728140666; val_accuracy: 0.1907842356687898 

The current subspace-distance is: 2.410977140243631e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.14
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.25; acc: 0.16
Batch: 80; loss: 2.18; acc: 0.19
Batch: 100; loss: 2.17; acc: 0.19
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.12
Batch: 160; loss: 2.26; acc: 0.14
Batch: 180; loss: 2.16; acc: 0.27
Batch: 200; loss: 2.18; acc: 0.16
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.24; acc: 0.16
Batch: 260; loss: 2.26; acc: 0.11
Batch: 280; loss: 2.18; acc: 0.23
Batch: 300; loss: 2.27; acc: 0.16
Batch: 320; loss: 2.23; acc: 0.14
Batch: 340; loss: 2.22; acc: 0.17
Batch: 360; loss: 2.23; acc: 0.23
Batch: 380; loss: 2.31; acc: 0.17
Batch: 400; loss: 2.2; acc: 0.16
Batch: 420; loss: 2.27; acc: 0.08
Batch: 440; loss: 2.21; acc: 0.11
Batch: 460; loss: 2.23; acc: 0.3
Batch: 480; loss: 2.12; acc: 0.27
Batch: 500; loss: 2.24; acc: 0.2
Batch: 520; loss: 2.24; acc: 0.14
Batch: 540; loss: 2.25; acc: 0.14
Batch: 560; loss: 2.24; acc: 0.16
Batch: 580; loss: 2.25; acc: 0.2
Batch: 600; loss: 2.16; acc: 0.23
Batch: 620; loss: 2.16; acc: 0.19
Batch: 640; loss: 2.17; acc: 0.2
Batch: 660; loss: 2.24; acc: 0.14
Batch: 680; loss: 2.34; acc: 0.14
Batch: 700; loss: 2.16; acc: 0.3
Batch: 720; loss: 2.14; acc: 0.22
Batch: 740; loss: 2.26; acc: 0.09
Batch: 760; loss: 2.21; acc: 0.19
Batch: 780; loss: 2.26; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.19; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.27
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.21; acc: 0.16
Val Epoch over. val_loss: 2.2120846714943077; val_accuracy: 0.18690286624203822 

The current subspace-distance is: 2.627952562761493e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.25
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.22; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.21; acc: 0.23
Batch: 100; loss: 2.17; acc: 0.19
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.31; acc: 0.09
Batch: 160; loss: 2.2; acc: 0.2
Batch: 180; loss: 2.25; acc: 0.22
Batch: 200; loss: 2.18; acc: 0.19
Batch: 220; loss: 2.19; acc: 0.2
Batch: 240; loss: 2.18; acc: 0.19
Batch: 260; loss: 2.18; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.25; acc: 0.14
Batch: 340; loss: 2.22; acc: 0.17
Batch: 360; loss: 2.25; acc: 0.17
Batch: 380; loss: 2.24; acc: 0.19
Batch: 400; loss: 2.24; acc: 0.16
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.19; acc: 0.17
Batch: 460; loss: 2.24; acc: 0.19
Batch: 480; loss: 2.21; acc: 0.19
Batch: 500; loss: 2.18; acc: 0.2
Batch: 520; loss: 2.19; acc: 0.2
Batch: 540; loss: 2.25; acc: 0.09
Batch: 560; loss: 2.2; acc: 0.22
Batch: 580; loss: 2.25; acc: 0.19
Batch: 600; loss: 2.2; acc: 0.16
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.21; acc: 0.19
Batch: 660; loss: 2.2; acc: 0.2
Batch: 680; loss: 2.34; acc: 0.09
Batch: 700; loss: 2.36; acc: 0.11
Batch: 720; loss: 2.17; acc: 0.19
Batch: 740; loss: 2.25; acc: 0.14
Batch: 760; loss: 2.19; acc: 0.2
Batch: 780; loss: 2.26; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.27
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.21; acc: 0.17
Val Epoch over. val_loss: 2.2110054280347886; val_accuracy: 0.1846138535031847 

The current subspace-distance is: 2.4323477191501297e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.15; acc: 0.23
Batch: 40; loss: 2.23; acc: 0.11
Batch: 60; loss: 2.25; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.17
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.24; acc: 0.12
Batch: 180; loss: 2.24; acc: 0.19
Batch: 200; loss: 2.25; acc: 0.2
Batch: 220; loss: 2.27; acc: 0.2
Batch: 240; loss: 2.2; acc: 0.2
Batch: 260; loss: 2.31; acc: 0.12
Batch: 280; loss: 2.2; acc: 0.19
Batch: 300; loss: 2.29; acc: 0.12
Batch: 320; loss: 2.32; acc: 0.16
Batch: 340; loss: 2.21; acc: 0.2
Batch: 360; loss: 2.15; acc: 0.2
Batch: 380; loss: 2.26; acc: 0.16
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.22; acc: 0.2
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.27; acc: 0.17
Batch: 480; loss: 2.18; acc: 0.23
Batch: 500; loss: 2.22; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.2
Batch: 540; loss: 2.16; acc: 0.19
Batch: 560; loss: 2.24; acc: 0.16
Batch: 580; loss: 2.13; acc: 0.23
Batch: 600; loss: 2.22; acc: 0.17
Batch: 620; loss: 2.17; acc: 0.22
Batch: 640; loss: 2.17; acc: 0.16
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.27; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.17
Batch: 720; loss: 2.14; acc: 0.25
Batch: 740; loss: 2.19; acc: 0.17
Batch: 760; loss: 2.28; acc: 0.14
Batch: 780; loss: 2.24; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.27
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.17
Val Epoch over. val_loss: 2.210712861103617; val_accuracy: 0.1852109872611465 

The current subspace-distance is: 2.8937061870237812e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.16; acc: 0.27
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.23; acc: 0.22
Batch: 100; loss: 2.22; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.18; acc: 0.27
Batch: 160; loss: 2.22; acc: 0.23
Batch: 180; loss: 2.19; acc: 0.2
Batch: 200; loss: 2.29; acc: 0.14
Batch: 220; loss: 2.23; acc: 0.22
Batch: 240; loss: 2.22; acc: 0.19
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.32; acc: 0.16
Batch: 340; loss: 2.19; acc: 0.23
Batch: 360; loss: 2.31; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.25; acc: 0.09
Batch: 420; loss: 2.17; acc: 0.17
Batch: 440; loss: 2.21; acc: 0.16
Batch: 460; loss: 2.27; acc: 0.09
Batch: 480; loss: 2.21; acc: 0.19
Batch: 500; loss: 2.23; acc: 0.12
Batch: 520; loss: 2.18; acc: 0.22
Batch: 540; loss: 2.2; acc: 0.16
Batch: 560; loss: 2.18; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.12
Batch: 600; loss: 2.23; acc: 0.12
Batch: 620; loss: 2.18; acc: 0.28
Batch: 640; loss: 2.16; acc: 0.19
Batch: 660; loss: 2.16; acc: 0.22
Batch: 680; loss: 2.22; acc: 0.17
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.2; acc: 0.27
Batch: 740; loss: 2.21; acc: 0.19
Batch: 760; loss: 2.18; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.14
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.27
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.17
Val Epoch over. val_loss: 2.210463291520526; val_accuracy: 0.1845143312101911 

The current subspace-distance is: 2.940041304100305e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.22; acc: 0.2
Batch: 60; loss: 2.24; acc: 0.22
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 2.21; acc: 0.2
Batch: 120; loss: 2.18; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.27; acc: 0.2
Batch: 180; loss: 2.2; acc: 0.16
Batch: 200; loss: 2.17; acc: 0.19
Batch: 220; loss: 2.19; acc: 0.19
Batch: 240; loss: 2.28; acc: 0.17
Batch: 260; loss: 2.16; acc: 0.27
Batch: 280; loss: 2.23; acc: 0.2
Batch: 300; loss: 2.21; acc: 0.25
Batch: 320; loss: 2.21; acc: 0.19
Batch: 340; loss: 2.19; acc: 0.17
Batch: 360; loss: 2.32; acc: 0.06
Batch: 380; loss: 2.33; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.17
Batch: 420; loss: 2.33; acc: 0.06
Batch: 440; loss: 2.21; acc: 0.2
Batch: 460; loss: 2.2; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.19; acc: 0.16
Batch: 520; loss: 2.2; acc: 0.23
Batch: 540; loss: 2.29; acc: 0.08
Batch: 560; loss: 2.26; acc: 0.11
Batch: 580; loss: 2.17; acc: 0.23
Batch: 600; loss: 2.27; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.11
Batch: 640; loss: 2.23; acc: 0.2
Batch: 660; loss: 2.25; acc: 0.19
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.25; acc: 0.17
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.35; acc: 0.06
Batch: 760; loss: 2.21; acc: 0.2
Batch: 780; loss: 2.17; acc: 0.23
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.27
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.17
Val Epoch over. val_loss: 2.2102023294776867; val_accuracy: 0.18401671974522293 

The current subspace-distance is: 2.8132175430073403e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.11
Batch: 40; loss: 2.25; acc: 0.14
Batch: 60; loss: 2.16; acc: 0.3
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.25; acc: 0.19
Batch: 120; loss: 2.21; acc: 0.23
Batch: 140; loss: 2.25; acc: 0.11
Batch: 160; loss: 2.2; acc: 0.19
Batch: 180; loss: 2.28; acc: 0.17
Batch: 200; loss: 2.17; acc: 0.23
Batch: 220; loss: 2.23; acc: 0.16
Batch: 240; loss: 2.25; acc: 0.16
Batch: 260; loss: 2.26; acc: 0.14
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.2; acc: 0.2
Batch: 320; loss: 2.28; acc: 0.09
Batch: 340; loss: 2.19; acc: 0.16
Batch: 360; loss: 2.21; acc: 0.19
Batch: 380; loss: 2.14; acc: 0.16
Batch: 400; loss: 2.31; acc: 0.09
Batch: 420; loss: 2.27; acc: 0.09
Batch: 440; loss: 2.2; acc: 0.17
Batch: 460; loss: 2.22; acc: 0.2
Batch: 480; loss: 2.26; acc: 0.2
Batch: 500; loss: 2.22; acc: 0.17
Batch: 520; loss: 2.19; acc: 0.22
Batch: 540; loss: 2.22; acc: 0.19
Batch: 560; loss: 2.23; acc: 0.14
Batch: 580; loss: 2.2; acc: 0.19
Batch: 600; loss: 2.22; acc: 0.22
Batch: 620; loss: 2.22; acc: 0.2
Batch: 640; loss: 2.19; acc: 0.17
Batch: 660; loss: 2.19; acc: 0.2
Batch: 680; loss: 2.18; acc: 0.14
Batch: 700; loss: 2.13; acc: 0.31
Batch: 720; loss: 2.24; acc: 0.19
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.19; acc: 0.17
Batch: 780; loss: 2.24; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.25
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.17
Val Epoch over. val_loss: 2.2098713999341246; val_accuracy: 0.18252388535031847 

The current subspace-distance is: 2.8730280973832123e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.19
Batch: 20; loss: 2.21; acc: 0.2
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.16; acc: 0.33
Batch: 80; loss: 2.23; acc: 0.19
Batch: 100; loss: 2.23; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.22
Batch: 140; loss: 2.21; acc: 0.22
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.2; acc: 0.16
Batch: 220; loss: 2.2; acc: 0.12
Batch: 240; loss: 2.21; acc: 0.16
Batch: 260; loss: 2.16; acc: 0.22
Batch: 280; loss: 2.26; acc: 0.16
Batch: 300; loss: 2.26; acc: 0.16
Batch: 320; loss: 2.19; acc: 0.2
Batch: 340; loss: 2.26; acc: 0.12
Batch: 360; loss: 2.18; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.17
Batch: 400; loss: 2.25; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.23; acc: 0.17
Batch: 460; loss: 2.17; acc: 0.22
Batch: 480; loss: 2.18; acc: 0.2
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.19; acc: 0.22
Batch: 540; loss: 2.19; acc: 0.22
Batch: 560; loss: 2.17; acc: 0.28
Batch: 580; loss: 2.13; acc: 0.22
Batch: 600; loss: 2.26; acc: 0.11
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.2; acc: 0.22
Batch: 660; loss: 2.17; acc: 0.22
Batch: 680; loss: 2.22; acc: 0.16
Batch: 700; loss: 2.35; acc: 0.06
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.34; acc: 0.12
Batch: 760; loss: 2.19; acc: 0.28
Batch: 780; loss: 2.36; acc: 0.08
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.14
Batch: 100; loss: 2.13; acc: 0.25
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.19
Val Epoch over. val_loss: 2.2096089238573793; val_accuracy: 0.1819267515923567 

The current subspace-distance is: 2.9688946597161703e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.24; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.19; acc: 0.25
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.19; acc: 0.14
Batch: 140; loss: 2.17; acc: 0.22
Batch: 160; loss: 2.23; acc: 0.16
Batch: 180; loss: 2.24; acc: 0.17
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.22; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.16
Batch: 260; loss: 2.25; acc: 0.14
Batch: 280; loss: 2.2; acc: 0.22
Batch: 300; loss: 2.21; acc: 0.27
Batch: 320; loss: 2.22; acc: 0.16
Batch: 340; loss: 2.2; acc: 0.2
Batch: 360; loss: 2.22; acc: 0.12
Batch: 380; loss: 2.2; acc: 0.12
Batch: 400; loss: 2.19; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.27; acc: 0.11
Batch: 460; loss: 2.12; acc: 0.27
Batch: 480; loss: 2.27; acc: 0.14
Batch: 500; loss: 2.14; acc: 0.22
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.15; acc: 0.23
Batch: 560; loss: 2.26; acc: 0.12
Batch: 580; loss: 2.25; acc: 0.17
Batch: 600; loss: 2.26; acc: 0.08
Batch: 620; loss: 2.22; acc: 0.2
Batch: 640; loss: 2.33; acc: 0.09
Batch: 660; loss: 2.22; acc: 0.16
Batch: 680; loss: 2.19; acc: 0.17
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.2; acc: 0.2
Batch: 740; loss: 2.32; acc: 0.11
Batch: 760; loss: 2.26; acc: 0.17
Batch: 780; loss: 2.18; acc: 0.27
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.14
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.19
Val Epoch over. val_loss: 2.2093020594044095; val_accuracy: 0.18172770700636942 

The current subspace-distance is: 3.0334087568917312e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.27; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.27; acc: 0.23
Batch: 80; loss: 2.22; acc: 0.2
Batch: 100; loss: 2.22; acc: 0.16
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.22; acc: 0.19
Batch: 160; loss: 2.21; acc: 0.23
Batch: 180; loss: 2.24; acc: 0.12
Batch: 200; loss: 2.25; acc: 0.19
Batch: 220; loss: 2.21; acc: 0.2
Batch: 240; loss: 2.24; acc: 0.17
Batch: 260; loss: 2.2; acc: 0.22
Batch: 280; loss: 2.06; acc: 0.31
Batch: 300; loss: 2.17; acc: 0.16
Batch: 320; loss: 2.24; acc: 0.16
Batch: 340; loss: 2.21; acc: 0.14
Batch: 360; loss: 2.21; acc: 0.2
Batch: 380; loss: 2.25; acc: 0.09
Batch: 400; loss: 2.19; acc: 0.17
Batch: 420; loss: 2.32; acc: 0.11
Batch: 440; loss: 2.28; acc: 0.11
Batch: 460; loss: 2.11; acc: 0.25
Batch: 480; loss: 2.34; acc: 0.08
Batch: 500; loss: 2.18; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.16
Batch: 540; loss: 2.16; acc: 0.2
Batch: 560; loss: 2.24; acc: 0.17
Batch: 580; loss: 2.21; acc: 0.22
Batch: 600; loss: 2.18; acc: 0.17
Batch: 620; loss: 2.31; acc: 0.12
Batch: 640; loss: 2.36; acc: 0.11
Batch: 660; loss: 2.16; acc: 0.23
Batch: 680; loss: 2.19; acc: 0.27
Batch: 700; loss: 2.2; acc: 0.22
Batch: 720; loss: 2.3; acc: 0.16
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.25; acc: 0.14
Batch: 780; loss: 2.19; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.14
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.19
Val Epoch over. val_loss: 2.208955506610263; val_accuracy: 0.18182722929936307 

The current subspace-distance is: 3.083614865317941e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.22; acc: 0.2
Batch: 20; loss: 2.18; acc: 0.17
Batch: 40; loss: 2.21; acc: 0.22
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.24; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.14
Batch: 120; loss: 2.21; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.21; acc: 0.11
Batch: 180; loss: 2.27; acc: 0.11
Batch: 200; loss: 2.2; acc: 0.19
Batch: 220; loss: 2.23; acc: 0.22
Batch: 240; loss: 2.22; acc: 0.14
Batch: 260; loss: 2.15; acc: 0.2
Batch: 280; loss: 2.25; acc: 0.14
Batch: 300; loss: 2.26; acc: 0.17
Batch: 320; loss: 2.23; acc: 0.12
Batch: 340; loss: 2.2; acc: 0.12
Batch: 360; loss: 2.24; acc: 0.14
Batch: 380; loss: 2.3; acc: 0.16
Batch: 400; loss: 2.16; acc: 0.23
Batch: 420; loss: 2.31; acc: 0.06
Batch: 440; loss: 2.26; acc: 0.17
Batch: 460; loss: 2.24; acc: 0.16
Batch: 480; loss: 2.2; acc: 0.2
Batch: 500; loss: 2.21; acc: 0.22
Batch: 520; loss: 2.29; acc: 0.16
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.19; acc: 0.19
Batch: 580; loss: 2.23; acc: 0.16
Batch: 600; loss: 2.14; acc: 0.2
Batch: 620; loss: 2.21; acc: 0.16
Batch: 640; loss: 2.25; acc: 0.09
Batch: 660; loss: 2.32; acc: 0.14
Batch: 680; loss: 2.28; acc: 0.09
Batch: 700; loss: 2.2; acc: 0.23
Batch: 720; loss: 2.18; acc: 0.14
Batch: 740; loss: 2.27; acc: 0.14
Batch: 760; loss: 2.3; acc: 0.14
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2086080289950036; val_accuracy: 0.18172770700636942 

The current subspace-distance is: 3.189998824382201e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.22; acc: 0.11
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.23; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.17
Batch: 80; loss: 2.31; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.19
Batch: 120; loss: 2.18; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.17
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.25; acc: 0.12
Batch: 220; loss: 2.19; acc: 0.22
Batch: 240; loss: 2.2; acc: 0.2
Batch: 260; loss: 2.21; acc: 0.19
Batch: 280; loss: 2.17; acc: 0.19
Batch: 300; loss: 2.23; acc: 0.16
Batch: 320; loss: 2.19; acc: 0.19
Batch: 340; loss: 2.16; acc: 0.22
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.14
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.22; acc: 0.19
Batch: 440; loss: 2.28; acc: 0.16
Batch: 460; loss: 2.22; acc: 0.11
Batch: 480; loss: 2.26; acc: 0.09
Batch: 500; loss: 2.11; acc: 0.25
Batch: 520; loss: 2.27; acc: 0.12
Batch: 540; loss: 2.18; acc: 0.12
Batch: 560; loss: 2.19; acc: 0.2
Batch: 580; loss: 2.24; acc: 0.11
Batch: 600; loss: 2.18; acc: 0.25
Batch: 620; loss: 2.21; acc: 0.14
Batch: 640; loss: 2.24; acc: 0.12
Batch: 660; loss: 2.22; acc: 0.19
Batch: 680; loss: 2.17; acc: 0.19
Batch: 700; loss: 2.15; acc: 0.23
Batch: 720; loss: 2.19; acc: 0.23
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.22; acc: 0.22
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2082734047227603; val_accuracy: 0.1800358280254777 

The current subspace-distance is: 2.9780747354379855e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.28; acc: 0.17
Batch: 20; loss: 2.27; acc: 0.14
Batch: 40; loss: 2.23; acc: 0.09
Batch: 60; loss: 2.23; acc: 0.2
Batch: 80; loss: 2.26; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.19
Batch: 120; loss: 2.18; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.14
Batch: 160; loss: 2.18; acc: 0.22
Batch: 180; loss: 2.23; acc: 0.19
Batch: 200; loss: 2.19; acc: 0.16
Batch: 220; loss: 2.18; acc: 0.19
Batch: 240; loss: 2.22; acc: 0.22
Batch: 260; loss: 2.26; acc: 0.14
Batch: 280; loss: 2.22; acc: 0.23
Batch: 300; loss: 2.23; acc: 0.19
Batch: 320; loss: 2.2; acc: 0.2
Batch: 340; loss: 2.16; acc: 0.25
Batch: 360; loss: 2.17; acc: 0.27
Batch: 380; loss: 2.21; acc: 0.16
Batch: 400; loss: 2.23; acc: 0.2
Batch: 420; loss: 2.23; acc: 0.2
Batch: 440; loss: 2.24; acc: 0.16
Batch: 460; loss: 2.21; acc: 0.19
Batch: 480; loss: 2.23; acc: 0.16
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.19; acc: 0.19
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.26; acc: 0.12
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.26; acc: 0.16
Batch: 620; loss: 2.16; acc: 0.27
Batch: 640; loss: 2.19; acc: 0.14
Batch: 660; loss: 2.24; acc: 0.19
Batch: 680; loss: 2.24; acc: 0.16
Batch: 700; loss: 2.25; acc: 0.12
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.17; acc: 0.22
Batch: 760; loss: 2.24; acc: 0.16
Batch: 780; loss: 2.18; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.207909907505011; val_accuracy: 0.18013535031847133 

The current subspace-distance is: 3.056331479456276e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.16; acc: 0.22
Batch: 20; loss: 2.35; acc: 0.09
Batch: 40; loss: 2.17; acc: 0.23
Batch: 60; loss: 2.18; acc: 0.25
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.22; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.22
Batch: 140; loss: 2.14; acc: 0.17
Batch: 160; loss: 2.26; acc: 0.14
Batch: 180; loss: 2.17; acc: 0.16
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.1; acc: 0.31
Batch: 240; loss: 2.28; acc: 0.25
Batch: 260; loss: 2.31; acc: 0.12
Batch: 280; loss: 2.24; acc: 0.14
Batch: 300; loss: 2.25; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.2
Batch: 340; loss: 2.12; acc: 0.25
Batch: 360; loss: 2.21; acc: 0.19
Batch: 380; loss: 2.18; acc: 0.19
Batch: 400; loss: 2.2; acc: 0.17
Batch: 420; loss: 2.26; acc: 0.11
Batch: 440; loss: 2.2; acc: 0.25
Batch: 460; loss: 2.16; acc: 0.22
Batch: 480; loss: 2.19; acc: 0.17
Batch: 500; loss: 2.21; acc: 0.22
Batch: 520; loss: 2.18; acc: 0.17
Batch: 540; loss: 2.32; acc: 0.17
Batch: 560; loss: 2.24; acc: 0.17
Batch: 580; loss: 2.22; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.16
Batch: 620; loss: 2.33; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.16
Batch: 660; loss: 2.17; acc: 0.2
Batch: 680; loss: 2.19; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.16; acc: 0.19
Batch: 740; loss: 2.2; acc: 0.2
Batch: 760; loss: 2.13; acc: 0.19
Batch: 780; loss: 2.19; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.207743079798996; val_accuracy: 0.18043391719745222 

The current subspace-distance is: 3.063102121814154e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.21; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.27; acc: 0.11
Batch: 60; loss: 2.18; acc: 0.22
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.2; acc: 0.16
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.09; acc: 0.28
Batch: 160; loss: 2.19; acc: 0.16
Batch: 180; loss: 2.19; acc: 0.19
Batch: 200; loss: 2.19; acc: 0.23
Batch: 220; loss: 2.21; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.12
Batch: 260; loss: 2.18; acc: 0.22
Batch: 280; loss: 2.26; acc: 0.11
Batch: 300; loss: 2.29; acc: 0.16
Batch: 320; loss: 2.18; acc: 0.19
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.12
Batch: 380; loss: 2.26; acc: 0.17
Batch: 400; loss: 2.23; acc: 0.16
Batch: 420; loss: 2.21; acc: 0.19
Batch: 440; loss: 2.22; acc: 0.2
Batch: 460; loss: 2.24; acc: 0.22
Batch: 480; loss: 2.2; acc: 0.2
Batch: 500; loss: 2.26; acc: 0.09
Batch: 520; loss: 2.13; acc: 0.27
Batch: 540; loss: 2.07; acc: 0.25
Batch: 560; loss: 2.23; acc: 0.16
Batch: 580; loss: 2.21; acc: 0.19
Batch: 600; loss: 2.16; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.25
Batch: 640; loss: 2.19; acc: 0.22
Batch: 660; loss: 2.26; acc: 0.17
Batch: 680; loss: 2.12; acc: 0.27
Batch: 700; loss: 2.22; acc: 0.19
Batch: 720; loss: 2.31; acc: 0.08
Batch: 740; loss: 2.15; acc: 0.23
Batch: 760; loss: 2.31; acc: 0.16
Batch: 780; loss: 2.19; acc: 0.2
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.207577315105754; val_accuracy: 0.1806329617834395 

The current subspace-distance is: 3.2108830055221915e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.16
Batch: 20; loss: 2.25; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.16
Batch: 60; loss: 2.17; acc: 0.17
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.21; acc: 0.17
Batch: 120; loss: 2.26; acc: 0.09
Batch: 140; loss: 2.24; acc: 0.17
Batch: 160; loss: 2.19; acc: 0.19
Batch: 180; loss: 2.19; acc: 0.17
Batch: 200; loss: 2.24; acc: 0.17
Batch: 220; loss: 2.28; acc: 0.09
Batch: 240; loss: 2.27; acc: 0.09
Batch: 260; loss: 2.25; acc: 0.2
Batch: 280; loss: 2.12; acc: 0.27
Batch: 300; loss: 2.21; acc: 0.16
Batch: 320; loss: 2.23; acc: 0.19
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.17; acc: 0.2
Batch: 380; loss: 2.19; acc: 0.2
Batch: 400; loss: 2.18; acc: 0.22
Batch: 420; loss: 2.22; acc: 0.17
Batch: 440; loss: 2.23; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.17; acc: 0.22
Batch: 500; loss: 2.22; acc: 0.25
Batch: 520; loss: 2.23; acc: 0.17
Batch: 540; loss: 2.19; acc: 0.19
Batch: 560; loss: 2.27; acc: 0.16
Batch: 580; loss: 2.26; acc: 0.11
Batch: 600; loss: 2.17; acc: 0.2
Batch: 620; loss: 2.31; acc: 0.16
Batch: 640; loss: 2.33; acc: 0.09
Batch: 660; loss: 2.16; acc: 0.22
Batch: 680; loss: 2.22; acc: 0.17
Batch: 700; loss: 2.18; acc: 0.16
Batch: 720; loss: 2.23; acc: 0.17
Batch: 740; loss: 2.17; acc: 0.17
Batch: 760; loss: 2.39; acc: 0.11
Batch: 780; loss: 2.31; acc: 0.06
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.25
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.207401063032211; val_accuracy: 0.18093152866242038 

The current subspace-distance is: 3.166530586895533e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.16; acc: 0.22
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.24; acc: 0.16
Batch: 60; loss: 2.25; acc: 0.17
Batch: 80; loss: 2.15; acc: 0.2
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.16
Batch: 140; loss: 2.19; acc: 0.2
Batch: 160; loss: 2.26; acc: 0.12
Batch: 180; loss: 2.17; acc: 0.19
Batch: 200; loss: 2.21; acc: 0.22
Batch: 220; loss: 2.22; acc: 0.16
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.15; acc: 0.23
Batch: 280; loss: 2.18; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.17
Batch: 320; loss: 2.23; acc: 0.17
Batch: 340; loss: 2.26; acc: 0.14
Batch: 360; loss: 2.24; acc: 0.16
Batch: 380; loss: 2.26; acc: 0.12
Batch: 400; loss: 2.21; acc: 0.17
Batch: 420; loss: 2.26; acc: 0.17
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.25; acc: 0.16
Batch: 480; loss: 2.22; acc: 0.16
Batch: 500; loss: 2.23; acc: 0.11
Batch: 520; loss: 2.2; acc: 0.25
Batch: 540; loss: 2.2; acc: 0.12
Batch: 560; loss: 2.23; acc: 0.19
Batch: 580; loss: 2.22; acc: 0.2
Batch: 600; loss: 2.15; acc: 0.2
Batch: 620; loss: 2.13; acc: 0.2
Batch: 640; loss: 2.2; acc: 0.2
Batch: 660; loss: 2.23; acc: 0.19
Batch: 680; loss: 2.23; acc: 0.17
Batch: 700; loss: 2.17; acc: 0.2
Batch: 720; loss: 2.19; acc: 0.16
Batch: 740; loss: 2.23; acc: 0.12
Batch: 760; loss: 2.27; acc: 0.22
Batch: 780; loss: 2.18; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.25
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.207252087866425; val_accuracy: 0.1807324840764331 

The current subspace-distance is: 3.088375524384901e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.12; acc: 0.3
Batch: 20; loss: 2.2; acc: 0.19
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.26; acc: 0.14
Batch: 100; loss: 2.25; acc: 0.11
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.21; acc: 0.22
Batch: 160; loss: 2.27; acc: 0.09
Batch: 180; loss: 2.15; acc: 0.2
Batch: 200; loss: 2.31; acc: 0.09
Batch: 220; loss: 2.2; acc: 0.17
Batch: 240; loss: 2.19; acc: 0.2
Batch: 260; loss: 2.26; acc: 0.09
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.2; acc: 0.16
Batch: 320; loss: 2.34; acc: 0.11
Batch: 340; loss: 2.14; acc: 0.19
Batch: 360; loss: 2.14; acc: 0.23
Batch: 380; loss: 2.21; acc: 0.14
Batch: 400; loss: 2.2; acc: 0.2
Batch: 420; loss: 2.22; acc: 0.17
Batch: 440; loss: 2.18; acc: 0.17
Batch: 460; loss: 2.28; acc: 0.16
Batch: 480; loss: 2.17; acc: 0.2
Batch: 500; loss: 2.2; acc: 0.19
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.2; acc: 0.14
Batch: 560; loss: 2.14; acc: 0.23
Batch: 580; loss: 2.16; acc: 0.25
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.28; acc: 0.12
Batch: 640; loss: 2.36; acc: 0.08
Batch: 660; loss: 2.24; acc: 0.14
Batch: 680; loss: 2.28; acc: 0.14
Batch: 700; loss: 2.2; acc: 0.2
Batch: 720; loss: 2.27; acc: 0.16
Batch: 740; loss: 2.2; acc: 0.16
Batch: 760; loss: 2.1; acc: 0.23
Batch: 780; loss: 2.27; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.25
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2070989623950545; val_accuracy: 0.1807324840764331 

The current subspace-distance is: 3.0131528546917252e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.28; acc: 0.2
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.23; acc: 0.17
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.18; acc: 0.2
Batch: 100; loss: 2.15; acc: 0.28
Batch: 120; loss: 2.24; acc: 0.12
Batch: 140; loss: 2.19; acc: 0.22
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.12; acc: 0.2
Batch: 200; loss: 2.14; acc: 0.23
Batch: 220; loss: 2.19; acc: 0.17
Batch: 240; loss: 2.3; acc: 0.14
Batch: 260; loss: 2.18; acc: 0.19
Batch: 280; loss: 2.24; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.19
Batch: 320; loss: 2.25; acc: 0.19
Batch: 340; loss: 2.26; acc: 0.16
Batch: 360; loss: 2.26; acc: 0.12
Batch: 380; loss: 2.24; acc: 0.16
Batch: 400; loss: 2.17; acc: 0.17
Batch: 420; loss: 2.19; acc: 0.19
Batch: 440; loss: 2.2; acc: 0.14
Batch: 460; loss: 2.19; acc: 0.17
Batch: 480; loss: 2.22; acc: 0.17
Batch: 500; loss: 2.23; acc: 0.22
Batch: 520; loss: 2.22; acc: 0.22
Batch: 540; loss: 2.28; acc: 0.14
Batch: 560; loss: 2.18; acc: 0.19
Batch: 580; loss: 2.2; acc: 0.17
Batch: 600; loss: 2.17; acc: 0.16
Batch: 620; loss: 2.18; acc: 0.2
Batch: 640; loss: 2.28; acc: 0.16
Batch: 660; loss: 2.23; acc: 0.23
Batch: 680; loss: 2.26; acc: 0.09
Batch: 700; loss: 2.22; acc: 0.11
Batch: 720; loss: 2.26; acc: 0.14
Batch: 740; loss: 2.19; acc: 0.2
Batch: 760; loss: 2.25; acc: 0.14
Batch: 780; loss: 2.23; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.206907717285642; val_accuracy: 0.18053343949044587 

The current subspace-distance is: 3.046693018404767e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.23
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.25; acc: 0.16
Batch: 60; loss: 2.21; acc: 0.16
Batch: 80; loss: 2.24; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.14
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.19
Batch: 160; loss: 2.14; acc: 0.23
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.2; acc: 0.22
Batch: 220; loss: 2.2; acc: 0.16
Batch: 240; loss: 2.2; acc: 0.2
Batch: 260; loss: 2.27; acc: 0.2
Batch: 280; loss: 2.22; acc: 0.22
Batch: 300; loss: 2.25; acc: 0.12
Batch: 320; loss: 2.23; acc: 0.14
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.27; acc: 0.19
Batch: 380; loss: 2.16; acc: 0.2
Batch: 400; loss: 2.21; acc: 0.14
Batch: 420; loss: 2.26; acc: 0.14
Batch: 440; loss: 2.21; acc: 0.17
Batch: 460; loss: 2.15; acc: 0.17
Batch: 480; loss: 2.24; acc: 0.2
Batch: 500; loss: 2.17; acc: 0.28
Batch: 520; loss: 2.17; acc: 0.22
Batch: 540; loss: 2.39; acc: 0.05
Batch: 560; loss: 2.24; acc: 0.17
Batch: 580; loss: 2.17; acc: 0.2
Batch: 600; loss: 2.25; acc: 0.14
Batch: 620; loss: 2.16; acc: 0.19
Batch: 640; loss: 2.25; acc: 0.16
Batch: 660; loss: 2.27; acc: 0.11
Batch: 680; loss: 2.31; acc: 0.06
Batch: 700; loss: 2.25; acc: 0.2
Batch: 720; loss: 2.18; acc: 0.22
Batch: 740; loss: 2.24; acc: 0.19
Batch: 760; loss: 2.31; acc: 0.12
Batch: 780; loss: 2.14; acc: 0.22
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2067491127427217; val_accuracy: 0.1803343949044586 

The current subspace-distance is: 3.2269144867314026e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.25; acc: 0.14
Batch: 20; loss: 2.26; acc: 0.08
Batch: 40; loss: 2.24; acc: 0.19
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.23; acc: 0.17
Batch: 100; loss: 2.31; acc: 0.09
Batch: 120; loss: 2.17; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.14
Batch: 160; loss: 2.19; acc: 0.19
Batch: 180; loss: 2.21; acc: 0.19
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.18; acc: 0.12
Batch: 240; loss: 2.17; acc: 0.17
Batch: 260; loss: 2.17; acc: 0.22
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.25; acc: 0.14
Batch: 320; loss: 2.21; acc: 0.2
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.21; acc: 0.19
Batch: 380; loss: 2.23; acc: 0.16
Batch: 400; loss: 2.28; acc: 0.22
Batch: 420; loss: 2.25; acc: 0.2
Batch: 440; loss: 2.22; acc: 0.11
Batch: 460; loss: 2.2; acc: 0.19
Batch: 480; loss: 2.23; acc: 0.17
Batch: 500; loss: 2.33; acc: 0.08
Batch: 520; loss: 2.22; acc: 0.19
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.15; acc: 0.31
Batch: 580; loss: 2.16; acc: 0.2
Batch: 600; loss: 2.23; acc: 0.17
Batch: 620; loss: 2.25; acc: 0.22
Batch: 640; loss: 2.29; acc: 0.14
Batch: 660; loss: 2.19; acc: 0.2
Batch: 680; loss: 2.15; acc: 0.23
Batch: 700; loss: 2.19; acc: 0.22
Batch: 720; loss: 2.2; acc: 0.22
Batch: 740; loss: 2.15; acc: 0.19
Batch: 760; loss: 2.2; acc: 0.2
Batch: 780; loss: 2.24; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.206588881790258; val_accuracy: 0.18013535031847133 

The current subspace-distance is: 3.286267747171223e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.25; acc: 0.19
Batch: 20; loss: 2.14; acc: 0.2
Batch: 40; loss: 2.28; acc: 0.11
Batch: 60; loss: 2.26; acc: 0.19
Batch: 80; loss: 2.14; acc: 0.23
Batch: 100; loss: 2.19; acc: 0.17
Batch: 120; loss: 2.14; acc: 0.25
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.12
Batch: 200; loss: 2.15; acc: 0.22
Batch: 220; loss: 2.19; acc: 0.22
Batch: 240; loss: 2.28; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.23; acc: 0.17
Batch: 300; loss: 2.34; acc: 0.05
Batch: 320; loss: 2.15; acc: 0.17
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.29; acc: 0.12
Batch: 380; loss: 2.13; acc: 0.23
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.24; acc: 0.12
Batch: 440; loss: 2.24; acc: 0.14
Batch: 460; loss: 2.07; acc: 0.28
Batch: 480; loss: 2.31; acc: 0.16
Batch: 500; loss: 2.16; acc: 0.28
Batch: 520; loss: 2.18; acc: 0.25
Batch: 540; loss: 2.26; acc: 0.08
Batch: 560; loss: 2.22; acc: 0.16
Batch: 580; loss: 2.23; acc: 0.16
Batch: 600; loss: 2.24; acc: 0.16
Batch: 620; loss: 2.26; acc: 0.12
Batch: 640; loss: 2.18; acc: 0.17
Batch: 660; loss: 2.24; acc: 0.17
Batch: 680; loss: 2.25; acc: 0.19
Batch: 700; loss: 2.27; acc: 0.11
Batch: 720; loss: 2.13; acc: 0.2
Batch: 740; loss: 2.25; acc: 0.19
Batch: 760; loss: 2.16; acc: 0.2
Batch: 780; loss: 2.2; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.206407442214383; val_accuracy: 0.18023487261146498 

The current subspace-distance is: 3.464113251538947e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.16
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.17; acc: 0.23
Batch: 80; loss: 2.25; acc: 0.17
Batch: 100; loss: 2.25; acc: 0.14
Batch: 120; loss: 2.12; acc: 0.22
Batch: 140; loss: 2.19; acc: 0.23
Batch: 160; loss: 2.23; acc: 0.12
Batch: 180; loss: 2.2; acc: 0.25
Batch: 200; loss: 2.19; acc: 0.27
Batch: 220; loss: 2.22; acc: 0.16
Batch: 240; loss: 2.2; acc: 0.19
Batch: 260; loss: 2.12; acc: 0.27
Batch: 280; loss: 2.19; acc: 0.23
Batch: 300; loss: 2.24; acc: 0.09
Batch: 320; loss: 2.27; acc: 0.12
Batch: 340; loss: 2.25; acc: 0.2
Batch: 360; loss: 2.13; acc: 0.17
Batch: 380; loss: 2.34; acc: 0.14
Batch: 400; loss: 2.22; acc: 0.16
Batch: 420; loss: 2.26; acc: 0.16
Batch: 440; loss: 2.18; acc: 0.2
Batch: 460; loss: 2.22; acc: 0.19
Batch: 480; loss: 2.16; acc: 0.2
Batch: 500; loss: 2.24; acc: 0.09
Batch: 520; loss: 2.22; acc: 0.12
Batch: 540; loss: 2.2; acc: 0.2
Batch: 560; loss: 2.29; acc: 0.09
Batch: 580; loss: 2.22; acc: 0.16
Batch: 600; loss: 2.16; acc: 0.17
Batch: 620; loss: 2.17; acc: 0.19
Batch: 640; loss: 2.17; acc: 0.22
Batch: 660; loss: 2.18; acc: 0.19
Batch: 680; loss: 2.27; acc: 0.11
Batch: 700; loss: 2.25; acc: 0.16
Batch: 720; loss: 2.13; acc: 0.22
Batch: 740; loss: 2.22; acc: 0.22
Batch: 760; loss: 2.14; acc: 0.27
Batch: 780; loss: 2.24; acc: 0.11
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2062340116804573; val_accuracy: 0.17993630573248406 

The current subspace-distance is: 3.515561547828838e-05 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.24; acc: 0.2
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.21; acc: 0.16
Batch: 60; loss: 2.27; acc: 0.19
Batch: 80; loss: 2.34; acc: 0.05
Batch: 100; loss: 2.22; acc: 0.2
Batch: 120; loss: 2.14; acc: 0.23
Batch: 140; loss: 2.19; acc: 0.17
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.23; acc: 0.2
Batch: 220; loss: 2.17; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.12
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.17; acc: 0.2
Batch: 300; loss: 2.18; acc: 0.19
Batch: 320; loss: 2.16; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.14
Batch: 360; loss: 2.21; acc: 0.19
Batch: 380; loss: 2.21; acc: 0.14
Batch: 400; loss: 2.14; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.14
Batch: 440; loss: 2.19; acc: 0.22
Batch: 460; loss: 2.28; acc: 0.11
Batch: 480; loss: 2.16; acc: 0.17
Batch: 500; loss: 2.34; acc: 0.08
Batch: 520; loss: 2.18; acc: 0.25
Batch: 540; loss: 2.17; acc: 0.23
Batch: 560; loss: 2.29; acc: 0.09
Batch: 580; loss: 2.23; acc: 0.19
Batch: 600; loss: 2.24; acc: 0.17
Batch: 620; loss: 2.22; acc: 0.17
Batch: 640; loss: 2.21; acc: 0.2
Batch: 660; loss: 2.26; acc: 0.12
Batch: 680; loss: 2.19; acc: 0.19
Batch: 700; loss: 2.24; acc: 0.19
Batch: 720; loss: 2.26; acc: 0.11
Batch: 740; loss: 2.14; acc: 0.22
Batch: 760; loss: 2.21; acc: 0.2
Batch: 780; loss: 2.31; acc: 0.16
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.206162736674023; val_accuracy: 0.18023487261146498 

The current subspace-distance is: 3.741408363566734e-05 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.26; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.2
Batch: 60; loss: 2.15; acc: 0.27
Batch: 80; loss: 2.25; acc: 0.08
Batch: 100; loss: 2.2; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.17
Batch: 140; loss: 2.2; acc: 0.17
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.27; acc: 0.16
Batch: 200; loss: 2.2; acc: 0.16
Batch: 220; loss: 2.26; acc: 0.2
Batch: 240; loss: 2.2; acc: 0.19
Batch: 260; loss: 2.24; acc: 0.11
Batch: 280; loss: 2.18; acc: 0.23
Batch: 300; loss: 2.17; acc: 0.22
Batch: 320; loss: 2.25; acc: 0.2
Batch: 340; loss: 2.23; acc: 0.17
Batch: 360; loss: 2.25; acc: 0.19
Batch: 380; loss: 2.19; acc: 0.16
Batch: 400; loss: 2.28; acc: 0.17
Batch: 420; loss: 2.31; acc: 0.11
Batch: 440; loss: 2.22; acc: 0.19
Batch: 460; loss: 2.17; acc: 0.22
Batch: 480; loss: 2.17; acc: 0.16
Batch: 500; loss: 2.24; acc: 0.09
Batch: 520; loss: 2.22; acc: 0.19
Batch: 540; loss: 2.21; acc: 0.19
Batch: 560; loss: 2.21; acc: 0.17
Batch: 580; loss: 2.23; acc: 0.17
Batch: 600; loss: 2.13; acc: 0.31
Batch: 620; loss: 2.25; acc: 0.14
Batch: 640; loss: 2.24; acc: 0.16
Batch: 660; loss: 2.25; acc: 0.12
Batch: 680; loss: 2.16; acc: 0.2
Batch: 700; loss: 2.26; acc: 0.2
Batch: 720; loss: 2.3; acc: 0.09
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.23; acc: 0.2
Batch: 780; loss: 2.24; acc: 0.09
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2060899005574024; val_accuracy: 0.18023487261146498 

The current subspace-distance is: 3.916062996722758e-05 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.17
Batch: 20; loss: 2.21; acc: 0.28
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.13; acc: 0.23
Batch: 80; loss: 2.14; acc: 0.2
Batch: 100; loss: 2.24; acc: 0.17
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.32; acc: 0.11
Batch: 180; loss: 2.13; acc: 0.23
Batch: 200; loss: 2.24; acc: 0.16
Batch: 220; loss: 2.27; acc: 0.16
Batch: 240; loss: 2.26; acc: 0.09
Batch: 260; loss: 2.16; acc: 0.23
Batch: 280; loss: 2.16; acc: 0.17
Batch: 300; loss: 2.18; acc: 0.17
Batch: 320; loss: 2.3; acc: 0.14
Batch: 340; loss: 2.15; acc: 0.2
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.25; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.08
Batch: 420; loss: 2.21; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.22
Batch: 500; loss: 2.24; acc: 0.19
Batch: 520; loss: 2.23; acc: 0.19
Batch: 540; loss: 2.21; acc: 0.19
Batch: 560; loss: 2.19; acc: 0.14
Batch: 580; loss: 2.15; acc: 0.17
Batch: 600; loss: 2.22; acc: 0.19
Batch: 620; loss: 2.3; acc: 0.16
Batch: 640; loss: 2.24; acc: 0.22
Batch: 660; loss: 2.31; acc: 0.09
Batch: 680; loss: 2.23; acc: 0.16
Batch: 700; loss: 2.2; acc: 0.12
Batch: 720; loss: 2.26; acc: 0.14
Batch: 740; loss: 2.19; acc: 0.27
Batch: 760; loss: 2.19; acc: 0.11
Batch: 780; loss: 2.31; acc: 0.08
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.2060163370363273; val_accuracy: 0.18083200636942676 

The current subspace-distance is: 4.0879072912503034e-05 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.19; acc: 0.16
Batch: 20; loss: 2.25; acc: 0.09
Batch: 40; loss: 2.2; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.16
Batch: 80; loss: 2.18; acc: 0.2
Batch: 100; loss: 2.24; acc: 0.14
Batch: 120; loss: 2.23; acc: 0.17
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.26; acc: 0.16
Batch: 200; loss: 2.2; acc: 0.2
Batch: 220; loss: 2.35; acc: 0.17
Batch: 240; loss: 2.2; acc: 0.22
Batch: 260; loss: 2.33; acc: 0.11
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.19; acc: 0.17
Batch: 320; loss: 2.25; acc: 0.12
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.17; acc: 0.25
Batch: 380; loss: 2.12; acc: 0.31
Batch: 400; loss: 2.23; acc: 0.12
Batch: 420; loss: 2.24; acc: 0.16
Batch: 440; loss: 2.29; acc: 0.16
Batch: 460; loss: 2.11; acc: 0.25
Batch: 480; loss: 2.26; acc: 0.08
Batch: 500; loss: 2.25; acc: 0.12
Batch: 520; loss: 2.28; acc: 0.17
Batch: 540; loss: 2.25; acc: 0.16
Batch: 560; loss: 2.16; acc: 0.25
Batch: 580; loss: 2.24; acc: 0.14
Batch: 600; loss: 2.25; acc: 0.16
Batch: 620; loss: 2.18; acc: 0.17
Batch: 640; loss: 2.29; acc: 0.17
Batch: 660; loss: 2.23; acc: 0.16
Batch: 680; loss: 2.17; acc: 0.2
Batch: 700; loss: 2.17; acc: 0.22
Batch: 720; loss: 2.21; acc: 0.17
Batch: 740; loss: 2.18; acc: 0.2
Batch: 760; loss: 2.18; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.22
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.2059370514693533; val_accuracy: 0.18093152866242038 

The current subspace-distance is: 4.248157347319648e-05 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.28; acc: 0.11
Batch: 20; loss: 2.13; acc: 0.27
Batch: 40; loss: 2.17; acc: 0.2
Batch: 60; loss: 2.14; acc: 0.25
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.27; acc: 0.11
Batch: 120; loss: 2.23; acc: 0.2
Batch: 140; loss: 2.23; acc: 0.2
Batch: 160; loss: 2.21; acc: 0.14
Batch: 180; loss: 2.23; acc: 0.11
Batch: 200; loss: 2.22; acc: 0.2
Batch: 220; loss: 2.16; acc: 0.23
Batch: 240; loss: 2.19; acc: 0.17
Batch: 260; loss: 2.29; acc: 0.22
Batch: 280; loss: 2.25; acc: 0.19
Batch: 300; loss: 2.16; acc: 0.22
Batch: 320; loss: 2.14; acc: 0.25
Batch: 340; loss: 2.18; acc: 0.19
Batch: 360; loss: 2.21; acc: 0.14
Batch: 380; loss: 2.25; acc: 0.17
Batch: 400; loss: 2.26; acc: 0.16
Batch: 420; loss: 2.18; acc: 0.22
Batch: 440; loss: 2.24; acc: 0.16
Batch: 460; loss: 2.21; acc: 0.22
Batch: 480; loss: 2.1; acc: 0.23
Batch: 500; loss: 2.22; acc: 0.19
Batch: 520; loss: 2.23; acc: 0.14
Batch: 540; loss: 2.24; acc: 0.14
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.22; acc: 0.19
Batch: 600; loss: 2.23; acc: 0.14
Batch: 620; loss: 2.19; acc: 0.22
Batch: 640; loss: 2.28; acc: 0.16
Batch: 660; loss: 2.28; acc: 0.11
Batch: 680; loss: 2.25; acc: 0.19
Batch: 700; loss: 2.18; acc: 0.2
Batch: 720; loss: 2.18; acc: 0.19
Batch: 740; loss: 2.27; acc: 0.08
Batch: 760; loss: 2.21; acc: 0.2
Batch: 780; loss: 2.27; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.2058603581349563; val_accuracy: 0.18053343949044587 

The current subspace-distance is: 4.4307340431259945e-05 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.16; acc: 0.19
Batch: 20; loss: 2.25; acc: 0.09
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.19
Batch: 80; loss: 2.27; acc: 0.17
Batch: 100; loss: 2.18; acc: 0.19
Batch: 120; loss: 2.13; acc: 0.19
Batch: 140; loss: 2.21; acc: 0.17
Batch: 160; loss: 2.23; acc: 0.2
Batch: 180; loss: 2.19; acc: 0.17
Batch: 200; loss: 2.2; acc: 0.2
Batch: 220; loss: 2.2; acc: 0.22
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.25; acc: 0.16
Batch: 280; loss: 2.16; acc: 0.2
Batch: 300; loss: 2.22; acc: 0.2
Batch: 320; loss: 2.2; acc: 0.17
Batch: 340; loss: 2.16; acc: 0.28
Batch: 360; loss: 2.19; acc: 0.19
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.24; acc: 0.14
Batch: 420; loss: 2.17; acc: 0.19
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.23; acc: 0.16
Batch: 480; loss: 2.18; acc: 0.19
Batch: 500; loss: 2.23; acc: 0.19
Batch: 520; loss: 2.24; acc: 0.12
Batch: 540; loss: 2.15; acc: 0.22
Batch: 560; loss: 2.15; acc: 0.22
Batch: 580; loss: 2.2; acc: 0.17
Batch: 600; loss: 2.25; acc: 0.14
Batch: 620; loss: 2.18; acc: 0.17
Batch: 640; loss: 2.27; acc: 0.11
Batch: 660; loss: 2.18; acc: 0.22
Batch: 680; loss: 2.16; acc: 0.25
Batch: 700; loss: 2.19; acc: 0.23
Batch: 720; loss: 2.26; acc: 0.16
Batch: 740; loss: 2.25; acc: 0.16
Batch: 760; loss: 2.19; acc: 0.17
Batch: 780; loss: 2.25; acc: 0.12
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.2057823117371576; val_accuracy: 0.18093152866242038 

The current subspace-distance is: 4.453859946806915e-05 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.27; acc: 0.08
Batch: 20; loss: 2.42; acc: 0.08
Batch: 40; loss: 2.19; acc: 0.23
Batch: 60; loss: 2.28; acc: 0.17
Batch: 80; loss: 2.24; acc: 0.12
Batch: 100; loss: 2.24; acc: 0.17
Batch: 120; loss: 2.21; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.23
Batch: 160; loss: 2.16; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.21; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.06
Batch: 240; loss: 2.26; acc: 0.22
Batch: 260; loss: 2.25; acc: 0.14
Batch: 280; loss: 2.13; acc: 0.12
Batch: 300; loss: 2.28; acc: 0.16
Batch: 320; loss: 2.28; acc: 0.22
Batch: 340; loss: 2.19; acc: 0.28
Batch: 360; loss: 2.2; acc: 0.16
Batch: 380; loss: 2.21; acc: 0.17
Batch: 400; loss: 2.22; acc: 0.17
Batch: 420; loss: 2.16; acc: 0.17
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.34; acc: 0.11
Batch: 480; loss: 2.23; acc: 0.11
Batch: 500; loss: 2.18; acc: 0.2
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.26; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.17
Batch: 580; loss: 2.28; acc: 0.08
Batch: 600; loss: 2.2; acc: 0.22
Batch: 620; loss: 2.23; acc: 0.22
Batch: 640; loss: 2.29; acc: 0.09
Batch: 660; loss: 2.17; acc: 0.2
Batch: 680; loss: 2.14; acc: 0.23
Batch: 700; loss: 2.2; acc: 0.2
Batch: 720; loss: 2.16; acc: 0.22
Batch: 740; loss: 2.27; acc: 0.11
Batch: 760; loss: 2.26; acc: 0.09
Batch: 780; loss: 2.25; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.205700787769002; val_accuracy: 0.18103105095541402 

The current subspace-distance is: 5.017694638809189e-05 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.21; acc: 0.16
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.17
Batch: 60; loss: 2.27; acc: 0.16
Batch: 80; loss: 2.21; acc: 0.2
Batch: 100; loss: 2.2; acc: 0.16
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.22; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.24; acc: 0.17
Batch: 220; loss: 2.14; acc: 0.22
Batch: 240; loss: 2.18; acc: 0.2
Batch: 260; loss: 2.26; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.17
Batch: 320; loss: 2.22; acc: 0.19
Batch: 340; loss: 2.22; acc: 0.16
Batch: 360; loss: 2.13; acc: 0.23
Batch: 380; loss: 2.15; acc: 0.17
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.22; acc: 0.14
Batch: 440; loss: 2.26; acc: 0.19
Batch: 460; loss: 2.25; acc: 0.16
Batch: 480; loss: 2.27; acc: 0.11
Batch: 500; loss: 2.24; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.12
Batch: 540; loss: 2.25; acc: 0.16
Batch: 560; loss: 2.05; acc: 0.31
Batch: 580; loss: 2.19; acc: 0.19
Batch: 600; loss: 2.22; acc: 0.12
Batch: 620; loss: 2.21; acc: 0.2
Batch: 640; loss: 2.24; acc: 0.17
Batch: 660; loss: 2.18; acc: 0.2
Batch: 680; loss: 2.19; acc: 0.16
Batch: 700; loss: 2.23; acc: 0.16
Batch: 720; loss: 2.34; acc: 0.09
Batch: 740; loss: 2.13; acc: 0.25
Batch: 760; loss: 2.26; acc: 0.11
Batch: 780; loss: 2.34; acc: 0.19
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.2056173731566995; val_accuracy: 0.18113057324840764 

The current subspace-distance is: 4.94732812512666e-05 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.14
Batch: 20; loss: 2.19; acc: 0.2
Batch: 40; loss: 2.19; acc: 0.22
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.16; acc: 0.14
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.25; acc: 0.14
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.25; acc: 0.19
Batch: 200; loss: 2.22; acc: 0.23
Batch: 220; loss: 2.27; acc: 0.14
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.26; acc: 0.09
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.35; acc: 0.08
Batch: 320; loss: 2.14; acc: 0.2
Batch: 340; loss: 2.19; acc: 0.17
Batch: 360; loss: 2.19; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.18; acc: 0.17
Batch: 420; loss: 2.23; acc: 0.08
Batch: 440; loss: 2.21; acc: 0.19
Batch: 460; loss: 2.08; acc: 0.27
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.18; acc: 0.2
Batch: 560; loss: 2.09; acc: 0.23
Batch: 580; loss: 2.17; acc: 0.25
Batch: 600; loss: 2.26; acc: 0.14
Batch: 620; loss: 2.28; acc: 0.17
Batch: 640; loss: 2.28; acc: 0.19
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.25; acc: 0.12
Batch: 700; loss: 2.28; acc: 0.14
Batch: 720; loss: 2.3; acc: 0.17
Batch: 740; loss: 2.17; acc: 0.2
Batch: 760; loss: 2.21; acc: 0.14
Batch: 780; loss: 2.18; acc: 0.17
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.2055313055682335; val_accuracy: 0.18113057324840764 

The current subspace-distance is: 4.8103349399752915e-05 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.27; acc: 0.16
Batch: 20; loss: 2.31; acc: 0.11
Batch: 40; loss: 2.25; acc: 0.12
Batch: 60; loss: 2.25; acc: 0.17
Batch: 80; loss: 2.19; acc: 0.22
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.26; acc: 0.14
Batch: 160; loss: 2.14; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.19
Batch: 200; loss: 2.12; acc: 0.25
Batch: 220; loss: 2.2; acc: 0.19
Batch: 240; loss: 2.16; acc: 0.2
Batch: 260; loss: 2.23; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.09
Batch: 300; loss: 2.23; acc: 0.16
Batch: 320; loss: 2.2; acc: 0.19
Batch: 340; loss: 2.19; acc: 0.19
Batch: 360; loss: 2.2; acc: 0.17
Batch: 380; loss: 2.24; acc: 0.17
Batch: 400; loss: 2.07; acc: 0.3
Batch: 420; loss: 2.23; acc: 0.16
Batch: 440; loss: 2.19; acc: 0.16
Batch: 460; loss: 2.22; acc: 0.14
Batch: 480; loss: 2.16; acc: 0.2
Batch: 500; loss: 2.2; acc: 0.19
Batch: 520; loss: 2.24; acc: 0.17
Batch: 540; loss: 2.19; acc: 0.2
Batch: 560; loss: 2.27; acc: 0.17
Batch: 580; loss: 2.15; acc: 0.22
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.16; acc: 0.19
Batch: 640; loss: 2.12; acc: 0.28
Batch: 660; loss: 2.2; acc: 0.16
Batch: 680; loss: 2.21; acc: 0.19
Batch: 700; loss: 2.19; acc: 0.17
Batch: 720; loss: 2.31; acc: 0.11
Batch: 740; loss: 2.2; acc: 0.19
Batch: 760; loss: 2.18; acc: 0.22
Batch: 780; loss: 2.2; acc: 0.23
Train Epoch over. train_loss: 2.22; train_accuracy: 0.17 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.13; acc: 0.23
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.2054439274368773; val_accuracy: 0.18083200636942676 

The current subspace-distance is: 4.769915176439099e-05 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5243
elements in E: 1124750
fraction nonzero: 0.004661480328961991
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.16
Batch: 160; loss: 2.32; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.29; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.31; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.14
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.19
Batch: 600; loss: 2.29; acc: 0.17
Batch: 620; loss: 2.29; acc: 0.17
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.23
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.3; acc: 0.06
Batch: 720; loss: 2.29; acc: 0.19
Batch: 740; loss: 2.3; acc: 0.08
Batch: 760; loss: 2.29; acc: 0.08
Batch: 780; loss: 2.28; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.2
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.22
Batch: 120; loss: 2.28; acc: 0.19
Batch: 140; loss: 2.29; acc: 0.19
Val Epoch over. val_loss: 2.2947304704386715; val_accuracy: 0.12768710191082802 

The current subspace-distance is: 2.7117421268485487e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.23
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.16
Batch: 300; loss: 2.27; acc: 0.25
Batch: 320; loss: 2.29; acc: 0.25
Batch: 340; loss: 2.29; acc: 0.19
Batch: 360; loss: 2.28; acc: 0.19
Batch: 380; loss: 2.27; acc: 0.22
Batch: 400; loss: 2.29; acc: 0.17
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.29; acc: 0.14
Batch: 460; loss: 2.29; acc: 0.08
Batch: 480; loss: 2.28; acc: 0.19
Batch: 500; loss: 2.28; acc: 0.16
Batch: 520; loss: 2.29; acc: 0.05
Batch: 540; loss: 2.3; acc: 0.08
Batch: 560; loss: 2.28; acc: 0.2
Batch: 580; loss: 2.28; acc: 0.19
Batch: 600; loss: 2.28; acc: 0.11
Batch: 620; loss: 2.29; acc: 0.14
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.22
Batch: 680; loss: 2.27; acc: 0.2
Batch: 700; loss: 2.27; acc: 0.27
Batch: 720; loss: 2.26; acc: 0.19
Batch: 740; loss: 2.27; acc: 0.22
Batch: 760; loss: 2.28; acc: 0.2
Batch: 780; loss: 2.27; acc: 0.14
Train Epoch over. train_loss: 2.29; train_accuracy: 0.15 

Batch: 0; loss: 2.26; acc: 0.2
Batch: 20; loss: 2.27; acc: 0.19
Batch: 40; loss: 2.26; acc: 0.2
Batch: 60; loss: 2.28; acc: 0.23
Batch: 80; loss: 2.27; acc: 0.27
Batch: 100; loss: 2.27; acc: 0.17
Batch: 120; loss: 2.27; acc: 0.22
Batch: 140; loss: 2.27; acc: 0.23
Val Epoch over. val_loss: 2.273516916165686; val_accuracy: 0.18511146496815287 

The current subspace-distance is: 4.350005383457756e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.29; acc: 0.16
Batch: 40; loss: 2.25; acc: 0.17
Batch: 60; loss: 2.27; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.23
Batch: 100; loss: 2.27; acc: 0.27
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.27; acc: 0.23
Batch: 160; loss: 2.26; acc: 0.22
Batch: 180; loss: 2.27; acc: 0.17
Batch: 200; loss: 2.27; acc: 0.2
Batch: 220; loss: 2.25; acc: 0.34
Batch: 240; loss: 2.25; acc: 0.27
Batch: 260; loss: 2.26; acc: 0.19
Batch: 280; loss: 2.26; acc: 0.16
Batch: 300; loss: 2.27; acc: 0.19
Batch: 320; loss: 2.27; acc: 0.22
Batch: 340; loss: 2.27; acc: 0.22
Batch: 360; loss: 2.24; acc: 0.31
Batch: 380; loss: 2.24; acc: 0.3
Batch: 400; loss: 2.25; acc: 0.3
Batch: 420; loss: 2.24; acc: 0.3
Batch: 440; loss: 2.25; acc: 0.22
Batch: 460; loss: 2.27; acc: 0.12
Batch: 480; loss: 2.23; acc: 0.31
Batch: 500; loss: 2.26; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.17
Batch: 540; loss: 2.26; acc: 0.23
Batch: 560; loss: 2.25; acc: 0.22
Batch: 580; loss: 2.25; acc: 0.22
Batch: 600; loss: 2.23; acc: 0.28
Batch: 620; loss: 2.26; acc: 0.23
Batch: 640; loss: 2.2; acc: 0.34
Batch: 660; loss: 2.22; acc: 0.31
Batch: 680; loss: 2.19; acc: 0.28
Batch: 700; loss: 2.25; acc: 0.22
Batch: 720; loss: 2.26; acc: 0.14
Batch: 740; loss: 2.26; acc: 0.19
Batch: 760; loss: 2.24; acc: 0.2
Batch: 780; loss: 2.25; acc: 0.2
Train Epoch over. train_loss: 2.26; train_accuracy: 0.22 

Batch: 0; loss: 2.22; acc: 0.31
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.21; acc: 0.23
Batch: 60; loss: 2.24; acc: 0.28
Batch: 80; loss: 2.23; acc: 0.36
Batch: 100; loss: 2.24; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.34
Batch: 140; loss: 2.21; acc: 0.25
Val Epoch over. val_loss: 2.2319722464130183; val_accuracy: 0.2461186305732484 

The current subspace-distance is: 6.945211225684034e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.17; acc: 0.34
Batch: 20; loss: 2.21; acc: 0.25
Batch: 40; loss: 2.23; acc: 0.22
Batch: 60; loss: 2.24; acc: 0.25
Batch: 80; loss: 2.25; acc: 0.2
Batch: 100; loss: 2.19; acc: 0.38
Batch: 120; loss: 2.21; acc: 0.25
Batch: 140; loss: 2.25; acc: 0.2
Batch: 160; loss: 2.23; acc: 0.28
Batch: 180; loss: 2.17; acc: 0.33
Batch: 200; loss: 2.17; acc: 0.25
Batch: 220; loss: 2.22; acc: 0.19
Batch: 240; loss: 2.22; acc: 0.3
Batch: 260; loss: 2.19; acc: 0.25
Batch: 280; loss: 2.19; acc: 0.28
Batch: 300; loss: 2.25; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.3
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.14; acc: 0.36
Batch: 380; loss: 2.24; acc: 0.27
Batch: 400; loss: 2.21; acc: 0.36
Batch: 420; loss: 2.2; acc: 0.25
Batch: 440; loss: 2.15; acc: 0.39
Batch: 460; loss: 2.2; acc: 0.33
Batch: 480; loss: 2.16; acc: 0.31
Batch: 500; loss: 2.18; acc: 0.25
Batch: 520; loss: 2.13; acc: 0.28
Batch: 540; loss: 2.16; acc: 0.28
Batch: 560; loss: 2.07; acc: 0.3
Batch: 580; loss: 2.07; acc: 0.36
Batch: 600; loss: 2.01; acc: 0.44
Batch: 620; loss: 2.12; acc: 0.3
Batch: 640; loss: 2.1; acc: 0.33
Batch: 660; loss: 2.07; acc: 0.23
Batch: 680; loss: 2.13; acc: 0.33
Batch: 700; loss: 2.09; acc: 0.25
Batch: 720; loss: 2.09; acc: 0.23
Batch: 740; loss: 2.03; acc: 0.28
Batch: 760; loss: 2.0; acc: 0.27
Batch: 780; loss: 2.13; acc: 0.31
Train Epoch over. train_loss: 2.17; train_accuracy: 0.28 

Batch: 0; loss: 2.04; acc: 0.33
Batch: 20; loss: 2.13; acc: 0.22
Batch: 40; loss: 1.93; acc: 0.39
Batch: 60; loss: 2.0; acc: 0.42
Batch: 80; loss: 2.04; acc: 0.28
Batch: 100; loss: 2.07; acc: 0.31
Batch: 120; loss: 2.11; acc: 0.3
Batch: 140; loss: 2.05; acc: 0.28
Val Epoch over. val_loss: 2.0480806652907355; val_accuracy: 0.31080812101910826 

The current subspace-distance is: 1.21533521451056e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.13; acc: 0.22
Batch: 20; loss: 2.05; acc: 0.3
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 2.06; acc: 0.33
Batch: 80; loss: 2.02; acc: 0.27
Batch: 100; loss: 2.04; acc: 0.36
Batch: 120; loss: 2.11; acc: 0.25
Batch: 140; loss: 2.15; acc: 0.23
Batch: 160; loss: 1.83; acc: 0.38
Batch: 180; loss: 2.07; acc: 0.27
Batch: 200; loss: 1.86; acc: 0.36
Batch: 220; loss: 1.95; acc: 0.42
Batch: 240; loss: 2.11; acc: 0.36
Batch: 260; loss: 2.08; acc: 0.34
Batch: 280; loss: 1.9; acc: 0.39
Batch: 300; loss: 1.94; acc: 0.31
Batch: 320; loss: 2.06; acc: 0.31
Batch: 340; loss: 1.89; acc: 0.3
Batch: 360; loss: 2.0; acc: 0.36
Batch: 380; loss: 2.02; acc: 0.36
Batch: 400; loss: 1.88; acc: 0.44
Batch: 420; loss: 2.1; acc: 0.34
Batch: 440; loss: 1.92; acc: 0.39
Batch: 460; loss: 2.07; acc: 0.27
Batch: 480; loss: 2.01; acc: 0.36
Batch: 500; loss: 1.99; acc: 0.34
Batch: 520; loss: 2.0; acc: 0.36
Batch: 540; loss: 2.02; acc: 0.39
Batch: 560; loss: 1.99; acc: 0.33
Batch: 580; loss: 2.24; acc: 0.17
Batch: 600; loss: 1.75; acc: 0.47
Batch: 620; loss: 2.01; acc: 0.36
Batch: 640; loss: 1.92; acc: 0.41
Batch: 660; loss: 1.84; acc: 0.42
Batch: 680; loss: 1.94; acc: 0.33
Batch: 700; loss: 2.08; acc: 0.33
Batch: 720; loss: 1.93; acc: 0.41
Batch: 740; loss: 1.97; acc: 0.33
Batch: 760; loss: 2.02; acc: 0.25
Batch: 780; loss: 1.89; acc: 0.39
Train Epoch over. train_loss: 1.99; train_accuracy: 0.33 

Batch: 0; loss: 1.97; acc: 0.3
Batch: 20; loss: 2.13; acc: 0.3
Batch: 40; loss: 1.7; acc: 0.44
Batch: 60; loss: 1.77; acc: 0.44
Batch: 80; loss: 1.89; acc: 0.36
Batch: 100; loss: 1.89; acc: 0.36
Batch: 120; loss: 1.96; acc: 0.3
Batch: 140; loss: 1.84; acc: 0.41
Val Epoch over. val_loss: 1.8807620926267783; val_accuracy: 0.3443471337579618 

The current subspace-distance is: 1.47619066410698e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.01; acc: 0.33
Batch: 20; loss: 2.09; acc: 0.33
Batch: 40; loss: 1.72; acc: 0.39
Batch: 60; loss: 1.79; acc: 0.38
Batch: 80; loss: 2.04; acc: 0.23
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.93; acc: 0.22
Batch: 140; loss: 1.84; acc: 0.33
Batch: 160; loss: 1.77; acc: 0.36
Batch: 180; loss: 1.89; acc: 0.28
Batch: 200; loss: 1.79; acc: 0.38
Batch: 220; loss: 1.5; acc: 0.41
Batch: 240; loss: 1.98; acc: 0.28
Batch: 260; loss: 1.92; acc: 0.28
Batch: 280; loss: 1.78; acc: 0.34
Batch: 300; loss: 1.72; acc: 0.39
Batch: 320; loss: 1.79; acc: 0.39
Batch: 340; loss: 1.83; acc: 0.38
Batch: 360; loss: 1.69; acc: 0.39
Batch: 380; loss: 1.72; acc: 0.28
Batch: 400; loss: 1.73; acc: 0.36
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.76; acc: 0.36
Batch: 460; loss: 1.69; acc: 0.39
Batch: 480; loss: 1.62; acc: 0.48
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.7; acc: 0.42
Batch: 540; loss: 1.73; acc: 0.36
Batch: 560; loss: 1.75; acc: 0.42
Batch: 580; loss: 1.66; acc: 0.44
Batch: 600; loss: 1.68; acc: 0.39
Batch: 620; loss: 1.81; acc: 0.38
Batch: 640; loss: 1.67; acc: 0.36
Batch: 660; loss: 1.42; acc: 0.48
Batch: 680; loss: 2.02; acc: 0.27
Batch: 700; loss: 1.36; acc: 0.53
Batch: 720; loss: 1.61; acc: 0.44
Batch: 740; loss: 1.81; acc: 0.41
Batch: 760; loss: 1.69; acc: 0.42
Batch: 780; loss: 1.42; acc: 0.52
Train Epoch over. train_loss: 1.73; train_accuracy: 0.38 

Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.95; acc: 0.39
Batch: 40; loss: 1.35; acc: 0.58
Batch: 60; loss: 1.57; acc: 0.5
Batch: 80; loss: 1.62; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.44
Batch: 120; loss: 1.8; acc: 0.45
Batch: 140; loss: 1.58; acc: 0.38
Val Epoch over. val_loss: 1.6143218582602823; val_accuracy: 0.43929140127388533 

The current subspace-distance is: 1.7788388504413888e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.66; acc: 0.41
Batch: 40; loss: 1.47; acc: 0.48
Batch: 60; loss: 1.7; acc: 0.38
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.44
Batch: 140; loss: 1.85; acc: 0.33
Batch: 160; loss: 1.73; acc: 0.39
Batch: 180; loss: 1.77; acc: 0.44
Batch: 200; loss: 1.51; acc: 0.44
Batch: 220; loss: 1.46; acc: 0.5
Batch: 240; loss: 1.43; acc: 0.55
Batch: 260; loss: 1.32; acc: 0.62
Batch: 280; loss: 1.42; acc: 0.42
Batch: 300; loss: 1.56; acc: 0.44
Batch: 320; loss: 1.81; acc: 0.41
Batch: 340; loss: 1.6; acc: 0.48
Batch: 360; loss: 1.56; acc: 0.5
Batch: 380; loss: 1.83; acc: 0.33
Batch: 400; loss: 1.56; acc: 0.41
Batch: 420; loss: 1.66; acc: 0.42
Batch: 440; loss: 1.71; acc: 0.34
Batch: 460; loss: 1.64; acc: 0.39
Batch: 480; loss: 1.53; acc: 0.47
Batch: 500; loss: 1.63; acc: 0.41
Batch: 520; loss: 1.5; acc: 0.47
Batch: 540; loss: 1.5; acc: 0.53
Batch: 560; loss: 1.65; acc: 0.42
Batch: 580; loss: 1.57; acc: 0.5
Batch: 600; loss: 1.71; acc: 0.47
Batch: 620; loss: 1.86; acc: 0.38
Batch: 640; loss: 1.56; acc: 0.45
Batch: 660; loss: 1.64; acc: 0.45
Batch: 680; loss: 1.62; acc: 0.5
Batch: 700; loss: 1.64; acc: 0.45
Batch: 720; loss: 1.58; acc: 0.45
Batch: 740; loss: 1.65; acc: 0.45
Batch: 760; loss: 1.58; acc: 0.42
Batch: 780; loss: 1.58; acc: 0.44
Train Epoch over. train_loss: 1.62; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.45
Batch: 20; loss: 1.9; acc: 0.39
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.41
Batch: 80; loss: 1.61; acc: 0.47
Batch: 100; loss: 1.66; acc: 0.47
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.61; acc: 0.38
Val Epoch over. val_loss: 1.600739021969449; val_accuracy: 0.45461783439490444 

The current subspace-distance is: 2.1321915482985787e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.65; acc: 0.39
Batch: 40; loss: 1.77; acc: 0.41
Batch: 60; loss: 1.6; acc: 0.55
Batch: 80; loss: 1.49; acc: 0.52
Batch: 100; loss: 1.45; acc: 0.55
Batch: 120; loss: 1.78; acc: 0.36
Batch: 140; loss: 1.61; acc: 0.41
Batch: 160; loss: 1.53; acc: 0.44
Batch: 180; loss: 1.67; acc: 0.41
Batch: 200; loss: 1.6; acc: 0.44
Batch: 220; loss: 1.56; acc: 0.41
Batch: 240; loss: 1.57; acc: 0.45
Batch: 260; loss: 1.71; acc: 0.45
Batch: 280; loss: 1.56; acc: 0.48
Batch: 300; loss: 1.6; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.62; acc: 0.41
Batch: 360; loss: 1.49; acc: 0.48
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.52; acc: 0.53
Batch: 420; loss: 1.51; acc: 0.42
Batch: 440; loss: 1.45; acc: 0.5
Batch: 460; loss: 1.68; acc: 0.47
Batch: 480; loss: 1.34; acc: 0.53
Batch: 500; loss: 1.57; acc: 0.53
Batch: 520; loss: 1.58; acc: 0.44
Batch: 540; loss: 1.39; acc: 0.56
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.64; acc: 0.41
Batch: 600; loss: 1.88; acc: 0.38
Batch: 620; loss: 1.72; acc: 0.47
Batch: 640; loss: 1.56; acc: 0.47
Batch: 660; loss: 1.47; acc: 0.53
Batch: 680; loss: 1.52; acc: 0.47
Batch: 700; loss: 1.57; acc: 0.45
Batch: 720; loss: 1.64; acc: 0.34
Batch: 740; loss: 1.4; acc: 0.55
Batch: 760; loss: 1.66; acc: 0.48
Batch: 780; loss: 1.53; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.59; acc: 0.45
Batch: 20; loss: 1.91; acc: 0.38
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.45
Batch: 100; loss: 1.67; acc: 0.45
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.59; acc: 0.41
Val Epoch over. val_loss: 1.6032413737789082; val_accuracy: 0.4557125796178344 

The current subspace-distance is: 2.461193798808381e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.44
Batch: 20; loss: 1.58; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.47
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.89; acc: 0.38
Batch: 100; loss: 1.65; acc: 0.47
Batch: 120; loss: 1.65; acc: 0.39
Batch: 140; loss: 1.72; acc: 0.45
Batch: 160; loss: 1.67; acc: 0.44
Batch: 180; loss: 1.78; acc: 0.36
Batch: 200; loss: 1.57; acc: 0.48
Batch: 220; loss: 1.56; acc: 0.52
Batch: 240; loss: 1.64; acc: 0.41
Batch: 260; loss: 1.84; acc: 0.38
Batch: 280; loss: 1.65; acc: 0.44
Batch: 300; loss: 1.59; acc: 0.45
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.71; acc: 0.45
Batch: 360; loss: 1.54; acc: 0.48
Batch: 380; loss: 1.69; acc: 0.42
Batch: 400; loss: 1.67; acc: 0.41
Batch: 420; loss: 1.9; acc: 0.44
Batch: 440; loss: 1.62; acc: 0.39
Batch: 460; loss: 1.82; acc: 0.39
Batch: 480; loss: 1.39; acc: 0.58
Batch: 500; loss: 1.74; acc: 0.36
Batch: 520; loss: 1.37; acc: 0.48
Batch: 540; loss: 1.56; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.39
Batch: 580; loss: 1.82; acc: 0.31
Batch: 600; loss: 1.59; acc: 0.45
Batch: 620; loss: 1.43; acc: 0.53
Batch: 640; loss: 1.5; acc: 0.53
Batch: 660; loss: 1.39; acc: 0.48
Batch: 680; loss: 1.72; acc: 0.41
Batch: 700; loss: 1.57; acc: 0.38
Batch: 720; loss: 1.72; acc: 0.44
Batch: 740; loss: 1.62; acc: 0.42
Batch: 760; loss: 1.44; acc: 0.55
Batch: 780; loss: 1.59; acc: 0.47
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.53; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.45
Batch: 40; loss: 1.26; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.45
Batch: 140; loss: 1.56; acc: 0.41
Val Epoch over. val_loss: 1.5909761926930421; val_accuracy: 0.4576035031847134 

The current subspace-distance is: 2.838466207322199e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.52
Batch: 40; loss: 1.56; acc: 0.45
Batch: 60; loss: 1.87; acc: 0.34
Batch: 80; loss: 1.49; acc: 0.47
Batch: 100; loss: 1.64; acc: 0.45
Batch: 120; loss: 1.64; acc: 0.45
Batch: 140; loss: 1.65; acc: 0.39
Batch: 160; loss: 1.51; acc: 0.5
Batch: 180; loss: 1.94; acc: 0.34
Batch: 200; loss: 1.68; acc: 0.45
Batch: 220; loss: 1.58; acc: 0.41
Batch: 240; loss: 1.59; acc: 0.45
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.6; acc: 0.42
Batch: 300; loss: 1.57; acc: 0.38
Batch: 320; loss: 1.73; acc: 0.39
Batch: 340; loss: 1.56; acc: 0.42
Batch: 360; loss: 1.6; acc: 0.41
Batch: 380; loss: 1.37; acc: 0.53
Batch: 400; loss: 1.41; acc: 0.53
Batch: 420; loss: 1.51; acc: 0.58
Batch: 440; loss: 1.68; acc: 0.33
Batch: 460; loss: 1.59; acc: 0.48
Batch: 480; loss: 1.77; acc: 0.48
Batch: 500; loss: 1.66; acc: 0.44
Batch: 520; loss: 1.49; acc: 0.5
Batch: 540; loss: 1.32; acc: 0.58
Batch: 560; loss: 1.82; acc: 0.34
Batch: 580; loss: 1.6; acc: 0.45
Batch: 600; loss: 1.72; acc: 0.45
Batch: 620; loss: 1.53; acc: 0.56
Batch: 640; loss: 1.6; acc: 0.45
Batch: 660; loss: 1.57; acc: 0.45
Batch: 680; loss: 1.78; acc: 0.42
Batch: 700; loss: 1.6; acc: 0.44
Batch: 720; loss: 1.41; acc: 0.5
Batch: 740; loss: 1.4; acc: 0.52
Batch: 760; loss: 1.46; acc: 0.52
Batch: 780; loss: 1.73; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.41
Batch: 40; loss: 1.24; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.45
Batch: 80; loss: 1.6; acc: 0.48
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.62; acc: 0.39
Val Epoch over. val_loss: 1.5960753427189627; val_accuracy: 0.45173168789808915 

The current subspace-distance is: 2.9163988074287772e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.47; acc: 0.5
Batch: 20; loss: 1.69; acc: 0.38
Batch: 40; loss: 1.61; acc: 0.48
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.56; acc: 0.42
Batch: 120; loss: 1.78; acc: 0.39
Batch: 140; loss: 1.64; acc: 0.5
Batch: 160; loss: 1.58; acc: 0.42
Batch: 180; loss: 1.53; acc: 0.5
Batch: 200; loss: 1.39; acc: 0.61
Batch: 220; loss: 1.67; acc: 0.39
Batch: 240; loss: 1.52; acc: 0.47
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.49; acc: 0.47
Batch: 300; loss: 1.6; acc: 0.5
Batch: 320; loss: 1.36; acc: 0.56
Batch: 340; loss: 1.63; acc: 0.41
Batch: 360; loss: 1.48; acc: 0.45
Batch: 380; loss: 1.75; acc: 0.44
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.67; acc: 0.41
Batch: 440; loss: 1.69; acc: 0.42
Batch: 460; loss: 1.58; acc: 0.48
Batch: 480; loss: 1.67; acc: 0.41
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.52; acc: 0.45
Batch: 540; loss: 1.58; acc: 0.42
Batch: 560; loss: 1.86; acc: 0.39
Batch: 580; loss: 1.77; acc: 0.36
Batch: 600; loss: 1.47; acc: 0.53
Batch: 620; loss: 1.47; acc: 0.45
Batch: 640; loss: 1.78; acc: 0.38
Batch: 660; loss: 1.54; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.44
Batch: 700; loss: 1.55; acc: 0.44
Batch: 720; loss: 1.48; acc: 0.58
Batch: 740; loss: 1.54; acc: 0.5
Batch: 760; loss: 1.69; acc: 0.45
Batch: 780; loss: 1.95; acc: 0.3
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.45
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.41
Val Epoch over. val_loss: 1.584239331020671; val_accuracy: 0.4589968152866242 

The current subspace-distance is: 2.986149411299266e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.69; acc: 0.42
Batch: 20; loss: 1.73; acc: 0.44
Batch: 40; loss: 1.67; acc: 0.41
Batch: 60; loss: 1.66; acc: 0.44
Batch: 80; loss: 1.63; acc: 0.44
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.68; acc: 0.41
Batch: 160; loss: 1.71; acc: 0.41
Batch: 180; loss: 1.64; acc: 0.39
Batch: 200; loss: 1.69; acc: 0.39
Batch: 220; loss: 1.57; acc: 0.48
Batch: 240; loss: 1.54; acc: 0.47
Batch: 260; loss: 1.58; acc: 0.48
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.84; acc: 0.39
Batch: 320; loss: 1.4; acc: 0.5
Batch: 340; loss: 1.53; acc: 0.47
Batch: 360; loss: 1.48; acc: 0.47
Batch: 380; loss: 1.6; acc: 0.36
Batch: 400; loss: 1.41; acc: 0.61
Batch: 420; loss: 1.62; acc: 0.58
Batch: 440; loss: 1.52; acc: 0.56
Batch: 460; loss: 1.64; acc: 0.41
Batch: 480; loss: 1.57; acc: 0.45
Batch: 500; loss: 1.68; acc: 0.47
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.53; acc: 0.48
Batch: 560; loss: 1.63; acc: 0.42
Batch: 580; loss: 1.89; acc: 0.36
Batch: 600; loss: 1.8; acc: 0.42
Batch: 620; loss: 1.76; acc: 0.39
Batch: 640; loss: 1.46; acc: 0.53
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.67; acc: 0.53
Batch: 700; loss: 1.73; acc: 0.41
Batch: 720; loss: 1.58; acc: 0.41
Batch: 740; loss: 1.51; acc: 0.5
Batch: 760; loss: 1.45; acc: 0.5
Batch: 780; loss: 1.7; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.88; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.58; acc: 0.47
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5837387469164126; val_accuracy: 0.4612858280254777 

The current subspace-distance is: 3.229802314308472e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.42; acc: 0.5
Batch: 20; loss: 1.38; acc: 0.59
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.77; acc: 0.34
Batch: 80; loss: 1.75; acc: 0.39
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.61; acc: 0.47
Batch: 160; loss: 1.58; acc: 0.39
Batch: 180; loss: 1.52; acc: 0.5
Batch: 200; loss: 1.45; acc: 0.53
Batch: 220; loss: 1.75; acc: 0.42
Batch: 240; loss: 1.58; acc: 0.47
Batch: 260; loss: 1.59; acc: 0.47
Batch: 280; loss: 1.85; acc: 0.38
Batch: 300; loss: 1.78; acc: 0.3
Batch: 320; loss: 1.7; acc: 0.41
Batch: 340; loss: 1.58; acc: 0.44
Batch: 360; loss: 1.83; acc: 0.38
Batch: 380; loss: 1.56; acc: 0.45
Batch: 400; loss: 1.52; acc: 0.56
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.53; acc: 0.53
Batch: 480; loss: 2.0; acc: 0.28
Batch: 500; loss: 1.66; acc: 0.44
Batch: 520; loss: 1.65; acc: 0.44
Batch: 540; loss: 1.61; acc: 0.48
Batch: 560; loss: 1.81; acc: 0.36
Batch: 580; loss: 1.55; acc: 0.48
Batch: 600; loss: 1.51; acc: 0.42
Batch: 620; loss: 1.56; acc: 0.45
Batch: 640; loss: 1.71; acc: 0.36
Batch: 660; loss: 1.76; acc: 0.47
Batch: 680; loss: 1.43; acc: 0.52
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.57; acc: 0.45
Batch: 740; loss: 1.57; acc: 0.47
Batch: 760; loss: 1.32; acc: 0.58
Batch: 780; loss: 1.34; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.45
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 1.58; acc: 0.45
Val Epoch over. val_loss: 1.5840973034026518; val_accuracy: 0.4592953821656051 

The current subspace-distance is: 3.698951695696451e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.61; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.45
Batch: 40; loss: 1.43; acc: 0.55
Batch: 60; loss: 1.81; acc: 0.38
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.57; acc: 0.52
Batch: 140; loss: 1.58; acc: 0.44
Batch: 160; loss: 1.5; acc: 0.47
Batch: 180; loss: 1.58; acc: 0.41
Batch: 200; loss: 1.75; acc: 0.36
Batch: 220; loss: 1.38; acc: 0.56
Batch: 240; loss: 1.45; acc: 0.53
Batch: 260; loss: 1.61; acc: 0.39
Batch: 280; loss: 1.71; acc: 0.41
Batch: 300; loss: 1.52; acc: 0.55
Batch: 320; loss: 1.89; acc: 0.38
Batch: 340; loss: 1.73; acc: 0.48
Batch: 360; loss: 1.78; acc: 0.42
Batch: 380; loss: 1.33; acc: 0.56
Batch: 400; loss: 1.67; acc: 0.52
Batch: 420; loss: 1.75; acc: 0.42
Batch: 440; loss: 1.72; acc: 0.41
Batch: 460; loss: 1.59; acc: 0.38
Batch: 480; loss: 1.64; acc: 0.45
Batch: 500; loss: 1.66; acc: 0.44
Batch: 520; loss: 1.74; acc: 0.42
Batch: 540; loss: 1.58; acc: 0.42
Batch: 560; loss: 1.65; acc: 0.41
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.69; acc: 0.47
Batch: 620; loss: 1.58; acc: 0.5
Batch: 640; loss: 2.06; acc: 0.39
Batch: 660; loss: 1.5; acc: 0.52
Batch: 680; loss: 1.52; acc: 0.5
Batch: 700; loss: 1.51; acc: 0.42
Batch: 720; loss: 1.6; acc: 0.47
Batch: 740; loss: 1.54; acc: 0.48
Batch: 760; loss: 1.61; acc: 0.44
Batch: 780; loss: 1.56; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.53; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.45
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.51; acc: 0.47
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.41
Val Epoch over. val_loss: 1.590055204500818; val_accuracy: 0.45431926751592355 

The current subspace-distance is: 3.84092127205804e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.75; acc: 0.34
Batch: 20; loss: 1.4; acc: 0.42
Batch: 40; loss: 1.58; acc: 0.44
Batch: 60; loss: 1.79; acc: 0.42
Batch: 80; loss: 1.71; acc: 0.44
Batch: 100; loss: 1.75; acc: 0.42
Batch: 120; loss: 1.63; acc: 0.41
Batch: 140; loss: 1.62; acc: 0.42
Batch: 160; loss: 1.67; acc: 0.45
Batch: 180; loss: 1.41; acc: 0.62
Batch: 200; loss: 1.66; acc: 0.41
Batch: 220; loss: 1.7; acc: 0.39
Batch: 240; loss: 1.47; acc: 0.5
Batch: 260; loss: 1.52; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.52
Batch: 300; loss: 1.73; acc: 0.36
Batch: 320; loss: 1.55; acc: 0.44
Batch: 340; loss: 1.66; acc: 0.44
Batch: 360; loss: 1.66; acc: 0.5
Batch: 380; loss: 1.58; acc: 0.45
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.72; acc: 0.42
Batch: 440; loss: 1.55; acc: 0.53
Batch: 460; loss: 1.74; acc: 0.44
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.58; acc: 0.44
Batch: 520; loss: 1.45; acc: 0.52
Batch: 540; loss: 1.47; acc: 0.5
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.81; acc: 0.36
Batch: 600; loss: 1.58; acc: 0.44
Batch: 620; loss: 1.48; acc: 0.5
Batch: 640; loss: 1.6; acc: 0.45
Batch: 660; loss: 1.62; acc: 0.31
Batch: 680; loss: 1.48; acc: 0.47
Batch: 700; loss: 1.72; acc: 0.44
Batch: 720; loss: 1.54; acc: 0.53
Batch: 740; loss: 1.66; acc: 0.44
Batch: 760; loss: 1.77; acc: 0.39
Batch: 780; loss: 1.48; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.44
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.54; acc: 0.42
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 1.58; acc: 0.41
Val Epoch over. val_loss: 1.5871719357314382; val_accuracy: 0.4534235668789809 

The current subspace-distance is: 4.349893060862087e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.75; acc: 0.41
Batch: 40; loss: 1.59; acc: 0.47
Batch: 60; loss: 1.55; acc: 0.34
Batch: 80; loss: 1.53; acc: 0.48
Batch: 100; loss: 1.65; acc: 0.39
Batch: 120; loss: 1.61; acc: 0.42
Batch: 140; loss: 1.63; acc: 0.52
Batch: 160; loss: 1.56; acc: 0.39
Batch: 180; loss: 1.58; acc: 0.52
Batch: 200; loss: 1.48; acc: 0.61
Batch: 220; loss: 1.64; acc: 0.42
Batch: 240; loss: 1.58; acc: 0.45
Batch: 260; loss: 1.7; acc: 0.39
Batch: 280; loss: 1.58; acc: 0.45
Batch: 300; loss: 1.54; acc: 0.5
Batch: 320; loss: 1.61; acc: 0.45
Batch: 340; loss: 1.52; acc: 0.45
Batch: 360; loss: 1.77; acc: 0.36
Batch: 380; loss: 1.53; acc: 0.42
Batch: 400; loss: 1.83; acc: 0.39
Batch: 420; loss: 1.63; acc: 0.45
Batch: 440; loss: 1.69; acc: 0.34
Batch: 460; loss: 1.85; acc: 0.42
Batch: 480; loss: 1.62; acc: 0.42
Batch: 500; loss: 1.54; acc: 0.48
Batch: 520; loss: 1.68; acc: 0.41
Batch: 540; loss: 1.73; acc: 0.44
Batch: 560; loss: 1.53; acc: 0.45
Batch: 580; loss: 1.65; acc: 0.45
Batch: 600; loss: 1.63; acc: 0.45
Batch: 620; loss: 1.61; acc: 0.39
Batch: 640; loss: 1.49; acc: 0.55
Batch: 660; loss: 1.53; acc: 0.52
Batch: 680; loss: 1.73; acc: 0.52
Batch: 700; loss: 1.38; acc: 0.53
Batch: 720; loss: 1.71; acc: 0.39
Batch: 740; loss: 1.39; acc: 0.56
Batch: 760; loss: 1.64; acc: 0.47
Batch: 780; loss: 1.52; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.47
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.54; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.59; acc: 0.41
Val Epoch over. val_loss: 1.5869241117671797; val_accuracy: 0.45849920382165604 

The current subspace-distance is: 4.3716878280974925e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.47
Batch: 40; loss: 1.72; acc: 0.38
Batch: 60; loss: 1.74; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.38
Batch: 120; loss: 1.65; acc: 0.45
Batch: 140; loss: 1.55; acc: 0.47
Batch: 160; loss: 1.54; acc: 0.47
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.55; acc: 0.44
Batch: 220; loss: 1.55; acc: 0.44
Batch: 240; loss: 1.55; acc: 0.45
Batch: 260; loss: 1.52; acc: 0.42
Batch: 280; loss: 1.91; acc: 0.36
Batch: 300; loss: 1.71; acc: 0.44
Batch: 320; loss: 1.57; acc: 0.55
Batch: 340; loss: 1.57; acc: 0.41
Batch: 360; loss: 1.7; acc: 0.42
Batch: 380; loss: 1.6; acc: 0.5
Batch: 400; loss: 1.45; acc: 0.48
Batch: 420; loss: 1.6; acc: 0.42
Batch: 440; loss: 1.5; acc: 0.52
Batch: 460; loss: 1.58; acc: 0.48
Batch: 480; loss: 1.44; acc: 0.52
Batch: 500; loss: 1.42; acc: 0.47
Batch: 520; loss: 1.86; acc: 0.39
Batch: 540; loss: 1.52; acc: 0.56
Batch: 560; loss: 1.54; acc: 0.44
Batch: 580; loss: 1.83; acc: 0.33
Batch: 600; loss: 2.0; acc: 0.39
Batch: 620; loss: 1.41; acc: 0.56
Batch: 640; loss: 1.76; acc: 0.41
Batch: 660; loss: 1.53; acc: 0.53
Batch: 680; loss: 1.77; acc: 0.42
Batch: 700; loss: 1.64; acc: 0.38
Batch: 720; loss: 1.47; acc: 0.56
Batch: 740; loss: 1.54; acc: 0.45
Batch: 760; loss: 1.68; acc: 0.34
Batch: 780; loss: 1.53; acc: 0.38
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.89; acc: 0.38
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.55; acc: 0.45
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.64; acc: 0.44
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.44
Val Epoch over. val_loss: 1.585787404115033; val_accuracy: 0.4615843949044586 

The current subspace-distance is: 4.310964504838921e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.52; acc: 0.45
Batch: 60; loss: 1.53; acc: 0.5
Batch: 80; loss: 1.6; acc: 0.45
Batch: 100; loss: 1.56; acc: 0.48
Batch: 120; loss: 1.54; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.38
Batch: 160; loss: 1.56; acc: 0.52
Batch: 180; loss: 1.58; acc: 0.41
Batch: 200; loss: 1.57; acc: 0.39
Batch: 220; loss: 1.67; acc: 0.39
Batch: 240; loss: 1.72; acc: 0.45
Batch: 260; loss: 1.45; acc: 0.5
Batch: 280; loss: 1.64; acc: 0.42
Batch: 300; loss: 1.63; acc: 0.5
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.53; acc: 0.47
Batch: 360; loss: 1.62; acc: 0.41
Batch: 380; loss: 1.71; acc: 0.45
Batch: 400; loss: 1.6; acc: 0.41
Batch: 420; loss: 1.41; acc: 0.52
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.92; acc: 0.33
Batch: 480; loss: 1.74; acc: 0.33
Batch: 500; loss: 1.75; acc: 0.36
Batch: 520; loss: 1.75; acc: 0.39
Batch: 540; loss: 1.72; acc: 0.38
Batch: 560; loss: 1.76; acc: 0.42
Batch: 580; loss: 1.69; acc: 0.45
Batch: 600; loss: 1.77; acc: 0.34
Batch: 620; loss: 1.83; acc: 0.33
Batch: 640; loss: 1.77; acc: 0.44
Batch: 660; loss: 1.57; acc: 0.44
Batch: 680; loss: 1.39; acc: 0.58
Batch: 700; loss: 1.45; acc: 0.52
Batch: 720; loss: 1.55; acc: 0.45
Batch: 740; loss: 1.89; acc: 0.34
Batch: 760; loss: 1.85; acc: 0.42
Batch: 780; loss: 1.62; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.47
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.26; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 1.57; acc: 0.42
Val Epoch over. val_loss: 1.5841692769603364; val_accuracy: 0.46178343949044587 

The current subspace-distance is: 4.507232370087877e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.6; acc: 0.44
Batch: 80; loss: 1.49; acc: 0.48
Batch: 100; loss: 1.39; acc: 0.52
Batch: 120; loss: 1.54; acc: 0.41
Batch: 140; loss: 1.76; acc: 0.38
Batch: 160; loss: 1.73; acc: 0.42
Batch: 180; loss: 1.75; acc: 0.41
Batch: 200; loss: 1.5; acc: 0.55
Batch: 220; loss: 1.46; acc: 0.48
Batch: 240; loss: 1.65; acc: 0.47
Batch: 260; loss: 1.64; acc: 0.36
Batch: 280; loss: 1.67; acc: 0.38
Batch: 300; loss: 1.61; acc: 0.48
Batch: 320; loss: 1.44; acc: 0.5
Batch: 340; loss: 1.63; acc: 0.42
Batch: 360; loss: 1.37; acc: 0.58
Batch: 380; loss: 1.66; acc: 0.47
Batch: 400; loss: 1.38; acc: 0.45
Batch: 420; loss: 1.66; acc: 0.28
Batch: 440; loss: 1.84; acc: 0.34
Batch: 460; loss: 1.78; acc: 0.31
Batch: 480; loss: 1.41; acc: 0.53
Batch: 500; loss: 1.57; acc: 0.52
Batch: 520; loss: 1.48; acc: 0.52
Batch: 540; loss: 1.73; acc: 0.41
Batch: 560; loss: 1.66; acc: 0.41
Batch: 580; loss: 1.73; acc: 0.38
Batch: 600; loss: 1.45; acc: 0.56
Batch: 620; loss: 1.51; acc: 0.44
Batch: 640; loss: 1.6; acc: 0.45
Batch: 660; loss: 1.62; acc: 0.47
Batch: 680; loss: 1.67; acc: 0.47
Batch: 700; loss: 1.64; acc: 0.44
Batch: 720; loss: 1.7; acc: 0.42
Batch: 740; loss: 1.76; acc: 0.38
Batch: 760; loss: 1.6; acc: 0.48
Batch: 780; loss: 1.64; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.39
Batch: 40; loss: 1.25; acc: 0.64
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.42
Val Epoch over. val_loss: 1.5845679105467099; val_accuracy: 0.460390127388535 

The current subspace-distance is: 4.7674715460743755e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.75; acc: 0.48
Batch: 60; loss: 1.9; acc: 0.33
Batch: 80; loss: 1.6; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.41
Batch: 120; loss: 1.74; acc: 0.41
Batch: 140; loss: 1.46; acc: 0.53
Batch: 160; loss: 1.47; acc: 0.56
Batch: 180; loss: 1.4; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.41
Batch: 220; loss: 1.48; acc: 0.52
Batch: 240; loss: 1.66; acc: 0.38
Batch: 260; loss: 1.5; acc: 0.48
Batch: 280; loss: 1.72; acc: 0.47
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.66; acc: 0.45
Batch: 340; loss: 1.45; acc: 0.47
Batch: 360; loss: 1.4; acc: 0.52
Batch: 380; loss: 1.57; acc: 0.42
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.56; acc: 0.45
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.47; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.55
Batch: 500; loss: 1.44; acc: 0.52
Batch: 520; loss: 1.66; acc: 0.48
Batch: 540; loss: 1.48; acc: 0.47
Batch: 560; loss: 1.72; acc: 0.41
Batch: 580; loss: 1.6; acc: 0.36
Batch: 600; loss: 1.66; acc: 0.39
Batch: 620; loss: 1.45; acc: 0.55
Batch: 640; loss: 1.6; acc: 0.42
Batch: 660; loss: 1.49; acc: 0.47
Batch: 680; loss: 1.83; acc: 0.36
Batch: 700; loss: 1.46; acc: 0.55
Batch: 720; loss: 1.48; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.41
Batch: 760; loss: 1.9; acc: 0.3
Batch: 780; loss: 1.7; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.47
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.38
Val Epoch over. val_loss: 1.584655092020703; val_accuracy: 0.461484872611465 

The current subspace-distance is: 4.858737884205766e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.73; acc: 0.33
Batch: 40; loss: 1.88; acc: 0.31
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.76; acc: 0.41
Batch: 100; loss: 1.62; acc: 0.47
Batch: 120; loss: 1.5; acc: 0.48
Batch: 140; loss: 1.42; acc: 0.48
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.76; acc: 0.33
Batch: 200; loss: 1.57; acc: 0.48
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.74; acc: 0.38
Batch: 260; loss: 1.44; acc: 0.53
Batch: 280; loss: 1.58; acc: 0.47
Batch: 300; loss: 1.58; acc: 0.47
Batch: 320; loss: 1.72; acc: 0.47
Batch: 340; loss: 1.66; acc: 0.44
Batch: 360; loss: 1.47; acc: 0.52
Batch: 380; loss: 1.72; acc: 0.42
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.39
Batch: 440; loss: 1.43; acc: 0.53
Batch: 460; loss: 1.81; acc: 0.36
Batch: 480; loss: 1.57; acc: 0.39
Batch: 500; loss: 1.58; acc: 0.48
Batch: 520; loss: 1.45; acc: 0.52
Batch: 540; loss: 1.42; acc: 0.5
Batch: 560; loss: 1.64; acc: 0.44
Batch: 580; loss: 1.55; acc: 0.45
Batch: 600; loss: 1.9; acc: 0.3
Batch: 620; loss: 1.5; acc: 0.44
Batch: 640; loss: 1.58; acc: 0.42
Batch: 660; loss: 1.69; acc: 0.36
Batch: 680; loss: 1.65; acc: 0.42
Batch: 700; loss: 1.5; acc: 0.52
Batch: 720; loss: 1.5; acc: 0.47
Batch: 740; loss: 1.73; acc: 0.45
Batch: 760; loss: 1.69; acc: 0.42
Batch: 780; loss: 1.71; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.86; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.59
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.45
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.42
Val Epoch over. val_loss: 1.585131718854236; val_accuracy: 0.45740445859872614 

The current subspace-distance is: 4.98365807288792e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.66; acc: 0.44
Batch: 20; loss: 1.6; acc: 0.44
Batch: 40; loss: 1.5; acc: 0.41
Batch: 60; loss: 1.35; acc: 0.53
Batch: 80; loss: 1.43; acc: 0.48
Batch: 100; loss: 1.57; acc: 0.44
Batch: 120; loss: 1.5; acc: 0.44
Batch: 140; loss: 1.72; acc: 0.36
Batch: 160; loss: 1.53; acc: 0.5
Batch: 180; loss: 1.74; acc: 0.39
Batch: 200; loss: 1.81; acc: 0.52
Batch: 220; loss: 1.74; acc: 0.36
Batch: 240; loss: 1.86; acc: 0.3
Batch: 260; loss: 1.64; acc: 0.36
Batch: 280; loss: 1.69; acc: 0.42
Batch: 300; loss: 1.38; acc: 0.53
Batch: 320; loss: 1.76; acc: 0.33
Batch: 340; loss: 1.52; acc: 0.48
Batch: 360; loss: 1.81; acc: 0.41
Batch: 380; loss: 1.76; acc: 0.34
Batch: 400; loss: 1.64; acc: 0.42
Batch: 420; loss: 1.56; acc: 0.45
Batch: 440; loss: 1.48; acc: 0.48
Batch: 460; loss: 1.92; acc: 0.34
Batch: 480; loss: 1.66; acc: 0.48
Batch: 500; loss: 1.42; acc: 0.5
Batch: 520; loss: 1.45; acc: 0.53
Batch: 540; loss: 1.49; acc: 0.59
Batch: 560; loss: 1.63; acc: 0.38
Batch: 580; loss: 1.54; acc: 0.42
Batch: 600; loss: 1.66; acc: 0.45
Batch: 620; loss: 1.64; acc: 0.36
Batch: 640; loss: 1.36; acc: 0.52
Batch: 660; loss: 1.77; acc: 0.36
Batch: 680; loss: 1.7; acc: 0.34
Batch: 700; loss: 1.67; acc: 0.42
Batch: 720; loss: 1.81; acc: 0.36
Batch: 740; loss: 1.67; acc: 0.39
Batch: 760; loss: 1.61; acc: 0.42
Batch: 780; loss: 1.47; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.44
Batch: 40; loss: 1.24; acc: 0.62
Batch: 60; loss: 1.51; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.39
Val Epoch over. val_loss: 1.58529536663347; val_accuracy: 0.4589968152866242 

The current subspace-distance is: 5.05302450619638e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.55; acc: 0.44
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 1.47; acc: 0.55
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.48; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.36
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 2.09; acc: 0.3
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.59; acc: 0.42
Batch: 200; loss: 1.77; acc: 0.38
Batch: 220; loss: 1.85; acc: 0.38
Batch: 240; loss: 1.51; acc: 0.41
Batch: 260; loss: 1.58; acc: 0.48
Batch: 280; loss: 1.49; acc: 0.5
Batch: 300; loss: 1.38; acc: 0.56
Batch: 320; loss: 1.32; acc: 0.59
Batch: 340; loss: 1.54; acc: 0.44
Batch: 360; loss: 1.81; acc: 0.44
Batch: 380; loss: 1.6; acc: 0.41
Batch: 400; loss: 1.88; acc: 0.36
Batch: 420; loss: 1.69; acc: 0.45
Batch: 440; loss: 1.8; acc: 0.47
Batch: 460; loss: 1.67; acc: 0.39
Batch: 480; loss: 1.67; acc: 0.38
Batch: 500; loss: 1.62; acc: 0.42
Batch: 520; loss: 1.61; acc: 0.36
Batch: 540; loss: 1.56; acc: 0.45
Batch: 560; loss: 1.7; acc: 0.44
Batch: 580; loss: 1.48; acc: 0.5
Batch: 600; loss: 1.71; acc: 0.42
Batch: 620; loss: 1.59; acc: 0.45
Batch: 640; loss: 1.63; acc: 0.39
Batch: 660; loss: 1.73; acc: 0.41
Batch: 680; loss: 1.41; acc: 0.53
Batch: 700; loss: 1.55; acc: 0.59
Batch: 720; loss: 1.68; acc: 0.39
Batch: 740; loss: 1.47; acc: 0.5
Batch: 760; loss: 1.76; acc: 0.33
Batch: 780; loss: 1.55; acc: 0.39
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.45
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.41
Val Epoch over. val_loss: 1.5842171542963404; val_accuracy: 0.4598925159235669 

The current subspace-distance is: 5.0874394219135866e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.65; acc: 0.5
Batch: 20; loss: 1.62; acc: 0.36
Batch: 40; loss: 1.48; acc: 0.5
Batch: 60; loss: 1.38; acc: 0.55
Batch: 80; loss: 1.82; acc: 0.36
Batch: 100; loss: 1.81; acc: 0.38
Batch: 120; loss: 1.49; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.39
Batch: 160; loss: 1.62; acc: 0.45
Batch: 180; loss: 1.78; acc: 0.31
Batch: 200; loss: 1.79; acc: 0.41
Batch: 220; loss: 1.49; acc: 0.53
Batch: 240; loss: 1.63; acc: 0.48
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.66; acc: 0.45
Batch: 300; loss: 1.56; acc: 0.47
Batch: 320; loss: 1.76; acc: 0.41
Batch: 340; loss: 1.61; acc: 0.41
Batch: 360; loss: 1.67; acc: 0.45
Batch: 380; loss: 1.76; acc: 0.41
Batch: 400; loss: 1.42; acc: 0.55
Batch: 420; loss: 1.94; acc: 0.31
Batch: 440; loss: 1.54; acc: 0.47
Batch: 460; loss: 1.45; acc: 0.52
Batch: 480; loss: 1.56; acc: 0.45
Batch: 500; loss: 1.56; acc: 0.5
Batch: 520; loss: 1.74; acc: 0.48
Batch: 540; loss: 1.57; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.41
Batch: 580; loss: 1.43; acc: 0.53
Batch: 600; loss: 1.61; acc: 0.44
Batch: 620; loss: 1.67; acc: 0.44
Batch: 640; loss: 1.53; acc: 0.42
Batch: 660; loss: 1.75; acc: 0.47
Batch: 680; loss: 1.57; acc: 0.42
Batch: 700; loss: 1.6; acc: 0.44
Batch: 720; loss: 1.72; acc: 0.42
Batch: 740; loss: 1.47; acc: 0.55
Batch: 760; loss: 1.8; acc: 0.38
Batch: 780; loss: 1.48; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.42
Batch: 40; loss: 1.26; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5839959276709588; val_accuracy: 0.461484872611465 

The current subspace-distance is: 4.9919355660676956e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.81; acc: 0.39
Batch: 20; loss: 1.44; acc: 0.55
Batch: 40; loss: 1.38; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.41
Batch: 80; loss: 1.58; acc: 0.53
Batch: 100; loss: 1.75; acc: 0.42
Batch: 120; loss: 1.69; acc: 0.34
Batch: 140; loss: 1.88; acc: 0.31
Batch: 160; loss: 1.7; acc: 0.48
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.8; acc: 0.38
Batch: 220; loss: 1.78; acc: 0.38
Batch: 240; loss: 1.57; acc: 0.41
Batch: 260; loss: 1.55; acc: 0.48
Batch: 280; loss: 1.36; acc: 0.55
Batch: 300; loss: 1.75; acc: 0.41
Batch: 320; loss: 1.52; acc: 0.47
Batch: 340; loss: 1.7; acc: 0.45
Batch: 360; loss: 1.42; acc: 0.45
Batch: 380; loss: 1.57; acc: 0.44
Batch: 400; loss: 1.58; acc: 0.41
Batch: 420; loss: 1.43; acc: 0.55
Batch: 440; loss: 1.85; acc: 0.38
Batch: 460; loss: 1.78; acc: 0.48
Batch: 480; loss: 1.36; acc: 0.53
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.62; acc: 0.41
Batch: 540; loss: 1.5; acc: 0.53
Batch: 560; loss: 1.57; acc: 0.42
Batch: 580; loss: 1.57; acc: 0.44
Batch: 600; loss: 1.67; acc: 0.47
Batch: 620; loss: 1.76; acc: 0.5
Batch: 640; loss: 1.79; acc: 0.33
Batch: 660; loss: 1.43; acc: 0.56
Batch: 680; loss: 1.66; acc: 0.44
Batch: 700; loss: 1.64; acc: 0.42
Batch: 720; loss: 1.51; acc: 0.5
Batch: 740; loss: 1.49; acc: 0.48
Batch: 760; loss: 1.71; acc: 0.42
Batch: 780; loss: 1.47; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.42
Batch: 40; loss: 1.24; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.45
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.42
Val Epoch over. val_loss: 1.5859790288718643; val_accuracy: 0.4573049363057325 

The current subspace-distance is: 5.3737170674139634e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.53; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.73; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.42
Batch: 80; loss: 1.77; acc: 0.44
Batch: 100; loss: 1.56; acc: 0.48
Batch: 120; loss: 1.77; acc: 0.38
Batch: 140; loss: 1.62; acc: 0.5
Batch: 160; loss: 1.5; acc: 0.42
Batch: 180; loss: 1.22; acc: 0.58
Batch: 200; loss: 1.78; acc: 0.36
Batch: 220; loss: 1.79; acc: 0.41
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.84; acc: 0.34
Batch: 280; loss: 1.56; acc: 0.41
Batch: 300; loss: 1.82; acc: 0.33
Batch: 320; loss: 1.4; acc: 0.5
Batch: 340; loss: 1.62; acc: 0.52
Batch: 360; loss: 1.74; acc: 0.36
Batch: 380; loss: 1.49; acc: 0.48
Batch: 400; loss: 1.74; acc: 0.39
Batch: 420; loss: 1.52; acc: 0.41
Batch: 440; loss: 1.5; acc: 0.52
Batch: 460; loss: 1.46; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.47
Batch: 520; loss: 1.65; acc: 0.45
Batch: 540; loss: 1.52; acc: 0.41
Batch: 560; loss: 1.83; acc: 0.39
Batch: 580; loss: 1.63; acc: 0.41
Batch: 600; loss: 1.62; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.44
Batch: 640; loss: 1.78; acc: 0.39
Batch: 660; loss: 1.75; acc: 0.42
Batch: 680; loss: 1.61; acc: 0.44
Batch: 700; loss: 1.55; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.42
Batch: 740; loss: 1.51; acc: 0.55
Batch: 760; loss: 1.48; acc: 0.53
Batch: 780; loss: 1.71; acc: 0.41
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.48
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.57; acc: 0.42
Val Epoch over. val_loss: 1.5840536333193445; val_accuracy: 0.4619824840764331 

The current subspace-distance is: 5.812610106659122e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.51; acc: 0.56
Batch: 20; loss: 1.54; acc: 0.52
Batch: 40; loss: 1.71; acc: 0.47
Batch: 60; loss: 1.86; acc: 0.38
Batch: 80; loss: 1.47; acc: 0.45
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.52; acc: 0.44
Batch: 140; loss: 1.51; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.39
Batch: 180; loss: 1.67; acc: 0.38
Batch: 200; loss: 1.52; acc: 0.5
Batch: 220; loss: 1.51; acc: 0.48
Batch: 240; loss: 1.53; acc: 0.48
Batch: 260; loss: 1.7; acc: 0.42
Batch: 280; loss: 1.47; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.47
Batch: 340; loss: 1.54; acc: 0.45
Batch: 360; loss: 1.59; acc: 0.44
Batch: 380; loss: 1.54; acc: 0.56
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.7; acc: 0.42
Batch: 440; loss: 1.71; acc: 0.45
Batch: 460; loss: 1.55; acc: 0.48
Batch: 480; loss: 1.73; acc: 0.44
Batch: 500; loss: 1.72; acc: 0.45
Batch: 520; loss: 1.58; acc: 0.52
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.7; acc: 0.34
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.52; acc: 0.47
Batch: 620; loss: 1.55; acc: 0.48
Batch: 640; loss: 1.59; acc: 0.48
Batch: 660; loss: 1.58; acc: 0.48
Batch: 680; loss: 1.63; acc: 0.45
Batch: 700; loss: 1.65; acc: 0.45
Batch: 720; loss: 1.73; acc: 0.45
Batch: 740; loss: 1.68; acc: 0.42
Batch: 760; loss: 1.7; acc: 0.44
Batch: 780; loss: 1.68; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.39
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.58; acc: 0.44
Val Epoch over. val_loss: 1.5839345887967735; val_accuracy: 0.45869824840764334 

The current subspace-distance is: 6.135697185527533e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 2.07; acc: 0.28
Batch: 40; loss: 1.59; acc: 0.42
Batch: 60; loss: 1.49; acc: 0.5
Batch: 80; loss: 1.63; acc: 0.48
Batch: 100; loss: 1.51; acc: 0.55
Batch: 120; loss: 1.61; acc: 0.45
Batch: 140; loss: 1.64; acc: 0.48
Batch: 160; loss: 1.66; acc: 0.45
Batch: 180; loss: 1.69; acc: 0.38
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.68; acc: 0.48
Batch: 240; loss: 1.67; acc: 0.44
Batch: 260; loss: 1.46; acc: 0.52
Batch: 280; loss: 1.48; acc: 0.47
Batch: 300; loss: 1.53; acc: 0.48
Batch: 320; loss: 1.44; acc: 0.52
Batch: 340; loss: 1.58; acc: 0.42
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.67; acc: 0.36
Batch: 400; loss: 1.53; acc: 0.53
Batch: 420; loss: 1.65; acc: 0.47
Batch: 440; loss: 1.7; acc: 0.45
Batch: 460; loss: 1.42; acc: 0.55
Batch: 480; loss: 1.53; acc: 0.53
Batch: 500; loss: 1.66; acc: 0.39
Batch: 520; loss: 1.72; acc: 0.41
Batch: 540; loss: 1.82; acc: 0.48
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.66; acc: 0.47
Batch: 620; loss: 1.72; acc: 0.39
Batch: 640; loss: 1.75; acc: 0.47
Batch: 660; loss: 1.55; acc: 0.45
Batch: 680; loss: 1.65; acc: 0.45
Batch: 700; loss: 1.68; acc: 0.42
Batch: 720; loss: 1.63; acc: 0.48
Batch: 740; loss: 1.65; acc: 0.42
Batch: 760; loss: 1.71; acc: 0.41
Batch: 780; loss: 1.69; acc: 0.42
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.57; acc: 0.45
Val Epoch over. val_loss: 1.5836654051094299; val_accuracy: 0.4583001592356688 

The current subspace-distance is: 6.220211798790842e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.6; acc: 0.45
Batch: 40; loss: 1.54; acc: 0.44
Batch: 60; loss: 1.6; acc: 0.44
Batch: 80; loss: 1.69; acc: 0.41
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.52; acc: 0.48
Batch: 140; loss: 1.87; acc: 0.38
Batch: 160; loss: 1.88; acc: 0.36
Batch: 180; loss: 1.61; acc: 0.36
Batch: 200; loss: 1.39; acc: 0.52
Batch: 220; loss: 1.55; acc: 0.48
Batch: 240; loss: 1.64; acc: 0.42
Batch: 260; loss: 1.67; acc: 0.44
Batch: 280; loss: 1.6; acc: 0.45
Batch: 300; loss: 1.72; acc: 0.41
Batch: 320; loss: 1.63; acc: 0.45
Batch: 340; loss: 1.72; acc: 0.38
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.64; acc: 0.47
Batch: 400; loss: 1.54; acc: 0.5
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.73; acc: 0.38
Batch: 460; loss: 1.57; acc: 0.48
Batch: 480; loss: 1.52; acc: 0.55
Batch: 500; loss: 1.69; acc: 0.47
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.63; acc: 0.48
Batch: 560; loss: 1.65; acc: 0.42
Batch: 580; loss: 1.75; acc: 0.44
Batch: 600; loss: 1.58; acc: 0.42
Batch: 620; loss: 1.7; acc: 0.42
Batch: 640; loss: 1.67; acc: 0.41
Batch: 660; loss: 1.66; acc: 0.39
Batch: 680; loss: 1.55; acc: 0.5
Batch: 700; loss: 1.69; acc: 0.44
Batch: 720; loss: 1.66; acc: 0.47
Batch: 740; loss: 1.35; acc: 0.53
Batch: 760; loss: 1.82; acc: 0.39
Batch: 780; loss: 1.39; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5834953617897762; val_accuracy: 0.4592953821656051 

The current subspace-distance is: 6.191504508024082e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.75; acc: 0.36
Batch: 20; loss: 1.68; acc: 0.36
Batch: 40; loss: 1.5; acc: 0.44
Batch: 60; loss: 1.84; acc: 0.33
Batch: 80; loss: 1.58; acc: 0.5
Batch: 100; loss: 1.61; acc: 0.47
Batch: 120; loss: 1.71; acc: 0.39
Batch: 140; loss: 1.78; acc: 0.41
Batch: 160; loss: 1.74; acc: 0.39
Batch: 180; loss: 1.56; acc: 0.53
Batch: 200; loss: 1.78; acc: 0.36
Batch: 220; loss: 1.5; acc: 0.53
Batch: 240; loss: 1.63; acc: 0.41
Batch: 260; loss: 1.91; acc: 0.39
Batch: 280; loss: 1.87; acc: 0.39
Batch: 300; loss: 1.45; acc: 0.5
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.64; acc: 0.41
Batch: 360; loss: 1.56; acc: 0.48
Batch: 380; loss: 1.58; acc: 0.45
Batch: 400; loss: 1.77; acc: 0.41
Batch: 420; loss: 1.6; acc: 0.47
Batch: 440; loss: 1.73; acc: 0.31
Batch: 460; loss: 1.72; acc: 0.38
Batch: 480; loss: 1.6; acc: 0.48
Batch: 500; loss: 1.35; acc: 0.55
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.41; acc: 0.44
Batch: 560; loss: 1.77; acc: 0.36
Batch: 580; loss: 1.7; acc: 0.44
Batch: 600; loss: 1.8; acc: 0.38
Batch: 620; loss: 1.63; acc: 0.39
Batch: 640; loss: 1.49; acc: 0.42
Batch: 660; loss: 1.6; acc: 0.48
Batch: 680; loss: 1.62; acc: 0.47
Batch: 700; loss: 1.7; acc: 0.39
Batch: 720; loss: 1.93; acc: 0.41
Batch: 740; loss: 1.43; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.45
Batch: 780; loss: 1.51; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.45
Batch: 20; loss: 1.86; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.59
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.584576991712971; val_accuracy: 0.4589968152866242 

The current subspace-distance is: 6.358751852530986e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.47; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.39
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.5; acc: 0.53
Batch: 80; loss: 1.4; acc: 0.52
Batch: 100; loss: 1.89; acc: 0.38
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.53; acc: 0.48
Batch: 180; loss: 1.54; acc: 0.47
Batch: 200; loss: 1.48; acc: 0.5
Batch: 220; loss: 1.48; acc: 0.55
Batch: 240; loss: 1.52; acc: 0.48
Batch: 260; loss: 1.52; acc: 0.45
Batch: 280; loss: 1.56; acc: 0.5
Batch: 300; loss: 1.57; acc: 0.36
Batch: 320; loss: 1.66; acc: 0.45
Batch: 340; loss: 1.62; acc: 0.42
Batch: 360; loss: 1.54; acc: 0.52
Batch: 380; loss: 1.5; acc: 0.5
Batch: 400; loss: 1.66; acc: 0.41
Batch: 420; loss: 1.75; acc: 0.39
Batch: 440; loss: 1.76; acc: 0.39
Batch: 460; loss: 1.6; acc: 0.39
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.37; acc: 0.52
Batch: 520; loss: 1.88; acc: 0.3
Batch: 540; loss: 1.68; acc: 0.48
Batch: 560; loss: 1.68; acc: 0.38
Batch: 580; loss: 1.67; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.33
Batch: 620; loss: 1.54; acc: 0.42
Batch: 640; loss: 1.57; acc: 0.47
Batch: 660; loss: 1.64; acc: 0.47
Batch: 680; loss: 1.63; acc: 0.48
Batch: 700; loss: 1.72; acc: 0.36
Batch: 720; loss: 1.67; acc: 0.45
Batch: 740; loss: 1.57; acc: 0.36
Batch: 760; loss: 1.62; acc: 0.42
Batch: 780; loss: 1.7; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.86; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.44
Val Epoch over. val_loss: 1.5842952584005465; val_accuracy: 0.4585987261146497 

The current subspace-distance is: 5.9394806157797575e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.64; acc: 0.5
Batch: 40; loss: 1.52; acc: 0.53
Batch: 60; loss: 1.46; acc: 0.45
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.66; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.59; acc: 0.44
Batch: 160; loss: 1.6; acc: 0.39
Batch: 180; loss: 1.66; acc: 0.41
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.48
Batch: 240; loss: 1.81; acc: 0.33
Batch: 260; loss: 1.77; acc: 0.34
Batch: 280; loss: 1.66; acc: 0.41
Batch: 300; loss: 1.5; acc: 0.44
Batch: 320; loss: 1.48; acc: 0.44
Batch: 340; loss: 1.82; acc: 0.33
Batch: 360; loss: 1.73; acc: 0.36
Batch: 380; loss: 1.71; acc: 0.42
Batch: 400; loss: 1.44; acc: 0.52
Batch: 420; loss: 1.67; acc: 0.41
Batch: 440; loss: 1.43; acc: 0.56
Batch: 460; loss: 1.57; acc: 0.52
Batch: 480; loss: 1.55; acc: 0.44
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.55; acc: 0.44
Batch: 540; loss: 1.56; acc: 0.5
Batch: 560; loss: 1.65; acc: 0.44
Batch: 580; loss: 1.69; acc: 0.45
Batch: 600; loss: 1.68; acc: 0.47
Batch: 620; loss: 1.74; acc: 0.39
Batch: 640; loss: 1.61; acc: 0.42
Batch: 660; loss: 1.73; acc: 0.42
Batch: 680; loss: 1.52; acc: 0.53
Batch: 700; loss: 1.62; acc: 0.41
Batch: 720; loss: 1.47; acc: 0.45
Batch: 740; loss: 1.52; acc: 0.47
Batch: 760; loss: 1.59; acc: 0.36
Batch: 780; loss: 1.74; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.42
Val Epoch over. val_loss: 1.583816891263245; val_accuracy: 0.46048964968152867 

The current subspace-distance is: 6.262705574044958e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.93; acc: 0.33
Batch: 40; loss: 1.56; acc: 0.47
Batch: 60; loss: 1.58; acc: 0.44
Batch: 80; loss: 1.61; acc: 0.45
Batch: 100; loss: 1.59; acc: 0.55
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.91; acc: 0.33
Batch: 160; loss: 1.67; acc: 0.38
Batch: 180; loss: 1.8; acc: 0.36
Batch: 200; loss: 1.49; acc: 0.45
Batch: 220; loss: 1.78; acc: 0.38
Batch: 240; loss: 1.69; acc: 0.41
Batch: 260; loss: 1.57; acc: 0.48
Batch: 280; loss: 1.42; acc: 0.53
Batch: 300; loss: 1.68; acc: 0.39
Batch: 320; loss: 1.55; acc: 0.5
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.54; acc: 0.48
Batch: 380; loss: 1.66; acc: 0.45
Batch: 400; loss: 1.57; acc: 0.44
Batch: 420; loss: 1.49; acc: 0.53
Batch: 440; loss: 1.73; acc: 0.44
Batch: 460; loss: 1.49; acc: 0.45
Batch: 480; loss: 1.52; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.41
Batch: 520; loss: 1.66; acc: 0.38
Batch: 540; loss: 1.67; acc: 0.44
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.8; acc: 0.39
Batch: 600; loss: 1.59; acc: 0.47
Batch: 620; loss: 1.56; acc: 0.52
Batch: 640; loss: 1.65; acc: 0.44
Batch: 660; loss: 1.69; acc: 0.41
Batch: 680; loss: 1.55; acc: 0.39
Batch: 700; loss: 1.68; acc: 0.41
Batch: 720; loss: 1.57; acc: 0.42
Batch: 740; loss: 1.45; acc: 0.48
Batch: 760; loss: 1.8; acc: 0.31
Batch: 780; loss: 1.6; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.42
Val Epoch over. val_loss: 1.5841025584822248; val_accuracy: 0.46009156050955413 

The current subspace-distance is: 6.510917592095211e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.8; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.39
Batch: 80; loss: 1.45; acc: 0.56
Batch: 100; loss: 1.74; acc: 0.41
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.33; acc: 0.55
Batch: 160; loss: 1.39; acc: 0.55
Batch: 180; loss: 1.44; acc: 0.45
Batch: 200; loss: 1.36; acc: 0.61
Batch: 220; loss: 1.69; acc: 0.44
Batch: 240; loss: 1.52; acc: 0.58
Batch: 260; loss: 1.53; acc: 0.42
Batch: 280; loss: 1.41; acc: 0.5
Batch: 300; loss: 1.59; acc: 0.48
Batch: 320; loss: 1.64; acc: 0.38
Batch: 340; loss: 1.76; acc: 0.45
Batch: 360; loss: 1.62; acc: 0.44
Batch: 380; loss: 1.58; acc: 0.42
Batch: 400; loss: 1.67; acc: 0.42
Batch: 420; loss: 1.76; acc: 0.47
Batch: 440; loss: 1.59; acc: 0.44
Batch: 460; loss: 1.56; acc: 0.45
Batch: 480; loss: 1.83; acc: 0.38
Batch: 500; loss: 1.81; acc: 0.42
Batch: 520; loss: 1.53; acc: 0.5
Batch: 540; loss: 1.6; acc: 0.47
Batch: 560; loss: 1.47; acc: 0.56
Batch: 580; loss: 1.71; acc: 0.45
Batch: 600; loss: 1.53; acc: 0.5
Batch: 620; loss: 1.61; acc: 0.44
Batch: 640; loss: 1.77; acc: 0.39
Batch: 660; loss: 1.77; acc: 0.44
Batch: 680; loss: 1.47; acc: 0.5
Batch: 700; loss: 1.46; acc: 0.48
Batch: 720; loss: 1.65; acc: 0.47
Batch: 740; loss: 1.5; acc: 0.5
Batch: 760; loss: 1.65; acc: 0.44
Batch: 780; loss: 1.5; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5838651414130145; val_accuracy: 0.46138535031847133 

The current subspace-distance is: 6.758269591955468e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.61; acc: 0.39
Batch: 20; loss: 1.77; acc: 0.34
Batch: 40; loss: 1.74; acc: 0.42
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.71; acc: 0.47
Batch: 120; loss: 1.63; acc: 0.38
Batch: 140; loss: 1.85; acc: 0.42
Batch: 160; loss: 1.61; acc: 0.42
Batch: 180; loss: 1.58; acc: 0.44
Batch: 200; loss: 1.62; acc: 0.41
Batch: 220; loss: 1.71; acc: 0.38
Batch: 240; loss: 1.61; acc: 0.5
Batch: 260; loss: 1.67; acc: 0.38
Batch: 280; loss: 1.59; acc: 0.56
Batch: 300; loss: 1.66; acc: 0.52
Batch: 320; loss: 1.65; acc: 0.39
Batch: 340; loss: 1.71; acc: 0.39
Batch: 360; loss: 1.55; acc: 0.5
Batch: 380; loss: 1.48; acc: 0.58
Batch: 400; loss: 1.48; acc: 0.48
Batch: 420; loss: 1.61; acc: 0.44
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.72; acc: 0.44
Batch: 480; loss: 1.57; acc: 0.45
Batch: 500; loss: 1.74; acc: 0.36
Batch: 520; loss: 1.52; acc: 0.56
Batch: 540; loss: 1.61; acc: 0.47
Batch: 560; loss: 1.67; acc: 0.39
Batch: 580; loss: 1.54; acc: 0.45
Batch: 600; loss: 1.46; acc: 0.48
Batch: 620; loss: 1.69; acc: 0.45
Batch: 640; loss: 1.74; acc: 0.39
Batch: 660; loss: 1.43; acc: 0.45
Batch: 680; loss: 1.55; acc: 0.44
Batch: 700; loss: 1.55; acc: 0.48
Batch: 720; loss: 1.66; acc: 0.39
Batch: 740; loss: 1.73; acc: 0.42
Batch: 760; loss: 1.56; acc: 0.5
Batch: 780; loss: 1.46; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.55; acc: 0.5
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5838711011181972; val_accuracy: 0.4612858280254777 

The current subspace-distance is: 6.497182766906917e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 1.77; acc: 0.39
Batch: 60; loss: 1.59; acc: 0.42
Batch: 80; loss: 1.71; acc: 0.44
Batch: 100; loss: 1.76; acc: 0.42
Batch: 120; loss: 1.48; acc: 0.42
Batch: 140; loss: 1.62; acc: 0.44
Batch: 160; loss: 1.8; acc: 0.36
Batch: 180; loss: 1.68; acc: 0.41
Batch: 200; loss: 1.67; acc: 0.38
Batch: 220; loss: 1.68; acc: 0.34
Batch: 240; loss: 1.82; acc: 0.42
Batch: 260; loss: 1.62; acc: 0.47
Batch: 280; loss: 1.66; acc: 0.38
Batch: 300; loss: 1.52; acc: 0.52
Batch: 320; loss: 1.7; acc: 0.52
Batch: 340; loss: 1.75; acc: 0.39
Batch: 360; loss: 1.68; acc: 0.5
Batch: 380; loss: 1.47; acc: 0.53
Batch: 400; loss: 1.58; acc: 0.48
Batch: 420; loss: 1.67; acc: 0.39
Batch: 440; loss: 1.44; acc: 0.56
Batch: 460; loss: 1.39; acc: 0.47
Batch: 480; loss: 1.57; acc: 0.45
Batch: 500; loss: 1.58; acc: 0.47
Batch: 520; loss: 1.61; acc: 0.5
Batch: 540; loss: 1.68; acc: 0.55
Batch: 560; loss: 1.6; acc: 0.42
Batch: 580; loss: 1.83; acc: 0.45
Batch: 600; loss: 1.7; acc: 0.44
Batch: 620; loss: 1.65; acc: 0.41
Batch: 640; loss: 1.48; acc: 0.55
Batch: 660; loss: 1.78; acc: 0.39
Batch: 680; loss: 1.74; acc: 0.39
Batch: 700; loss: 1.52; acc: 0.45
Batch: 720; loss: 1.64; acc: 0.31
Batch: 740; loss: 1.64; acc: 0.45
Batch: 760; loss: 1.64; acc: 0.44
Batch: 780; loss: 1.35; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.86; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.58; acc: 0.44
Val Epoch over. val_loss: 1.5843583208740137; val_accuracy: 0.4576035031847134 

The current subspace-distance is: 6.415956158889458e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.87; acc: 0.41
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.62; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.77; acc: 0.34
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.67; acc: 0.41
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.92; acc: 0.41
Batch: 200; loss: 1.59; acc: 0.44
Batch: 220; loss: 1.67; acc: 0.36
Batch: 240; loss: 1.71; acc: 0.34
Batch: 260; loss: 1.59; acc: 0.48
Batch: 280; loss: 1.39; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.47
Batch: 320; loss: 1.42; acc: 0.56
Batch: 340; loss: 1.73; acc: 0.41
Batch: 360; loss: 1.43; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.5
Batch: 400; loss: 1.8; acc: 0.39
Batch: 420; loss: 1.77; acc: 0.33
Batch: 440; loss: 1.39; acc: 0.61
Batch: 460; loss: 1.62; acc: 0.42
Batch: 480; loss: 1.83; acc: 0.38
Batch: 500; loss: 1.77; acc: 0.44
Batch: 520; loss: 1.59; acc: 0.5
Batch: 540; loss: 1.62; acc: 0.47
Batch: 560; loss: 1.43; acc: 0.45
Batch: 580; loss: 1.6; acc: 0.5
Batch: 600; loss: 1.55; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.61; acc: 0.48
Batch: 680; loss: 1.7; acc: 0.42
Batch: 700; loss: 1.44; acc: 0.53
Batch: 720; loss: 1.55; acc: 0.45
Batch: 740; loss: 1.74; acc: 0.42
Batch: 760; loss: 1.62; acc: 0.44
Batch: 780; loss: 1.4; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5840842108817617; val_accuracy: 0.45740445859872614 

The current subspace-distance is: 6.853324157418683e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.68; acc: 0.47
Batch: 60; loss: 1.59; acc: 0.45
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.86; acc: 0.33
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.62; acc: 0.48
Batch: 160; loss: 1.52; acc: 0.5
Batch: 180; loss: 1.6; acc: 0.5
Batch: 200; loss: 1.59; acc: 0.45
Batch: 220; loss: 1.58; acc: 0.53
Batch: 240; loss: 1.69; acc: 0.41
Batch: 260; loss: 1.78; acc: 0.45
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.76; acc: 0.42
Batch: 320; loss: 1.62; acc: 0.45
Batch: 340; loss: 1.59; acc: 0.5
Batch: 360; loss: 1.57; acc: 0.41
Batch: 380; loss: 1.51; acc: 0.42
Batch: 400; loss: 1.4; acc: 0.53
Batch: 420; loss: 1.63; acc: 0.44
Batch: 440; loss: 1.67; acc: 0.42
Batch: 460; loss: 1.7; acc: 0.39
Batch: 480; loss: 1.63; acc: 0.39
Batch: 500; loss: 1.71; acc: 0.34
Batch: 520; loss: 1.71; acc: 0.36
Batch: 540; loss: 1.57; acc: 0.44
Batch: 560; loss: 1.49; acc: 0.48
Batch: 580; loss: 1.64; acc: 0.41
Batch: 600; loss: 1.7; acc: 0.44
Batch: 620; loss: 1.59; acc: 0.55
Batch: 640; loss: 1.58; acc: 0.52
Batch: 660; loss: 1.59; acc: 0.45
Batch: 680; loss: 1.42; acc: 0.52
Batch: 700; loss: 1.48; acc: 0.47
Batch: 720; loss: 1.69; acc: 0.44
Batch: 740; loss: 1.66; acc: 0.44
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.44; acc: 0.56
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5836988512877446; val_accuracy: 0.4588972929936306 

The current subspace-distance is: 6.673247116850689e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.56; acc: 0.44
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 1.66; acc: 0.45
Batch: 60; loss: 1.66; acc: 0.42
Batch: 80; loss: 1.52; acc: 0.44
Batch: 100; loss: 1.46; acc: 0.55
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.72; acc: 0.41
Batch: 160; loss: 1.4; acc: 0.56
Batch: 180; loss: 1.51; acc: 0.42
Batch: 200; loss: 1.42; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.45
Batch: 240; loss: 1.58; acc: 0.47
Batch: 260; loss: 1.54; acc: 0.47
Batch: 280; loss: 1.49; acc: 0.5
Batch: 300; loss: 1.9; acc: 0.3
Batch: 320; loss: 1.47; acc: 0.5
Batch: 340; loss: 1.72; acc: 0.38
Batch: 360; loss: 1.61; acc: 0.47
Batch: 380; loss: 1.49; acc: 0.53
Batch: 400; loss: 1.78; acc: 0.33
Batch: 420; loss: 1.6; acc: 0.45
Batch: 440; loss: 1.53; acc: 0.5
Batch: 460; loss: 1.57; acc: 0.45
Batch: 480; loss: 1.48; acc: 0.45
Batch: 500; loss: 1.6; acc: 0.5
Batch: 520; loss: 1.76; acc: 0.42
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.52; acc: 0.41
Batch: 580; loss: 1.96; acc: 0.28
Batch: 600; loss: 1.94; acc: 0.33
Batch: 620; loss: 1.68; acc: 0.41
Batch: 640; loss: 1.69; acc: 0.38
Batch: 660; loss: 1.65; acc: 0.48
Batch: 680; loss: 1.74; acc: 0.5
Batch: 700; loss: 1.69; acc: 0.39
Batch: 720; loss: 1.33; acc: 0.59
Batch: 740; loss: 1.7; acc: 0.36
Batch: 760; loss: 1.86; acc: 0.33
Batch: 780; loss: 1.7; acc: 0.36
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.583922810615248; val_accuracy: 0.4590963375796178 

The current subspace-distance is: 6.809880869695917e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.51; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.42
Batch: 40; loss: 1.69; acc: 0.44
Batch: 60; loss: 1.65; acc: 0.45
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.67; acc: 0.34
Batch: 120; loss: 1.48; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.42
Batch: 160; loss: 1.68; acc: 0.41
Batch: 180; loss: 1.71; acc: 0.45
Batch: 200; loss: 1.62; acc: 0.48
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.47; acc: 0.5
Batch: 260; loss: 1.51; acc: 0.48
Batch: 280; loss: 1.57; acc: 0.45
Batch: 300; loss: 1.72; acc: 0.39
Batch: 320; loss: 1.62; acc: 0.38
Batch: 340; loss: 1.71; acc: 0.38
Batch: 360; loss: 1.5; acc: 0.53
Batch: 380; loss: 1.6; acc: 0.44
Batch: 400; loss: 1.9; acc: 0.33
Batch: 420; loss: 1.75; acc: 0.41
Batch: 440; loss: 1.66; acc: 0.44
Batch: 460; loss: 1.57; acc: 0.48
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.63; acc: 0.45
Batch: 520; loss: 1.7; acc: 0.42
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.62; acc: 0.42
Batch: 580; loss: 1.58; acc: 0.5
Batch: 600; loss: 1.53; acc: 0.48
Batch: 620; loss: 1.6; acc: 0.47
Batch: 640; loss: 1.7; acc: 0.41
Batch: 660; loss: 1.52; acc: 0.5
Batch: 680; loss: 1.46; acc: 0.55
Batch: 700; loss: 1.82; acc: 0.33
Batch: 720; loss: 1.92; acc: 0.3
Batch: 740; loss: 1.68; acc: 0.48
Batch: 760; loss: 1.54; acc: 0.5
Batch: 780; loss: 1.81; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.86; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5844537862546884; val_accuracy: 0.4581011146496815 

The current subspace-distance is: 7.031606219243258e-05 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.67; acc: 0.52
Batch: 20; loss: 1.44; acc: 0.48
Batch: 40; loss: 1.75; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.42
Batch: 80; loss: 1.48; acc: 0.5
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.51; acc: 0.48
Batch: 140; loss: 1.63; acc: 0.41
Batch: 160; loss: 1.51; acc: 0.5
Batch: 180; loss: 1.92; acc: 0.39
Batch: 200; loss: 1.85; acc: 0.34
Batch: 220; loss: 1.39; acc: 0.53
Batch: 240; loss: 1.75; acc: 0.39
Batch: 260; loss: 1.68; acc: 0.41
Batch: 280; loss: 1.51; acc: 0.48
Batch: 300; loss: 1.72; acc: 0.41
Batch: 320; loss: 1.73; acc: 0.36
Batch: 340; loss: 1.51; acc: 0.42
Batch: 360; loss: 1.6; acc: 0.39
Batch: 380; loss: 1.42; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.45
Batch: 420; loss: 1.46; acc: 0.53
Batch: 440; loss: 1.5; acc: 0.53
Batch: 460; loss: 1.63; acc: 0.45
Batch: 480; loss: 1.34; acc: 0.59
Batch: 500; loss: 1.62; acc: 0.41
Batch: 520; loss: 1.65; acc: 0.42
Batch: 540; loss: 1.48; acc: 0.41
Batch: 560; loss: 1.63; acc: 0.45
Batch: 580; loss: 1.68; acc: 0.47
Batch: 600; loss: 1.57; acc: 0.47
Batch: 620; loss: 1.77; acc: 0.44
Batch: 640; loss: 1.75; acc: 0.36
Batch: 660; loss: 1.65; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.42
Batch: 700; loss: 1.56; acc: 0.44
Batch: 720; loss: 1.41; acc: 0.53
Batch: 740; loss: 1.6; acc: 0.44
Batch: 760; loss: 1.49; acc: 0.52
Batch: 780; loss: 1.82; acc: 0.33
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.42
Val Epoch over. val_loss: 1.583679349559128; val_accuracy: 0.4601910828025478 

The current subspace-distance is: 7.212105265352875e-05 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 1.53; acc: 0.52
Batch: 40; loss: 1.69; acc: 0.39
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.77; acc: 0.39
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.57; acc: 0.39
Batch: 140; loss: 1.67; acc: 0.34
Batch: 160; loss: 1.38; acc: 0.53
Batch: 180; loss: 1.48; acc: 0.45
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.83; acc: 0.34
Batch: 240; loss: 1.53; acc: 0.48
Batch: 260; loss: 1.51; acc: 0.44
Batch: 280; loss: 1.52; acc: 0.44
Batch: 300; loss: 1.38; acc: 0.55
Batch: 320; loss: 1.8; acc: 0.39
Batch: 340; loss: 1.77; acc: 0.34
Batch: 360; loss: 1.72; acc: 0.41
Batch: 380; loss: 1.5; acc: 0.45
Batch: 400; loss: 1.91; acc: 0.31
Batch: 420; loss: 1.68; acc: 0.48
Batch: 440; loss: 1.87; acc: 0.34
Batch: 460; loss: 1.48; acc: 0.44
Batch: 480; loss: 1.58; acc: 0.48
Batch: 500; loss: 1.67; acc: 0.44
Batch: 520; loss: 1.26; acc: 0.56
Batch: 540; loss: 1.47; acc: 0.55
Batch: 560; loss: 1.71; acc: 0.41
Batch: 580; loss: 1.57; acc: 0.38
Batch: 600; loss: 1.49; acc: 0.44
Batch: 620; loss: 1.41; acc: 0.55
Batch: 640; loss: 1.75; acc: 0.39
Batch: 660; loss: 1.75; acc: 0.38
Batch: 680; loss: 1.57; acc: 0.48
Batch: 700; loss: 1.61; acc: 0.39
Batch: 720; loss: 1.57; acc: 0.48
Batch: 740; loss: 1.44; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.5
Batch: 780; loss: 1.67; acc: 0.38
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5836909318425854; val_accuracy: 0.4599920382165605 

The current subspace-distance is: 7.598022784804925e-05 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.67; acc: 0.36
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.65; acc: 0.47
Batch: 80; loss: 1.57; acc: 0.47
Batch: 100; loss: 1.56; acc: 0.47
Batch: 120; loss: 1.7; acc: 0.47
Batch: 140; loss: 1.63; acc: 0.47
Batch: 160; loss: 1.79; acc: 0.34
Batch: 180; loss: 1.56; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.47
Batch: 220; loss: 1.64; acc: 0.45
Batch: 240; loss: 1.77; acc: 0.5
Batch: 260; loss: 1.5; acc: 0.48
Batch: 280; loss: 1.42; acc: 0.52
Batch: 300; loss: 1.31; acc: 0.61
Batch: 320; loss: 1.76; acc: 0.38
Batch: 340; loss: 1.54; acc: 0.5
Batch: 360; loss: 1.46; acc: 0.53
Batch: 380; loss: 1.69; acc: 0.39
Batch: 400; loss: 1.47; acc: 0.52
Batch: 420; loss: 1.6; acc: 0.48
Batch: 440; loss: 1.51; acc: 0.55
Batch: 460; loss: 1.59; acc: 0.44
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.64; acc: 0.47
Batch: 520; loss: 1.65; acc: 0.41
Batch: 540; loss: 1.56; acc: 0.5
Batch: 560; loss: 1.53; acc: 0.5
Batch: 580; loss: 1.64; acc: 0.45
Batch: 600; loss: 1.53; acc: 0.48
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.69; acc: 0.42
Batch: 660; loss: 1.77; acc: 0.42
Batch: 680; loss: 1.64; acc: 0.52
Batch: 700; loss: 1.77; acc: 0.39
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.48; acc: 0.53
Batch: 760; loss: 1.74; acc: 0.45
Batch: 780; loss: 1.49; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.42
Val Epoch over. val_loss: 1.5836124852964073; val_accuracy: 0.459593949044586 

The current subspace-distance is: 7.951441511977464e-05 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.41
Batch: 20; loss: 1.48; acc: 0.45
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.63; acc: 0.39
Batch: 80; loss: 1.78; acc: 0.3
Batch: 100; loss: 1.68; acc: 0.39
Batch: 120; loss: 1.54; acc: 0.47
Batch: 140; loss: 1.64; acc: 0.42
Batch: 160; loss: 1.79; acc: 0.38
Batch: 180; loss: 1.51; acc: 0.52
Batch: 200; loss: 1.7; acc: 0.45
Batch: 220; loss: 1.75; acc: 0.39
Batch: 240; loss: 1.65; acc: 0.52
Batch: 260; loss: 1.53; acc: 0.44
Batch: 280; loss: 1.62; acc: 0.5
Batch: 300; loss: 1.41; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.39
Batch: 340; loss: 1.86; acc: 0.36
Batch: 360; loss: 1.78; acc: 0.41
Batch: 380; loss: 1.6; acc: 0.47
Batch: 400; loss: 1.75; acc: 0.33
Batch: 420; loss: 1.57; acc: 0.45
Batch: 440; loss: 1.42; acc: 0.48
Batch: 460; loss: 1.48; acc: 0.56
Batch: 480; loss: 1.6; acc: 0.42
Batch: 500; loss: 1.75; acc: 0.38
Batch: 520; loss: 1.76; acc: 0.38
Batch: 540; loss: 1.37; acc: 0.58
Batch: 560; loss: 1.46; acc: 0.48
Batch: 580; loss: 1.7; acc: 0.39
Batch: 600; loss: 1.58; acc: 0.52
Batch: 620; loss: 1.71; acc: 0.41
Batch: 640; loss: 1.64; acc: 0.41
Batch: 660; loss: 1.8; acc: 0.42
Batch: 680; loss: 1.53; acc: 0.48
Batch: 700; loss: 1.39; acc: 0.55
Batch: 720; loss: 1.75; acc: 0.42
Batch: 740; loss: 1.37; acc: 0.53
Batch: 760; loss: 1.78; acc: 0.36
Batch: 780; loss: 1.71; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.583627162465624; val_accuracy: 0.4589968152866242 

The current subspace-distance is: 7.884252408985049e-05 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.79; acc: 0.33
Batch: 20; loss: 1.63; acc: 0.41
Batch: 40; loss: 1.54; acc: 0.48
Batch: 60; loss: 1.44; acc: 0.52
Batch: 80; loss: 1.46; acc: 0.53
Batch: 100; loss: 1.43; acc: 0.5
Batch: 120; loss: 1.88; acc: 0.31
Batch: 140; loss: 1.73; acc: 0.42
Batch: 160; loss: 1.67; acc: 0.38
Batch: 180; loss: 1.72; acc: 0.39
Batch: 200; loss: 1.71; acc: 0.42
Batch: 220; loss: 1.58; acc: 0.44
Batch: 240; loss: 1.72; acc: 0.39
Batch: 260; loss: 1.31; acc: 0.58
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.47; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.44
Batch: 340; loss: 1.65; acc: 0.38
Batch: 360; loss: 1.69; acc: 0.44
Batch: 380; loss: 1.67; acc: 0.41
Batch: 400; loss: 1.43; acc: 0.52
Batch: 420; loss: 1.6; acc: 0.41
Batch: 440; loss: 1.6; acc: 0.44
Batch: 460; loss: 1.57; acc: 0.47
Batch: 480; loss: 1.62; acc: 0.47
Batch: 500; loss: 1.72; acc: 0.39
Batch: 520; loss: 1.52; acc: 0.5
Batch: 540; loss: 1.65; acc: 0.52
Batch: 560; loss: 1.88; acc: 0.38
Batch: 580; loss: 1.6; acc: 0.53
Batch: 600; loss: 1.44; acc: 0.5
Batch: 620; loss: 1.56; acc: 0.52
Batch: 640; loss: 1.88; acc: 0.42
Batch: 660; loss: 1.64; acc: 0.53
Batch: 680; loss: 1.64; acc: 0.42
Batch: 700; loss: 1.61; acc: 0.44
Batch: 720; loss: 1.97; acc: 0.42
Batch: 740; loss: 1.74; acc: 0.36
Batch: 760; loss: 1.7; acc: 0.41
Batch: 780; loss: 1.75; acc: 0.39
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5837453208911192; val_accuracy: 0.45949442675159236 

The current subspace-distance is: 8.207261271309108e-05 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.36
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.57; acc: 0.41
Batch: 100; loss: 1.49; acc: 0.47
Batch: 120; loss: 1.45; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.45
Batch: 160; loss: 1.65; acc: 0.42
Batch: 180; loss: 1.62; acc: 0.39
Batch: 200; loss: 1.69; acc: 0.39
Batch: 220; loss: 1.78; acc: 0.38
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.48
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.53; acc: 0.52
Batch: 320; loss: 1.5; acc: 0.42
Batch: 340; loss: 1.77; acc: 0.39
Batch: 360; loss: 1.52; acc: 0.44
Batch: 380; loss: 1.9; acc: 0.38
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.54; acc: 0.39
Batch: 440; loss: 1.55; acc: 0.42
Batch: 460; loss: 1.73; acc: 0.44
Batch: 480; loss: 1.34; acc: 0.47
Batch: 500; loss: 1.88; acc: 0.36
Batch: 520; loss: 1.55; acc: 0.53
Batch: 540; loss: 1.67; acc: 0.41
Batch: 560; loss: 1.48; acc: 0.47
Batch: 580; loss: 1.51; acc: 0.47
Batch: 600; loss: 1.71; acc: 0.39
Batch: 620; loss: 1.64; acc: 0.44
Batch: 640; loss: 1.73; acc: 0.39
Batch: 660; loss: 1.62; acc: 0.41
Batch: 680; loss: 1.63; acc: 0.44
Batch: 700; loss: 1.25; acc: 0.67
Batch: 720; loss: 1.69; acc: 0.44
Batch: 740; loss: 1.54; acc: 0.48
Batch: 760; loss: 1.75; acc: 0.45
Batch: 780; loss: 1.68; acc: 0.47
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5838579038146194; val_accuracy: 0.4590963375796178 

The current subspace-distance is: 8.004730625543743e-05 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.43; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.5
Batch: 60; loss: 1.82; acc: 0.36
Batch: 80; loss: 1.61; acc: 0.41
Batch: 100; loss: 1.72; acc: 0.33
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.57; acc: 0.48
Batch: 160; loss: 1.6; acc: 0.5
Batch: 180; loss: 1.73; acc: 0.41
Batch: 200; loss: 1.81; acc: 0.41
Batch: 220; loss: 1.57; acc: 0.56
Batch: 240; loss: 1.74; acc: 0.39
Batch: 260; loss: 1.56; acc: 0.48
Batch: 280; loss: 1.7; acc: 0.38
Batch: 300; loss: 1.64; acc: 0.41
Batch: 320; loss: 1.6; acc: 0.41
Batch: 340; loss: 1.48; acc: 0.52
Batch: 360; loss: 1.5; acc: 0.59
Batch: 380; loss: 1.78; acc: 0.42
Batch: 400; loss: 1.5; acc: 0.5
Batch: 420; loss: 1.64; acc: 0.41
Batch: 440; loss: 1.72; acc: 0.42
Batch: 460; loss: 1.57; acc: 0.48
Batch: 480; loss: 1.59; acc: 0.44
Batch: 500; loss: 1.72; acc: 0.39
Batch: 520; loss: 1.89; acc: 0.34
Batch: 540; loss: 1.66; acc: 0.47
Batch: 560; loss: 1.51; acc: 0.5
Batch: 580; loss: 1.67; acc: 0.39
Batch: 600; loss: 1.29; acc: 0.61
Batch: 620; loss: 1.55; acc: 0.39
Batch: 640; loss: 1.59; acc: 0.45
Batch: 660; loss: 1.62; acc: 0.48
Batch: 680; loss: 1.59; acc: 0.45
Batch: 700; loss: 1.36; acc: 0.47
Batch: 720; loss: 1.61; acc: 0.5
Batch: 740; loss: 1.67; acc: 0.36
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.67; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5836327797288348; val_accuracy: 0.4589968152866242 

The current subspace-distance is: 8.278367749881e-05 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.59; acc: 0.38
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.52; acc: 0.47
Batch: 60; loss: 1.51; acc: 0.44
Batch: 80; loss: 1.44; acc: 0.48
Batch: 100; loss: 1.64; acc: 0.47
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.41
Batch: 160; loss: 1.69; acc: 0.38
Batch: 180; loss: 1.45; acc: 0.5
Batch: 200; loss: 1.54; acc: 0.44
Batch: 220; loss: 1.54; acc: 0.48
Batch: 240; loss: 1.45; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.42
Batch: 280; loss: 1.39; acc: 0.52
Batch: 300; loss: 1.5; acc: 0.48
Batch: 320; loss: 1.55; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.42
Batch: 360; loss: 1.54; acc: 0.47
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 2.03; acc: 0.34
Batch: 420; loss: 1.68; acc: 0.42
Batch: 440; loss: 1.45; acc: 0.52
Batch: 460; loss: 1.69; acc: 0.47
Batch: 480; loss: 1.4; acc: 0.53
Batch: 500; loss: 1.61; acc: 0.5
Batch: 520; loss: 1.88; acc: 0.39
Batch: 540; loss: 1.62; acc: 0.42
Batch: 560; loss: 1.38; acc: 0.55
Batch: 580; loss: 1.46; acc: 0.55
Batch: 600; loss: 1.74; acc: 0.42
Batch: 620; loss: 1.48; acc: 0.45
Batch: 640; loss: 1.56; acc: 0.45
Batch: 660; loss: 1.62; acc: 0.47
Batch: 680; loss: 1.49; acc: 0.47
Batch: 700; loss: 1.58; acc: 0.45
Batch: 720; loss: 1.81; acc: 0.42
Batch: 740; loss: 1.43; acc: 0.53
Batch: 760; loss: 1.45; acc: 0.52
Batch: 780; loss: 1.6; acc: 0.47
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5835853951751806; val_accuracy: 0.4596934713375796 

The current subspace-distance is: 8.485994476359338e-05 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.5; acc: 0.5
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.55; acc: 0.55
Batch: 60; loss: 1.61; acc: 0.41
Batch: 80; loss: 1.63; acc: 0.41
Batch: 100; loss: 1.33; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.44
Batch: 140; loss: 1.67; acc: 0.48
Batch: 160; loss: 1.67; acc: 0.38
Batch: 180; loss: 1.55; acc: 0.48
Batch: 200; loss: 1.77; acc: 0.41
Batch: 220; loss: 1.54; acc: 0.5
Batch: 240; loss: 1.43; acc: 0.55
Batch: 260; loss: 1.85; acc: 0.39
Batch: 280; loss: 1.5; acc: 0.48
Batch: 300; loss: 1.74; acc: 0.45
Batch: 320; loss: 1.37; acc: 0.58
Batch: 340; loss: 1.56; acc: 0.42
Batch: 360; loss: 1.81; acc: 0.36
Batch: 380; loss: 1.55; acc: 0.55
Batch: 400; loss: 1.58; acc: 0.42
Batch: 420; loss: 1.68; acc: 0.52
Batch: 440; loss: 1.48; acc: 0.5
Batch: 460; loss: 1.32; acc: 0.58
Batch: 480; loss: 1.43; acc: 0.44
Batch: 500; loss: 1.59; acc: 0.47
Batch: 520; loss: 1.57; acc: 0.53
Batch: 540; loss: 1.76; acc: 0.38
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.43; acc: 0.56
Batch: 600; loss: 1.53; acc: 0.52
Batch: 620; loss: 1.42; acc: 0.5
Batch: 640; loss: 1.63; acc: 0.45
Batch: 660; loss: 1.65; acc: 0.44
Batch: 680; loss: 1.66; acc: 0.42
Batch: 700; loss: 1.6; acc: 0.42
Batch: 720; loss: 1.48; acc: 0.53
Batch: 740; loss: 1.57; acc: 0.47
Batch: 760; loss: 1.7; acc: 0.41
Batch: 780; loss: 1.55; acc: 0.34
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5836867283863627; val_accuracy: 0.4602906050955414 

The current subspace-distance is: 8.406778943026438e-05 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.74; acc: 0.47
Batch: 60; loss: 1.51; acc: 0.45
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.47
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.65; acc: 0.42
Batch: 160; loss: 1.52; acc: 0.45
Batch: 180; loss: 1.72; acc: 0.42
Batch: 200; loss: 1.48; acc: 0.47
Batch: 220; loss: 1.72; acc: 0.38
Batch: 240; loss: 1.72; acc: 0.41
Batch: 260; loss: 1.73; acc: 0.41
Batch: 280; loss: 1.77; acc: 0.38
Batch: 300; loss: 1.59; acc: 0.42
Batch: 320; loss: 1.49; acc: 0.5
Batch: 340; loss: 1.53; acc: 0.5
Batch: 360; loss: 1.47; acc: 0.45
Batch: 380; loss: 1.79; acc: 0.34
Batch: 400; loss: 1.48; acc: 0.42
Batch: 420; loss: 1.43; acc: 0.56
Batch: 440; loss: 1.69; acc: 0.39
Batch: 460; loss: 1.53; acc: 0.41
Batch: 480; loss: 1.78; acc: 0.33
Batch: 500; loss: 1.86; acc: 0.36
Batch: 520; loss: 1.43; acc: 0.52
Batch: 540; loss: 1.5; acc: 0.41
Batch: 560; loss: 1.68; acc: 0.36
Batch: 580; loss: 1.54; acc: 0.48
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.86; acc: 0.36
Batch: 660; loss: 1.6; acc: 0.38
Batch: 680; loss: 1.49; acc: 0.52
Batch: 700; loss: 1.54; acc: 0.52
Batch: 720; loss: 1.47; acc: 0.55
Batch: 740; loss: 1.47; acc: 0.55
Batch: 760; loss: 1.53; acc: 0.47
Batch: 780; loss: 1.53; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.45 

Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.57; acc: 0.44
Val Epoch over. val_loss: 1.5837483892015591; val_accuracy: 0.4596934713375796 

The current subspace-distance is: 8.252722182078287e-05 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 10720
elements in E: 2249500
fraction nonzero: 0.004765503445210047
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.16
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.29; acc: 0.16
Batch: 620; loss: 2.29; acc: 0.12
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.29; acc: 0.09
Batch: 700; loss: 2.29; acc: 0.06
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.3; acc: 0.08
Batch: 760; loss: 2.29; acc: 0.06
Batch: 780; loss: 2.27; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.2884829454361255; val_accuracy: 0.09882563694267515 

The current subspace-distance is: 3.9090405152819585e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.17
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.28; acc: 0.19
Batch: 180; loss: 2.29; acc: 0.23
Batch: 200; loss: 2.27; acc: 0.22
Batch: 220; loss: 2.28; acc: 0.23
Batch: 240; loss: 2.28; acc: 0.22
Batch: 260; loss: 2.27; acc: 0.27
Batch: 280; loss: 2.27; acc: 0.33
Batch: 300; loss: 2.25; acc: 0.36
Batch: 320; loss: 2.26; acc: 0.28
Batch: 340; loss: 2.28; acc: 0.23
Batch: 360; loss: 2.27; acc: 0.2
Batch: 380; loss: 2.26; acc: 0.27
Batch: 400; loss: 2.27; acc: 0.25
Batch: 420; loss: 2.24; acc: 0.31
Batch: 440; loss: 2.25; acc: 0.27
Batch: 460; loss: 2.23; acc: 0.3
Batch: 480; loss: 2.25; acc: 0.22
Batch: 500; loss: 2.23; acc: 0.23
Batch: 520; loss: 2.19; acc: 0.38
Batch: 540; loss: 2.25; acc: 0.25
Batch: 560; loss: 2.23; acc: 0.22
Batch: 580; loss: 2.22; acc: 0.19
Batch: 600; loss: 2.22; acc: 0.28
Batch: 620; loss: 2.21; acc: 0.25
Batch: 640; loss: 2.2; acc: 0.25
Batch: 660; loss: 2.21; acc: 0.2
Batch: 680; loss: 2.18; acc: 0.23
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.2; acc: 0.16
Batch: 740; loss: 2.1; acc: 0.28
Batch: 760; loss: 2.19; acc: 0.17
Batch: 780; loss: 2.09; acc: 0.27
Train Epoch over. train_loss: 2.24; train_accuracy: 0.22 

Batch: 0; loss: 2.13; acc: 0.2
Batch: 20; loss: 2.17; acc: 0.17
Batch: 40; loss: 2.12; acc: 0.25
Batch: 60; loss: 2.15; acc: 0.19
Batch: 80; loss: 2.14; acc: 0.25
Batch: 100; loss: 2.14; acc: 0.25
Batch: 120; loss: 2.16; acc: 0.2
Batch: 140; loss: 2.19; acc: 0.16
Val Epoch over. val_loss: 2.140305901788602; val_accuracy: 0.2360668789808917 

The current subspace-distance is: 6.907093847985379e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.15; acc: 0.25
Batch: 20; loss: 2.19; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.16
Batch: 60; loss: 2.12; acc: 0.23
Batch: 80; loss: 2.12; acc: 0.19
Batch: 100; loss: 2.12; acc: 0.23
Batch: 120; loss: 2.06; acc: 0.22
Batch: 140; loss: 2.07; acc: 0.22
Batch: 160; loss: 2.04; acc: 0.27
Batch: 180; loss: 1.96; acc: 0.3
Batch: 200; loss: 2.05; acc: 0.25
Batch: 220; loss: 2.07; acc: 0.19
Batch: 240; loss: 1.89; acc: 0.3
Batch: 260; loss: 1.96; acc: 0.36
Batch: 280; loss: 1.97; acc: 0.23
Batch: 300; loss: 1.88; acc: 0.33
Batch: 320; loss: 1.94; acc: 0.2
Batch: 340; loss: 1.96; acc: 0.3
Batch: 360; loss: 1.79; acc: 0.34
Batch: 380; loss: 1.78; acc: 0.38
Batch: 400; loss: 1.85; acc: 0.39
Batch: 420; loss: 1.83; acc: 0.39
Batch: 440; loss: 1.85; acc: 0.38
Batch: 460; loss: 1.61; acc: 0.42
Batch: 480; loss: 1.77; acc: 0.39
Batch: 500; loss: 1.56; acc: 0.44
Batch: 520; loss: 1.56; acc: 0.47
Batch: 540; loss: 1.75; acc: 0.33
Batch: 560; loss: 1.51; acc: 0.45
Batch: 580; loss: 1.45; acc: 0.52
Batch: 600; loss: 1.64; acc: 0.47
Batch: 620; loss: 1.27; acc: 0.56
Batch: 640; loss: 1.41; acc: 0.58
Batch: 660; loss: 1.47; acc: 0.56
Batch: 680; loss: 1.55; acc: 0.48
Batch: 700; loss: 1.54; acc: 0.47
Batch: 720; loss: 1.62; acc: 0.47
Batch: 740; loss: 1.23; acc: 0.62
Batch: 760; loss: 1.24; acc: 0.61
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.75; train_accuracy: 0.39 

Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.77; acc: 0.41
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 1.32; acc: 0.62
Batch: 80; loss: 1.36; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.21; acc: 0.59
Val Epoch over. val_loss: 1.3186447631781268; val_accuracy: 0.5685708598726115 

The current subspace-distance is: 1.3881952327210456e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.36; acc: 0.53
Batch: 40; loss: 1.32; acc: 0.64
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 1.29; acc: 0.56
Batch: 100; loss: 1.3; acc: 0.53
Batch: 120; loss: 1.25; acc: 0.55
Batch: 140; loss: 1.19; acc: 0.58
Batch: 160; loss: 1.15; acc: 0.58
Batch: 180; loss: 1.11; acc: 0.55
Batch: 200; loss: 1.32; acc: 0.59
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.52; acc: 0.53
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.02; acc: 0.72
Batch: 300; loss: 1.52; acc: 0.55
Batch: 320; loss: 1.09; acc: 0.64
Batch: 340; loss: 1.37; acc: 0.5
Batch: 360; loss: 1.1; acc: 0.66
Batch: 380; loss: 1.19; acc: 0.58
Batch: 400; loss: 1.31; acc: 0.56
Batch: 420; loss: 1.3; acc: 0.56
Batch: 440; loss: 1.03; acc: 0.7
Batch: 460; loss: 0.95; acc: 0.67
Batch: 480; loss: 1.02; acc: 0.55
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 1.28; acc: 0.55
Batch: 540; loss: 1.31; acc: 0.56
Batch: 560; loss: 1.31; acc: 0.64
Batch: 580; loss: 0.93; acc: 0.7
Batch: 600; loss: 1.26; acc: 0.69
Batch: 620; loss: 1.28; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.03; acc: 0.7
Batch: 680; loss: 1.31; acc: 0.53
Batch: 700; loss: 1.36; acc: 0.55
Batch: 720; loss: 1.42; acc: 0.53
Batch: 740; loss: 1.07; acc: 0.67
Batch: 760; loss: 1.17; acc: 0.62
Batch: 780; loss: 1.01; acc: 0.59
Train Epoch over. train_loss: 1.24; train_accuracy: 0.61 

Batch: 0; loss: 1.47; acc: 0.55
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 0.84; acc: 0.75
Batch: 60; loss: 1.15; acc: 0.61
Batch: 80; loss: 1.23; acc: 0.61
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.11; acc: 0.62
Val Epoch over. val_loss: 1.1704921866678129; val_accuracy: 0.6156449044585988 

The current subspace-distance is: 1.9229126337449998e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.59
Batch: 20; loss: 1.3; acc: 0.48
Batch: 40; loss: 1.12; acc: 0.62
Batch: 60; loss: 1.26; acc: 0.56
Batch: 80; loss: 1.29; acc: 0.62
Batch: 100; loss: 1.05; acc: 0.66
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 1.44; acc: 0.52
Batch: 160; loss: 0.96; acc: 0.66
Batch: 180; loss: 1.06; acc: 0.69
Batch: 200; loss: 1.26; acc: 0.61
Batch: 220; loss: 1.31; acc: 0.53
Batch: 240; loss: 0.97; acc: 0.67
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.0; acc: 0.62
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.22; acc: 0.66
Batch: 340; loss: 1.04; acc: 0.67
Batch: 360; loss: 1.48; acc: 0.45
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 1.21; acc: 0.58
Batch: 420; loss: 1.24; acc: 0.55
Batch: 440; loss: 1.23; acc: 0.66
Batch: 460; loss: 1.19; acc: 0.62
Batch: 480; loss: 1.17; acc: 0.58
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 1.11; acc: 0.64
Batch: 540; loss: 1.23; acc: 0.53
Batch: 560; loss: 1.31; acc: 0.56
Batch: 580; loss: 1.32; acc: 0.58
Batch: 600; loss: 1.02; acc: 0.61
Batch: 620; loss: 1.16; acc: 0.62
Batch: 640; loss: 1.0; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.62
Batch: 680; loss: 1.26; acc: 0.64
Batch: 700; loss: 1.54; acc: 0.56
Batch: 720; loss: 1.23; acc: 0.59
Batch: 740; loss: 1.08; acc: 0.62
Batch: 760; loss: 1.03; acc: 0.72
Batch: 780; loss: 1.04; acc: 0.69
Train Epoch over. train_loss: 1.16; train_accuracy: 0.62 

Batch: 0; loss: 1.4; acc: 0.5
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.59
Batch: 140; loss: 1.15; acc: 0.61
Val Epoch over. val_loss: 1.1114997996646128; val_accuracy: 0.6362460191082803 

The current subspace-distance is: 2.4535260308766738e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.66
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 1.12; acc: 0.59
Batch: 100; loss: 1.16; acc: 0.61
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.66
Batch: 160; loss: 0.92; acc: 0.73
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.05; acc: 0.61
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 1.31; acc: 0.56
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 0.89; acc: 0.75
Batch: 300; loss: 1.11; acc: 0.66
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 1.13; acc: 0.67
Batch: 360; loss: 1.08; acc: 0.58
Batch: 380; loss: 1.31; acc: 0.58
Batch: 400; loss: 1.1; acc: 0.62
Batch: 420; loss: 0.96; acc: 0.67
Batch: 440; loss: 0.96; acc: 0.66
Batch: 460; loss: 0.97; acc: 0.66
Batch: 480; loss: 1.16; acc: 0.66
Batch: 500; loss: 1.34; acc: 0.58
Batch: 520; loss: 1.29; acc: 0.56
Batch: 540; loss: 1.15; acc: 0.64
Batch: 560; loss: 1.52; acc: 0.52
Batch: 580; loss: 1.16; acc: 0.61
Batch: 600; loss: 1.01; acc: 0.62
Batch: 620; loss: 1.36; acc: 0.56
Batch: 640; loss: 1.17; acc: 0.61
Batch: 660; loss: 1.42; acc: 0.55
Batch: 680; loss: 1.44; acc: 0.53
Batch: 700; loss: 1.17; acc: 0.62
Batch: 720; loss: 1.36; acc: 0.62
Batch: 740; loss: 0.97; acc: 0.7
Batch: 760; loss: 1.01; acc: 0.64
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.15; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.56
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 1.12; acc: 0.66
Batch: 80; loss: 1.07; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.61
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 1.16; acc: 0.62
Val Epoch over. val_loss: 1.1070092381185788; val_accuracy: 0.6307722929936306 

The current subspace-distance is: 2.9868067940697074e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.61
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 1.1; acc: 0.62
Batch: 60; loss: 1.15; acc: 0.61
Batch: 80; loss: 1.15; acc: 0.58
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 1.12; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.53
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.4; acc: 0.53
Batch: 200; loss: 0.95; acc: 0.66
Batch: 220; loss: 0.94; acc: 0.77
Batch: 240; loss: 1.06; acc: 0.67
Batch: 260; loss: 1.09; acc: 0.72
Batch: 280; loss: 0.77; acc: 0.67
Batch: 300; loss: 1.02; acc: 0.61
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 1.1; acc: 0.62
Batch: 380; loss: 1.5; acc: 0.53
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.94; acc: 0.7
Batch: 440; loss: 1.1; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.56
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.69
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.03; acc: 0.64
Batch: 560; loss: 1.12; acc: 0.64
Batch: 580; loss: 0.88; acc: 0.67
Batch: 600; loss: 1.23; acc: 0.64
Batch: 620; loss: 1.2; acc: 0.64
Batch: 640; loss: 1.16; acc: 0.62
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.67
Batch: 700; loss: 1.09; acc: 0.61
Batch: 720; loss: 1.22; acc: 0.56
Batch: 740; loss: 1.07; acc: 0.61
Batch: 760; loss: 1.02; acc: 0.66
Batch: 780; loss: 1.18; acc: 0.66
Train Epoch over. train_loss: 1.13; train_accuracy: 0.64 

Batch: 0; loss: 1.21; acc: 0.53
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.62
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 1.23; acc: 0.64
Val Epoch over. val_loss: 1.0932871463951792; val_accuracy: 0.644406847133758 

The current subspace-distance is: 3.373788422322832e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 1.14; acc: 0.66
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 0.8; acc: 0.73
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 1.04; acc: 0.69
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 1.21; acc: 0.59
Batch: 220; loss: 1.11; acc: 0.61
Batch: 240; loss: 1.15; acc: 0.61
Batch: 260; loss: 0.96; acc: 0.67
Batch: 280; loss: 1.25; acc: 0.64
Batch: 300; loss: 1.21; acc: 0.62
Batch: 320; loss: 0.97; acc: 0.66
Batch: 340; loss: 1.1; acc: 0.61
Batch: 360; loss: 1.18; acc: 0.62
Batch: 380; loss: 1.18; acc: 0.62
Batch: 400; loss: 1.17; acc: 0.66
Batch: 420; loss: 0.9; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.73
Batch: 460; loss: 1.2; acc: 0.53
Batch: 480; loss: 0.95; acc: 0.64
Batch: 500; loss: 1.03; acc: 0.66
Batch: 520; loss: 1.13; acc: 0.62
Batch: 540; loss: 0.95; acc: 0.64
Batch: 560; loss: 1.39; acc: 0.61
Batch: 580; loss: 1.02; acc: 0.67
Batch: 600; loss: 1.06; acc: 0.59
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 1.16; acc: 0.67
Batch: 660; loss: 1.41; acc: 0.59
Batch: 680; loss: 0.92; acc: 0.67
Batch: 700; loss: 1.02; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.61
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 1.2; acc: 0.62
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.32; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.5
Batch: 40; loss: 0.75; acc: 0.77
Batch: 60; loss: 1.11; acc: 0.56
Batch: 80; loss: 0.95; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.56
Val Epoch over. val_loss: 1.1047509046876507; val_accuracy: 0.6358479299363057 

The current subspace-distance is: 3.542942431522533e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.61
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 1.28; acc: 0.58
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 1.17; acc: 0.58
Batch: 100; loss: 1.15; acc: 0.64
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 1.12; acc: 0.66
Batch: 160; loss: 1.21; acc: 0.55
Batch: 180; loss: 1.18; acc: 0.61
Batch: 200; loss: 0.95; acc: 0.7
Batch: 220; loss: 1.02; acc: 0.7
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.15; acc: 0.59
Batch: 280; loss: 1.0; acc: 0.64
Batch: 300; loss: 1.27; acc: 0.59
Batch: 320; loss: 0.97; acc: 0.67
Batch: 340; loss: 1.36; acc: 0.53
Batch: 360; loss: 1.23; acc: 0.56
Batch: 380; loss: 0.83; acc: 0.7
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.23; acc: 0.59
Batch: 460; loss: 1.32; acc: 0.52
Batch: 480; loss: 1.02; acc: 0.72
Batch: 500; loss: 1.42; acc: 0.61
Batch: 520; loss: 1.21; acc: 0.62
Batch: 540; loss: 1.06; acc: 0.64
Batch: 560; loss: 1.06; acc: 0.7
Batch: 580; loss: 1.2; acc: 0.59
Batch: 600; loss: 1.26; acc: 0.58
Batch: 620; loss: 0.96; acc: 0.66
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.64
Batch: 680; loss: 1.08; acc: 0.66
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.64
Batch: 740; loss: 1.17; acc: 0.62
Batch: 760; loss: 1.08; acc: 0.67
Batch: 780; loss: 0.94; acc: 0.73
Train Epoch over. train_loss: 1.1; train_accuracy: 0.65 

Batch: 0; loss: 1.17; acc: 0.56
Batch: 20; loss: 1.34; acc: 0.53
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.69
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.55
Batch: 140; loss: 1.04; acc: 0.7
Val Epoch over. val_loss: 1.063030638132885; val_accuracy: 0.6667993630573248 

The current subspace-distance is: 3.990876939496957e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.87; acc: 0.7
Batch: 60; loss: 1.22; acc: 0.58
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 1.1; acc: 0.62
Batch: 160; loss: 0.95; acc: 0.61
Batch: 180; loss: 1.32; acc: 0.59
Batch: 200; loss: 1.18; acc: 0.58
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.13; acc: 0.59
Batch: 260; loss: 1.09; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.67
Batch: 300; loss: 1.02; acc: 0.67
Batch: 320; loss: 1.08; acc: 0.62
Batch: 340; loss: 1.16; acc: 0.7
Batch: 360; loss: 1.0; acc: 0.61
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.98; acc: 0.67
Batch: 420; loss: 0.9; acc: 0.72
Batch: 440; loss: 0.95; acc: 0.72
Batch: 460; loss: 1.13; acc: 0.61
Batch: 480; loss: 1.15; acc: 0.62
Batch: 500; loss: 1.21; acc: 0.61
Batch: 520; loss: 0.96; acc: 0.66
Batch: 540; loss: 0.9; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.7
Batch: 580; loss: 0.87; acc: 0.7
Batch: 600; loss: 1.11; acc: 0.73
Batch: 620; loss: 1.04; acc: 0.72
Batch: 640; loss: 1.0; acc: 0.7
Batch: 660; loss: 1.08; acc: 0.66
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.08; acc: 0.67
Batch: 760; loss: 0.99; acc: 0.64
Batch: 780; loss: 1.39; acc: 0.61
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.17; acc: 0.56
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 1.01; acc: 0.62
Batch: 80; loss: 0.81; acc: 0.73
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 1.16; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.69
Val Epoch over. val_loss: 1.0379334126308466; val_accuracy: 0.6606289808917197 

The current subspace-distance is: 4.401555997901596e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 1.21; acc: 0.58
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.67
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.1; acc: 0.64
Batch: 160; loss: 1.18; acc: 0.58
Batch: 180; loss: 0.94; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.09; acc: 0.59
Batch: 240; loss: 0.97; acc: 0.7
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.91; acc: 0.73
Batch: 300; loss: 0.85; acc: 0.69
Batch: 320; loss: 0.82; acc: 0.78
Batch: 340; loss: 1.05; acc: 0.67
Batch: 360; loss: 0.96; acc: 0.7
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 1.14; acc: 0.64
Batch: 420; loss: 1.11; acc: 0.56
Batch: 440; loss: 1.0; acc: 0.69
Batch: 460; loss: 1.01; acc: 0.7
Batch: 480; loss: 0.98; acc: 0.7
Batch: 500; loss: 1.33; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.64
Batch: 540; loss: 1.26; acc: 0.59
Batch: 560; loss: 1.31; acc: 0.55
Batch: 580; loss: 1.19; acc: 0.64
Batch: 600; loss: 0.98; acc: 0.69
Batch: 620; loss: 1.04; acc: 0.69
Batch: 640; loss: 0.88; acc: 0.7
Batch: 660; loss: 0.91; acc: 0.72
Batch: 680; loss: 1.04; acc: 0.66
Batch: 700; loss: 1.03; acc: 0.67
Batch: 720; loss: 0.97; acc: 0.66
Batch: 740; loss: 0.92; acc: 0.66
Batch: 760; loss: 1.03; acc: 0.66
Batch: 780; loss: 1.4; acc: 0.56
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.59
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 1.01; acc: 0.62
Batch: 80; loss: 0.77; acc: 0.73
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 0.95; acc: 0.66
Val Epoch over. val_loss: 1.0108150836009129; val_accuracy: 0.6734673566878981 

The current subspace-distance is: 4.730376531369984e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.64
Batch: 120; loss: 1.43; acc: 0.59
Batch: 140; loss: 1.15; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.7
Batch: 180; loss: 1.09; acc: 0.64
Batch: 200; loss: 1.15; acc: 0.61
Batch: 220; loss: 0.98; acc: 0.69
Batch: 240; loss: 1.0; acc: 0.62
Batch: 260; loss: 0.98; acc: 0.66
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 0.95; acc: 0.66
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.85; acc: 0.73
Batch: 360; loss: 0.97; acc: 0.67
Batch: 380; loss: 0.98; acc: 0.73
Batch: 400; loss: 0.98; acc: 0.62
Batch: 420; loss: 0.92; acc: 0.67
Batch: 440; loss: 1.24; acc: 0.69
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 0.83; acc: 0.66
Batch: 500; loss: 1.13; acc: 0.61
Batch: 520; loss: 0.81; acc: 0.7
Batch: 540; loss: 0.85; acc: 0.75
Batch: 560; loss: 1.13; acc: 0.61
Batch: 580; loss: 1.17; acc: 0.64
Batch: 600; loss: 1.3; acc: 0.58
Batch: 620; loss: 1.15; acc: 0.58
Batch: 640; loss: 0.79; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.58
Batch: 680; loss: 1.19; acc: 0.66
Batch: 700; loss: 1.06; acc: 0.67
Batch: 720; loss: 1.16; acc: 0.61
Batch: 740; loss: 1.02; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.66
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.18; acc: 0.58
Batch: 20; loss: 1.31; acc: 0.55
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 1.05; acc: 0.61
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 0.96; acc: 0.7
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.66
Val Epoch over. val_loss: 1.0405613187771694; val_accuracy: 0.6610270700636943 

The current subspace-distance is: 5.1160237489966676e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.16; acc: 0.58
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 1.1; acc: 0.59
Batch: 60; loss: 1.19; acc: 0.62
Batch: 80; loss: 1.08; acc: 0.66
Batch: 100; loss: 1.02; acc: 0.59
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.9; acc: 0.75
Batch: 160; loss: 1.01; acc: 0.67
Batch: 180; loss: 1.39; acc: 0.55
Batch: 200; loss: 1.05; acc: 0.61
Batch: 220; loss: 1.29; acc: 0.59
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 1.17; acc: 0.56
Batch: 280; loss: 1.27; acc: 0.66
Batch: 300; loss: 1.05; acc: 0.7
Batch: 320; loss: 1.14; acc: 0.66
Batch: 340; loss: 1.03; acc: 0.7
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 1.04; acc: 0.67
Batch: 400; loss: 0.98; acc: 0.66
Batch: 420; loss: 1.24; acc: 0.59
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 1.09; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.61
Batch: 500; loss: 0.95; acc: 0.67
Batch: 520; loss: 1.27; acc: 0.61
Batch: 540; loss: 1.1; acc: 0.66
Batch: 560; loss: 1.14; acc: 0.62
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.22; acc: 0.58
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 1.01; acc: 0.58
Batch: 660; loss: 1.17; acc: 0.61
Batch: 680; loss: 0.86; acc: 0.77
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.69
Batch: 740; loss: 1.22; acc: 0.64
Batch: 760; loss: 1.06; acc: 0.7
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 1.33; acc: 0.58
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.61
Batch: 80; loss: 0.79; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.67
Batch: 120; loss: 1.19; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.64
Val Epoch over. val_loss: 1.021001302512588; val_accuracy: 0.6678941082802548 

The current subspace-distance is: 5.353346932679415e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.01; acc: 0.64
Batch: 40; loss: 1.03; acc: 0.69
Batch: 60; loss: 0.78; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.62
Batch: 100; loss: 1.22; acc: 0.67
Batch: 120; loss: 1.16; acc: 0.7
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.93; acc: 0.69
Batch: 180; loss: 0.98; acc: 0.72
Batch: 200; loss: 1.18; acc: 0.53
Batch: 220; loss: 1.03; acc: 0.7
Batch: 240; loss: 1.12; acc: 0.58
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 1.11; acc: 0.67
Batch: 300; loss: 0.87; acc: 0.73
Batch: 320; loss: 1.35; acc: 0.67
Batch: 340; loss: 1.3; acc: 0.62
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 1.07; acc: 0.59
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 1.03; acc: 0.59
Batch: 440; loss: 1.17; acc: 0.56
Batch: 460; loss: 1.39; acc: 0.52
Batch: 480; loss: 0.85; acc: 0.78
Batch: 500; loss: 0.9; acc: 0.7
Batch: 520; loss: 1.09; acc: 0.64
Batch: 540; loss: 1.11; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.59
Batch: 580; loss: 0.84; acc: 0.75
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.53; acc: 0.56
Batch: 640; loss: 1.08; acc: 0.61
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.1; acc: 0.56
Batch: 720; loss: 0.95; acc: 0.7
Batch: 740; loss: 1.17; acc: 0.61
Batch: 760; loss: 1.06; acc: 0.62
Batch: 780; loss: 1.03; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.62
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.64
Val Epoch over. val_loss: 1.0268781739435378; val_accuracy: 0.6720740445859873 

The current subspace-distance is: 5.471038457471877e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 1.09; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.59
Batch: 80; loss: 1.16; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 1.16; acc: 0.64
Batch: 160; loss: 0.91; acc: 0.7
Batch: 180; loss: 0.94; acc: 0.64
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 1.05; acc: 0.58
Batch: 240; loss: 1.12; acc: 0.73
Batch: 260; loss: 0.94; acc: 0.75
Batch: 280; loss: 0.97; acc: 0.7
Batch: 300; loss: 1.28; acc: 0.61
Batch: 320; loss: 0.98; acc: 0.77
Batch: 340; loss: 1.13; acc: 0.62
Batch: 360; loss: 1.19; acc: 0.62
Batch: 380; loss: 1.14; acc: 0.61
Batch: 400; loss: 1.11; acc: 0.61
Batch: 420; loss: 1.2; acc: 0.62
Batch: 440; loss: 1.0; acc: 0.61
Batch: 460; loss: 1.2; acc: 0.58
Batch: 480; loss: 1.22; acc: 0.56
Batch: 500; loss: 0.94; acc: 0.7
Batch: 520; loss: 0.97; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.17; acc: 0.58
Batch: 600; loss: 1.2; acc: 0.61
Batch: 620; loss: 0.91; acc: 0.72
Batch: 640; loss: 1.17; acc: 0.66
Batch: 660; loss: 0.92; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.59
Batch: 700; loss: 1.22; acc: 0.66
Batch: 720; loss: 0.96; acc: 0.62
Batch: 740; loss: 1.28; acc: 0.64
Batch: 760; loss: 1.05; acc: 0.67
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 1.01; acc: 0.62
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 1.01; acc: 0.66
Val Epoch over. val_loss: 1.0180068513390366; val_accuracy: 0.6697850318471338 

The current subspace-distance is: 5.7554141676519066e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.07; acc: 0.66
Batch: 20; loss: 1.48; acc: 0.53
Batch: 40; loss: 0.95; acc: 0.72
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.58
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.03; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.67
Batch: 160; loss: 1.04; acc: 0.72
Batch: 180; loss: 1.02; acc: 0.69
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 0.92; acc: 0.7
Batch: 240; loss: 1.02; acc: 0.69
Batch: 260; loss: 1.03; acc: 0.69
Batch: 280; loss: 1.16; acc: 0.66
Batch: 300; loss: 1.07; acc: 0.61
Batch: 320; loss: 1.1; acc: 0.62
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 1.09; acc: 0.73
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 1.11; acc: 0.61
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.2; acc: 0.59
Batch: 480; loss: 1.25; acc: 0.55
Batch: 500; loss: 1.04; acc: 0.69
Batch: 520; loss: 1.02; acc: 0.67
Batch: 540; loss: 1.26; acc: 0.61
Batch: 560; loss: 1.15; acc: 0.67
Batch: 580; loss: 1.14; acc: 0.66
Batch: 600; loss: 0.87; acc: 0.72
Batch: 620; loss: 0.88; acc: 0.66
Batch: 640; loss: 1.05; acc: 0.72
Batch: 660; loss: 1.1; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.53
Batch: 700; loss: 0.82; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 0.98; acc: 0.62
Batch: 760; loss: 1.02; acc: 0.61
Batch: 780; loss: 0.97; acc: 0.67
Train Epoch over. train_loss: 1.08; train_accuracy: 0.65 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.34; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.61
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.66
Val Epoch over. val_loss: 1.0193877189782015; val_accuracy: 0.6677945859872612 

The current subspace-distance is: 5.864066770300269e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.05; acc: 0.61
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.29; acc: 0.56
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 1.2; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 0.95; acc: 0.7
Batch: 220; loss: 1.08; acc: 0.66
Batch: 240; loss: 1.05; acc: 0.66
Batch: 260; loss: 0.95; acc: 0.66
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.12; acc: 0.62
Batch: 320; loss: 0.99; acc: 0.73
Batch: 340; loss: 1.07; acc: 0.69
Batch: 360; loss: 1.08; acc: 0.69
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 1.01; acc: 0.66
Batch: 440; loss: 0.86; acc: 0.73
Batch: 460; loss: 1.01; acc: 0.73
Batch: 480; loss: 0.98; acc: 0.75
Batch: 500; loss: 1.13; acc: 0.59
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.35; acc: 0.59
Batch: 560; loss: 1.21; acc: 0.56
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 1.3; acc: 0.58
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 0.83; acc: 0.77
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 0.84; acc: 0.66
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.75; acc: 0.84
Batch: 780; loss: 1.07; acc: 0.62
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.56
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.62
Batch: 80; loss: 0.8; acc: 0.69
Batch: 100; loss: 0.96; acc: 0.69
Batch: 120; loss: 1.19; acc: 0.58
Batch: 140; loss: 1.0; acc: 0.66
Val Epoch over. val_loss: 1.0209426211703354; val_accuracy: 0.666202229299363 

The current subspace-distance is: 6.0439033404691145e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 1.06; acc: 0.64
Batch: 40; loss: 0.97; acc: 0.69
Batch: 60; loss: 1.01; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.64
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 0.84; acc: 0.73
Batch: 160; loss: 0.98; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.56
Batch: 200; loss: 0.88; acc: 0.73
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.59
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 1.22; acc: 0.59
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.28; acc: 0.66
Batch: 360; loss: 1.15; acc: 0.62
Batch: 380; loss: 1.55; acc: 0.5
Batch: 400; loss: 1.22; acc: 0.62
Batch: 420; loss: 1.06; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.58
Batch: 460; loss: 1.4; acc: 0.58
Batch: 480; loss: 1.18; acc: 0.66
Batch: 500; loss: 1.08; acc: 0.64
Batch: 520; loss: 1.02; acc: 0.59
Batch: 540; loss: 1.15; acc: 0.67
Batch: 560; loss: 1.44; acc: 0.53
Batch: 580; loss: 0.94; acc: 0.64
Batch: 600; loss: 1.06; acc: 0.69
Batch: 620; loss: 1.14; acc: 0.67
Batch: 640; loss: 1.03; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.56
Batch: 680; loss: 1.06; acc: 0.64
Batch: 700; loss: 0.92; acc: 0.69
Batch: 720; loss: 0.91; acc: 0.73
Batch: 740; loss: 1.29; acc: 0.56
Batch: 760; loss: 1.5; acc: 0.59
Batch: 780; loss: 1.08; acc: 0.64
Train Epoch over. train_loss: 1.08; train_accuracy: 0.65 

Batch: 0; loss: 1.14; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 1.17; acc: 0.56
Batch: 140; loss: 0.94; acc: 0.67
Val Epoch over. val_loss: 1.0193952732025438; val_accuracy: 0.6721735668789809 

The current subspace-distance is: 6.551491969730705e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 1.0; acc: 0.64
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.88; acc: 0.7
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.62
Batch: 160; loss: 0.91; acc: 0.67
Batch: 180; loss: 1.39; acc: 0.64
Batch: 200; loss: 1.0; acc: 0.64
Batch: 220; loss: 0.95; acc: 0.64
Batch: 240; loss: 1.27; acc: 0.53
Batch: 260; loss: 0.99; acc: 0.7
Batch: 280; loss: 1.17; acc: 0.72
Batch: 300; loss: 1.26; acc: 0.59
Batch: 320; loss: 0.86; acc: 0.7
Batch: 340; loss: 1.12; acc: 0.64
Batch: 360; loss: 0.83; acc: 0.72
Batch: 380; loss: 1.1; acc: 0.62
Batch: 400; loss: 1.16; acc: 0.56
Batch: 420; loss: 1.16; acc: 0.58
Batch: 440; loss: 1.43; acc: 0.59
Batch: 460; loss: 0.94; acc: 0.64
Batch: 480; loss: 0.98; acc: 0.7
Batch: 500; loss: 1.05; acc: 0.69
Batch: 520; loss: 1.12; acc: 0.62
Batch: 540; loss: 1.07; acc: 0.7
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 0.75; acc: 0.75
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 0.92; acc: 0.7
Batch: 640; loss: 0.92; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.62
Batch: 680; loss: 1.19; acc: 0.61
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 1.18; acc: 0.67
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 0.84; acc: 0.72
Batch: 780; loss: 1.16; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.97; acc: 0.67
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.55
Batch: 140; loss: 1.01; acc: 0.67
Val Epoch over. val_loss: 1.0290574978111655; val_accuracy: 0.6685907643312102 

The current subspace-distance is: 6.582871719729155e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.43; acc: 0.55
Batch: 80; loss: 0.9; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.59
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 0.96; acc: 0.69
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 1.02; acc: 0.66
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 1.25; acc: 0.62
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 1.19; acc: 0.55
Batch: 320; loss: 1.27; acc: 0.67
Batch: 340; loss: 1.0; acc: 0.72
Batch: 360; loss: 0.95; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.58
Batch: 420; loss: 1.0; acc: 0.67
Batch: 440; loss: 0.89; acc: 0.72
Batch: 460; loss: 0.97; acc: 0.7
Batch: 480; loss: 0.83; acc: 0.69
Batch: 500; loss: 0.85; acc: 0.75
Batch: 520; loss: 1.45; acc: 0.67
Batch: 540; loss: 1.09; acc: 0.62
Batch: 560; loss: 1.36; acc: 0.56
Batch: 580; loss: 1.14; acc: 0.58
Batch: 600; loss: 0.99; acc: 0.67
Batch: 620; loss: 1.04; acc: 0.62
Batch: 640; loss: 1.16; acc: 0.61
Batch: 660; loss: 1.22; acc: 0.59
Batch: 680; loss: 0.99; acc: 0.62
Batch: 700; loss: 1.01; acc: 0.64
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 1.32; acc: 0.61
Batch: 780; loss: 1.06; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.65 

Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 0.95; acc: 0.69
Val Epoch over. val_loss: 1.01666352285701; val_accuracy: 0.67296974522293 

The current subspace-distance is: 6.748486339347437e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 0.97; acc: 0.66
Batch: 80; loss: 1.14; acc: 0.67
Batch: 100; loss: 1.0; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 0.88; acc: 0.62
Batch: 180; loss: 0.97; acc: 0.67
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 1.2; acc: 0.58
Batch: 240; loss: 1.19; acc: 0.66
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.86; acc: 0.7
Batch: 300; loss: 1.29; acc: 0.55
Batch: 320; loss: 1.04; acc: 0.7
Batch: 340; loss: 0.97; acc: 0.72
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 1.17; acc: 0.59
Batch: 400; loss: 0.91; acc: 0.8
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 0.97; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.67
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 1.15; acc: 0.58
Batch: 520; loss: 1.01; acc: 0.64
Batch: 540; loss: 1.01; acc: 0.64
Batch: 560; loss: 1.06; acc: 0.55
Batch: 580; loss: 0.99; acc: 0.62
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 0.82; acc: 0.72
Batch: 640; loss: 1.01; acc: 0.72
Batch: 660; loss: 1.02; acc: 0.62
Batch: 680; loss: 1.22; acc: 0.59
Batch: 700; loss: 1.04; acc: 0.67
Batch: 720; loss: 0.78; acc: 0.72
Batch: 740; loss: 1.23; acc: 0.62
Batch: 760; loss: 1.18; acc: 0.55
Batch: 780; loss: 0.98; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.98; acc: 0.64
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 0.96; acc: 0.7
Val Epoch over. val_loss: 1.011494678676508; val_accuracy: 0.6724721337579618 

The current subspace-distance is: 7.221677515190095e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.03; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.62
Batch: 60; loss: 0.94; acc: 0.66
Batch: 80; loss: 0.87; acc: 0.72
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 0.87; acc: 0.69
Batch: 140; loss: 1.37; acc: 0.59
Batch: 160; loss: 0.98; acc: 0.64
Batch: 180; loss: 1.05; acc: 0.67
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 1.09; acc: 0.66
Batch: 240; loss: 1.23; acc: 0.67
Batch: 260; loss: 1.15; acc: 0.61
Batch: 280; loss: 1.02; acc: 0.64
Batch: 300; loss: 0.91; acc: 0.73
Batch: 320; loss: 1.11; acc: 0.66
Batch: 340; loss: 0.91; acc: 0.7
Batch: 360; loss: 1.28; acc: 0.61
Batch: 380; loss: 1.53; acc: 0.53
Batch: 400; loss: 1.12; acc: 0.67
Batch: 420; loss: 0.92; acc: 0.67
Batch: 440; loss: 1.09; acc: 0.64
Batch: 460; loss: 1.21; acc: 0.58
Batch: 480; loss: 0.99; acc: 0.7
Batch: 500; loss: 0.9; acc: 0.77
Batch: 520; loss: 1.01; acc: 0.67
Batch: 540; loss: 0.97; acc: 0.77
Batch: 560; loss: 0.95; acc: 0.69
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 1.18; acc: 0.67
Batch: 620; loss: 1.06; acc: 0.58
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.16; acc: 0.58
Batch: 680; loss: 1.07; acc: 0.61
Batch: 700; loss: 1.14; acc: 0.58
Batch: 720; loss: 1.04; acc: 0.75
Batch: 740; loss: 1.3; acc: 0.61
Batch: 760; loss: 1.02; acc: 0.7
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.62
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.96; acc: 0.67
Val Epoch over. val_loss: 1.0121538202474072; val_accuracy: 0.6758558917197452 

The current subspace-distance is: 7.087965786922723e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.83; acc: 0.7
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.56
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 1.27; acc: 0.58
Batch: 160; loss: 0.89; acc: 0.69
Batch: 180; loss: 1.02; acc: 0.7
Batch: 200; loss: 0.96; acc: 0.66
Batch: 220; loss: 1.08; acc: 0.64
Batch: 240; loss: 0.98; acc: 0.67
Batch: 260; loss: 1.06; acc: 0.64
Batch: 280; loss: 0.95; acc: 0.66
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 1.03; acc: 0.72
Batch: 340; loss: 1.07; acc: 0.7
Batch: 360; loss: 1.22; acc: 0.59
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.31; acc: 0.58
Batch: 440; loss: 1.13; acc: 0.61
Batch: 460; loss: 1.26; acc: 0.61
Batch: 480; loss: 0.98; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.61
Batch: 520; loss: 0.96; acc: 0.73
Batch: 540; loss: 0.93; acc: 0.78
Batch: 560; loss: 1.04; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.66
Batch: 600; loss: 1.21; acc: 0.64
Batch: 620; loss: 1.02; acc: 0.64
Batch: 640; loss: 0.98; acc: 0.64
Batch: 660; loss: 1.17; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.61
Batch: 720; loss: 1.24; acc: 0.58
Batch: 740; loss: 0.84; acc: 0.75
Batch: 760; loss: 1.09; acc: 0.58
Batch: 780; loss: 0.89; acc: 0.7
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.59
Batch: 20; loss: 1.31; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.64
Val Epoch over. val_loss: 1.0110311990330934; val_accuracy: 0.6732683121019108 

The current subspace-distance is: 7.374055712716654e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.59
Batch: 40; loss: 1.06; acc: 0.67
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 1.18; acc: 0.58
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.17; acc: 0.72
Batch: 180; loss: 1.12; acc: 0.64
Batch: 200; loss: 1.5; acc: 0.61
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 0.99; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.06; acc: 0.62
Batch: 320; loss: 1.21; acc: 0.66
Batch: 340; loss: 1.22; acc: 0.69
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.03; acc: 0.64
Batch: 400; loss: 0.99; acc: 0.62
Batch: 420; loss: 1.3; acc: 0.55
Batch: 440; loss: 0.87; acc: 0.8
Batch: 460; loss: 1.0; acc: 0.73
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 0.8; acc: 0.75
Batch: 520; loss: 1.21; acc: 0.56
Batch: 540; loss: 1.09; acc: 0.67
Batch: 560; loss: 1.21; acc: 0.55
Batch: 580; loss: 1.04; acc: 0.67
Batch: 600; loss: 1.13; acc: 0.58
Batch: 620; loss: 1.36; acc: 0.58
Batch: 640; loss: 0.96; acc: 0.69
Batch: 660; loss: 1.41; acc: 0.48
Batch: 680; loss: 0.92; acc: 0.7
Batch: 700; loss: 1.34; acc: 0.59
Batch: 720; loss: 1.5; acc: 0.61
Batch: 740; loss: 1.01; acc: 0.7
Batch: 760; loss: 1.49; acc: 0.5
Batch: 780; loss: 0.95; acc: 0.62
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.58
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.64
Batch: 80; loss: 0.77; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.69
Val Epoch over. val_loss: 1.0111761468990592; val_accuracy: 0.6711783439490446 

The current subspace-distance is: 7.557103526778519e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.18; acc: 0.55
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.87; acc: 0.67
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.66
Batch: 100; loss: 0.96; acc: 0.73
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.66
Batch: 160; loss: 1.2; acc: 0.64
Batch: 180; loss: 1.06; acc: 0.61
Batch: 200; loss: 1.0; acc: 0.67
Batch: 220; loss: 1.29; acc: 0.48
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 1.27; acc: 0.64
Batch: 320; loss: 0.91; acc: 0.67
Batch: 340; loss: 1.07; acc: 0.62
Batch: 360; loss: 1.02; acc: 0.67
Batch: 380; loss: 1.02; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.61
Batch: 420; loss: 1.05; acc: 0.66
Batch: 440; loss: 1.34; acc: 0.66
Batch: 460; loss: 1.1; acc: 0.67
Batch: 480; loss: 1.0; acc: 0.62
Batch: 500; loss: 0.97; acc: 0.7
Batch: 520; loss: 0.96; acc: 0.75
Batch: 540; loss: 1.1; acc: 0.62
Batch: 560; loss: 1.06; acc: 0.7
Batch: 580; loss: 1.11; acc: 0.64
Batch: 600; loss: 0.91; acc: 0.69
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.56
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 0.91; acc: 0.69
Batch: 740; loss: 1.0; acc: 0.62
Batch: 760; loss: 1.09; acc: 0.69
Batch: 780; loss: 1.01; acc: 0.69
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.56
Batch: 140; loss: 1.0; acc: 0.66
Val Epoch over. val_loss: 1.016796139395161; val_accuracy: 0.675656847133758 

The current subspace-distance is: 7.798552542226389e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 1.14; acc: 0.62
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.18; acc: 0.62
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 0.9; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.62
Batch: 180; loss: 0.96; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.75
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 1.13; acc: 0.64
Batch: 260; loss: 1.08; acc: 0.66
Batch: 280; loss: 0.84; acc: 0.66
Batch: 300; loss: 1.33; acc: 0.53
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 1.13; acc: 0.66
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 1.1; acc: 0.64
Batch: 400; loss: 1.24; acc: 0.59
Batch: 420; loss: 1.13; acc: 0.52
Batch: 440; loss: 1.09; acc: 0.64
Batch: 460; loss: 1.12; acc: 0.61
Batch: 480; loss: 1.1; acc: 0.56
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 1.18; acc: 0.56
Batch: 540; loss: 0.84; acc: 0.69
Batch: 560; loss: 1.36; acc: 0.58
Batch: 580; loss: 1.13; acc: 0.61
Batch: 600; loss: 1.03; acc: 0.64
Batch: 620; loss: 1.05; acc: 0.69
Batch: 640; loss: 1.1; acc: 0.59
Batch: 660; loss: 1.16; acc: 0.59
Batch: 680; loss: 0.84; acc: 0.73
Batch: 700; loss: 0.96; acc: 0.73
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 1.14; acc: 0.61
Batch: 760; loss: 0.97; acc: 0.69
Batch: 780; loss: 1.11; acc: 0.56
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.31; acc: 0.58
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 1.02; acc: 0.59
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 1.0; acc: 0.66
Val Epoch over. val_loss: 1.014080250718791; val_accuracy: 0.6732683121019108 

The current subspace-distance is: 8.032857294892892e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.0; acc: 0.73
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 1.42; acc: 0.5
Batch: 120; loss: 0.87; acc: 0.66
Batch: 140; loss: 1.05; acc: 0.66
Batch: 160; loss: 0.98; acc: 0.62
Batch: 180; loss: 1.26; acc: 0.59
Batch: 200; loss: 0.98; acc: 0.64
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 0.82; acc: 0.73
Batch: 300; loss: 0.98; acc: 0.62
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 0.87; acc: 0.69
Batch: 360; loss: 1.14; acc: 0.62
Batch: 380; loss: 1.06; acc: 0.66
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 1.27; acc: 0.59
Batch: 440; loss: 1.15; acc: 0.59
Batch: 460; loss: 1.08; acc: 0.7
Batch: 480; loss: 1.05; acc: 0.67
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.19; acc: 0.58
Batch: 540; loss: 1.29; acc: 0.59
Batch: 560; loss: 1.28; acc: 0.61
Batch: 580; loss: 0.75; acc: 0.73
Batch: 600; loss: 0.95; acc: 0.69
Batch: 620; loss: 1.0; acc: 0.73
Batch: 640; loss: 1.07; acc: 0.61
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 0.96; acc: 0.77
Batch: 720; loss: 1.11; acc: 0.58
Batch: 740; loss: 1.12; acc: 0.66
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 0.98; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.59
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.66
Val Epoch over. val_loss: 1.010362471744513; val_accuracy: 0.6735668789808917 

The current subspace-distance is: 8.34664679132402e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.95; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.67
Batch: 40; loss: 1.07; acc: 0.61
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 0.98; acc: 0.73
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 1.15; acc: 0.62
Batch: 180; loss: 1.07; acc: 0.62
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 0.98; acc: 0.62
Batch: 240; loss: 1.12; acc: 0.59
Batch: 260; loss: 1.05; acc: 0.58
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 1.24; acc: 0.62
Batch: 320; loss: 0.96; acc: 0.64
Batch: 340; loss: 0.97; acc: 0.66
Batch: 360; loss: 1.3; acc: 0.66
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.23; acc: 0.58
Batch: 420; loss: 0.91; acc: 0.72
Batch: 440; loss: 1.27; acc: 0.56
Batch: 460; loss: 1.28; acc: 0.61
Batch: 480; loss: 1.16; acc: 0.55
Batch: 500; loss: 1.13; acc: 0.58
Batch: 520; loss: 1.03; acc: 0.64
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 0.97; acc: 0.67
Batch: 580; loss: 1.11; acc: 0.56
Batch: 600; loss: 1.13; acc: 0.61
Batch: 620; loss: 1.16; acc: 0.62
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.01; acc: 0.72
Batch: 680; loss: 1.05; acc: 0.77
Batch: 700; loss: 1.39; acc: 0.53
Batch: 720; loss: 1.09; acc: 0.67
Batch: 740; loss: 1.25; acc: 0.59
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.24; acc: 0.53
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.67
Val Epoch over. val_loss: 1.011155538118569; val_accuracy: 0.6736664012738853 

The current subspace-distance is: 8.647693175589666e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.08; acc: 0.61
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 1.08; acc: 0.66
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.03; acc: 0.61
Batch: 140; loss: 1.19; acc: 0.58
Batch: 160; loss: 1.16; acc: 0.66
Batch: 180; loss: 1.01; acc: 0.61
Batch: 200; loss: 0.86; acc: 0.7
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 0.9; acc: 0.69
Batch: 260; loss: 1.1; acc: 0.58
Batch: 280; loss: 0.97; acc: 0.73
Batch: 300; loss: 0.94; acc: 0.67
Batch: 320; loss: 1.01; acc: 0.62
Batch: 340; loss: 0.88; acc: 0.73
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 0.95; acc: 0.7
Batch: 400; loss: 1.01; acc: 0.7
Batch: 420; loss: 1.12; acc: 0.58
Batch: 440; loss: 1.2; acc: 0.61
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.49; acc: 0.52
Batch: 520; loss: 0.99; acc: 0.7
Batch: 540; loss: 0.98; acc: 0.67
Batch: 560; loss: 1.13; acc: 0.61
Batch: 580; loss: 1.08; acc: 0.66
Batch: 600; loss: 1.09; acc: 0.66
Batch: 620; loss: 1.09; acc: 0.66
Batch: 640; loss: 0.97; acc: 0.67
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.36; acc: 0.64
Batch: 700; loss: 1.07; acc: 0.67
Batch: 720; loss: 0.95; acc: 0.67
Batch: 740; loss: 0.98; acc: 0.69
Batch: 760; loss: 0.95; acc: 0.72
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.73
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.96; acc: 0.69
Val Epoch over. val_loss: 1.0088783373498613; val_accuracy: 0.6747611464968153 

The current subspace-distance is: 8.788050035946071e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.34; acc: 0.5
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 1.03; acc: 0.69
Batch: 60; loss: 1.46; acc: 0.53
Batch: 80; loss: 1.26; acc: 0.55
Batch: 100; loss: 0.81; acc: 0.72
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.59
Batch: 180; loss: 1.01; acc: 0.69
Batch: 200; loss: 1.07; acc: 0.61
Batch: 220; loss: 1.01; acc: 0.67
Batch: 240; loss: 0.83; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.69
Batch: 280; loss: 1.18; acc: 0.59
Batch: 300; loss: 0.89; acc: 0.73
Batch: 320; loss: 1.01; acc: 0.69
Batch: 340; loss: 1.1; acc: 0.69
Batch: 360; loss: 1.12; acc: 0.64
Batch: 380; loss: 1.37; acc: 0.58
Batch: 400; loss: 1.09; acc: 0.59
Batch: 420; loss: 0.99; acc: 0.64
Batch: 440; loss: 1.1; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.58
Batch: 500; loss: 0.89; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.62
Batch: 540; loss: 1.01; acc: 0.62
Batch: 560; loss: 1.12; acc: 0.61
Batch: 580; loss: 1.3; acc: 0.62
Batch: 600; loss: 1.29; acc: 0.62
Batch: 620; loss: 1.09; acc: 0.59
Batch: 640; loss: 1.03; acc: 0.66
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.55
Batch: 700; loss: 1.52; acc: 0.53
Batch: 720; loss: 1.12; acc: 0.61
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 1.04; acc: 0.7
Batch: 780; loss: 1.0; acc: 0.69
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.64
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.58
Batch: 140; loss: 0.96; acc: 0.67
Val Epoch over. val_loss: 1.0085615846002178; val_accuracy: 0.6751592356687898 

The current subspace-distance is: 8.806508412817493e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.21; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 1.13; acc: 0.61
Batch: 60; loss: 0.85; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.64
Batch: 100; loss: 1.35; acc: 0.59
Batch: 120; loss: 1.24; acc: 0.59
Batch: 140; loss: 1.32; acc: 0.55
Batch: 160; loss: 1.05; acc: 0.61
Batch: 180; loss: 0.98; acc: 0.64
Batch: 200; loss: 1.3; acc: 0.55
Batch: 220; loss: 1.19; acc: 0.66
Batch: 240; loss: 1.09; acc: 0.61
Batch: 260; loss: 1.13; acc: 0.61
Batch: 280; loss: 0.87; acc: 0.69
Batch: 300; loss: 1.11; acc: 0.72
Batch: 320; loss: 1.09; acc: 0.59
Batch: 340; loss: 1.09; acc: 0.7
Batch: 360; loss: 0.95; acc: 0.69
Batch: 380; loss: 0.91; acc: 0.66
Batch: 400; loss: 1.04; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.62
Batch: 440; loss: 1.22; acc: 0.58
Batch: 460; loss: 1.04; acc: 0.62
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 1.12; acc: 0.62
Batch: 520; loss: 1.27; acc: 0.61
Batch: 540; loss: 1.11; acc: 0.61
Batch: 560; loss: 1.04; acc: 0.66
Batch: 580; loss: 1.31; acc: 0.53
Batch: 600; loss: 1.48; acc: 0.53
Batch: 620; loss: 1.34; acc: 0.61
Batch: 640; loss: 1.12; acc: 0.67
Batch: 660; loss: 1.15; acc: 0.59
Batch: 680; loss: 1.17; acc: 0.64
Batch: 700; loss: 1.02; acc: 0.66
Batch: 720; loss: 1.01; acc: 0.59
Batch: 740; loss: 1.2; acc: 0.62
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.04; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.99; acc: 0.64
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.56
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0102881762632139; val_accuracy: 0.6741640127388535 

The current subspace-distance is: 9.305724233854562e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 1.31; acc: 0.55
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 1.01; acc: 0.64
Batch: 100; loss: 0.96; acc: 0.62
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.64
Batch: 160; loss: 1.11; acc: 0.59
Batch: 180; loss: 0.93; acc: 0.73
Batch: 200; loss: 0.9; acc: 0.66
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.56
Batch: 280; loss: 1.05; acc: 0.64
Batch: 300; loss: 0.9; acc: 0.75
Batch: 320; loss: 1.06; acc: 0.62
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 1.03; acc: 0.66
Batch: 380; loss: 1.06; acc: 0.67
Batch: 400; loss: 0.98; acc: 0.59
Batch: 420; loss: 0.96; acc: 0.67
Batch: 440; loss: 1.04; acc: 0.72
Batch: 460; loss: 1.33; acc: 0.53
Batch: 480; loss: 0.93; acc: 0.66
Batch: 500; loss: 1.1; acc: 0.61
Batch: 520; loss: 0.86; acc: 0.75
Batch: 540; loss: 1.28; acc: 0.69
Batch: 560; loss: 1.22; acc: 0.62
Batch: 580; loss: 1.22; acc: 0.58
Batch: 600; loss: 1.1; acc: 0.67
Batch: 620; loss: 1.16; acc: 0.59
Batch: 640; loss: 1.05; acc: 0.64
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 0.9; acc: 0.72
Batch: 700; loss: 0.98; acc: 0.69
Batch: 720; loss: 1.1; acc: 0.66
Batch: 740; loss: 1.07; acc: 0.61
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 1.07; acc: 0.66
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 0.96; acc: 0.66
Val Epoch over. val_loss: 1.008766747584009; val_accuracy: 0.6754578025477707 

The current subspace-distance is: 9.373937064083293e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.06; acc: 0.62
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.66
Batch: 160; loss: 1.2; acc: 0.58
Batch: 180; loss: 1.39; acc: 0.56
Batch: 200; loss: 0.98; acc: 0.64
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 1.25; acc: 0.59
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 0.95; acc: 0.69
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 1.28; acc: 0.55
Batch: 340; loss: 0.93; acc: 0.66
Batch: 360; loss: 0.92; acc: 0.75
Batch: 380; loss: 0.97; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.59
Batch: 420; loss: 0.99; acc: 0.73
Batch: 440; loss: 1.09; acc: 0.67
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 0.86; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.69
Batch: 520; loss: 1.18; acc: 0.59
Batch: 540; loss: 1.04; acc: 0.7
Batch: 560; loss: 1.0; acc: 0.72
Batch: 580; loss: 1.35; acc: 0.59
Batch: 600; loss: 1.13; acc: 0.69
Batch: 620; loss: 0.89; acc: 0.67
Batch: 640; loss: 1.35; acc: 0.53
Batch: 660; loss: 1.18; acc: 0.59
Batch: 680; loss: 1.09; acc: 0.64
Batch: 700; loss: 1.02; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.59
Batch: 740; loss: 0.98; acc: 0.64
Batch: 760; loss: 1.19; acc: 0.73
Batch: 780; loss: 1.13; acc: 0.67
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.1; acc: 0.59
Batch: 20; loss: 1.3; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.66
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.58
Batch: 140; loss: 0.98; acc: 0.66
Val Epoch over. val_loss: 1.0123007255754652; val_accuracy: 0.6749601910828026 

The current subspace-distance is: 9.400689305039123e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 1.13; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.62
Batch: 80; loss: 0.94; acc: 0.72
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 1.03; acc: 0.67
Batch: 180; loss: 0.97; acc: 0.67
Batch: 200; loss: 0.74; acc: 0.75
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 0.89; acc: 0.77
Batch: 260; loss: 1.0; acc: 0.69
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 0.81; acc: 0.7
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.07; acc: 0.66
Batch: 380; loss: 0.91; acc: 0.69
Batch: 400; loss: 0.97; acc: 0.59
Batch: 420; loss: 1.24; acc: 0.67
Batch: 440; loss: 1.2; acc: 0.67
Batch: 460; loss: 0.95; acc: 0.73
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.18; acc: 0.56
Batch: 520; loss: 0.98; acc: 0.72
Batch: 540; loss: 0.96; acc: 0.66
Batch: 560; loss: 0.82; acc: 0.77
Batch: 580; loss: 0.91; acc: 0.73
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 1.24; acc: 0.67
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.66
Batch: 700; loss: 1.15; acc: 0.58
Batch: 720; loss: 0.81; acc: 0.78
Batch: 740; loss: 1.11; acc: 0.56
Batch: 760; loss: 1.26; acc: 0.53
Batch: 780; loss: 1.02; acc: 0.72
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.58
Batch: 140; loss: 0.98; acc: 0.66
Val Epoch over. val_loss: 1.0113956381560891; val_accuracy: 0.674562101910828 

The current subspace-distance is: 9.67642044997774e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.0; acc: 0.61
Batch: 20; loss: 1.49; acc: 0.55
Batch: 40; loss: 1.15; acc: 0.62
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 1.01; acc: 0.62
Batch: 120; loss: 1.07; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.3; acc: 0.53
Batch: 180; loss: 0.95; acc: 0.73
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 0.9; acc: 0.62
Batch: 260; loss: 1.01; acc: 0.67
Batch: 280; loss: 1.02; acc: 0.7
Batch: 300; loss: 1.26; acc: 0.58
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.09; acc: 0.69
Batch: 360; loss: 0.93; acc: 0.72
Batch: 380; loss: 1.17; acc: 0.58
Batch: 400; loss: 0.88; acc: 0.72
Batch: 420; loss: 1.25; acc: 0.56
Batch: 440; loss: 1.13; acc: 0.66
Batch: 460; loss: 1.04; acc: 0.61
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 1.24; acc: 0.56
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 1.0; acc: 0.67
Batch: 560; loss: 1.36; acc: 0.64
Batch: 580; loss: 1.02; acc: 0.64
Batch: 600; loss: 0.97; acc: 0.64
Batch: 620; loss: 0.99; acc: 0.67
Batch: 640; loss: 1.3; acc: 0.61
Batch: 660; loss: 1.04; acc: 0.58
Batch: 680; loss: 1.11; acc: 0.67
Batch: 700; loss: 0.88; acc: 0.72
Batch: 720; loss: 1.09; acc: 0.61
Batch: 740; loss: 1.05; acc: 0.64
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 1.07; acc: 0.66
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0109096131506998; val_accuracy: 0.6721735668789809 

The current subspace-distance is: 9.616936586098745e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.05; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.26; acc: 0.52
Batch: 60; loss: 1.15; acc: 0.55
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 1.11; acc: 0.64
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.04; acc: 0.73
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.25; acc: 0.55
Batch: 260; loss: 1.1; acc: 0.62
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.17; acc: 0.59
Batch: 320; loss: 1.04; acc: 0.7
Batch: 340; loss: 1.1; acc: 0.67
Batch: 360; loss: 1.15; acc: 0.62
Batch: 380; loss: 0.99; acc: 0.62
Batch: 400; loss: 1.0; acc: 0.75
Batch: 420; loss: 1.01; acc: 0.69
Batch: 440; loss: 0.9; acc: 0.69
Batch: 460; loss: 0.93; acc: 0.72
Batch: 480; loss: 0.72; acc: 0.8
Batch: 500; loss: 1.0; acc: 0.75
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 1.08; acc: 0.72
Batch: 560; loss: 1.1; acc: 0.67
Batch: 580; loss: 1.13; acc: 0.69
Batch: 600; loss: 1.05; acc: 0.61
Batch: 620; loss: 1.32; acc: 0.59
Batch: 640; loss: 1.03; acc: 0.62
Batch: 660; loss: 1.14; acc: 0.58
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 1.05; acc: 0.72
Batch: 740; loss: 1.24; acc: 0.62
Batch: 760; loss: 1.01; acc: 0.69
Batch: 780; loss: 1.04; acc: 0.66
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.59
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.64
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 0.99; acc: 0.67
Val Epoch over. val_loss: 1.0116517976590782; val_accuracy: 0.6712778662420382 

The current subspace-distance is: 9.583110659150407e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.19; acc: 0.62
Batch: 20; loss: 0.88; acc: 0.75
Batch: 40; loss: 1.48; acc: 0.61
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 0.88; acc: 0.73
Batch: 100; loss: 1.07; acc: 0.64
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 1.3; acc: 0.59
Batch: 160; loss: 1.16; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.61
Batch: 200; loss: 1.12; acc: 0.62
Batch: 220; loss: 1.15; acc: 0.59
Batch: 240; loss: 0.99; acc: 0.69
Batch: 260; loss: 0.98; acc: 0.59
Batch: 280; loss: 1.04; acc: 0.66
Batch: 300; loss: 0.86; acc: 0.66
Batch: 320; loss: 1.05; acc: 0.72
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 0.96; acc: 0.64
Batch: 380; loss: 1.26; acc: 0.64
Batch: 400; loss: 1.09; acc: 0.69
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 0.85; acc: 0.73
Batch: 460; loss: 0.97; acc: 0.73
Batch: 480; loss: 1.2; acc: 0.59
Batch: 500; loss: 1.12; acc: 0.69
Batch: 520; loss: 1.13; acc: 0.67
Batch: 540; loss: 1.04; acc: 0.7
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.93; acc: 0.73
Batch: 600; loss: 0.99; acc: 0.72
Batch: 620; loss: 1.03; acc: 0.59
Batch: 640; loss: 1.07; acc: 0.72
Batch: 660; loss: 1.27; acc: 0.64
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.13; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.59
Batch: 740; loss: 1.09; acc: 0.64
Batch: 760; loss: 1.19; acc: 0.59
Batch: 780; loss: 0.84; acc: 0.69
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0093879187183015; val_accuracy: 0.67296974522293 

The current subspace-distance is: 9.440745634492487e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.32; acc: 0.61
Batch: 20; loss: 1.06; acc: 0.61
Batch: 40; loss: 1.19; acc: 0.61
Batch: 60; loss: 1.38; acc: 0.58
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.04; acc: 0.64
Batch: 160; loss: 0.87; acc: 0.67
Batch: 180; loss: 0.93; acc: 0.67
Batch: 200; loss: 0.89; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.72
Batch: 260; loss: 1.18; acc: 0.66
Batch: 280; loss: 0.94; acc: 0.7
Batch: 300; loss: 1.03; acc: 0.67
Batch: 320; loss: 1.21; acc: 0.73
Batch: 340; loss: 1.25; acc: 0.52
Batch: 360; loss: 1.13; acc: 0.72
Batch: 380; loss: 0.8; acc: 0.77
Batch: 400; loss: 0.8; acc: 0.7
Batch: 420; loss: 0.99; acc: 0.69
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.16; acc: 0.62
Batch: 480; loss: 1.09; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.59
Batch: 520; loss: 1.04; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.62
Batch: 560; loss: 0.91; acc: 0.69
Batch: 580; loss: 0.89; acc: 0.75
Batch: 600; loss: 0.96; acc: 0.7
Batch: 620; loss: 1.24; acc: 0.62
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 1.06; acc: 0.67
Batch: 680; loss: 0.84; acc: 0.73
Batch: 700; loss: 0.97; acc: 0.67
Batch: 720; loss: 0.96; acc: 0.75
Batch: 740; loss: 1.19; acc: 0.59
Batch: 760; loss: 0.92; acc: 0.69
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.59
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.96; acc: 0.66
Val Epoch over. val_loss: 1.0090304776361794; val_accuracy: 0.6753582802547771 

The current subspace-distance is: 9.621505159884691e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.67
Batch: 40; loss: 1.06; acc: 0.56
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.09; acc: 0.62
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 1.24; acc: 0.61
Batch: 160; loss: 1.05; acc: 0.62
Batch: 180; loss: 0.97; acc: 0.69
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 1.37; acc: 0.66
Batch: 240; loss: 1.07; acc: 0.64
Batch: 260; loss: 0.95; acc: 0.75
Batch: 280; loss: 0.99; acc: 0.75
Batch: 300; loss: 1.13; acc: 0.64
Batch: 320; loss: 1.19; acc: 0.66
Batch: 340; loss: 1.22; acc: 0.67
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 1.04; acc: 0.73
Batch: 400; loss: 1.27; acc: 0.61
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 1.09; acc: 0.59
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.58
Batch: 540; loss: 1.06; acc: 0.61
Batch: 560; loss: 1.02; acc: 0.66
Batch: 580; loss: 1.24; acc: 0.61
Batch: 600; loss: 1.17; acc: 0.61
Batch: 620; loss: 1.02; acc: 0.73
Batch: 640; loss: 0.97; acc: 0.77
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 0.88; acc: 0.73
Batch: 700; loss: 0.92; acc: 0.67
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.37; acc: 0.52
Batch: 780; loss: 1.29; acc: 0.61
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.96; acc: 0.66
Val Epoch over. val_loss: 1.0085503413419055; val_accuracy: 0.6747611464968153 

The current subspace-distance is: 9.864520689006895e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.27; acc: 0.58
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 1.25; acc: 0.66
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.67
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 1.2; acc: 0.61
Batch: 160; loss: 1.02; acc: 0.62
Batch: 180; loss: 1.18; acc: 0.66
Batch: 200; loss: 1.24; acc: 0.62
Batch: 220; loss: 1.08; acc: 0.64
Batch: 240; loss: 0.8; acc: 0.73
Batch: 260; loss: 0.96; acc: 0.7
Batch: 280; loss: 1.3; acc: 0.59
Batch: 300; loss: 1.16; acc: 0.64
Batch: 320; loss: 1.21; acc: 0.61
Batch: 340; loss: 1.34; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.61
Batch: 400; loss: 1.48; acc: 0.61
Batch: 420; loss: 1.17; acc: 0.59
Batch: 440; loss: 0.9; acc: 0.67
Batch: 460; loss: 1.06; acc: 0.64
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.04; acc: 0.66
Batch: 540; loss: 1.0; acc: 0.64
Batch: 560; loss: 1.05; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.67
Batch: 600; loss: 1.33; acc: 0.64
Batch: 620; loss: 1.38; acc: 0.55
Batch: 640; loss: 1.07; acc: 0.73
Batch: 660; loss: 0.99; acc: 0.72
Batch: 680; loss: 0.85; acc: 0.67
Batch: 700; loss: 1.32; acc: 0.5
Batch: 720; loss: 0.91; acc: 0.69
Batch: 740; loss: 1.21; acc: 0.59
Batch: 760; loss: 0.97; acc: 0.69
Batch: 780; loss: 1.21; acc: 0.61
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0094302285249066; val_accuracy: 0.6747611464968153 

The current subspace-distance is: 0.00010191622277488932 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 1.05; acc: 0.58
Batch: 40; loss: 1.35; acc: 0.56
Batch: 60; loss: 1.28; acc: 0.52
Batch: 80; loss: 1.09; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.61
Batch: 160; loss: 1.04; acc: 0.64
Batch: 180; loss: 1.21; acc: 0.66
Batch: 200; loss: 1.12; acc: 0.64
Batch: 220; loss: 1.07; acc: 0.66
Batch: 240; loss: 1.09; acc: 0.66
Batch: 260; loss: 1.26; acc: 0.69
Batch: 280; loss: 0.9; acc: 0.66
Batch: 300; loss: 1.16; acc: 0.7
Batch: 320; loss: 0.86; acc: 0.72
Batch: 340; loss: 0.97; acc: 0.75
Batch: 360; loss: 0.96; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.67
Batch: 400; loss: 0.95; acc: 0.69
Batch: 420; loss: 0.85; acc: 0.77
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.99; acc: 0.69
Batch: 520; loss: 0.96; acc: 0.77
Batch: 540; loss: 1.21; acc: 0.64
Batch: 560; loss: 0.97; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.56
Batch: 600; loss: 1.15; acc: 0.53
Batch: 620; loss: 1.38; acc: 0.52
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.23; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.64
Batch: 720; loss: 0.97; acc: 0.67
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.67
Batch: 780; loss: 1.06; acc: 0.67
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.66
Val Epoch over. val_loss: 1.0095609255657074; val_accuracy: 0.673765923566879 

The current subspace-distance is: 0.00010456104064360261 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.2; acc: 0.56
Batch: 40; loss: 1.39; acc: 0.53
Batch: 60; loss: 1.14; acc: 0.64
Batch: 80; loss: 0.97; acc: 0.66
Batch: 100; loss: 1.21; acc: 0.59
Batch: 120; loss: 1.01; acc: 0.67
Batch: 140; loss: 1.03; acc: 0.7
Batch: 160; loss: 1.03; acc: 0.7
Batch: 180; loss: 1.02; acc: 0.64
Batch: 200; loss: 1.12; acc: 0.56
Batch: 220; loss: 1.16; acc: 0.58
Batch: 240; loss: 1.12; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.66
Batch: 280; loss: 1.3; acc: 0.56
Batch: 300; loss: 0.99; acc: 0.7
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 1.12; acc: 0.62
Batch: 360; loss: 1.07; acc: 0.64
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 1.3; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.58
Batch: 440; loss: 1.4; acc: 0.59
Batch: 460; loss: 1.05; acc: 0.64
Batch: 480; loss: 1.24; acc: 0.64
Batch: 500; loss: 0.95; acc: 0.69
Batch: 520; loss: 0.86; acc: 0.7
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.97; acc: 0.75
Batch: 580; loss: 0.94; acc: 0.62
Batch: 600; loss: 1.0; acc: 0.64
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 1.05; acc: 0.67
Batch: 660; loss: 0.96; acc: 0.72
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.16; acc: 0.69
Batch: 740; loss: 0.91; acc: 0.77
Batch: 760; loss: 1.09; acc: 0.64
Batch: 780; loss: 0.93; acc: 0.7
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.62
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0102953861473472; val_accuracy: 0.6749601910828026 

The current subspace-distance is: 0.00010953331366181374 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.93; acc: 0.69
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.78
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.59
Batch: 180; loss: 1.06; acc: 0.61
Batch: 200; loss: 1.29; acc: 0.64
Batch: 220; loss: 1.13; acc: 0.58
Batch: 240; loss: 1.14; acc: 0.59
Batch: 260; loss: 0.86; acc: 0.69
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 1.0; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.7
Batch: 340; loss: 0.91; acc: 0.72
Batch: 360; loss: 0.99; acc: 0.73
Batch: 380; loss: 1.34; acc: 0.52
Batch: 400; loss: 1.03; acc: 0.67
Batch: 420; loss: 1.17; acc: 0.62
Batch: 440; loss: 1.06; acc: 0.67
Batch: 460; loss: 1.01; acc: 0.67
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 0.96; acc: 0.67
Batch: 520; loss: 1.06; acc: 0.64
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.52
Batch: 580; loss: 0.92; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 0.96; acc: 0.66
Batch: 660; loss: 0.95; acc: 0.7
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.11; acc: 0.64
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 0.94; acc: 0.61
Batch: 760; loss: 1.07; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.78
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.66
Val Epoch over. val_loss: 1.009957466915155; val_accuracy: 0.6746616242038217 

The current subspace-distance is: 0.00010828582890098915 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.19; acc: 0.55
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 1.33; acc: 0.58
Batch: 80; loss: 1.18; acc: 0.58
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 1.16; acc: 0.62
Batch: 160; loss: 1.02; acc: 0.73
Batch: 180; loss: 1.34; acc: 0.61
Batch: 200; loss: 0.97; acc: 0.61
Batch: 220; loss: 1.36; acc: 0.55
Batch: 240; loss: 1.3; acc: 0.61
Batch: 260; loss: 1.02; acc: 0.69
Batch: 280; loss: 0.96; acc: 0.7
Batch: 300; loss: 1.05; acc: 0.67
Batch: 320; loss: 0.89; acc: 0.75
Batch: 340; loss: 1.16; acc: 0.69
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 1.19; acc: 0.58
Batch: 400; loss: 1.02; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.66
Batch: 440; loss: 0.83; acc: 0.72
Batch: 460; loss: 0.86; acc: 0.75
Batch: 480; loss: 1.09; acc: 0.69
Batch: 500; loss: 1.02; acc: 0.64
Batch: 520; loss: 1.21; acc: 0.58
Batch: 540; loss: 0.92; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.73
Batch: 580; loss: 1.22; acc: 0.62
Batch: 600; loss: 0.87; acc: 0.75
Batch: 620; loss: 0.93; acc: 0.73
Batch: 640; loss: 0.87; acc: 0.66
Batch: 660; loss: 1.02; acc: 0.61
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 1.04; acc: 0.58
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 0.88; acc: 0.67
Batch: 760; loss: 1.13; acc: 0.58
Batch: 780; loss: 1.32; acc: 0.66
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 1.0; acc: 0.62
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0093365247082557; val_accuracy: 0.6742635350318471 

The current subspace-distance is: 0.00011053671914851293 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.62
Batch: 180; loss: 1.1; acc: 0.62
Batch: 200; loss: 1.05; acc: 0.72
Batch: 220; loss: 1.1; acc: 0.75
Batch: 240; loss: 1.21; acc: 0.55
Batch: 260; loss: 1.01; acc: 0.72
Batch: 280; loss: 1.02; acc: 0.59
Batch: 300; loss: 1.08; acc: 0.66
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 1.42; acc: 0.55
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 1.33; acc: 0.62
Batch: 400; loss: 0.94; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.67
Batch: 440; loss: 0.82; acc: 0.7
Batch: 460; loss: 0.88; acc: 0.66
Batch: 480; loss: 1.0; acc: 0.67
Batch: 500; loss: 1.26; acc: 0.62
Batch: 520; loss: 0.96; acc: 0.73
Batch: 540; loss: 1.21; acc: 0.59
Batch: 560; loss: 1.07; acc: 0.7
Batch: 580; loss: 1.2; acc: 0.62
Batch: 600; loss: 0.89; acc: 0.69
Batch: 620; loss: 1.07; acc: 0.7
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 1.01; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.67
Batch: 700; loss: 1.15; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.53
Batch: 740; loss: 1.15; acc: 0.59
Batch: 760; loss: 0.93; acc: 0.67
Batch: 780; loss: 0.93; acc: 0.72
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 0.97; acc: 0.67
Val Epoch over. val_loss: 1.009452087863995; val_accuracy: 0.6738654458598726 

The current subspace-distance is: 0.00010981175000779331 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.96; acc: 0.64
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 0.87; acc: 0.72
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.69
Batch: 160; loss: 0.98; acc: 0.73
Batch: 180; loss: 1.05; acc: 0.67
Batch: 200; loss: 1.12; acc: 0.61
Batch: 220; loss: 1.01; acc: 0.72
Batch: 240; loss: 0.91; acc: 0.72
Batch: 260; loss: 0.82; acc: 0.73
Batch: 280; loss: 1.0; acc: 0.62
Batch: 300; loss: 1.16; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.61
Batch: 340; loss: 1.18; acc: 0.66
Batch: 360; loss: 0.89; acc: 0.67
Batch: 380; loss: 1.18; acc: 0.64
Batch: 400; loss: 1.07; acc: 0.61
Batch: 420; loss: 0.99; acc: 0.66
Batch: 440; loss: 0.83; acc: 0.69
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 1.43; acc: 0.56
Batch: 520; loss: 1.07; acc: 0.66
Batch: 540; loss: 1.18; acc: 0.58
Batch: 560; loss: 0.85; acc: 0.73
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.05; acc: 0.7
Batch: 620; loss: 0.89; acc: 0.69
Batch: 640; loss: 0.97; acc: 0.78
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 0.84; acc: 0.75
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 0.89; acc: 0.67
Batch: 740; loss: 1.13; acc: 0.62
Batch: 760; loss: 0.97; acc: 0.69
Batch: 780; loss: 1.32; acc: 0.52
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.010033900950365; val_accuracy: 0.6742635350318471 

The current subspace-distance is: 0.00011075003567384556 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.18; acc: 0.59
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 0.94; acc: 0.59
Batch: 60; loss: 1.1; acc: 0.58
Batch: 80; loss: 0.83; acc: 0.75
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 0.97; acc: 0.69
Batch: 180; loss: 0.79; acc: 0.77
Batch: 200; loss: 0.87; acc: 0.62
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 0.98; acc: 0.64
Batch: 260; loss: 1.1; acc: 0.62
Batch: 280; loss: 1.35; acc: 0.59
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.02; acc: 0.67
Batch: 340; loss: 0.85; acc: 0.67
Batch: 360; loss: 0.91; acc: 0.7
Batch: 380; loss: 1.15; acc: 0.59
Batch: 400; loss: 0.95; acc: 0.7
Batch: 420; loss: 0.94; acc: 0.72
Batch: 440; loss: 1.05; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.61
Batch: 480; loss: 1.08; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.55
Batch: 520; loss: 1.28; acc: 0.64
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 0.88; acc: 0.64
Batch: 580; loss: 1.21; acc: 0.56
Batch: 600; loss: 0.81; acc: 0.75
Batch: 620; loss: 1.03; acc: 0.67
Batch: 640; loss: 1.03; acc: 0.64
Batch: 660; loss: 1.01; acc: 0.7
Batch: 680; loss: 1.3; acc: 0.66
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 1.17; acc: 0.58
Batch: 740; loss: 0.69; acc: 0.75
Batch: 760; loss: 1.09; acc: 0.66
Batch: 780; loss: 0.97; acc: 0.67
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0093759848813342; val_accuracy: 0.6741640127388535 

The current subspace-distance is: 0.00011446966527728364 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.9; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.72
Batch: 60; loss: 1.06; acc: 0.59
Batch: 80; loss: 1.32; acc: 0.52
Batch: 100; loss: 1.34; acc: 0.53
Batch: 120; loss: 1.06; acc: 0.58
Batch: 140; loss: 0.98; acc: 0.66
Batch: 160; loss: 1.07; acc: 0.62
Batch: 180; loss: 1.23; acc: 0.61
Batch: 200; loss: 1.13; acc: 0.67
Batch: 220; loss: 0.99; acc: 0.66
Batch: 240; loss: 0.9; acc: 0.69
Batch: 260; loss: 0.91; acc: 0.66
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 0.95; acc: 0.69
Batch: 320; loss: 1.32; acc: 0.64
Batch: 340; loss: 1.04; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.59
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.63; acc: 0.44
Batch: 420; loss: 1.15; acc: 0.66
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 1.1; acc: 0.59
Batch: 480; loss: 0.99; acc: 0.67
Batch: 500; loss: 1.15; acc: 0.66
Batch: 520; loss: 1.27; acc: 0.59
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.03; acc: 0.69
Batch: 580; loss: 0.93; acc: 0.69
Batch: 600; loss: 1.2; acc: 0.7
Batch: 620; loss: 0.88; acc: 0.77
Batch: 640; loss: 1.07; acc: 0.73
Batch: 660; loss: 1.12; acc: 0.61
Batch: 680; loss: 1.07; acc: 0.62
Batch: 700; loss: 1.14; acc: 0.64
Batch: 720; loss: 0.99; acc: 0.67
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 0.95; acc: 0.66
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0089194546839235; val_accuracy: 0.6742635350318471 

The current subspace-distance is: 0.00011443018593126908 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.97; acc: 0.72
Batch: 60; loss: 0.94; acc: 0.62
Batch: 80; loss: 0.9; acc: 0.73
Batch: 100; loss: 0.96; acc: 0.69
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.96; acc: 0.62
Batch: 160; loss: 1.26; acc: 0.59
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 1.08; acc: 0.64
Batch: 220; loss: 1.06; acc: 0.69
Batch: 240; loss: 1.02; acc: 0.64
Batch: 260; loss: 1.46; acc: 0.62
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 1.23; acc: 0.61
Batch: 320; loss: 0.95; acc: 0.62
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 1.26; acc: 0.62
Batch: 380; loss: 1.12; acc: 0.58
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 1.05; acc: 0.67
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.98; acc: 0.66
Batch: 500; loss: 1.2; acc: 0.62
Batch: 520; loss: 0.99; acc: 0.69
Batch: 540; loss: 1.11; acc: 0.7
Batch: 560; loss: 1.05; acc: 0.62
Batch: 580; loss: 0.87; acc: 0.77
Batch: 600; loss: 1.07; acc: 0.62
Batch: 620; loss: 1.01; acc: 0.69
Batch: 640; loss: 1.29; acc: 0.62
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 1.06; acc: 0.62
Batch: 700; loss: 1.31; acc: 0.61
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.26; acc: 0.53
Batch: 780; loss: 1.0; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.59
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0092410471788638; val_accuracy: 0.6738654458598726 

The current subspace-distance is: 0.00011786013055825606 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 1.18; acc: 0.56
Batch: 40; loss: 1.06; acc: 0.67
Batch: 60; loss: 0.96; acc: 0.7
Batch: 80; loss: 0.93; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.58
Batch: 120; loss: 1.08; acc: 0.67
Batch: 140; loss: 1.4; acc: 0.66
Batch: 160; loss: 1.12; acc: 0.7
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 0.86; acc: 0.72
Batch: 220; loss: 1.04; acc: 0.64
Batch: 240; loss: 1.12; acc: 0.62
Batch: 260; loss: 0.95; acc: 0.7
Batch: 280; loss: 1.14; acc: 0.62
Batch: 300; loss: 0.93; acc: 0.75
Batch: 320; loss: 0.9; acc: 0.72
Batch: 340; loss: 1.18; acc: 0.59
Batch: 360; loss: 1.03; acc: 0.64
Batch: 380; loss: 1.26; acc: 0.56
Batch: 400; loss: 1.06; acc: 0.72
Batch: 420; loss: 0.92; acc: 0.67
Batch: 440; loss: 1.49; acc: 0.56
Batch: 460; loss: 0.98; acc: 0.64
Batch: 480; loss: 1.01; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 1.11; acc: 0.62
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 0.92; acc: 0.73
Batch: 580; loss: 0.94; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.7
Batch: 620; loss: 1.02; acc: 0.67
Batch: 640; loss: 1.3; acc: 0.58
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.62
Batch: 720; loss: 0.97; acc: 0.7
Batch: 740; loss: 0.94; acc: 0.7
Batch: 760; loss: 1.35; acc: 0.59
Batch: 780; loss: 1.19; acc: 0.61
Train Epoch over. train_loss: 1.07; train_accuracy: 0.66 

Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.61
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.0093291595483282; val_accuracy: 0.6735668789808917 

The current subspace-distance is: 0.00011750846897484735 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 15942
elements in E: 3374250
fraction nonzero: 0.004724605467881751
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.29; acc: 0.06
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.28; acc: 0.17
Batch: 600; loss: 2.27; acc: 0.16
Batch: 620; loss: 2.27; acc: 0.14
Batch: 640; loss: 2.26; acc: 0.2
Batch: 660; loss: 2.26; acc: 0.2
Batch: 680; loss: 2.27; acc: 0.17
Batch: 700; loss: 2.27; acc: 0.09
Batch: 720; loss: 2.24; acc: 0.16
Batch: 740; loss: 2.28; acc: 0.08
Batch: 760; loss: 2.26; acc: 0.12
Batch: 780; loss: 2.24; acc: 0.23
Train Epoch over. train_loss: 2.29; train_accuracy: 0.11 

Batch: 0; loss: 2.26; acc: 0.11
Batch: 20; loss: 2.27; acc: 0.14
Batch: 40; loss: 2.25; acc: 0.16
Batch: 60; loss: 2.25; acc: 0.25
Batch: 80; loss: 2.25; acc: 0.12
Batch: 100; loss: 2.26; acc: 0.17
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.26; acc: 0.14
Val Epoch over. val_loss: 2.2572086328154155; val_accuracy: 0.14261544585987262 

The current subspace-distance is: 4.78402307635406e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.24; acc: 0.17
Batch: 40; loss: 2.24; acc: 0.19
Batch: 60; loss: 2.24; acc: 0.23
Batch: 80; loss: 2.23; acc: 0.22
Batch: 100; loss: 2.23; acc: 0.19
Batch: 120; loss: 2.21; acc: 0.22
Batch: 140; loss: 2.24; acc: 0.12
Batch: 160; loss: 2.2; acc: 0.2
Batch: 180; loss: 2.23; acc: 0.11
Batch: 200; loss: 2.2; acc: 0.2
Batch: 220; loss: 2.19; acc: 0.17
Batch: 240; loss: 2.13; acc: 0.25
Batch: 260; loss: 2.12; acc: 0.22
Batch: 280; loss: 2.11; acc: 0.2
Batch: 300; loss: 2.06; acc: 0.14
Batch: 320; loss: 1.99; acc: 0.3
Batch: 340; loss: 2.01; acc: 0.31
Batch: 360; loss: 1.77; acc: 0.39
Batch: 380; loss: 1.77; acc: 0.39
Batch: 400; loss: 1.68; acc: 0.38
Batch: 420; loss: 1.37; acc: 0.53
Batch: 440; loss: 1.45; acc: 0.52
Batch: 460; loss: 1.27; acc: 0.64
Batch: 480; loss: 1.46; acc: 0.48
Batch: 500; loss: 1.2; acc: 0.53
Batch: 520; loss: 0.99; acc: 0.59
Batch: 540; loss: 1.36; acc: 0.5
Batch: 560; loss: 1.13; acc: 0.58
Batch: 580; loss: 0.92; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.72
Batch: 620; loss: 1.5; acc: 0.52
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 1.1; acc: 0.53
Batch: 680; loss: 0.96; acc: 0.73
Batch: 700; loss: 1.12; acc: 0.62
Batch: 720; loss: 1.01; acc: 0.67
Batch: 740; loss: 1.0; acc: 0.62
Batch: 760; loss: 1.18; acc: 0.66
Batch: 780; loss: 0.8; acc: 0.81
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.38; acc: 0.45
Batch: 20; loss: 1.73; acc: 0.48
Batch: 40; loss: 0.99; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.52
Batch: 80; loss: 1.31; acc: 0.55
Batch: 100; loss: 1.35; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.38
Batch: 140; loss: 1.26; acc: 0.59
Val Epoch over. val_loss: 1.3197910713542038; val_accuracy: 0.5362261146496815 

The current subspace-distance is: 1.2827006685256492e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.58
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 1.01; acc: 0.61
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 0.77; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.69
Batch: 160; loss: 0.92; acc: 0.72
Batch: 180; loss: 1.1; acc: 0.67
Batch: 200; loss: 0.8; acc: 0.77
Batch: 220; loss: 0.91; acc: 0.69
Batch: 240; loss: 0.96; acc: 0.64
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.83; acc: 0.72
Batch: 300; loss: 0.98; acc: 0.69
Batch: 320; loss: 0.79; acc: 0.7
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.72
Batch: 380; loss: 0.96; acc: 0.61
Batch: 400; loss: 0.76; acc: 0.7
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.21; acc: 0.62
Batch: 460; loss: 1.25; acc: 0.58
Batch: 480; loss: 0.86; acc: 0.8
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.9; acc: 0.72
Batch: 540; loss: 0.82; acc: 0.72
Batch: 560; loss: 1.3; acc: 0.58
Batch: 580; loss: 0.96; acc: 0.64
Batch: 600; loss: 0.73; acc: 0.73
Batch: 620; loss: 0.93; acc: 0.66
Batch: 640; loss: 0.96; acc: 0.66
Batch: 660; loss: 0.78; acc: 0.72
Batch: 680; loss: 1.19; acc: 0.61
Batch: 700; loss: 0.95; acc: 0.73
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.84; acc: 0.69
Batch: 780; loss: 0.81; acc: 0.72
Train Epoch over. train_loss: 0.96; train_accuracy: 0.68 

Batch: 0; loss: 0.93; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.7; acc: 0.73
Val Epoch over. val_loss: 0.7927100180061; val_accuracy: 0.7349721337579618 

The current subspace-distance is: 1.942549533850979e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.66
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.72
Batch: 60; loss: 0.9; acc: 0.61
Batch: 80; loss: 1.07; acc: 0.59
Batch: 100; loss: 0.95; acc: 0.69
Batch: 120; loss: 2.03; acc: 0.53
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.67
Batch: 200; loss: 0.78; acc: 0.8
Batch: 220; loss: 0.96; acc: 0.73
Batch: 240; loss: 1.04; acc: 0.61
Batch: 260; loss: 0.81; acc: 0.72
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.89; acc: 0.67
Batch: 320; loss: 0.87; acc: 0.72
Batch: 340; loss: 0.97; acc: 0.69
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.82; acc: 0.73
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.69
Batch: 480; loss: 1.0; acc: 0.58
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.95; acc: 0.67
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 1.12; acc: 0.61
Batch: 640; loss: 0.67; acc: 0.78
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.9; acc: 0.73
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 1.02; acc: 0.7
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.98; acc: 0.73
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.18; acc: 0.72
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.89; acc: 0.67
Val Epoch over. val_loss: 0.8438034017754209; val_accuracy: 0.7156648089171974 

The current subspace-distance is: 2.68194326054072e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.72
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.94; acc: 0.75
Batch: 160; loss: 0.81; acc: 0.73
Batch: 180; loss: 0.77; acc: 0.72
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 1.06; acc: 0.69
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.83; acc: 0.72
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.77
Batch: 360; loss: 1.08; acc: 0.58
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.78; acc: 0.73
Batch: 440; loss: 0.66; acc: 0.77
Batch: 460; loss: 0.94; acc: 0.67
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 1.1; acc: 0.58
Batch: 520; loss: 0.97; acc: 0.72
Batch: 540; loss: 1.02; acc: 0.72
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.81; acc: 0.7
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 1.1; acc: 0.56
Batch: 640; loss: 0.91; acc: 0.78
Batch: 660; loss: 0.92; acc: 0.69
Batch: 680; loss: 0.95; acc: 0.73
Batch: 700; loss: 0.87; acc: 0.73
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.9; acc: 0.66
Batch: 760; loss: 0.81; acc: 0.75
Batch: 780; loss: 0.7; acc: 0.75
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 1.08; acc: 0.61
Batch: 20; loss: 1.85; acc: 0.61
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 1.5; acc: 0.67
Batch: 140; loss: 1.13; acc: 0.67
Val Epoch over. val_loss: 1.0408818256703152; val_accuracy: 0.6752587579617835 

The current subspace-distance is: 3.151441705995239e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 2.25; acc: 0.44
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.72; acc: 0.77
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 0.56; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.88; acc: 0.7
Batch: 260; loss: 0.81; acc: 0.69
Batch: 280; loss: 0.74; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.72
Batch: 320; loss: 1.09; acc: 0.53
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.86; acc: 0.69
Batch: 380; loss: 1.0; acc: 0.67
Batch: 400; loss: 0.87; acc: 0.7
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 0.86; acc: 0.69
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 1.49; acc: 0.56
Batch: 560; loss: 1.05; acc: 0.81
Batch: 580; loss: 0.73; acc: 0.77
Batch: 600; loss: 0.93; acc: 0.61
Batch: 620; loss: 1.28; acc: 0.59
Batch: 640; loss: 0.86; acc: 0.75
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 1.07; acc: 0.59
Batch: 700; loss: 0.77; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.75
Batch: 740; loss: 0.67; acc: 0.78
Batch: 760; loss: 0.73; acc: 0.75
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 1.22; acc: 0.66
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 0.99; acc: 0.69
Batch: 120; loss: 1.45; acc: 0.66
Batch: 140; loss: 1.08; acc: 0.56
Val Epoch over. val_loss: 1.0599827610763015; val_accuracy: 0.6658041401273885 

The current subspace-distance is: 3.722583642229438e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 1.02; acc: 0.59
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.87; acc: 0.67
Batch: 160; loss: 1.77; acc: 0.45
Batch: 180; loss: 1.01; acc: 0.7
Batch: 200; loss: 0.74; acc: 0.78
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 0.62; acc: 0.75
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.57; acc: 0.73
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.56; acc: 0.77
Batch: 360; loss: 1.01; acc: 0.66
Batch: 380; loss: 0.82; acc: 0.73
Batch: 400; loss: 0.78; acc: 0.72
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 1.1; acc: 0.66
Batch: 500; loss: 1.81; acc: 0.58
Batch: 520; loss: 0.6; acc: 0.75
Batch: 540; loss: 0.68; acc: 0.75
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 0.76; acc: 0.75
Batch: 600; loss: 0.89; acc: 0.73
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 0.66; acc: 0.75
Batch: 700; loss: 0.71; acc: 0.7
Batch: 720; loss: 0.94; acc: 0.67
Batch: 740; loss: 0.52; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 1.12; acc: 0.64
Train Epoch over. train_loss: 0.84; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.67
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 0.87; acc: 0.66
Val Epoch over. val_loss: 0.8700862039046683; val_accuracy: 0.7082006369426752 

The current subspace-distance is: 3.9269223634619266e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 1.11; acc: 0.69
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.75
Batch: 120; loss: 0.86; acc: 0.67
Batch: 140; loss: 0.74; acc: 0.7
Batch: 160; loss: 0.6; acc: 0.77
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.86; acc: 0.72
Batch: 220; loss: 0.79; acc: 0.67
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.6; acc: 0.75
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.73
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.82; acc: 0.73
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.9; acc: 0.72
Batch: 460; loss: 0.77; acc: 0.75
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.78
Batch: 520; loss: 1.15; acc: 0.61
Batch: 540; loss: 0.59; acc: 0.73
Batch: 560; loss: 0.98; acc: 0.61
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 1.1; acc: 0.7
Batch: 620; loss: 0.69; acc: 0.75
Batch: 640; loss: 0.75; acc: 0.69
Batch: 660; loss: 0.88; acc: 0.73
Batch: 680; loss: 0.82; acc: 0.73
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.7
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.75
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.77; train_accuracy: 0.75 

Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.51; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.54; acc: 0.78
Val Epoch over. val_loss: 0.893426806683753; val_accuracy: 0.7011345541401274 

The current subspace-distance is: 4.304852700443007e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.72
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.89; acc: 0.7
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.69
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.72
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 1.08; acc: 0.58
Batch: 280; loss: 0.82; acc: 0.75
Batch: 300; loss: 0.71; acc: 0.7
Batch: 320; loss: 0.8; acc: 0.75
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.88; acc: 0.73
Batch: 380; loss: 0.76; acc: 0.73
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 1.01; acc: 0.61
Batch: 440; loss: 0.67; acc: 0.73
Batch: 460; loss: 0.91; acc: 0.72
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.92; acc: 0.72
Batch: 520; loss: 0.7; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.75
Batch: 560; loss: 0.93; acc: 0.73
Batch: 580; loss: 0.79; acc: 0.72
Batch: 600; loss: 0.89; acc: 0.73
Batch: 620; loss: 0.55; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.69
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 0.8; acc: 0.69
Batch: 700; loss: 0.71; acc: 0.75
Batch: 720; loss: 0.9; acc: 0.72
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.66
Batch: 780; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.81; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.61
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.82; acc: 0.7
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.9329175871268959; val_accuracy: 0.6821257961783439 

The current subspace-distance is: 4.6893452235963196e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 0.82; acc: 0.67
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 1.19; acc: 0.69
Batch: 100; loss: 0.8; acc: 0.7
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 0.66; acc: 0.77
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.69
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.85; acc: 0.7
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.96; acc: 0.64
Batch: 300; loss: 0.81; acc: 0.7
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.77
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 1.16; acc: 0.69
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.59; acc: 0.77
Batch: 520; loss: 0.62; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.98; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.8
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 0.5; acc: 0.77
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 1.08; acc: 0.7
Batch: 700; loss: 0.69; acc: 0.73
Batch: 720; loss: 0.78; acc: 0.75
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 1.05; acc: 0.62
Batch: 780; loss: 0.74; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.67
Batch: 140; loss: 0.51; acc: 0.81
Val Epoch over. val_loss: 0.7189854560004678; val_accuracy: 0.7641321656050956 

The current subspace-distance is: 4.958590216119774e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.73
Batch: 20; loss: 0.74; acc: 0.72
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.75; acc: 0.75
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.73
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.82; acc: 0.77
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.64; acc: 0.78
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.78
Batch: 640; loss: 0.56; acc: 0.78
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.81
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.86; acc: 0.7
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.86
Val Epoch over. val_loss: 0.5550421041667841; val_accuracy: 0.8211584394904459 

The current subspace-distance is: 5.497601887327619e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.67; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.77
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.84; acc: 0.77
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.82; acc: 0.72
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.81
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.72; acc: 0.69
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.72
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.77
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.86
Val Epoch over. val_loss: 0.7540591886848401; val_accuracy: 0.7402468152866242 

The current subspace-distance is: 5.6943590607261285e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.9; acc: 0.72
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.73
Batch: 420; loss: 0.78; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.77
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.63; acc: 0.78
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.7
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.53; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.5830670137693927; val_accuracy: 0.8200636942675159 

The current subspace-distance is: 5.875022543477826e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.79; acc: 0.7
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.8
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.9; acc: 0.66
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.79; acc: 0.7
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 0.73; acc: 0.77
Batch: 660; loss: 0.55; acc: 0.77
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.77
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.7
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.5900383373354651; val_accuracy: 0.8119028662420382 

The current subspace-distance is: 6.333010969683528e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.76; acc: 0.73
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.81; acc: 0.72
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.5; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.74; acc: 0.72
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 1.01; acc: 0.7
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.78
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.69
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.6; acc: 0.75
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.73
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5544685131045664; val_accuracy: 0.8267316878980892 

The current subspace-distance is: 6.491442036349326e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.7
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.97; acc: 0.66
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 0.32; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.74; acc: 0.73
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.37; acc: 0.88
Val Epoch over. val_loss: 0.5583194013993451; val_accuracy: 0.8213574840764332 

The current subspace-distance is: 6.767464947188273e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.73
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.8
Batch: 320; loss: 0.55; acc: 0.78
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.81; acc: 0.77
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.77
Batch: 420; loss: 0.85; acc: 0.72
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.73
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.61; acc: 0.78
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.67
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.78
Batch: 780; loss: 0.59; acc: 0.77
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5574248565040576; val_accuracy: 0.8282245222929936 

The current subspace-distance is: 7.126876880647615e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.73
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.75
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.72; acc: 0.73
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.75; acc: 0.72
Batch: 480; loss: 0.56; acc: 0.78
Batch: 500; loss: 0.91; acc: 0.75
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.91; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.73
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.75
Batch: 620; loss: 0.76; acc: 0.77
Batch: 640; loss: 0.55; acc: 0.78
Batch: 660; loss: 0.82; acc: 0.7
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.9; acc: 0.73
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.7
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.6231949367340962; val_accuracy: 0.8044386942675159 

The current subspace-distance is: 7.48890161048621e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.62; acc: 0.8
Batch: 180; loss: 0.98; acc: 0.73
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.8
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.77
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.75
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 1.05; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.45; acc: 0.81
Val Epoch over. val_loss: 0.6907105505656285; val_accuracy: 0.7871218152866242 

The current subspace-distance is: 7.758870196994394e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.75
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.77
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.62; acc: 0.77
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.75; acc: 0.7
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.65; acc: 0.77
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5486740884697361; val_accuracy: 0.8285230891719745 

The current subspace-distance is: 8.072772470768541e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.77
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.75
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.77; acc: 0.75
Batch: 240; loss: 0.75; acc: 0.75
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.77
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.83; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.5343710136166804; val_accuracy: 0.8303144904458599 

The current subspace-distance is: 8.263569179689512e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.69
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.77
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.84; acc: 0.73
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.94; acc: 0.77
Batch: 380; loss: 0.82; acc: 0.77
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.79; acc: 0.73
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.78
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.64; acc: 0.75
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5301887426216891; val_accuracy: 0.8331011146496815 

The current subspace-distance is: 8.63558379933238e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.81
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 0.58; acc: 0.73
Batch: 180; loss: 0.77; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.48; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.54; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.8
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.72
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5251103447880715; val_accuracy: 0.8369824840764332 

The current subspace-distance is: 8.986701868707314e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.72
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.43; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.75
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.78
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.75; acc: 0.72
Batch: 640; loss: 0.46; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.75; acc: 0.78
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.92; acc: 0.69
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.55; acc: 0.73
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.5547320750203861; val_accuracy: 0.8294187898089171 

The current subspace-distance is: 9.143103670794517e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.78
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.8
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.61; acc: 0.73
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.81
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.75
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.78
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.529197671848118; val_accuracy: 0.8343949044585988 

The current subspace-distance is: 9.434056119062006e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.86; acc: 0.67
Batch: 120; loss: 0.79; acc: 0.67
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.44; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.81
Batch: 200; loss: 0.72; acc: 0.81
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.81
Batch: 300; loss: 0.8; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.83; acc: 0.7
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.75
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.81
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.5356038519816034; val_accuracy: 0.8296178343949044 

The current subspace-distance is: 9.478173160459846e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.75
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.82; acc: 0.75
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.75
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.59; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.81; acc: 0.75
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5272442535230308; val_accuracy: 0.8346934713375797 

The current subspace-distance is: 9.873838280327618e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.58; acc: 0.8
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.71; acc: 0.75
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.61; acc: 0.77
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.86; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.7
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.78
Batch: 720; loss: 0.76; acc: 0.78
Batch: 740; loss: 0.5; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.9; acc: 0.73
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.5552563866612258; val_accuracy: 0.8213574840764332 

The current subspace-distance is: 9.975042485166341e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.9; acc: 0.72
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.78
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.81
Batch: 220; loss: 0.64; acc: 0.77
Batch: 240; loss: 0.64; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.78
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.97; acc: 0.72
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.93; acc: 0.72
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5307956578055765; val_accuracy: 0.8323049363057324 

The current subspace-distance is: 0.0001029291088343598 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.78
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.71; acc: 0.73
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.82; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.88; acc: 0.75
Batch: 400; loss: 0.6; acc: 0.77
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.59; acc: 0.78
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.67; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.73
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.81; acc: 0.72
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.86; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5343468074396158; val_accuracy: 0.831906847133758 

The current subspace-distance is: 0.0001046889228746295 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.48; acc: 0.8
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.53; acc: 0.8
Batch: 360; loss: 0.84; acc: 0.7
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.63; acc: 0.75
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.94; acc: 0.75
Batch: 620; loss: 0.72; acc: 0.77
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.519921997455275; val_accuracy: 0.8370820063694268 

The current subspace-distance is: 0.00010720369755290449 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.75
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.78
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.75
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.9; acc: 0.75
Batch: 560; loss: 1.01; acc: 0.62
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.78
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.75
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5236810592898897; val_accuracy: 0.8365843949044586 

The current subspace-distance is: 0.00010712732910178602 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.79; acc: 0.73
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.73
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.76; acc: 0.72
Batch: 460; loss: 0.46; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.78
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.72
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.78
Batch: 640; loss: 0.78; acc: 0.77
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5243751549037399; val_accuracy: 0.8340963375796179 

The current subspace-distance is: 0.00011034588533220813 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.85; acc: 0.73
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.72
Batch: 780; loss: 0.57; acc: 0.77
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5240396664590593; val_accuracy: 0.8348925159235668 

The current subspace-distance is: 0.00011229130905121565 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.96; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 0.61; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.72
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.73; acc: 0.77
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.53; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.72
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.56; acc: 0.77
Batch: 580; loss: 0.75; acc: 0.73
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.94; acc: 0.72
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5218081583927392; val_accuracy: 0.8367834394904459 

The current subspace-distance is: 0.00011241334868827835 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.75
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 0.57; acc: 0.78
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.78
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.75
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.75
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.69
Batch: 680; loss: 0.58; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.87; acc: 0.73
Batch: 760; loss: 0.72; acc: 0.75
Batch: 780; loss: 0.63; acc: 0.77
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5225700743638786; val_accuracy: 0.8369824840764332 

The current subspace-distance is: 0.0001144646666944027 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.78; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.75
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.75
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.75
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.7; acc: 0.77
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.526722106014847; val_accuracy: 0.8346934713375797 

The current subspace-distance is: 0.0001175240395241417 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.83; acc: 0.73
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5204545239069659; val_accuracy: 0.8378781847133758 

The current subspace-distance is: 0.00012194692681077868 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.9; acc: 0.75
Batch: 160; loss: 0.48; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.78
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.77
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.81
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.77
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5266156196119679; val_accuracy: 0.8341958598726115 

The current subspace-distance is: 0.0001225620653713122 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.77
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.86; acc: 0.7
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.95; acc: 0.62
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.52; acc: 0.81
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.83; acc: 0.77
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5199394089401148; val_accuracy: 0.8358877388535032 

The current subspace-distance is: 0.0001232590584550053 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.6; acc: 0.75
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.51; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.77
Batch: 620; loss: 0.67; acc: 0.73
Batch: 640; loss: 0.63; acc: 0.78
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5192195670619891; val_accuracy: 0.836484872611465 

The current subspace-distance is: 0.0001264251914108172 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.71; acc: 0.72
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.8
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.78
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5194120729804799; val_accuracy: 0.8367834394904459 

The current subspace-distance is: 0.00012547121150419116 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.75
Batch: 240; loss: 0.78; acc: 0.73
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.75; acc: 0.7
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.77
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5205613185835493; val_accuracy: 0.8360867834394905 

The current subspace-distance is: 0.00012633235019166023 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 0.62; acc: 0.73
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.78
Batch: 160; loss: 0.85; acc: 0.73
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.84; acc: 0.72
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.73; acc: 0.7
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.8
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.73
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.8
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5190766155719757; val_accuracy: 0.8358877388535032 

The current subspace-distance is: 0.00012803179561160505 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.7
Batch: 140; loss: 0.75; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.75
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.65; acc: 0.78
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.77
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.73; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.8
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5189202846425354; val_accuracy: 0.8359872611464968 

The current subspace-distance is: 0.0001284210738958791 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.73
Batch: 200; loss: 0.75; acc: 0.73
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.73; acc: 0.75
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.78
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 0.55; acc: 0.78
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.77
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.5188747723201278; val_accuracy: 0.836484872611465 

The current subspace-distance is: 0.0001304960169363767 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.86; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.55; acc: 0.78
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.73
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.73
Batch: 480; loss: 0.84; acc: 0.75
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.78
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.51; acc: 0.8
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.75
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.5188035087031164; val_accuracy: 0.8367834394904459 

The current subspace-distance is: 0.00012937842984683812 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.81
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.85; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.75
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.95; acc: 0.73
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.73
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.8
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5193901205328619; val_accuracy: 0.8377786624203821 

The current subspace-distance is: 0.00013209377357270569 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.81; acc: 0.77
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.75
Batch: 240; loss: 0.84; acc: 0.83
Batch: 260; loss: 1.01; acc: 0.75
Batch: 280; loss: 0.65; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.78
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.8
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.77
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5179791852926753; val_accuracy: 0.8371815286624203 

The current subspace-distance is: 0.0001327425125055015 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.53; acc: 0.8
Batch: 180; loss: 0.81; acc: 0.72
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.75
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.9; acc: 0.72
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.78
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.5; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.5190288427339238; val_accuracy: 0.8369824840764332 

The current subspace-distance is: 0.0001339019654551521 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 21026
elements in E: 4499000
fraction nonzero: 0.004673482996221383
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.17
Batch: 160; loss: 2.31; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.06
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.14
Batch: 240; loss: 2.28; acc: 0.14
Batch: 260; loss: 2.28; acc: 0.14
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.27; acc: 0.17
Batch: 340; loss: 2.28; acc: 0.12
Batch: 360; loss: 2.27; acc: 0.12
Batch: 380; loss: 2.27; acc: 0.08
Batch: 400; loss: 2.26; acc: 0.16
Batch: 420; loss: 2.26; acc: 0.22
Batch: 440; loss: 2.24; acc: 0.11
Batch: 460; loss: 2.25; acc: 0.2
Batch: 480; loss: 2.24; acc: 0.19
Batch: 500; loss: 2.24; acc: 0.25
Batch: 520; loss: 2.21; acc: 0.28
Batch: 540; loss: 2.19; acc: 0.23
Batch: 560; loss: 2.15; acc: 0.36
Batch: 580; loss: 2.15; acc: 0.25
Batch: 600; loss: 2.07; acc: 0.45
Batch: 620; loss: 2.09; acc: 0.44
Batch: 640; loss: 2.01; acc: 0.33
Batch: 660; loss: 1.91; acc: 0.48
Batch: 680; loss: 1.8; acc: 0.47
Batch: 700; loss: 1.63; acc: 0.59
Batch: 720; loss: 1.56; acc: 0.5
Batch: 740; loss: 1.43; acc: 0.56
Batch: 760; loss: 1.34; acc: 0.59
Batch: 780; loss: 1.38; acc: 0.58
Train Epoch over. train_loss: 2.14; train_accuracy: 0.22 

Batch: 0; loss: 1.52; acc: 0.44
Batch: 20; loss: 1.63; acc: 0.47
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.59
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.37; acc: 0.53
Batch: 120; loss: 1.46; acc: 0.52
Batch: 140; loss: 1.21; acc: 0.55
Val Epoch over. val_loss: 1.3536250188851813; val_accuracy: 0.5365246815286624 

The current subspace-distance is: 8.241988325607963e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.53
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 1.19; acc: 0.59
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 1.05; acc: 0.67
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 1.27; acc: 0.55
Batch: 200; loss: 0.88; acc: 0.78
Batch: 220; loss: 0.92; acc: 0.7
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 1.23; acc: 0.62
Batch: 280; loss: 0.99; acc: 0.66
Batch: 300; loss: 0.83; acc: 0.67
Batch: 320; loss: 1.02; acc: 0.62
Batch: 340; loss: 1.01; acc: 0.69
Batch: 360; loss: 0.83; acc: 0.7
Batch: 380; loss: 0.85; acc: 0.7
Batch: 400; loss: 1.18; acc: 0.59
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.78; acc: 0.8
Batch: 460; loss: 0.83; acc: 0.67
Batch: 480; loss: 0.87; acc: 0.67
Batch: 500; loss: 0.94; acc: 0.62
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.87; acc: 0.72
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.69
Batch: 620; loss: 1.02; acc: 0.69
Batch: 640; loss: 1.58; acc: 0.59
Batch: 660; loss: 0.71; acc: 0.77
Batch: 680; loss: 0.83; acc: 0.7
Batch: 700; loss: 0.77; acc: 0.77
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.72; acc: 0.72
Batch: 760; loss: 0.78; acc: 0.7
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.95; acc: 0.64
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.8108207317674236; val_accuracy: 0.727906050955414 

The current subspace-distance is: 1.8432057913742028e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.67
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.78
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.75
Batch: 160; loss: 0.96; acc: 0.64
Batch: 180; loss: 0.65; acc: 0.66
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 1.05; acc: 0.69
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.98; acc: 0.67
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 1.27; acc: 0.58
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.73
Batch: 520; loss: 0.91; acc: 0.7
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.83; acc: 0.78
Batch: 600; loss: 0.68; acc: 0.73
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.86; acc: 0.72
Batch: 680; loss: 0.73; acc: 0.7
Batch: 700; loss: 0.84; acc: 0.73
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.63; acc: 0.73
Batch: 780; loss: 0.72; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 1.82; acc: 0.47
Batch: 20; loss: 1.74; acc: 0.44
Batch: 40; loss: 1.31; acc: 0.59
Batch: 60; loss: 1.55; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.61
Batch: 100; loss: 1.65; acc: 0.59
Batch: 120; loss: 2.14; acc: 0.44
Batch: 140; loss: 1.33; acc: 0.53
Val Epoch over. val_loss: 1.5604919802611041; val_accuracy: 0.5612062101910829 

The current subspace-distance is: 2.592002238088753e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.83; acc: 0.78
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.66
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 1.17; acc: 0.62
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.68; acc: 0.69
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.73; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.59
Batch: 340; loss: 1.12; acc: 0.66
Batch: 360; loss: 0.71; acc: 0.75
Batch: 380; loss: 0.82; acc: 0.73
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 1.01; acc: 0.62
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.69
Batch: 640; loss: 0.7; acc: 0.77
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 1.0; acc: 0.72
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.83; acc: 0.77
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 1.32; acc: 0.5
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.75; acc: 0.7
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 0.78; acc: 0.72
Val Epoch over. val_loss: 1.104924045740419; val_accuracy: 0.637937898089172 

The current subspace-distance is: 3.175933670718223e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.64
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 0.49; acc: 0.8
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.77
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 1.08; acc: 0.75
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.99; acc: 0.69
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.59; acc: 0.78
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 0.72; acc: 0.77
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.5; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 0.76; acc: 0.77
Batch: 680; loss: 0.71; acc: 0.75
Batch: 700; loss: 0.83; acc: 0.8
Batch: 720; loss: 0.8; acc: 0.72
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.77 

Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.5808022174106282; val_accuracy: 0.8148885350318471 

The current subspace-distance is: 3.8809317629784346e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.76; acc: 0.72
Batch: 100; loss: 0.7; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.7
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.73; acc: 0.75
Batch: 400; loss: 0.64; acc: 0.81
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.68; acc: 0.78
Batch: 480; loss: 0.87; acc: 0.7
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.6; acc: 0.75
Batch: 560; loss: 1.11; acc: 0.8
Batch: 580; loss: 0.53; acc: 0.8
Batch: 600; loss: 0.96; acc: 0.7
Batch: 620; loss: 0.89; acc: 0.78
Batch: 640; loss: 0.41; acc: 0.84
Batch: 660; loss: 1.13; acc: 0.64
Batch: 680; loss: 0.87; acc: 0.72
Batch: 700; loss: 0.71; acc: 0.75
Batch: 720; loss: 0.57; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 0.86; acc: 0.67
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.73
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.7089642726691665; val_accuracy: 0.7676154458598726 

The current subspace-distance is: 4.3123087380081415e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.72
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.71; acc: 0.75
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.77
Batch: 300; loss: 0.52; acc: 0.81
Batch: 320; loss: 0.85; acc: 0.73
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.75
Batch: 380; loss: 0.89; acc: 0.66
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.8
Batch: 440; loss: 0.82; acc: 0.69
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.77; acc: 0.67
Batch: 520; loss: 0.57; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.74; acc: 0.73
Batch: 600; loss: 0.71; acc: 0.72
Batch: 620; loss: 0.65; acc: 0.73
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.85; acc: 0.69
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.77
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 0.53; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 0.63; acc: 0.7
Val Epoch over. val_loss: 0.7910995356216552; val_accuracy: 0.7352707006369427 

The current subspace-distance is: 4.597328006639145e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.7
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.75
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.74; acc: 0.75
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.95; acc: 0.64
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.75
Batch: 360; loss: 0.9; acc: 0.72
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.79; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.78; acc: 0.73
Batch: 460; loss: 0.87; acc: 0.72
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.77
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.48; acc: 0.78
Batch: 560; loss: 0.83; acc: 0.67
Batch: 580; loss: 0.71; acc: 0.73
Batch: 600; loss: 1.45; acc: 0.61
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 1.31; acc: 0.55
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.9; acc: 0.72
Batch: 760; loss: 0.95; acc: 0.75
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 0.36; acc: 0.86
Val Epoch over. val_loss: 0.787243726337032; val_accuracy: 0.7412420382165605 

The current subspace-distance is: 5.116921602166258e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.78; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.89; acc: 0.77
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.75
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 1.17; acc: 0.61
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 1.06; acc: 0.7
Batch: 360; loss: 0.96; acc: 0.67
Batch: 380; loss: 0.66; acc: 0.72
Batch: 400; loss: 0.43; acc: 0.81
Batch: 420; loss: 0.89; acc: 0.77
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.67
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.78; acc: 0.72
Batch: 520; loss: 0.55; acc: 0.78
Batch: 540; loss: 0.63; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.66
Batch: 580; loss: 0.91; acc: 0.69
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.77
Batch: 680; loss: 0.78; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.68; acc: 0.75
Batch: 740; loss: 1.06; acc: 0.72
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.88; acc: 0.69
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 1.35; acc: 0.61
Batch: 20; loss: 1.48; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 1.24; acc: 0.75
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.78
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 0.78; acc: 0.67
Val Epoch over. val_loss: 1.1221759543297396; val_accuracy: 0.6751592356687898 

The current subspace-distance is: 5.602081000688486e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.66
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.52; acc: 0.77
Batch: 180; loss: 0.69; acc: 0.75
Batch: 200; loss: 0.96; acc: 0.73
Batch: 220; loss: 0.81; acc: 0.75
Batch: 240; loss: 0.84; acc: 0.72
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.98; acc: 0.69
Batch: 300; loss: 0.85; acc: 0.7
Batch: 320; loss: 0.74; acc: 0.73
Batch: 340; loss: 0.72; acc: 0.77
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.96; acc: 0.69
Batch: 460; loss: 0.81; acc: 0.7
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 0.55; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.72
Batch: 560; loss: 0.66; acc: 0.75
Batch: 580; loss: 0.78; acc: 0.77
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.94; acc: 0.67
Batch: 700; loss: 0.7; acc: 0.77
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.4; acc: 0.84
Val Epoch over. val_loss: 0.5821239589505894; val_accuracy: 0.8113057324840764 

The current subspace-distance is: 6.019544889568351e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.75
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.54; acc: 0.77
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.77
Batch: 160; loss: 0.57; acc: 0.77
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.81
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.5272715949708489; val_accuracy: 0.8335987261146497 

The current subspace-distance is: 6.267334538279101e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.72; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.59; acc: 0.77
Batch: 600; loss: 0.75; acc: 0.8
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.77
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.75
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 0.36; acc: 0.86
Val Epoch over. val_loss: 0.6502412701867948; val_accuracy: 0.7879179936305732 

The current subspace-distance is: 6.783799472032115e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.8
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.73
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.64; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.55; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.5549370842944285; val_accuracy: 0.8255374203821656 

The current subspace-distance is: 6.968992965994403e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.7; acc: 0.72
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.77
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.73
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.48; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.5153601169586182; val_accuracy: 0.8351910828025477 

The current subspace-distance is: 7.384717173408717e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.75; acc: 0.75
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.73
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.92; acc: 0.7
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.58; acc: 0.78
Batch: 740; loss: 0.81; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.9; acc: 0.73
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.5189717457552624; val_accuracy: 0.8341958598726115 

The current subspace-distance is: 7.651520718354732e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.83; acc: 0.7
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.77
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.78
Batch: 660; loss: 0.69; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.77
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.5078514386324366; val_accuracy: 0.8395700636942676 

The current subspace-distance is: 7.843815546948463e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.73
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.87; acc: 0.78
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.73
Batch: 540; loss: 0.99; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.92; acc: 0.7
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.76; acc: 0.72
Batch: 700; loss: 0.51; acc: 0.8
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.88; acc: 0.75
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.57; acc: 0.75
Val Epoch over. val_loss: 0.8529117338976283; val_accuracy: 0.71984474522293 

The current subspace-distance is: 8.068435272434726e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.55; acc: 0.77
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.8
Batch: 320; loss: 0.65; acc: 0.72
Batch: 340; loss: 0.68; acc: 0.73
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.78
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.77
Batch: 620; loss: 0.56; acc: 0.77
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.73
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.85; acc: 0.8
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 1.01; acc: 0.73
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.7
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.5441208557718119; val_accuracy: 0.8290207006369427 

The current subspace-distance is: 8.278543828055263e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.83; acc: 0.73
Batch: 180; loss: 0.86; acc: 0.7
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.7
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.51; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.77
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.86
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 0.42; acc: 0.81
Val Epoch over. val_loss: 0.7246573116559132; val_accuracy: 0.7767714968152867 

The current subspace-distance is: 8.558564877603203e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.78
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.8
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.77
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.8
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.55; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.77
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.77
Train Epoch over. train_loss: 0.55; train_accuracy: 0.82 

Batch: 0; loss: 1.0; acc: 0.61
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.5926372049150953; val_accuracy: 0.8053343949044586 

The current subspace-distance is: 8.722696657059714e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.64; acc: 0.75
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.58; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.83; acc: 0.7
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.5480021561027333; val_accuracy: 0.8226512738853503 

The current subspace-distance is: 8.996514225145802e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.78
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.73
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.77
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.74; acc: 0.77
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.63; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.75; acc: 0.73
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.88
Val Epoch over. val_loss: 0.47962797352462816; val_accuracy: 0.8472332802547771 

The current subspace-distance is: 9.227505506714806e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.55; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.73
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.75
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.78
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.48557150439851604; val_accuracy: 0.8473328025477707 

The current subspace-distance is: 9.418504487257451e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.75
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.67
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.71; acc: 0.77
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.77
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.7
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.4905828452034361; val_accuracy: 0.8449442675159236 

The current subspace-distance is: 9.50977555476129e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.75
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.81
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.78
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.7
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.49577238652736516; val_accuracy: 0.8419585987261147 

The current subspace-distance is: 9.75507718976587e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.81
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.8
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.67; acc: 0.78
Batch: 660; loss: 0.6; acc: 0.75
Batch: 680; loss: 0.61; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.4824928386006386; val_accuracy: 0.8471337579617835 

The current subspace-distance is: 0.00010056736937258393 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.75
Batch: 120; loss: 0.47; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.78
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.8
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.81
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.4866280158993545; val_accuracy: 0.8457404458598726 

The current subspace-distance is: 0.00010414851567475125 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.8
Batch: 360; loss: 0.44; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.78
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.74; acc: 0.75
Batch: 500; loss: 0.55; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.78
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.69
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.5137063524905284; val_accuracy: 0.8336982484076433 

The current subspace-distance is: 0.0001067276461981237 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.73
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.41; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.8
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.72
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.4782195268732727; val_accuracy: 0.8506170382165605 

The current subspace-distance is: 0.00011149015335831791 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.98; acc: 0.72
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.84; acc: 0.67
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.78
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.79; acc: 0.72
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.8
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.79; acc: 0.73
Batch: 580; loss: 0.79; acc: 0.77
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.38; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.78
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.49664213674463287; val_accuracy: 0.8415605095541401 

The current subspace-distance is: 0.00011326526146149263 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.75
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.77
Batch: 220; loss: 0.4; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.7; acc: 0.8
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.75
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.48; acc: 0.81
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.47986088389424003; val_accuracy: 0.847531847133758 

The current subspace-distance is: 0.00011505474685691297 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.94; acc: 0.73
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.47424427111437367; val_accuracy: 0.849422770700637 

The current subspace-distance is: 0.00011599223216762766 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.76; acc: 0.72
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.77
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.75
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.44; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.74; acc: 0.75
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.76; acc: 0.7
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.7
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.47819358176866156; val_accuracy: 0.8484275477707006 

The current subspace-distance is: 0.0001181210100185126 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.72
Batch: 40; loss: 0.54; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.75
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.78
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 0.58; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.73
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.8
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.7
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.47799182146977465; val_accuracy: 0.8473328025477707 

The current subspace-distance is: 0.00011749508121283725 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.75
Batch: 40; loss: 0.86; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.77
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.8
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.8; acc: 0.73
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.7
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4744057988475083; val_accuracy: 0.8514132165605095 

The current subspace-distance is: 0.00011945116420974955 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.77; acc: 0.81
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.66; acc: 0.77
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.61; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.18; acc: 0.98
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.71; acc: 0.73
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.78
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.75
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.89
Val Epoch over. val_loss: 0.47833024743635943; val_accuracy: 0.8503184713375797 

The current subspace-distance is: 0.00012179069017292932 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.73
Batch: 140; loss: 0.8; acc: 0.73
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.76; acc: 0.77
Batch: 220; loss: 0.57; acc: 0.77
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.77
Batch: 300; loss: 0.48; acc: 0.8
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.38; acc: 0.83
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.78
Batch: 560; loss: 0.4; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.78
Batch: 680; loss: 0.62; acc: 0.77
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.4754614298510703; val_accuracy: 0.8512141719745223 

The current subspace-distance is: 0.0001237913966178894 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.36; acc: 0.84
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.76; acc: 0.8
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.68; acc: 0.75
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.77
Batch: 620; loss: 0.62; acc: 0.75
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.73
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.78
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.7
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.4761254404001175; val_accuracy: 0.8514132165605095 

The current subspace-distance is: 0.0001267768384423107 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.77
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.67; acc: 0.75
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.8
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.81
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.8
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.89
Val Epoch over. val_loss: 0.47122085388678653; val_accuracy: 0.8520103503184714 

The current subspace-distance is: 0.00012929753575008363 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.86; acc: 0.78
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.75
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.47594552226127335; val_accuracy: 0.8503184713375797 

The current subspace-distance is: 0.00013140177179593593 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.77
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.78
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.75
Batch: 400; loss: 0.62; acc: 0.78
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.89; acc: 0.72
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.91; acc: 0.73
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.89
Val Epoch over. val_loss: 0.472008762086273; val_accuracy: 0.8513136942675159 

The current subspace-distance is: 0.00013284815941005945 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.81; acc: 0.73
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.7
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.46986514605154656; val_accuracy: 0.8524084394904459 

The current subspace-distance is: 0.00013253708311822265 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.73
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.57; acc: 0.75
Batch: 400; loss: 0.37; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.7
Batch: 140; loss: 0.18; acc: 0.89
Val Epoch over. val_loss: 0.47183674108830226; val_accuracy: 0.8513136942675159 

The current subspace-distance is: 0.00013322700397111475 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.75
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.77
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.83
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.8
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.81
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.8
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.4722359244041382; val_accuracy: 0.8534036624203821 

The current subspace-distance is: 0.00013319523714017123 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.81
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.78
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.8
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.77
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.8
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.73
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.7
Batch: 140; loss: 0.19; acc: 0.89
Val Epoch over. val_loss: 0.470292583962155; val_accuracy: 0.8516122611464968 

The current subspace-distance is: 0.00013736468099523336 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.81
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.78
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.8
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.77
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.77
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.77
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.7
Batch: 140; loss: 0.19; acc: 0.89
Val Epoch over. val_loss: 0.470394617622825; val_accuracy: 0.8513136942675159 

The current subspace-distance is: 0.0001395873405272141 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.77; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.89
Val Epoch over. val_loss: 0.47233930277596614; val_accuracy: 0.8509156050955414 

The current subspace-distance is: 0.00014447988360188901 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.78
Batch: 220; loss: 0.43; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.81; acc: 0.77
Batch: 420; loss: 0.46; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.8; acc: 0.7
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.75
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.89
Val Epoch over. val_loss: 0.46928496468978326; val_accuracy: 0.8526074840764332 

The current subspace-distance is: 0.00014493759954348207 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.45; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.78
Batch: 220; loss: 0.51; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.77
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.8
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.77
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.8; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.71; acc: 0.75
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.73
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.89
Val Epoch over. val_loss: 0.47007984985971146; val_accuracy: 0.852906050955414 

The current subspace-distance is: 0.0001484989479649812 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 0.52; acc: 0.78
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.78
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.78
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.78
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.84; acc: 0.8
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.4717923574576712; val_accuracy: 0.8511146496815286 

The current subspace-distance is: 0.00014971758355386555 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 23633
elements in E: 4948900
fraction nonzero: 0.004775404635373517
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.27; acc: 0.16
Batch: 340; loss: 2.29; acc: 0.12
Batch: 360; loss: 2.29; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.19
Batch: 400; loss: 2.27; acc: 0.22
Batch: 420; loss: 2.28; acc: 0.3
Batch: 440; loss: 2.28; acc: 0.2
Batch: 460; loss: 2.28; acc: 0.23
Batch: 480; loss: 2.27; acc: 0.28
Batch: 500; loss: 2.28; acc: 0.25
Batch: 520; loss: 2.26; acc: 0.36
Batch: 540; loss: 2.27; acc: 0.34
Batch: 560; loss: 2.24; acc: 0.41
Batch: 580; loss: 2.24; acc: 0.3
Batch: 600; loss: 2.24; acc: 0.3
Batch: 620; loss: 2.25; acc: 0.31
Batch: 640; loss: 2.23; acc: 0.3
Batch: 660; loss: 2.21; acc: 0.3
Batch: 680; loss: 2.22; acc: 0.2
Batch: 700; loss: 2.19; acc: 0.28
Batch: 720; loss: 2.16; acc: 0.34
Batch: 740; loss: 2.16; acc: 0.38
Batch: 760; loss: 2.1; acc: 0.44
Batch: 780; loss: 2.06; acc: 0.39
Train Epoch over. train_loss: 2.26; train_accuracy: 0.21 

Batch: 0; loss: 2.09; acc: 0.33
Batch: 20; loss: 2.13; acc: 0.23
Batch: 40; loss: 2.03; acc: 0.45
Batch: 60; loss: 2.04; acc: 0.41
Batch: 80; loss: 2.1; acc: 0.34
Batch: 100; loss: 2.08; acc: 0.38
Batch: 120; loss: 2.04; acc: 0.42
Batch: 140; loss: 2.09; acc: 0.31
Val Epoch over. val_loss: 2.0976988039198954; val_accuracy: 0.3166799363057325 

The current subspace-distance is: 6.146436135168187e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.1; acc: 0.27
Batch: 20; loss: 2.09; acc: 0.34
Batch: 40; loss: 1.94; acc: 0.34
Batch: 60; loss: 1.89; acc: 0.42
Batch: 80; loss: 1.69; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.42
Batch: 120; loss: 1.58; acc: 0.45
Batch: 140; loss: 1.42; acc: 0.55
Batch: 160; loss: 1.33; acc: 0.61
Batch: 180; loss: 1.29; acc: 0.56
Batch: 200; loss: 1.43; acc: 0.5
Batch: 220; loss: 1.38; acc: 0.52
Batch: 240; loss: 1.17; acc: 0.59
Batch: 260; loss: 1.42; acc: 0.53
Batch: 280; loss: 1.42; acc: 0.52
Batch: 300; loss: 1.08; acc: 0.59
Batch: 320; loss: 1.33; acc: 0.52
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 0.88; acc: 0.72
Batch: 380; loss: 1.22; acc: 0.55
Batch: 400; loss: 1.32; acc: 0.52
Batch: 420; loss: 1.09; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.69
Batch: 460; loss: 1.03; acc: 0.72
Batch: 480; loss: 1.12; acc: 0.64
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 0.86; acc: 0.77
Batch: 560; loss: 1.18; acc: 0.58
Batch: 580; loss: 1.11; acc: 0.55
Batch: 600; loss: 0.73; acc: 0.77
Batch: 620; loss: 1.35; acc: 0.66
Batch: 640; loss: 0.87; acc: 0.75
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.77; acc: 0.73
Batch: 700; loss: 0.96; acc: 0.7
Batch: 720; loss: 0.96; acc: 0.7
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.85; acc: 0.78
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 1.2; train_accuracy: 0.61 

Batch: 0; loss: 1.07; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.61
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.69
Val Epoch over. val_loss: 1.0497579096229213; val_accuracy: 0.6696855095541401 

The current subspace-distance is: 1.6953357771853916e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 1.01; acc: 0.66
Batch: 180; loss: 0.69; acc: 0.72
Batch: 200; loss: 1.12; acc: 0.58
Batch: 220; loss: 0.81; acc: 0.78
Batch: 240; loss: 0.64; acc: 0.75
Batch: 260; loss: 0.61; acc: 0.78
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.86; acc: 0.69
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.84; acc: 0.66
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.77
Batch: 400; loss: 0.78; acc: 0.73
Batch: 420; loss: 0.84; acc: 0.72
Batch: 440; loss: 0.71; acc: 0.78
Batch: 460; loss: 0.89; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.75
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.7; acc: 0.75
Batch: 580; loss: 0.93; acc: 0.7
Batch: 600; loss: 1.33; acc: 0.59
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 1.06; acc: 0.61
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.83; acc: 0.75
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 1.27; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 0.84; acc: 0.75
Batch: 60; loss: 1.33; acc: 0.55
Batch: 80; loss: 1.11; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 1.34; acc: 0.59
Val Epoch over. val_loss: 1.3014670378842932; val_accuracy: 0.5908638535031847 

The current subspace-distance is: 2.4162192858057097e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 0.58; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.75
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.66
Batch: 100; loss: 0.96; acc: 0.7
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 0.9; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.79; acc: 0.7
Batch: 240; loss: 0.8; acc: 0.73
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 1.11; acc: 0.69
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.8
Batch: 420; loss: 0.72; acc: 0.75
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.84; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.75
Batch: 620; loss: 1.04; acc: 0.64
Batch: 640; loss: 0.76; acc: 0.8
Batch: 660; loss: 0.98; acc: 0.75
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.84; acc: 0.69
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.77
Train Epoch over. train_loss: 0.78; train_accuracy: 0.76 

Batch: 0; loss: 0.7; acc: 0.73
Batch: 20; loss: 1.03; acc: 0.7
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.59
Batch: 140; loss: 0.72; acc: 0.77
Val Epoch over. val_loss: 0.7261025142517818; val_accuracy: 0.7680135350318471 

The current subspace-distance is: 3.023458157258574e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.69
Batch: 40; loss: 0.81; acc: 0.7
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.99; acc: 0.62
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 1.26; acc: 0.64
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.65; acc: 0.77
Batch: 280; loss: 1.13; acc: 0.64
Batch: 300; loss: 0.68; acc: 0.75
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.98; acc: 0.67
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.86; acc: 0.72
Batch: 480; loss: 0.81; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.75
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.73
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.91; acc: 0.7
Batch: 640; loss: 0.97; acc: 0.67
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.84
Batch: 720; loss: 0.83; acc: 0.75
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.78 

Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.89; acc: 0.7
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 0.45; acc: 0.83
Val Epoch over. val_loss: 0.848235869673407; val_accuracy: 0.7229299363057324 

The current subspace-distance is: 3.597127215471119e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 1.29; acc: 0.75
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.69
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.77; acc: 0.73
Batch: 380; loss: 0.89; acc: 0.78
Batch: 400; loss: 0.51; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 1.38; acc: 0.66
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.72; acc: 0.73
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.72
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.8; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.8 

Batch: 0; loss: 0.7; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.88
Val Epoch over. val_loss: 0.6330706442047835; val_accuracy: 0.7973726114649682 

The current subspace-distance is: 4.1559625969966874e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 0.53; acc: 0.78
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.72
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.75
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.86; acc: 0.75
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.66
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.92; acc: 0.67
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 0.67; acc: 0.7
Val Epoch over. val_loss: 0.6862437889264648; val_accuracy: 0.7911027070063694 

The current subspace-distance is: 4.717761112260632e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.75
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.87; acc: 0.66
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.71; acc: 0.77
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.77
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.72
Batch: 740; loss: 0.73; acc: 0.72
Batch: 760; loss: 0.92; acc: 0.8
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.81 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.6832487746408791; val_accuracy: 0.7803542993630573 

The current subspace-distance is: 5.249523019301705e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.73
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.73
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.78; acc: 0.72
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.99; acc: 0.77
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.92; acc: 0.73
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.83; acc: 0.72
Batch: 480; loss: 0.5; acc: 0.8
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.79; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.92; acc: 0.73
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.5512583852763389; val_accuracy: 0.8255374203821656 

The current subspace-distance is: 5.545817475649528e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 0.75; acc: 0.73
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.78
Batch: 300; loss: 0.78; acc: 0.72
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.77
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.52; acc: 0.81
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 1.26; acc: 0.64
Batch: 780; loss: 0.62; acc: 0.75
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.62; acc: 0.73
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.77
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.84
Val Epoch over. val_loss: 0.5060141423515453; val_accuracy: 0.8443471337579618 

The current subspace-distance is: 5.8766752772498876e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.65; acc: 0.78
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.78; acc: 0.77
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.45699702782236085; val_accuracy: 0.8644506369426752 

The current subspace-distance is: 6.296234641922638e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.78
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.78
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.83; acc: 0.75
Batch: 680; loss: 0.47; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.49522713263323354; val_accuracy: 0.8507165605095541 

The current subspace-distance is: 6.789744656998664e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.75
Batch: 240; loss: 0.46; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.86; acc: 0.72
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.52; acc: 0.8
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.49068377095802573; val_accuracy: 0.848328025477707 

The current subspace-distance is: 7.089976861607283e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.9; acc: 0.77
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.87; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.77
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.81
Val Epoch over. val_loss: 0.6194886024210863; val_accuracy: 0.8174761146496815 

The current subspace-distance is: 7.252212526509538e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.94; acc: 0.75
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.48; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.74; acc: 0.75
Batch: 720; loss: 0.56; acc: 0.8
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.4947622369049461; val_accuracy: 0.8459394904458599 

The current subspace-distance is: 7.671827188460156e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.92; acc: 0.81
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.7; acc: 0.77
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.78
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.77
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4372599421033434; val_accuracy: 0.8683320063694268 

The current subspace-distance is: 7.899366755736992e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.91; acc: 0.78
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.67; acc: 0.75
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 1.08; acc: 0.66
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.84
Val Epoch over. val_loss: 0.6454503012310927; val_accuracy: 0.8045382165605095 

The current subspace-distance is: 8.259269088739529e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.69; acc: 0.77
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.8
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.78
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.61; acc: 0.75
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.72
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.89
Val Epoch over. val_loss: 0.5723712127299825; val_accuracy: 0.8156847133757962 

The current subspace-distance is: 8.684669592184946e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.88; acc: 0.7
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.77
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.65; acc: 0.73
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.77
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.67; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.73; acc: 0.75
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.86
Val Epoch over. val_loss: 0.5166157433751283; val_accuracy: 0.8425557324840764 

The current subspace-distance is: 9.045554179465398e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.78
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.8
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.81
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.47098677401330064; val_accuracy: 0.8544984076433121 

The current subspace-distance is: 9.207861148752272e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.8
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4129083956692629; val_accuracy: 0.8701234076433121 

The current subspace-distance is: 9.634482557885349e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.4237453984986445; val_accuracy: 0.8629578025477707 

The current subspace-distance is: 0.00010007579840021208 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.83
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.75
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.4141186566869165; val_accuracy: 0.8689291401273885 

The current subspace-distance is: 0.0001037134206853807 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.78
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.81
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.78; acc: 0.75
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.81
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.42; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.4165493627167811; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.00010727014159783721 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.81
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.4423294026570715; val_accuracy: 0.863953025477707 

The current subspace-distance is: 0.00010913075675489381 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.77
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.78
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.83
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.39779026085024427; val_accuracy: 0.8751990445859873 

The current subspace-distance is: 0.0001096761116059497 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.87; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.41063387995692574; val_accuracy: 0.8683320063694268 

The current subspace-distance is: 0.00011235953570576385 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.81
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.77
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3904599700668815; val_accuracy: 0.8800756369426752 

The current subspace-distance is: 0.00011510444892337546 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.92; acc: 0.8
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.81
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3862681540239389; val_accuracy: 0.882265127388535 

The current subspace-distance is: 0.00011706288205459714 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.81
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.78
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.92; acc: 0.77
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4051125191009728; val_accuracy: 0.8766918789808917 

The current subspace-distance is: 0.00011900180106749758 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.8
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.8
Batch: 620; loss: 0.38; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.38293020901786295; val_accuracy: 0.8812699044585988 

The current subspace-distance is: 0.0001207709065056406 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.8
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3803161338066599; val_accuracy: 0.8827627388535032 

The current subspace-distance is: 0.0001207144814543426 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.38608118374446393; val_accuracy: 0.8803742038216561 

The current subspace-distance is: 0.0001228230685228482 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.38458090356201124; val_accuracy: 0.8792794585987261 

The current subspace-distance is: 0.00012585626973304898 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3832763869576393; val_accuracy: 0.8801751592356688 

The current subspace-distance is: 0.00012865020835306495 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.28; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.83
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.83
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.81
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.49; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3982896531937988; val_accuracy: 0.8778861464968153 

The current subspace-distance is: 0.000130578875541687 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.84
Batch: 340; loss: 0.75; acc: 0.78
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3824759281839535; val_accuracy: 0.8803742038216561 

The current subspace-distance is: 0.00013219815446063876 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.56; acc: 0.75
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.3829957793947238; val_accuracy: 0.8835589171974523 

The current subspace-distance is: 0.00013603392289951444 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.78
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.8
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.38456936118898877; val_accuracy: 0.882265127388535 

The current subspace-distance is: 0.00013597494398709387 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.46; acc: 0.8
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.81
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.38529461527326303; val_accuracy: 0.8796775477707006 

The current subspace-distance is: 0.00013885957014281303 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.37982152658662977; val_accuracy: 0.884156050955414 

The current subspace-distance is: 0.00014164854655973613 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3788986246392226; val_accuracy: 0.882265127388535 

The current subspace-distance is: 0.0001429675321560353 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.75; acc: 0.8
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3814711388508985; val_accuracy: 0.8799761146496815 

The current subspace-distance is: 0.0001449376723030582 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.78; acc: 0.81
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.37866252281103924; val_accuracy: 0.883359872611465 

The current subspace-distance is: 0.0001483716769143939 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.77
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3788033981042303; val_accuracy: 0.8831608280254777 

The current subspace-distance is: 0.00014830140571575612 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.93; acc: 0.73
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3803747206167051; val_accuracy: 0.8812699044585988 

The current subspace-distance is: 0.0001498267229180783 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.75
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3795276446042547; val_accuracy: 0.8831608280254777 

The current subspace-distance is: 0.00015417141548823565 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.8
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3785447154644948; val_accuracy: 0.883359872611465 

The current subspace-distance is: 0.00015741704555694014 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.37891792154805676; val_accuracy: 0.8839570063694268 

The current subspace-distance is: 0.00015986590005923063 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.28; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3793478416409462; val_accuracy: 0.8826632165605095 

The current subspace-distance is: 0.0001617412781342864 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_110_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 25707
elements in E: 5398800
fraction nonzero: 0.00476161369193154
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.28; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.23
Batch: 340; loss: 2.29; acc: 0.17
Batch: 360; loss: 2.28; acc: 0.23
Batch: 380; loss: 2.28; acc: 0.17
Batch: 400; loss: 2.28; acc: 0.12
Batch: 420; loss: 2.27; acc: 0.23
Batch: 440; loss: 2.27; acc: 0.16
Batch: 460; loss: 2.28; acc: 0.11
Batch: 480; loss: 2.28; acc: 0.16
Batch: 500; loss: 2.26; acc: 0.23
Batch: 520; loss: 2.27; acc: 0.19
Batch: 540; loss: 2.27; acc: 0.17
Batch: 560; loss: 2.25; acc: 0.27
Batch: 580; loss: 2.26; acc: 0.2
Batch: 600; loss: 2.24; acc: 0.28
Batch: 620; loss: 2.24; acc: 0.2
Batch: 640; loss: 2.24; acc: 0.14
Batch: 660; loss: 2.22; acc: 0.25
Batch: 680; loss: 2.22; acc: 0.25
Batch: 700; loss: 2.21; acc: 0.27
Batch: 720; loss: 2.16; acc: 0.33
Batch: 740; loss: 2.17; acc: 0.27
Batch: 760; loss: 2.16; acc: 0.17
Batch: 780; loss: 2.15; acc: 0.31
Train Epoch over. train_loss: 2.27; train_accuracy: 0.17 

Batch: 0; loss: 2.12; acc: 0.38
Batch: 20; loss: 2.13; acc: 0.34
Batch: 40; loss: 2.06; acc: 0.41
Batch: 60; loss: 2.1; acc: 0.39
Batch: 80; loss: 2.13; acc: 0.33
Batch: 100; loss: 2.08; acc: 0.44
Batch: 120; loss: 2.11; acc: 0.39
Batch: 140; loss: 2.12; acc: 0.34
Val Epoch over. val_loss: 2.138759764896077; val_accuracy: 0.321656050955414 

The current subspace-distance is: 6.138671778899152e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.15; acc: 0.33
Batch: 20; loss: 2.13; acc: 0.23
Batch: 40; loss: 2.02; acc: 0.42
Batch: 60; loss: 2.01; acc: 0.44
Batch: 80; loss: 1.86; acc: 0.48
Batch: 100; loss: 1.83; acc: 0.38
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.61
Batch: 160; loss: 1.43; acc: 0.55
Batch: 180; loss: 1.37; acc: 0.58
Batch: 200; loss: 1.28; acc: 0.59
Batch: 220; loss: 1.38; acc: 0.52
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 1.35; acc: 0.62
Batch: 280; loss: 1.13; acc: 0.7
Batch: 300; loss: 1.07; acc: 0.67
Batch: 320; loss: 1.31; acc: 0.53
Batch: 340; loss: 1.32; acc: 0.58
Batch: 360; loss: 1.07; acc: 0.66
Batch: 380; loss: 0.99; acc: 0.75
Batch: 400; loss: 1.31; acc: 0.56
Batch: 420; loss: 1.11; acc: 0.72
Batch: 440; loss: 0.89; acc: 0.73
Batch: 460; loss: 0.98; acc: 0.67
Batch: 480; loss: 0.87; acc: 0.69
Batch: 500; loss: 0.91; acc: 0.72
Batch: 520; loss: 0.72; acc: 0.77
Batch: 540; loss: 1.13; acc: 0.64
Batch: 560; loss: 0.75; acc: 0.7
Batch: 580; loss: 0.91; acc: 0.81
Batch: 600; loss: 0.83; acc: 0.75
Batch: 620; loss: 1.27; acc: 0.7
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 1.01; acc: 0.64
Batch: 720; loss: 0.7; acc: 0.75
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 1.04; acc: 0.72
Batch: 780; loss: 0.8; acc: 0.72
Train Epoch over. train_loss: 1.2; train_accuracy: 0.62 

Batch: 0; loss: 0.98; acc: 0.67
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 1.06; acc: 0.66
Batch: 80; loss: 0.94; acc: 0.62
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.46; acc: 0.66
Batch: 140; loss: 0.54; acc: 0.8
Val Epoch over. val_loss: 0.9883771859536505; val_accuracy: 0.6852109872611465 

The current subspace-distance is: 1.9186541976523586e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.66
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.78
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.82; acc: 0.7
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.9; acc: 0.7
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.73; acc: 0.75
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.77
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.64; acc: 0.78
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.76; acc: 0.72
Batch: 540; loss: 0.66; acc: 0.75
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.91; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.8
Batch: 720; loss: 0.8; acc: 0.69
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 0.54; acc: 0.83
Val Epoch over. val_loss: 0.7574119495738084; val_accuracy: 0.7682125796178344 

The current subspace-distance is: 2.6198737032245845e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 0.69; acc: 0.72
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.91; acc: 0.7
Batch: 260; loss: 0.75; acc: 0.72
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.72
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.71; acc: 0.73
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.87; acc: 0.72
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.56; acc: 0.77
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.6501223839771976; val_accuracy: 0.7844347133757962 

The current subspace-distance is: 3.226079206797294e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.67; acc: 0.77
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.77
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.77
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.86; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.81
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.582884569361711; val_accuracy: 0.8204617834394905 

The current subspace-distance is: 3.835998722934164e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.69
Batch: 40; loss: 0.57; acc: 0.78
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.75
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.77
Batch: 380; loss: 0.72; acc: 0.75
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.77
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.78
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.75
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.69
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.5203186368012125; val_accuracy: 0.8417595541401274 

The current subspace-distance is: 4.4038595660822466e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.89; acc: 0.73
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 1.0; acc: 0.66
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.5; acc: 0.8
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.51; acc: 0.78
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 1.01; acc: 0.67
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.88
Val Epoch over. val_loss: 0.5251065821025023; val_accuracy: 0.8406648089171974 

The current subspace-distance is: 4.888943658443168e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.7
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.5672478221214501; val_accuracy: 0.8206608280254777 

The current subspace-distance is: 5.333883746061474e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.8
Batch: 360; loss: 0.57; acc: 0.77
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.7; acc: 0.75
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.76; acc: 0.72
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.81
Val Epoch over. val_loss: 0.5480819740303003; val_accuracy: 0.8276273885350318 

The current subspace-distance is: 5.7206569181289524e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.4349764352960951; val_accuracy: 0.8625597133757962 

The current subspace-distance is: 6.132075213827193e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.77
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.84
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.72
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4006810736883977; val_accuracy: 0.8784832802547771 

The current subspace-distance is: 6.523154297610745e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.81
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.78
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.39995758292401673; val_accuracy: 0.8782842356687898 

The current subspace-distance is: 6.908471550559625e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.41171534255052067; val_accuracy: 0.8756966560509554 

The current subspace-distance is: 7.301520963665098e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.8
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.81
Batch: 580; loss: 0.3; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.69
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4111388476601072; val_accuracy: 0.8752985668789809 

The current subspace-distance is: 7.603861740790308e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.74; acc: 0.7
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.75
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.8
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.75
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.67; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.75
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.4020100822494288; val_accuracy: 0.876890923566879 

The current subspace-distance is: 7.976517372298986e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.73; acc: 0.78
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.78
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4023460987836692; val_accuracy: 0.8757961783439491 

The current subspace-distance is: 8.360570791410282e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.86
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.83; acc: 0.67
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.41829768060498934; val_accuracy: 0.8707205414012739 

The current subspace-distance is: 8.678483573021367e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.65; acc: 0.78
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.75
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.83; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.66
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4147083059807492; val_accuracy: 0.8717157643312102 

The current subspace-distance is: 8.993053779704496e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.74; acc: 0.73
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.8
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.4464183628179465; val_accuracy: 0.8589769108280255 

The current subspace-distance is: 9.236344340024516e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.83
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.62; acc: 0.77
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.7
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.407615828713414; val_accuracy: 0.8755971337579618 

The current subspace-distance is: 9.446541662327945e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.51; acc: 0.77
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3728771466453364; val_accuracy: 0.8890326433121019 

The current subspace-distance is: 9.790027979761362e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.8
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.69; acc: 0.7
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37693852537376865; val_accuracy: 0.8850517515923567 

The current subspace-distance is: 0.00010056495375465602 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.3; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.75
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37952914041508534; val_accuracy: 0.886046974522293 

The current subspace-distance is: 0.00010245925659546629 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.72
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.38463787620614287; val_accuracy: 0.8824641719745223 

The current subspace-distance is: 0.00010364416084485129 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3733918247803761; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.00010368241055402905 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.8
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.78
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.75
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.37310091994560446; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.0001058305861079134 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.72
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.37772164797517144; val_accuracy: 0.8863455414012739 

The current subspace-distance is: 0.00010743094753706828 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.81
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3913517494680016; val_accuracy: 0.8800756369426752 

The current subspace-distance is: 0.00010958224447676912 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.64; acc: 0.8
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.81
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.75
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37255556017729885; val_accuracy: 0.8871417197452229 

The current subspace-distance is: 0.00011138574336655438 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.81
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.8
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.78
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.7
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37589019071903956; val_accuracy: 0.8862460191082803 

The current subspace-distance is: 0.00011526082380441949 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.94
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.84; acc: 0.78
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3720783176980201; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.00011804760288214311 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.72
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.37618887258373246; val_accuracy: 0.8835589171974523 

The current subspace-distance is: 0.00011796072794822976 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.6; acc: 0.77
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37511014729548414; val_accuracy: 0.8878383757961783 

The current subspace-distance is: 0.00011995872046099976 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.83
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.72
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3757056750974078; val_accuracy: 0.8872412420382165 

The current subspace-distance is: 0.00012236874317750335 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.81
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3731339373596155; val_accuracy: 0.8888335987261147 

The current subspace-distance is: 0.00012616936874110252 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37370816665660045; val_accuracy: 0.8877388535031847 

The current subspace-distance is: 0.00012736798089463264 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.78
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.78
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3743186499092989; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.0001302771270275116 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.77
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.372188665732077; val_accuracy: 0.8876393312101911 

The current subspace-distance is: 0.00013050415145698935 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.47; acc: 0.8
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.78
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.75
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37241888387947325; val_accuracy: 0.8886345541401274 

The current subspace-distance is: 0.00013273864169605076 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.8
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3737520869275567; val_accuracy: 0.8878383757961783 

The current subspace-distance is: 0.0001345607015537098 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.78
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37051044803136474; val_accuracy: 0.8892316878980892 

The current subspace-distance is: 0.00013577527715824544 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.81
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37222193537434195; val_accuracy: 0.8871417197452229 

The current subspace-distance is: 0.00013666157610714436 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.77
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.81
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3740360297879596; val_accuracy: 0.8865445859872612 

The current subspace-distance is: 0.00013739206769969314 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.41; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.81
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3709220053378943; val_accuracy: 0.8888335987261147 

The current subspace-distance is: 0.00013954164751339704 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.75
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.370816350220495; val_accuracy: 0.8889331210191083 

The current subspace-distance is: 0.00014045431453268975 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.77
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.77
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37220827741607737; val_accuracy: 0.8883359872611465 

The current subspace-distance is: 0.00014259577437769622 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.77
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.75
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37220126428421896; val_accuracy: 0.8888335987261147 

The current subspace-distance is: 0.0001442521606804803 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.8
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.78
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37098978065950855; val_accuracy: 0.8883359872611465 

The current subspace-distance is: 0.0001465126988478005 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.86; acc: 0.81
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.8
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.75
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37095388404692814; val_accuracy: 0.8885350318471338 

The current subspace-distance is: 0.0001491919538239017 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.83; acc: 0.75
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.73
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37151179666731765; val_accuracy: 0.8890326433121019 

The current subspace-distance is: 0.00015144208737183362 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_120_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 27212
elements in E: 5848700
fraction nonzero: 0.004652657855591841
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.17
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.28; acc: 0.14
Batch: 220; loss: 2.28; acc: 0.14
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.28; acc: 0.2
Batch: 280; loss: 2.29; acc: 0.17
Batch: 300; loss: 2.27; acc: 0.27
Batch: 320; loss: 2.27; acc: 0.2
Batch: 340; loss: 2.28; acc: 0.22
Batch: 360; loss: 2.28; acc: 0.2
Batch: 380; loss: 2.27; acc: 0.17
Batch: 400; loss: 2.26; acc: 0.23
Batch: 420; loss: 2.26; acc: 0.25
Batch: 440; loss: 2.24; acc: 0.34
Batch: 460; loss: 2.25; acc: 0.33
Batch: 480; loss: 2.25; acc: 0.2
Batch: 500; loss: 2.25; acc: 0.27
Batch: 520; loss: 2.19; acc: 0.44
Batch: 540; loss: 2.2; acc: 0.3
Batch: 560; loss: 2.14; acc: 0.38
Batch: 580; loss: 2.15; acc: 0.34
Batch: 600; loss: 2.09; acc: 0.42
Batch: 620; loss: 2.08; acc: 0.34
Batch: 640; loss: 1.94; acc: 0.38
Batch: 660; loss: 1.86; acc: 0.44
Batch: 680; loss: 1.68; acc: 0.48
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.3; acc: 0.61
Batch: 740; loss: 1.28; acc: 0.53
Batch: 760; loss: 1.14; acc: 0.64
Batch: 780; loss: 1.18; acc: 0.59
Train Epoch over. train_loss: 2.11; train_accuracy: 0.28 

Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 2.14; acc: 0.36
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.72; acc: 0.52
Batch: 100; loss: 1.5; acc: 0.55
Batch: 120; loss: 1.66; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.66
Val Epoch over. val_loss: 1.645578328211596; val_accuracy: 0.51671974522293 

The current subspace-distance is: 8.230730600189418e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.92; acc: 0.48
Batch: 20; loss: 1.28; acc: 0.5
Batch: 40; loss: 1.12; acc: 0.59
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 0.94; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 1.05; acc: 0.64
Batch: 160; loss: 1.38; acc: 0.58
Batch: 180; loss: 0.9; acc: 0.73
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.74; acc: 0.73
Batch: 240; loss: 0.8; acc: 0.75
Batch: 260; loss: 1.08; acc: 0.66
Batch: 280; loss: 0.85; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 1.01; acc: 0.72
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.72
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.87; acc: 0.78
Batch: 420; loss: 1.15; acc: 0.66
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.94; acc: 0.72
Batch: 480; loss: 0.63; acc: 0.77
Batch: 500; loss: 0.85; acc: 0.69
Batch: 520; loss: 0.79; acc: 0.73
Batch: 540; loss: 0.87; acc: 0.75
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.92; acc: 0.77
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.81; train_accuracy: 0.75 

Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 1.42; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 0.78; acc: 0.73
Val Epoch over. val_loss: 0.9564189836857425; val_accuracy: 0.7439291401273885 

The current subspace-distance is: 1.9324952518218197e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.75
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.8; acc: 0.77
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.73; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.7
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.75
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.84
Val Epoch over. val_loss: 0.6075978878007573; val_accuracy: 0.8075238853503185 

The current subspace-distance is: 2.693416172405705e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.21; acc: 0.98
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.78; acc: 0.75
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.72
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.4880981647474751; val_accuracy: 0.8485270700636943 

The current subspace-distance is: 3.214316529920325e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.81
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.8
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.425489584209433; val_accuracy: 0.8682324840764332 

The current subspace-distance is: 3.8271169614745304e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.75
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.88
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.8
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.65; acc: 0.8
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.85; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.42262265475312616; val_accuracy: 0.8771894904458599 

The current subspace-distance is: 4.41578304162249e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.78
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.82; acc: 0.73
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.89
Val Epoch over. val_loss: 0.4625015067067116; val_accuracy: 0.8561902866242038 

The current subspace-distance is: 4.905962850898504e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.81
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.78; acc: 0.77
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.83
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.49324444657678057; val_accuracy: 0.8444466560509554 

The current subspace-distance is: 5.4030580940889195e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.74; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.81; acc: 0.81
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.73
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.84
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.89
Val Epoch over. val_loss: 0.4917231945759931; val_accuracy: 0.8530055732484076 

The current subspace-distance is: 5.804651300422847e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.61; acc: 0.78
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.4302322135134867; val_accuracy: 0.8665406050955414 

The current subspace-distance is: 6.18494741502218e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.37679486242449206; val_accuracy: 0.88953025477707 

The current subspace-distance is: 6.564492650795728e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.85; acc: 0.81
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.8
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.36235354864483427; val_accuracy: 0.8947054140127388 

The current subspace-distance is: 6.898015271872282e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.81
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.4044153384247403; val_accuracy: 0.8802746815286624 

The current subspace-distance is: 7.277602708199993e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.4209917381785478; val_accuracy: 0.8738057324840764 

The current subspace-distance is: 7.638119132025167e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.77
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3910255135054801; val_accuracy: 0.8865445859872612 

The current subspace-distance is: 7.71408376749605e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.84
Batch: 240; loss: 0.26; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3417275069150955; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 7.955157343531027e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.379180192093181; val_accuracy: 0.8891321656050956 

The current subspace-distance is: 8.183710451703519e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.39870031538662637; val_accuracy: 0.8809713375796179 

The current subspace-distance is: 8.396998600801453e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.36031977751642275; val_accuracy: 0.8945063694267515 

The current subspace-distance is: 8.780870120972395e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.98
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3860355813507062; val_accuracy: 0.8851512738853503 

The current subspace-distance is: 9.013047383632511e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.34122648612139334; val_accuracy: 0.9013734076433121 

The current subspace-distance is: 9.252186282537878e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.46; acc: 0.8
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3331510623928848; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 9.51446418184787e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.09; acc: 1.0
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3450974352230692; val_accuracy: 0.8983877388535032 

The current subspace-distance is: 9.658329508965835e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.77
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3371411712401232; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 9.963748743757606e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3386862053992642; val_accuracy: 0.9006767515923567 

The current subspace-distance is: 0.0001006792881526053 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3379770563856052; val_accuracy: 0.9025676751592356 

The current subspace-distance is: 0.00010285302414558828 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.33740878098045185; val_accuracy: 0.9020700636942676 

The current subspace-distance is: 0.00010523645323701203 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3404873784892498; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 0.00010708046465879306 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.81
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.81
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3335940274795529; val_accuracy: 0.9058519108280255 

The current subspace-distance is: 0.00010998193465638906 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.83
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.33373540689706044; val_accuracy: 0.903562898089172 

The current subspace-distance is: 0.00011325540981488302 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.330692229614516; val_accuracy: 0.9032643312101911 

The current subspace-distance is: 0.00011582308070501313 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.33687661773270106; val_accuracy: 0.9025676751592356 

The current subspace-distance is: 0.00011786864342866465 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3313142156145375; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 0.00011869204899994656 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.33095311207376465; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 0.00012044244795106351 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3319745484953094; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 0.00012560057803057134 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.83
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3321341061430752; val_accuracy: 0.9036624203821656 

The current subspace-distance is: 0.00012837207759730518 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.33101825501508775; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 0.0001297558774240315 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.8
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3361945971372021; val_accuracy: 0.9037619426751592 

The current subspace-distance is: 0.0001319002331001684 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.76; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.77
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.329275800876177; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 0.00013526540715247393 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.77
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.59; acc: 0.78
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3300475105168713; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 0.00013800831220578402 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32889288530987537; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.00013731421495322138 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.8
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3298996746255334; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.00013696214591618627 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.8
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3287375654289677; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 0.00013801094610244036 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3288206696795051; val_accuracy: 0.9049562101910829 

The current subspace-distance is: 0.00014162618026603013 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.32949371099661867; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.00014494708739221096 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.81
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3292950491189577; val_accuracy: 0.904359076433121 

The current subspace-distance is: 0.0001466074463678524 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.32921671573143857; val_accuracy: 0.9045581210191083 

The current subspace-distance is: 0.00015004364831838757 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32934821857388613; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.0001507273263996467 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.32844257513713687; val_accuracy: 0.9049562101910829 

The current subspace-distance is: 0.00015040051948744804 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.8
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3288366199251573; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 0.00015264669491443783 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_130_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 29716
elements in E: 6298600
fraction nonzero: 0.004717873813228336
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.06
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.28; acc: 0.16
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.28; acc: 0.28
Batch: 340; loss: 2.29; acc: 0.25
Batch: 360; loss: 2.29; acc: 0.23
Batch: 380; loss: 2.28; acc: 0.23
Batch: 400; loss: 2.28; acc: 0.27
Batch: 420; loss: 2.27; acc: 0.44
Batch: 440; loss: 2.28; acc: 0.27
Batch: 460; loss: 2.28; acc: 0.27
Batch: 480; loss: 2.28; acc: 0.25
Batch: 500; loss: 2.28; acc: 0.31
Batch: 520; loss: 2.26; acc: 0.27
Batch: 540; loss: 2.27; acc: 0.31
Batch: 560; loss: 2.26; acc: 0.34
Batch: 580; loss: 2.26; acc: 0.31
Batch: 600; loss: 2.26; acc: 0.31
Batch: 620; loss: 2.27; acc: 0.28
Batch: 640; loss: 2.24; acc: 0.34
Batch: 660; loss: 2.24; acc: 0.36
Batch: 680; loss: 2.24; acc: 0.27
Batch: 700; loss: 2.23; acc: 0.31
Batch: 720; loss: 2.22; acc: 0.39
Batch: 740; loss: 2.23; acc: 0.34
Batch: 760; loss: 2.2; acc: 0.45
Batch: 780; loss: 2.17; acc: 0.47
Train Epoch over. train_loss: 2.27; train_accuracy: 0.24 

Batch: 0; loss: 2.19; acc: 0.36
Batch: 20; loss: 2.22; acc: 0.34
Batch: 40; loss: 2.17; acc: 0.45
Batch: 60; loss: 2.19; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.38
Batch: 100; loss: 2.2; acc: 0.39
Batch: 120; loss: 2.18; acc: 0.42
Batch: 140; loss: 2.2; acc: 0.34
Val Epoch over. val_loss: 2.200554155240393; val_accuracy: 0.3450437898089172 

The current subspace-distance is: 6.288187250902411e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.31
Batch: 20; loss: 2.16; acc: 0.42
Batch: 40; loss: 2.17; acc: 0.31
Batch: 60; loss: 2.14; acc: 0.44
Batch: 80; loss: 2.11; acc: 0.33
Batch: 100; loss: 2.16; acc: 0.41
Batch: 120; loss: 2.08; acc: 0.38
Batch: 140; loss: 2.05; acc: 0.55
Batch: 160; loss: 2.02; acc: 0.45
Batch: 180; loss: 1.99; acc: 0.45
Batch: 200; loss: 1.92; acc: 0.41
Batch: 220; loss: 1.78; acc: 0.47
Batch: 240; loss: 1.55; acc: 0.59
Batch: 260; loss: 1.42; acc: 0.64
Batch: 280; loss: 1.26; acc: 0.62
Batch: 300; loss: 1.19; acc: 0.66
Batch: 320; loss: 1.24; acc: 0.59
Batch: 340; loss: 0.96; acc: 0.75
Batch: 360; loss: 0.92; acc: 0.69
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 1.02; acc: 0.77
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.8; acc: 0.72
Batch: 460; loss: 0.91; acc: 0.72
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.66; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.97; acc: 0.73
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.77
Batch: 680; loss: 0.58; acc: 0.8
Batch: 700; loss: 0.9; acc: 0.66
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 1.18; train_accuracy: 0.65 

Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 1.36; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 0.85; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.6; acc: 0.77
Val Epoch over. val_loss: 0.9684980374992274; val_accuracy: 0.7139729299363057 

The current subspace-distance is: 1.868575236585457e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.64
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.75
Batch: 240; loss: 0.6; acc: 0.78
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.78
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.93; acc: 0.75
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.73
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.8
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.88
Val Epoch over. val_loss: 0.5305124656030327; val_accuracy: 0.8324044585987261 

The current subspace-distance is: 2.7760033844970167e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.65; acc: 0.77
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.83
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.67; acc: 0.77
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 0.99; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.41; acc: 0.8
Val Epoch over. val_loss: 0.8022391714487865; val_accuracy: 0.761843152866242 

The current subspace-distance is: 3.434311656747013e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.35307978632248893; val_accuracy: 0.8974920382165605 

The current subspace-distance is: 3.9847891457611695e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.4047376090648827; val_accuracy: 0.8813694267515924 

The current subspace-distance is: 4.867904863203876e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.73
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.14; acc: 0.98
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.42133955785613153; val_accuracy: 0.8798765923566879 

The current subspace-distance is: 5.684321149601601e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.41986120943051236; val_accuracy: 0.8679339171974523 

The current subspace-distance is: 6.307626608759165e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.78
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.4177666320732445; val_accuracy: 0.8744028662420382 

The current subspace-distance is: 6.753687921445817e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.78
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.79; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.36955578041494275; val_accuracy: 0.8880374203821656 

The current subspace-distance is: 6.881714216433465e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.33065287640709784; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 7.49816172174178e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.8
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.35095848902395577; val_accuracy: 0.8928144904458599 

The current subspace-distance is: 7.998320506885648e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.63; acc: 0.75
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3384903974973472; val_accuracy: 0.8974920382165605 

The current subspace-distance is: 8.354533201782033e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.8
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.387350279007368; val_accuracy: 0.8855493630573248 

The current subspace-distance is: 8.512201748089865e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.8
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3722643988432398; val_accuracy: 0.8869426751592356 

The current subspace-distance is: 8.745786908548325e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.83
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.33071307425096536; val_accuracy: 0.9008757961783439 

The current subspace-distance is: 9.08255678950809e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.34621642809954417; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 9.380087431054562e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.77
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.3444749618981295; val_accuracy: 0.8985867834394905 

The current subspace-distance is: 9.798202518140897e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.95; acc: 0.75
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3544193956123036; val_accuracy: 0.8974920382165605 

The current subspace-distance is: 0.00010067108814837411 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.78
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.38333300909229145; val_accuracy: 0.8851512738853503 

The current subspace-distance is: 0.00010185460996581241 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3197799596911783; val_accuracy: 0.9065485668789809 

The current subspace-distance is: 0.0001053847445291467 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.3218065034242193; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00010695616947486997 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.83
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3247078946156866; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.00010991217277478427 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.32; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3313430125831039; val_accuracy: 0.9013734076433121 

The current subspace-distance is: 0.00011255295248702168 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.81
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.32313280400766686; val_accuracy: 0.9056528662420382 

The current subspace-distance is: 0.00011564099986571819 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.83
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.31757999107146717; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.00011730877304216847 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.94
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.32040393157939245; val_accuracy: 0.9059514331210191 

The current subspace-distance is: 0.00011828830611193553 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.8
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.8
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.32891787626561086; val_accuracy: 0.9020700636942676 

The current subspace-distance is: 0.00012026070908177644 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.81
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.32125334489117763; val_accuracy: 0.9059514331210191 

The current subspace-distance is: 0.0001216276505147107 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.81
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.78
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3158582801556891; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.00012514175614342093 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3163902648979691; val_accuracy: 0.9075437898089171 

The current subspace-distance is: 0.0001250153436558321 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.31556071231889116; val_accuracy: 0.9083399681528662 

The current subspace-distance is: 0.00012666441034525633 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3171264682510856; val_accuracy: 0.9074442675159236 

The current subspace-distance is: 0.00012663810048252344 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.3171446185772586; val_accuracy: 0.9071457006369427 

The current subspace-distance is: 0.00013015756849199533 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.31; acc: 0.84
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31716970297371505; val_accuracy: 0.9069466560509554 

The current subspace-distance is: 0.0001301105075981468 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.3178672936217041; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.00013142041279934347 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.78
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.31540844869461787; val_accuracy: 0.9090366242038217 

The current subspace-distance is: 0.00013470591511577368 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.31530539848052774; val_accuracy: 0.9080414012738853 

The current subspace-distance is: 0.00013906123058404773 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.3159804673995941; val_accuracy: 0.9088375796178344 

The current subspace-distance is: 0.0001404300273861736 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.3153298221955633; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.00014049930905457586 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3149766618279135; val_accuracy: 0.908937101910828 

The current subspace-distance is: 0.00014272157568484545 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.6; acc: 0.75
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31456266604601196; val_accuracy: 0.9085390127388535 

The current subspace-distance is: 0.00014347181422635913 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.83
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.69; acc: 0.78
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.3146751628370042; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 0.00014609828940592706 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.31520197792038035; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 0.00014803162775933743 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.31419176247659003; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 0.00014793095760978758 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.31543727442147623; val_accuracy: 0.9080414012738853 

The current subspace-distance is: 0.00015070462541189045 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3160444145464593; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.00015163520583882928 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.3; acc: 0.86
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.83
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3161009340339406; val_accuracy: 0.9079418789808917 

The current subspace-distance is: 0.00015381879347842187 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.31471814039596324; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 0.0001557589857839048 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.3144421384782548; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 0.00015818716201465577 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_140_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 31685
elements in E: 6748500
fraction nonzero: 0.004695117433503742
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.27; acc: 0.12
Batch: 320; loss: 2.27; acc: 0.22
Batch: 340; loss: 2.28; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.26; acc: 0.2
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.25; acc: 0.31
Batch: 440; loss: 2.24; acc: 0.25
Batch: 460; loss: 2.24; acc: 0.28
Batch: 480; loss: 2.23; acc: 0.2
Batch: 500; loss: 2.2; acc: 0.34
Batch: 520; loss: 2.16; acc: 0.3
Batch: 540; loss: 2.13; acc: 0.36
Batch: 560; loss: 2.0; acc: 0.47
Batch: 580; loss: 1.98; acc: 0.38
Batch: 600; loss: 1.71; acc: 0.45
Batch: 620; loss: 1.5; acc: 0.48
Batch: 640; loss: 1.16; acc: 0.61
Batch: 660; loss: 1.32; acc: 0.48
Batch: 680; loss: 1.07; acc: 0.62
Batch: 700; loss: 0.95; acc: 0.69
Batch: 720; loss: 0.81; acc: 0.72
Batch: 740; loss: 0.81; acc: 0.73
Batch: 760; loss: 0.82; acc: 0.72
Batch: 780; loss: 1.3; acc: 0.59
Train Epoch over. train_loss: 1.98; train_accuracy: 0.3 

Batch: 0; loss: 1.42; acc: 0.59
Batch: 20; loss: 1.92; acc: 0.52
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 1.09; acc: 0.62
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 0.94; acc: 0.67
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.58
Val Epoch over. val_loss: 1.336751246528261; val_accuracy: 0.6114649681528662 

The current subspace-distance is: 1.0013300197897479e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.61
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.72
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.54; acc: 0.77
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 1.62; acc: 0.69
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.73
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.75
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.72
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 1.01; acc: 0.7
Batch: 140; loss: 0.53; acc: 0.81
Val Epoch over. val_loss: 0.6425561571766616; val_accuracy: 0.7961783439490446 

The current subspace-distance is: 1.9585093468776904e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.77
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.81
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.81
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.4582262285480833; val_accuracy: 0.8589769108280255 

The current subspace-distance is: 2.552409023337532e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.8; acc: 0.73
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.72
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.88
Val Epoch over. val_loss: 0.4162427480718133; val_accuracy: 0.8641520700636943 

The current subspace-distance is: 3.286181890871376e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.75
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.4218704313229603; val_accuracy: 0.868531050955414 

The current subspace-distance is: 3.739331805263646e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 1.07; acc: 0.75
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.81
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.93; acc: 0.64
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.44022515050734684; val_accuracy: 0.8675358280254777 

The current subspace-distance is: 4.2037820094265044e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.75
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 1.13; acc: 0.66
Batch: 140; loss: 0.49; acc: 0.81
Val Epoch over. val_loss: 0.5452143969429526; val_accuracy: 0.8257364649681529 

The current subspace-distance is: 4.646616798709147e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.73
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.78
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.81
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.4881053755807269; val_accuracy: 0.8403662420382165 

The current subspace-distance is: 5.231918839854188e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.77
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.81
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.75
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.47718942246049834; val_accuracy: 0.8459394904458599 

The current subspace-distance is: 5.625763151329011e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.30766377033321723; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 6.125421350589022e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.83
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.2859241737966325; val_accuracy: 0.9107285031847133 

The current subspace-distance is: 6.551093247253448e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.8
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.65; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.4377715660698095; val_accuracy: 0.8642515923566879 

The current subspace-distance is: 6.897556158946827e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.73
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.325462081772127; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 7.229347102111205e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.34037587682532655; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 7.44415883673355e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.81
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.83
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.27861369591040214; val_accuracy: 0.9138136942675159 

The current subspace-distance is: 7.806660141795874e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.274557953854655; val_accuracy: 0.9137141719745223 

The current subspace-distance is: 8.03086586529389e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.12; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.83
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2726167258658227; val_accuracy: 0.915406050955414 

The current subspace-distance is: 8.262194751296192e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.38; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.8
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3388125747916805; val_accuracy: 0.8940087579617835 

The current subspace-distance is: 8.580440771766007e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.77
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.83
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.27075174301388155; val_accuracy: 0.9168988853503185 

The current subspace-distance is: 8.793757297098637e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2878839580496405; val_accuracy: 0.9109275477707006 

The current subspace-distance is: 9.087348007597029e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.81
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.84
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.26036993594496116; val_accuracy: 0.9185907643312102 

The current subspace-distance is: 9.336257789982483e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.7; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.25969798701583957; val_accuracy: 0.9220740445859873 

The current subspace-distance is: 9.647956903791055e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.88
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2638999844432636; val_accuracy: 0.919187898089172 

The current subspace-distance is: 9.871601650957018e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.86
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.84
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2614683383352058; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 0.0001012719512800686 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.26082942976503615; val_accuracy: 0.9183917197452229 

The current subspace-distance is: 0.00010417088924441487 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2540872241755959; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 0.00010697833204176277 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.09; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25594566639061944; val_accuracy: 0.9205812101910829 

The current subspace-distance is: 0.00010863485658774152 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2678982481170612; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 0.00011160282883793116 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.86
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.260602591499971; val_accuracy: 0.9166998407643312 

The current subspace-distance is: 0.00011375922622391954 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.62; acc: 0.75
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2544670370733662; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 0.00011404597171349451 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.83
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.25033920324721914; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00011693032138282433 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.84
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.88
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2535470690649406; val_accuracy: 0.9219745222929936 

The current subspace-distance is: 0.00011939950491068885 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.2564978326202198; val_accuracy: 0.9219745222929936 

The current subspace-distance is: 0.00012054872058797628 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2514483325990142; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 0.00012334019993431866 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.25011764884374704; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 0.00012702772801276296 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.81
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2554269818126396; val_accuracy: 0.9204816878980892 

The current subspace-distance is: 0.0001287391351070255 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.84
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2515262510081765; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 0.00013021632912568748 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2492337484078802; val_accuracy: 0.9250597133757962 

The current subspace-distance is: 0.00013387997751124203 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2493037926685658; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 0.00013688887702301145 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.24940029526971708; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 0.00013948380365036428 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.84
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.81; acc: 0.81
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.78
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.24978037517826268; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 0.00014142082363832742 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.8
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.24889548493039076; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 0.00014377012848854065 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.24; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24882337405898008; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 0.00014464231207966805 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.89
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.24921629167381365; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 0.00014719643513672054 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24848342539778182; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 0.00015051406808197498 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24923758778222807; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 0.00015370214532595128 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.88
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2493211048518776; val_accuracy: 0.9232683121019108 

The current subspace-distance is: 0.0001556884089950472 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24784998510293899; val_accuracy: 0.9254578025477707 

The current subspace-distance is: 0.00015896838158369064 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.84
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.24970344413712525; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 0.00016144079563673586 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.34; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2483255225619313; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00016250312910415232 

plots/subspace_training/reg_lenet_3/2020-01-22 14:40:21/d_dim_150_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 33905
elements in E: 7198400
fraction nonzero: 0.004710074460991332
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.12
Batch: 260; loss: 2.28; acc: 0.19
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.27; acc: 0.19
Batch: 320; loss: 2.26; acc: 0.33
Batch: 340; loss: 2.27; acc: 0.31
Batch: 360; loss: 2.27; acc: 0.3
Batch: 380; loss: 2.26; acc: 0.33
Batch: 400; loss: 2.25; acc: 0.23
Batch: 420; loss: 2.24; acc: 0.36
Batch: 440; loss: 2.24; acc: 0.23
Batch: 460; loss: 2.24; acc: 0.23
Batch: 480; loss: 2.22; acc: 0.25
Batch: 500; loss: 2.22; acc: 0.17
Batch: 520; loss: 2.15; acc: 0.33
Batch: 540; loss: 2.12; acc: 0.38
Batch: 560; loss: 2.04; acc: 0.36
Batch: 580; loss: 2.03; acc: 0.34
Batch: 600; loss: 1.88; acc: 0.41
Batch: 620; loss: 1.91; acc: 0.36
Batch: 640; loss: 1.56; acc: 0.52
Batch: 660; loss: 1.52; acc: 0.48
Batch: 680; loss: 1.39; acc: 0.58
Batch: 700; loss: 1.04; acc: 0.69
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.97; acc: 0.64
Batch: 760; loss: 0.85; acc: 0.8
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 2.02; train_accuracy: 0.32 

Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 1.34; acc: 0.48
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 1.32; acc: 0.62
Batch: 140; loss: 1.01; acc: 0.58
Val Epoch over. val_loss: 0.9216884040528801; val_accuracy: 0.6890923566878981 

The current subspace-distance is: 9.143082934315316e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.67
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.72
Batch: 100; loss: 0.72; acc: 0.72
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 1.08; acc: 0.69
Batch: 280; loss: 0.82; acc: 0.73
Batch: 300; loss: 0.98; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.85; acc: 0.73
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.83; acc: 0.73
Batch: 420; loss: 0.77; acc: 0.75
Batch: 440; loss: 0.84; acc: 0.73
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.76; acc: 0.7
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 0.99; acc: 0.69
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.74; acc: 0.73
Batch: 720; loss: 0.67; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.77
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 1.0; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.77
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.57; acc: 0.77
Val Epoch over. val_loss: 0.6475024427388124; val_accuracy: 0.7937898089171974 

The current subspace-distance is: 1.990741839108523e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.78
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.67; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.32; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.4259964030734293; val_accuracy: 0.8661425159235668 

The current subspace-distance is: 2.71979170065606e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.8
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.37757881607409494; val_accuracy: 0.8755971337579618 

The current subspace-distance is: 3.403146183700301e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.95
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.32771482247455863; val_accuracy: 0.8985867834394905 

The current subspace-distance is: 3.975466461270116e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.64; acc: 0.73
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.81; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3440618610401062; val_accuracy: 0.8937101910828026 

The current subspace-distance is: 4.519143112702295e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.51; acc: 0.78
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.383283797841353; val_accuracy: 0.879578025477707 

The current subspace-distance is: 5.1317099860170856e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.3184747065708136; val_accuracy: 0.9008757961783439 

The current subspace-distance is: 5.68476170883514e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.81
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.62; acc: 0.73
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.353549968807181; val_accuracy: 0.8923168789808917 

The current subspace-distance is: 6.0271373513387516e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.34581769732343165; val_accuracy: 0.894406847133758 

The current subspace-distance is: 6.439102435251698e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.27039510723511884; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 6.857039261376485e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26814360005460725; val_accuracy: 0.9193869426751592 

The current subspace-distance is: 7.215980440378189e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.84
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.84
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.1; acc: 1.0
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.31403768029372403; val_accuracy: 0.903562898089172 

The current subspace-distance is: 7.708649354754016e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.8
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.31282950318448105; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 8.115525270113721e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.27701462449351694; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 8.63615277921781e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.84
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.26572368894318105; val_accuracy: 0.9184912420382165 

The current subspace-distance is: 9.086056525120512e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.27597729263791615; val_accuracy: 0.9141122611464968 

The current subspace-distance is: 9.289288573199883e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2771965358524945; val_accuracy: 0.914609872611465 

The current subspace-distance is: 9.856440738076344e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2760662045923008; val_accuracy: 0.9142117834394905 

The current subspace-distance is: 9.987712110159919e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2779109211294514; val_accuracy: 0.9168988853503185 

The current subspace-distance is: 0.00010293898230884224 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2580308495623291; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 0.00010791843669721857 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2570834905383693; val_accuracy: 0.9219745222929936 

The current subspace-distance is: 0.00010923137597274035 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.259226231509523; val_accuracy: 0.9203821656050956 

The current subspace-distance is: 0.00011180995352333412 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.25985111678197126; val_accuracy: 0.9203821656050956 

The current subspace-distance is: 0.00011263584747212008 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2586567652927842; val_accuracy: 0.921875 

The current subspace-distance is: 0.00011454884224804118 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.86
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.25531458401471185; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 0.0001166183064924553 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.25674615109896964; val_accuracy: 0.9232683121019108 

The current subspace-distance is: 0.00011788055417127907 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2585947111747257; val_accuracy: 0.9234673566878981 

The current subspace-distance is: 0.00011965224257437512 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2544138923667039; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 0.00012221101496834308 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2616608010688026; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 0.00012433086521923542 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2534194500986368; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 0.00012610007252078503 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25151150099411135; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00012756210344377905 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.83
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.25214847542677715; val_accuracy: 0.9227707006369427 

The current subspace-distance is: 0.00012833016808144748 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.83
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2525448196561663; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 0.0001305610639974475 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.25224605066239075; val_accuracy: 0.9251592356687898 

The current subspace-distance is: 0.00013171460886951536 

Epoch 36 start
The current lr is: 0.06400000000000002
