model : MLP
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : False
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-22 14:34:24
nonzero elements in E: 44640
elements in E: 19921000
fraction nonzero: 0.0022408513628833894
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.32; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.17
Batch: 140; loss: 2.31; acc: 0.06
Batch: 160; loss: 2.31; acc: 0.12
Batch: 180; loss: 2.29; acc: 0.08
Batch: 200; loss: 2.29; acc: 0.19
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.19
Batch: 260; loss: 2.28; acc: 0.16
Batch: 280; loss: 2.27; acc: 0.2
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.28; acc: 0.14
Batch: 340; loss: 2.26; acc: 0.27
Batch: 360; loss: 2.26; acc: 0.23
Batch: 380; loss: 2.24; acc: 0.3
Batch: 400; loss: 2.23; acc: 0.31
Batch: 420; loss: 2.24; acc: 0.3
Batch: 440; loss: 2.24; acc: 0.22
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.25
Batch: 500; loss: 2.22; acc: 0.3
Batch: 520; loss: 2.25; acc: 0.3
Batch: 540; loss: 2.23; acc: 0.28
Batch: 560; loss: 2.21; acc: 0.38
Batch: 580; loss: 2.22; acc: 0.3
Batch: 600; loss: 2.22; acc: 0.33
Batch: 620; loss: 2.18; acc: 0.38
Batch: 640; loss: 2.18; acc: 0.34
Batch: 660; loss: 2.18; acc: 0.44
Batch: 680; loss: 2.12; acc: 0.47
Batch: 700; loss: 2.16; acc: 0.41
Batch: 720; loss: 2.15; acc: 0.41
Batch: 740; loss: 2.15; acc: 0.36
Batch: 760; loss: 2.12; acc: 0.48
Batch: 780; loss: 2.11; acc: 0.45
Train Epoch over. train_loss: 2.24; train_accuracy: 0.27 

Batch: 0; loss: 2.14; acc: 0.42
Batch: 20; loss: 2.06; acc: 0.53
Batch: 40; loss: 2.06; acc: 0.48
Batch: 60; loss: 2.1; acc: 0.44
Batch: 80; loss: 2.09; acc: 0.47
Batch: 100; loss: 2.1; acc: 0.5
Batch: 120; loss: 2.15; acc: 0.34
Batch: 140; loss: 2.05; acc: 0.52
Val Epoch over. val_loss: 2.0998780393296745; val_accuracy: 0.4680533439490446 

The current subspace-distance is: 6.9968564275768586e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.11; acc: 0.47
Batch: 20; loss: 2.11; acc: 0.41
Batch: 40; loss: 2.08; acc: 0.45
Batch: 60; loss: 2.09; acc: 0.39
Batch: 80; loss: 2.0; acc: 0.53
Batch: 100; loss: 2.01; acc: 0.44
Batch: 120; loss: 2.05; acc: 0.48
Batch: 140; loss: 2.0; acc: 0.42
Batch: 160; loss: 1.98; acc: 0.59
Batch: 180; loss: 1.99; acc: 0.45
Batch: 200; loss: 1.89; acc: 0.58
Batch: 220; loss: 1.97; acc: 0.48
Batch: 240; loss: 1.93; acc: 0.45
Batch: 260; loss: 1.94; acc: 0.44
Batch: 280; loss: 1.82; acc: 0.5
Batch: 300; loss: 1.88; acc: 0.45
Batch: 320; loss: 1.86; acc: 0.47
Batch: 340; loss: 1.68; acc: 0.53
Batch: 360; loss: 1.73; acc: 0.44
Batch: 380; loss: 1.8; acc: 0.45
Batch: 400; loss: 1.85; acc: 0.44
Batch: 420; loss: 1.64; acc: 0.59
Batch: 440; loss: 1.86; acc: 0.41
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.62; acc: 0.5
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.68; acc: 0.38
Batch: 540; loss: 1.59; acc: 0.48
Batch: 560; loss: 1.49; acc: 0.47
Batch: 580; loss: 1.38; acc: 0.59
Batch: 600; loss: 1.77; acc: 0.41
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.49; acc: 0.48
Batch: 660; loss: 1.53; acc: 0.59
Batch: 680; loss: 1.44; acc: 0.53
Batch: 700; loss: 1.53; acc: 0.5
Batch: 720; loss: 1.55; acc: 0.47
Batch: 740; loss: 1.41; acc: 0.58
Batch: 760; loss: 1.38; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.45
Train Epoch over. train_loss: 1.76; train_accuracy: 0.48 

Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 1.15; acc: 0.64
Batch: 60; loss: 1.43; acc: 0.5
Batch: 80; loss: 1.3; acc: 0.55
Batch: 100; loss: 1.32; acc: 0.56
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.21; acc: 0.61
Val Epoch over. val_loss: 1.4187178786393184; val_accuracy: 0.5305533439490446 

The current subspace-distance is: 1.904130658658687e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.52
Batch: 20; loss: 1.33; acc: 0.55
Batch: 40; loss: 1.47; acc: 0.45
Batch: 60; loss: 1.52; acc: 0.5
Batch: 80; loss: 1.42; acc: 0.5
Batch: 100; loss: 1.37; acc: 0.53
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 1.46; acc: 0.5
Batch: 160; loss: 1.75; acc: 0.45
Batch: 180; loss: 1.43; acc: 0.52
Batch: 200; loss: 1.48; acc: 0.47
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.41; acc: 0.58
Batch: 260; loss: 1.36; acc: 0.55
Batch: 280; loss: 1.55; acc: 0.5
Batch: 300; loss: 1.22; acc: 0.61
Batch: 320; loss: 1.21; acc: 0.58
Batch: 340; loss: 1.54; acc: 0.52
Batch: 360; loss: 1.11; acc: 0.62
Batch: 380; loss: 1.35; acc: 0.52
Batch: 400; loss: 1.33; acc: 0.58
Batch: 420; loss: 1.35; acc: 0.61
Batch: 440; loss: 1.16; acc: 0.64
Batch: 460; loss: 1.44; acc: 0.48
Batch: 480; loss: 1.23; acc: 0.58
Batch: 500; loss: 1.31; acc: 0.55
Batch: 520; loss: 1.28; acc: 0.59
Batch: 540; loss: 1.3; acc: 0.52
Batch: 560; loss: 1.22; acc: 0.56
Batch: 580; loss: 1.41; acc: 0.59
Batch: 600; loss: 1.33; acc: 0.48
Batch: 620; loss: 1.25; acc: 0.59
Batch: 640; loss: 1.18; acc: 0.59
Batch: 660; loss: 1.42; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.59
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.5; acc: 0.52
Batch: 740; loss: 1.46; acc: 0.55
Batch: 760; loss: 1.57; acc: 0.47
Batch: 780; loss: 1.1; acc: 0.59
Train Epoch over. train_loss: 1.35; train_accuracy: 0.55 

Batch: 0; loss: 1.45; acc: 0.5
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.92; acc: 0.67
Batch: 60; loss: 1.29; acc: 0.55
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 1.15; acc: 0.64
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.01; acc: 0.64
Val Epoch over. val_loss: 1.2512653731996086; val_accuracy: 0.5801154458598726 

The current subspace-distance is: 2.7768019208451733e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.17; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 1.4; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.53
Batch: 80; loss: 1.09; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.45
Batch: 120; loss: 1.23; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.55
Batch: 160; loss: 1.44; acc: 0.5
Batch: 180; loss: 0.92; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.61
Batch: 220; loss: 1.42; acc: 0.53
Batch: 240; loss: 1.24; acc: 0.56
Batch: 260; loss: 1.58; acc: 0.45
Batch: 280; loss: 1.17; acc: 0.61
Batch: 300; loss: 1.42; acc: 0.45
Batch: 320; loss: 1.34; acc: 0.58
Batch: 340; loss: 1.61; acc: 0.41
Batch: 360; loss: 1.0; acc: 0.66
Batch: 380; loss: 1.28; acc: 0.47
Batch: 400; loss: 1.43; acc: 0.52
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 1.11; acc: 0.64
Batch: 460; loss: 1.26; acc: 0.61
Batch: 480; loss: 1.02; acc: 0.66
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.22; acc: 0.52
Batch: 540; loss: 1.22; acc: 0.59
Batch: 560; loss: 1.35; acc: 0.56
Batch: 580; loss: 1.38; acc: 0.62
Batch: 600; loss: 1.22; acc: 0.56
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.33; acc: 0.56
Batch: 660; loss: 1.55; acc: 0.45
Batch: 680; loss: 1.29; acc: 0.52
Batch: 700; loss: 1.34; acc: 0.58
Batch: 720; loss: 1.13; acc: 0.61
Batch: 740; loss: 1.19; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.58
Batch: 780; loss: 1.38; acc: 0.48
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.38; acc: 0.47
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.7
Batch: 60; loss: 1.22; acc: 0.56
Batch: 80; loss: 0.97; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 0.91; acc: 0.67
Val Epoch over. val_loss: 1.1797108236391833; val_accuracy: 0.6026074840764332 

The current subspace-distance is: 3.362231291248463e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.52
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.59
Batch: 80; loss: 1.23; acc: 0.53
Batch: 100; loss: 1.45; acc: 0.52
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 1.24; acc: 0.59
Batch: 160; loss: 1.18; acc: 0.62
Batch: 180; loss: 1.21; acc: 0.59
Batch: 200; loss: 1.1; acc: 0.67
Batch: 220; loss: 1.36; acc: 0.59
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.3; acc: 0.58
Batch: 280; loss: 1.44; acc: 0.47
Batch: 300; loss: 1.22; acc: 0.48
Batch: 320; loss: 1.22; acc: 0.59
Batch: 340; loss: 1.35; acc: 0.5
Batch: 360; loss: 1.23; acc: 0.64
Batch: 380; loss: 1.3; acc: 0.52
Batch: 400; loss: 1.2; acc: 0.61
Batch: 420; loss: 1.26; acc: 0.5
Batch: 440; loss: 0.88; acc: 0.78
Batch: 460; loss: 1.25; acc: 0.58
Batch: 480; loss: 1.18; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.59
Batch: 520; loss: 1.14; acc: 0.62
Batch: 540; loss: 1.15; acc: 0.58
Batch: 560; loss: 1.19; acc: 0.58
Batch: 580; loss: 1.02; acc: 0.64
Batch: 600; loss: 1.26; acc: 0.55
Batch: 620; loss: 1.3; acc: 0.55
Batch: 640; loss: 1.21; acc: 0.67
Batch: 660; loss: 1.16; acc: 0.55
Batch: 680; loss: 1.33; acc: 0.53
Batch: 700; loss: 1.25; acc: 0.55
Batch: 720; loss: 1.24; acc: 0.59
Batch: 740; loss: 1.21; acc: 0.52
Batch: 760; loss: 1.34; acc: 0.52
Batch: 780; loss: 1.43; acc: 0.52
Train Epoch over. train_loss: 1.21; train_accuracy: 0.59 

Batch: 0; loss: 1.32; acc: 0.52
Batch: 20; loss: 1.21; acc: 0.59
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.53
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 0.84; acc: 0.7
Val Epoch over. val_loss: 1.1457784957946486; val_accuracy: 0.6140525477707006 

The current subspace-distance is: 3.9476522943004966e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.61
Batch: 20; loss: 1.49; acc: 0.42
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.15; acc: 0.56
Batch: 100; loss: 1.24; acc: 0.61
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 1.02; acc: 0.67
Batch: 160; loss: 1.17; acc: 0.52
Batch: 180; loss: 1.25; acc: 0.62
Batch: 200; loss: 1.15; acc: 0.59
Batch: 220; loss: 1.16; acc: 0.59
Batch: 240; loss: 1.29; acc: 0.56
Batch: 260; loss: 1.28; acc: 0.7
Batch: 280; loss: 1.11; acc: 0.58
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 1.01; acc: 0.67
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 1.13; acc: 0.66
Batch: 380; loss: 1.14; acc: 0.59
Batch: 400; loss: 1.3; acc: 0.55
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 1.42; acc: 0.5
Batch: 460; loss: 1.49; acc: 0.48
Batch: 480; loss: 1.11; acc: 0.62
Batch: 500; loss: 1.06; acc: 0.58
Batch: 520; loss: 1.08; acc: 0.62
Batch: 540; loss: 1.25; acc: 0.64
Batch: 560; loss: 1.17; acc: 0.59
Batch: 580; loss: 0.93; acc: 0.69
Batch: 600; loss: 1.1; acc: 0.64
Batch: 620; loss: 1.26; acc: 0.58
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.01; acc: 0.62
Batch: 680; loss: 1.34; acc: 0.64
Batch: 700; loss: 1.18; acc: 0.62
Batch: 720; loss: 0.97; acc: 0.64
Batch: 740; loss: 1.25; acc: 0.62
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.6 

Batch: 0; loss: 1.32; acc: 0.53
Batch: 20; loss: 1.19; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.59
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 0.77; acc: 0.75
Val Epoch over. val_loss: 1.1228639946621695; val_accuracy: 0.6213176751592356 

The current subspace-distance is: 4.3334151996532455e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.61
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 1.18; acc: 0.56
Batch: 60; loss: 1.29; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.58
Batch: 100; loss: 1.15; acc: 0.62
Batch: 120; loss: 1.27; acc: 0.58
Batch: 140; loss: 1.02; acc: 0.69
Batch: 160; loss: 1.44; acc: 0.52
Batch: 180; loss: 1.24; acc: 0.53
Batch: 200; loss: 1.28; acc: 0.55
Batch: 220; loss: 1.32; acc: 0.56
Batch: 240; loss: 1.14; acc: 0.61
Batch: 260; loss: 0.91; acc: 0.7
Batch: 280; loss: 1.41; acc: 0.53
Batch: 300; loss: 1.02; acc: 0.7
Batch: 320; loss: 1.24; acc: 0.62
Batch: 340; loss: 1.19; acc: 0.58
Batch: 360; loss: 1.22; acc: 0.61
Batch: 380; loss: 0.97; acc: 0.64
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.12; acc: 0.62
Batch: 440; loss: 1.03; acc: 0.64
Batch: 460; loss: 1.22; acc: 0.58
Batch: 480; loss: 1.06; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.58
Batch: 520; loss: 1.07; acc: 0.62
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 1.42; acc: 0.52
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.96; acc: 0.69
Batch: 620; loss: 0.97; acc: 0.7
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.61
Batch: 680; loss: 1.19; acc: 0.59
Batch: 700; loss: 1.02; acc: 0.59
Batch: 720; loss: 1.31; acc: 0.58
Batch: 740; loss: 1.36; acc: 0.59
Batch: 760; loss: 1.54; acc: 0.48
Batch: 780; loss: 1.35; acc: 0.55
Train Epoch over. train_loss: 1.16; train_accuracy: 0.61 

Batch: 0; loss: 1.31; acc: 0.53
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 0.82; acc: 0.69
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0991735112894871; val_accuracy: 0.6328622611464968 

The current subspace-distance is: 4.8272744606947526e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 1.14; acc: 0.58
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.11; acc: 0.61
Batch: 140; loss: 1.08; acc: 0.64
Batch: 160; loss: 1.08; acc: 0.66
Batch: 180; loss: 1.08; acc: 0.67
Batch: 200; loss: 1.15; acc: 0.56
Batch: 220; loss: 1.51; acc: 0.5
Batch: 240; loss: 1.11; acc: 0.64
Batch: 260; loss: 1.31; acc: 0.52
Batch: 280; loss: 0.99; acc: 0.66
Batch: 300; loss: 1.18; acc: 0.62
Batch: 320; loss: 0.76; acc: 0.72
Batch: 340; loss: 1.18; acc: 0.61
Batch: 360; loss: 1.19; acc: 0.58
Batch: 380; loss: 1.37; acc: 0.56
Batch: 400; loss: 1.29; acc: 0.61
Batch: 420; loss: 1.04; acc: 0.56
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 0.89; acc: 0.67
Batch: 480; loss: 1.33; acc: 0.56
Batch: 500; loss: 1.19; acc: 0.59
Batch: 520; loss: 1.18; acc: 0.56
Batch: 540; loss: 1.23; acc: 0.56
Batch: 560; loss: 1.27; acc: 0.59
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.18; acc: 0.59
Batch: 620; loss: 1.29; acc: 0.59
Batch: 640; loss: 1.4; acc: 0.59
Batch: 660; loss: 1.06; acc: 0.62
Batch: 680; loss: 1.01; acc: 0.64
Batch: 700; loss: 1.08; acc: 0.62
Batch: 720; loss: 1.16; acc: 0.58
Batch: 740; loss: 1.24; acc: 0.61
Batch: 760; loss: 1.6; acc: 0.48
Batch: 780; loss: 1.13; acc: 0.66
Train Epoch over. train_loss: 1.15; train_accuracy: 0.62 

Batch: 0; loss: 1.32; acc: 0.56
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 1.17; acc: 0.58
Batch: 80; loss: 0.8; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.7
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 0.7; acc: 0.83
Val Epoch over. val_loss: 1.081553910947909; val_accuracy: 0.64171974522293 

The current subspace-distance is: 5.297760071698576e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.58
Batch: 40; loss: 1.1; acc: 0.61
Batch: 60; loss: 1.1; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.53
Batch: 100; loss: 0.99; acc: 0.67
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.13; acc: 0.59
Batch: 160; loss: 1.24; acc: 0.59
Batch: 180; loss: 1.34; acc: 0.55
Batch: 200; loss: 1.06; acc: 0.64
Batch: 220; loss: 1.13; acc: 0.61
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 0.73; acc: 0.73
Batch: 280; loss: 1.16; acc: 0.7
Batch: 300; loss: 1.12; acc: 0.62
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 1.29; acc: 0.66
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.12; acc: 0.62
Batch: 400; loss: 0.93; acc: 0.61
Batch: 420; loss: 1.18; acc: 0.62
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.02; acc: 0.64
Batch: 480; loss: 1.27; acc: 0.59
Batch: 500; loss: 0.85; acc: 0.73
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 0.99; acc: 0.64
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 1.12; acc: 0.66
Batch: 600; loss: 1.04; acc: 0.62
Batch: 620; loss: 1.2; acc: 0.7
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 1.22; acc: 0.64
Batch: 680; loss: 1.11; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.61
Batch: 720; loss: 1.22; acc: 0.62
Batch: 740; loss: 1.08; acc: 0.59
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.34; acc: 0.56
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 1.33; acc: 0.55
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.78
Batch: 60; loss: 1.17; acc: 0.61
Batch: 80; loss: 0.83; acc: 0.69
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0764039603008586; val_accuracy: 0.6455015923566879 

The current subspace-distance is: 5.5095861171139404e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.7
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 0.83; acc: 0.69
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.45; acc: 0.48
Batch: 100; loss: 1.05; acc: 0.61
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.62
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 0.93; acc: 0.64
Batch: 220; loss: 0.93; acc: 0.72
Batch: 240; loss: 1.05; acc: 0.62
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 1.12; acc: 0.61
Batch: 300; loss: 1.17; acc: 0.69
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.91; acc: 0.67
Batch: 360; loss: 1.6; acc: 0.55
Batch: 380; loss: 1.24; acc: 0.62
Batch: 400; loss: 1.06; acc: 0.62
Batch: 420; loss: 1.12; acc: 0.67
Batch: 440; loss: 1.21; acc: 0.59
Batch: 460; loss: 1.11; acc: 0.64
Batch: 480; loss: 1.43; acc: 0.58
Batch: 500; loss: 1.25; acc: 0.64
Batch: 520; loss: 1.06; acc: 0.59
Batch: 540; loss: 1.24; acc: 0.59
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.16; acc: 0.61
Batch: 600; loss: 1.19; acc: 0.61
Batch: 620; loss: 1.09; acc: 0.64
Batch: 640; loss: 1.11; acc: 0.66
Batch: 660; loss: 0.98; acc: 0.7
Batch: 680; loss: 0.8; acc: 0.77
Batch: 700; loss: 1.31; acc: 0.52
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 0.97; acc: 0.69
Batch: 760; loss: 0.89; acc: 0.69
Batch: 780; loss: 1.18; acc: 0.62
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 0.8; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0677698562099676; val_accuracy: 0.6482882165605095 

The current subspace-distance is: 5.817824785481207e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 1.35; acc: 0.5
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.5
Batch: 120; loss: 1.02; acc: 0.69
Batch: 140; loss: 1.3; acc: 0.55
Batch: 160; loss: 1.33; acc: 0.58
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 1.12; acc: 0.67
Batch: 220; loss: 1.2; acc: 0.64
Batch: 240; loss: 1.09; acc: 0.62
Batch: 260; loss: 1.19; acc: 0.56
Batch: 280; loss: 1.27; acc: 0.55
Batch: 300; loss: 1.26; acc: 0.59
Batch: 320; loss: 1.17; acc: 0.67
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.34; acc: 0.56
Batch: 380; loss: 1.01; acc: 0.67
Batch: 400; loss: 1.33; acc: 0.59
Batch: 420; loss: 1.03; acc: 0.58
Batch: 440; loss: 0.95; acc: 0.73
Batch: 460; loss: 1.43; acc: 0.56
Batch: 480; loss: 0.95; acc: 0.66
Batch: 500; loss: 1.37; acc: 0.55
Batch: 520; loss: 1.22; acc: 0.59
Batch: 540; loss: 1.17; acc: 0.59
Batch: 560; loss: 1.28; acc: 0.55
Batch: 580; loss: 0.9; acc: 0.69
Batch: 600; loss: 1.28; acc: 0.53
Batch: 620; loss: 1.17; acc: 0.59
Batch: 640; loss: 1.1; acc: 0.64
Batch: 660; loss: 1.02; acc: 0.67
Batch: 680; loss: 1.02; acc: 0.61
Batch: 700; loss: 1.03; acc: 0.64
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 0.94; acc: 0.64
Batch: 760; loss: 1.03; acc: 0.59
Batch: 780; loss: 1.23; acc: 0.58
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.55
Batch: 20; loss: 1.12; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0637868403629134; val_accuracy: 0.6475915605095541 

The current subspace-distance is: 6.259323708945885e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.0; acc: 0.64
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.26; acc: 0.59
Batch: 100; loss: 1.19; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.55
Batch: 140; loss: 0.93; acc: 0.72
Batch: 160; loss: 1.14; acc: 0.56
Batch: 180; loss: 1.04; acc: 0.62
Batch: 200; loss: 0.89; acc: 0.7
Batch: 220; loss: 1.09; acc: 0.67
Batch: 240; loss: 1.25; acc: 0.52
Batch: 260; loss: 1.13; acc: 0.55
Batch: 280; loss: 1.62; acc: 0.41
Batch: 300; loss: 0.93; acc: 0.72
Batch: 320; loss: 1.07; acc: 0.62
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 1.1; acc: 0.62
Batch: 380; loss: 1.43; acc: 0.5
Batch: 400; loss: 1.15; acc: 0.61
Batch: 420; loss: 1.12; acc: 0.62
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.03; acc: 0.69
Batch: 480; loss: 0.98; acc: 0.66
Batch: 500; loss: 1.15; acc: 0.64
Batch: 520; loss: 1.26; acc: 0.62
Batch: 540; loss: 1.17; acc: 0.67
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 1.25; acc: 0.59
Batch: 620; loss: 1.14; acc: 0.59
Batch: 640; loss: 1.12; acc: 0.64
Batch: 660; loss: 0.92; acc: 0.67
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.12; acc: 0.66
Batch: 720; loss: 0.9; acc: 0.7
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.21; acc: 0.66
Batch: 780; loss: 1.12; acc: 0.59
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.32; acc: 0.5
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 0.7; acc: 0.78
Val Epoch over. val_loss: 1.0635000038298832; val_accuracy: 0.647890127388535 

The current subspace-distance is: 6.952194962650537e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.15; acc: 0.56
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.64
Batch: 100; loss: 1.19; acc: 0.58
Batch: 120; loss: 1.26; acc: 0.59
Batch: 140; loss: 1.11; acc: 0.61
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.09; acc: 0.62
Batch: 200; loss: 1.07; acc: 0.66
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 1.07; acc: 0.62
Batch: 260; loss: 1.1; acc: 0.59
Batch: 280; loss: 1.16; acc: 0.66
Batch: 300; loss: 1.36; acc: 0.5
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 1.45; acc: 0.55
Batch: 360; loss: 1.18; acc: 0.58
Batch: 380; loss: 1.34; acc: 0.62
Batch: 400; loss: 1.14; acc: 0.55
Batch: 420; loss: 1.08; acc: 0.64
Batch: 440; loss: 1.2; acc: 0.62
Batch: 460; loss: 1.1; acc: 0.62
Batch: 480; loss: 1.11; acc: 0.61
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.31; acc: 0.52
Batch: 540; loss: 1.15; acc: 0.66
Batch: 560; loss: 1.1; acc: 0.67
Batch: 580; loss: 0.93; acc: 0.7
Batch: 600; loss: 1.14; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.69
Batch: 660; loss: 1.01; acc: 0.62
Batch: 680; loss: 1.46; acc: 0.52
Batch: 700; loss: 1.22; acc: 0.59
Batch: 720; loss: 1.02; acc: 0.75
Batch: 740; loss: 1.3; acc: 0.62
Batch: 760; loss: 1.18; acc: 0.62
Batch: 780; loss: 1.24; acc: 0.58
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.32; acc: 0.48
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.062684107737936; val_accuracy: 0.6482882165605095 

The current subspace-distance is: 7.233142241602764e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 1.28; acc: 0.61
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.61
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 1.06; acc: 0.58
Batch: 200; loss: 1.2; acc: 0.64
Batch: 220; loss: 0.96; acc: 0.73
Batch: 240; loss: 1.38; acc: 0.55
Batch: 260; loss: 1.22; acc: 0.61
Batch: 280; loss: 1.38; acc: 0.56
Batch: 300; loss: 1.13; acc: 0.67
Batch: 320; loss: 1.1; acc: 0.61
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 1.53; acc: 0.52
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 1.34; acc: 0.62
Batch: 420; loss: 1.41; acc: 0.61
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 1.11; acc: 0.61
Batch: 480; loss: 1.1; acc: 0.66
Batch: 500; loss: 1.29; acc: 0.56
Batch: 520; loss: 1.31; acc: 0.5
Batch: 540; loss: 1.19; acc: 0.58
Batch: 560; loss: 1.33; acc: 0.55
Batch: 580; loss: 1.18; acc: 0.58
Batch: 600; loss: 0.99; acc: 0.7
Batch: 620; loss: 0.85; acc: 0.69
Batch: 640; loss: 1.05; acc: 0.67
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.44; acc: 0.45
Batch: 700; loss: 0.96; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.56
Batch: 740; loss: 1.38; acc: 0.58
Batch: 760; loss: 1.15; acc: 0.58
Batch: 780; loss: 1.11; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.48
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 0.71; acc: 0.77
Val Epoch over. val_loss: 1.0596317394523864; val_accuracy: 0.6491839171974523 

The current subspace-distance is: 7.684067531954497e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.56
Batch: 20; loss: 1.06; acc: 0.61
Batch: 40; loss: 1.05; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.62
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 0.98; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.61
Batch: 160; loss: 1.07; acc: 0.66
Batch: 180; loss: 1.2; acc: 0.58
Batch: 200; loss: 0.92; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.62
Batch: 240; loss: 1.37; acc: 0.62
Batch: 260; loss: 1.23; acc: 0.61
Batch: 280; loss: 1.11; acc: 0.62
Batch: 300; loss: 1.16; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.55
Batch: 340; loss: 1.39; acc: 0.58
Batch: 360; loss: 0.99; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 1.03; acc: 0.64
Batch: 420; loss: 1.27; acc: 0.52
Batch: 440; loss: 0.9; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.62
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 1.26; acc: 0.59
Batch: 520; loss: 1.03; acc: 0.62
Batch: 540; loss: 1.28; acc: 0.66
Batch: 560; loss: 1.3; acc: 0.64
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.64
Batch: 620; loss: 1.12; acc: 0.64
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.02; acc: 0.64
Batch: 680; loss: 1.15; acc: 0.62
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.11; acc: 0.62
Batch: 740; loss: 1.21; acc: 0.59
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.22; acc: 0.59
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.45
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.67
Batch: 100; loss: 1.08; acc: 0.7
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 0.72; acc: 0.77
Val Epoch over. val_loss: 1.0596123011248886; val_accuracy: 0.6491839171974523 

The current subspace-distance is: 7.990671292645857e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.19; acc: 0.59
Batch: 40; loss: 1.06; acc: 0.67
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 0.98; acc: 0.75
Batch: 100; loss: 1.37; acc: 0.61
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.64
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.61
Batch: 200; loss: 1.3; acc: 0.59
Batch: 220; loss: 1.19; acc: 0.59
Batch: 240; loss: 1.3; acc: 0.58
Batch: 260; loss: 1.23; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.67
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 0.99; acc: 0.75
Batch: 360; loss: 1.11; acc: 0.56
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 0.98; acc: 0.7
Batch: 420; loss: 1.26; acc: 0.58
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 1.08; acc: 0.59
Batch: 480; loss: 1.02; acc: 0.64
Batch: 500; loss: 1.03; acc: 0.67
Batch: 520; loss: 1.27; acc: 0.58
Batch: 540; loss: 1.01; acc: 0.64
Batch: 560; loss: 1.47; acc: 0.55
Batch: 580; loss: 1.53; acc: 0.5
Batch: 600; loss: 1.16; acc: 0.61
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 1.06; acc: 0.67
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 1.0; acc: 0.75
Batch: 700; loss: 1.34; acc: 0.59
Batch: 720; loss: 1.27; acc: 0.64
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.38; acc: 0.59
Batch: 780; loss: 0.98; acc: 0.73
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.32; acc: 0.5
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.67
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.46; acc: 0.52
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0593506105386528; val_accuracy: 0.6499800955414012 

The current subspace-distance is: 8.334675658261403e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.03; acc: 0.64
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.61
Batch: 100; loss: 1.08; acc: 0.67
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.64
Batch: 180; loss: 1.2; acc: 0.61
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 1.18; acc: 0.61
Batch: 240; loss: 1.04; acc: 0.62
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.16; acc: 0.59
Batch: 300; loss: 1.24; acc: 0.58
Batch: 320; loss: 1.16; acc: 0.61
Batch: 340; loss: 1.16; acc: 0.55
Batch: 360; loss: 1.08; acc: 0.66
Batch: 380; loss: 1.16; acc: 0.59
Batch: 400; loss: 1.07; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.55
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.38; acc: 0.56
Batch: 480; loss: 1.32; acc: 0.62
Batch: 500; loss: 0.98; acc: 0.64
Batch: 520; loss: 1.43; acc: 0.56
Batch: 540; loss: 1.02; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.62
Batch: 580; loss: 1.14; acc: 0.59
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 1.26; acc: 0.53
Batch: 640; loss: 1.12; acc: 0.53
Batch: 660; loss: 1.31; acc: 0.56
Batch: 680; loss: 1.05; acc: 0.69
Batch: 700; loss: 1.25; acc: 0.62
Batch: 720; loss: 1.07; acc: 0.66
Batch: 740; loss: 0.96; acc: 0.67
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.1; acc: 0.72
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.53
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 1.1; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.45; acc: 0.5
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.057151745838724; val_accuracy: 0.6502786624203821 

The current subspace-distance is: 8.722282655071467e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.28; acc: 0.58
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 1.06; acc: 0.75
Batch: 60; loss: 1.05; acc: 0.64
Batch: 80; loss: 1.39; acc: 0.55
Batch: 100; loss: 1.1; acc: 0.62
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 1.41; acc: 0.59
Batch: 160; loss: 1.16; acc: 0.61
Batch: 180; loss: 1.23; acc: 0.61
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.99; acc: 0.67
Batch: 240; loss: 0.87; acc: 0.75
Batch: 260; loss: 1.19; acc: 0.59
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.09; acc: 0.62
Batch: 320; loss: 1.02; acc: 0.69
Batch: 340; loss: 1.2; acc: 0.61
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 0.81; acc: 0.75
Batch: 400; loss: 1.09; acc: 0.59
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 1.13; acc: 0.58
Batch: 460; loss: 0.9; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.53
Batch: 500; loss: 1.08; acc: 0.69
Batch: 520; loss: 1.25; acc: 0.61
Batch: 540; loss: 1.18; acc: 0.61
Batch: 560; loss: 0.84; acc: 0.69
Batch: 580; loss: 1.38; acc: 0.48
Batch: 600; loss: 1.2; acc: 0.59
Batch: 620; loss: 1.3; acc: 0.56
Batch: 640; loss: 1.15; acc: 0.61
Batch: 660; loss: 1.26; acc: 0.64
Batch: 680; loss: 0.95; acc: 0.66
Batch: 700; loss: 0.98; acc: 0.7
Batch: 720; loss: 1.1; acc: 0.61
Batch: 740; loss: 1.31; acc: 0.61
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.08; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.52
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.69
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.45; acc: 0.52
Batch: 140; loss: 0.73; acc: 0.77
Val Epoch over. val_loss: 1.0570771876414111; val_accuracy: 0.6500796178343949 

The current subspace-distance is: 8.904204878490418e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.7
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.52
Batch: 140; loss: 1.12; acc: 0.59
Batch: 160; loss: 1.18; acc: 0.58
Batch: 180; loss: 1.02; acc: 0.62
Batch: 200; loss: 1.0; acc: 0.7
Batch: 220; loss: 1.03; acc: 0.59
Batch: 240; loss: 0.98; acc: 0.73
Batch: 260; loss: 1.13; acc: 0.64
Batch: 280; loss: 1.31; acc: 0.52
Batch: 300; loss: 1.42; acc: 0.56
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 1.32; acc: 0.56
Batch: 360; loss: 1.1; acc: 0.61
Batch: 380; loss: 0.98; acc: 0.73
Batch: 400; loss: 0.98; acc: 0.69
Batch: 420; loss: 1.1; acc: 0.66
Batch: 440; loss: 1.01; acc: 0.66
Batch: 460; loss: 1.17; acc: 0.59
Batch: 480; loss: 0.94; acc: 0.67
Batch: 500; loss: 1.14; acc: 0.66
Batch: 520; loss: 1.21; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.66
Batch: 560; loss: 1.03; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.62
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.84; acc: 0.69
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.09; acc: 0.64
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.05; acc: 0.59
Batch: 740; loss: 1.41; acc: 0.58
Batch: 760; loss: 1.16; acc: 0.53
Batch: 780; loss: 1.17; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 0.8; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 0.73; acc: 0.77
Val Epoch over. val_loss: 1.0565451106448083; val_accuracy: 0.6513734076433121 

The current subspace-distance is: 9.171796409646049e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.21; acc: 0.58
Batch: 20; loss: 0.91; acc: 0.67
Batch: 40; loss: 1.1; acc: 0.62
Batch: 60; loss: 1.08; acc: 0.59
Batch: 80; loss: 0.91; acc: 0.73
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.16; acc: 0.59
Batch: 140; loss: 0.95; acc: 0.67
Batch: 160; loss: 0.93; acc: 0.64
Batch: 180; loss: 1.12; acc: 0.62
Batch: 200; loss: 1.29; acc: 0.58
Batch: 220; loss: 1.11; acc: 0.64
Batch: 240; loss: 0.98; acc: 0.69
Batch: 260; loss: 1.15; acc: 0.62
Batch: 280; loss: 1.23; acc: 0.55
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 0.95; acc: 0.7
Batch: 340; loss: 1.36; acc: 0.52
Batch: 360; loss: 1.05; acc: 0.72
Batch: 380; loss: 0.74; acc: 0.75
Batch: 400; loss: 1.03; acc: 0.66
Batch: 420; loss: 1.04; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.58
Batch: 460; loss: 0.85; acc: 0.75
Batch: 480; loss: 1.34; acc: 0.59
Batch: 500; loss: 1.06; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.61
Batch: 540; loss: 1.01; acc: 0.69
Batch: 560; loss: 1.05; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.69
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 1.06; acc: 0.58
Batch: 640; loss: 1.49; acc: 0.44
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.28; acc: 0.62
Batch: 700; loss: 1.29; acc: 0.59
Batch: 720; loss: 1.15; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 0.89; acc: 0.69
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.5
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 0.74; acc: 0.78
Val Epoch over. val_loss: 1.0565607779344934; val_accuracy: 0.6513734076433121 

The current subspace-distance is: 9.45703504839912e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.02; acc: 0.7
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 1.12; acc: 0.59
Batch: 60; loss: 1.13; acc: 0.62
Batch: 80; loss: 1.11; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.44
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.15; acc: 0.64
Batch: 160; loss: 1.01; acc: 0.61
Batch: 180; loss: 0.98; acc: 0.66
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.94; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.35; acc: 0.56
Batch: 280; loss: 1.05; acc: 0.66
Batch: 300; loss: 0.78; acc: 0.72
Batch: 320; loss: 1.02; acc: 0.64
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.01; acc: 0.62
Batch: 380; loss: 1.13; acc: 0.58
Batch: 400; loss: 1.25; acc: 0.58
Batch: 420; loss: 1.39; acc: 0.59
Batch: 440; loss: 0.85; acc: 0.73
Batch: 460; loss: 1.06; acc: 0.69
Batch: 480; loss: 1.05; acc: 0.64
Batch: 500; loss: 1.2; acc: 0.59
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 1.26; acc: 0.58
Batch: 580; loss: 1.19; acc: 0.61
Batch: 600; loss: 1.22; acc: 0.66
Batch: 620; loss: 1.22; acc: 0.53
Batch: 640; loss: 1.35; acc: 0.58
Batch: 660; loss: 1.12; acc: 0.67
Batch: 680; loss: 1.03; acc: 0.66
Batch: 700; loss: 1.03; acc: 0.69
Batch: 720; loss: 1.19; acc: 0.58
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.07; acc: 0.69
Batch: 780; loss: 1.1; acc: 0.66
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.74; acc: 0.77
Val Epoch over. val_loss: 1.0557141448282132; val_accuracy: 0.6518710191082803 

The current subspace-distance is: 0.00010003280476666987 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 1.3; acc: 0.55
Batch: 60; loss: 1.04; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 1.32; acc: 0.55
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.93; acc: 0.7
Batch: 200; loss: 1.41; acc: 0.61
Batch: 220; loss: 1.0; acc: 0.67
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.59
Batch: 280; loss: 0.78; acc: 0.69
Batch: 300; loss: 0.91; acc: 0.7
Batch: 320; loss: 0.96; acc: 0.61
Batch: 340; loss: 1.12; acc: 0.61
Batch: 360; loss: 0.85; acc: 0.73
Batch: 380; loss: 0.93; acc: 0.7
Batch: 400; loss: 1.22; acc: 0.61
Batch: 420; loss: 0.88; acc: 0.67
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 1.4; acc: 0.58
Batch: 480; loss: 1.43; acc: 0.53
Batch: 500; loss: 1.13; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.56
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.11; acc: 0.66
Batch: 580; loss: 1.06; acc: 0.61
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.08; acc: 0.66
Batch: 640; loss: 1.3; acc: 0.59
Batch: 660; loss: 1.32; acc: 0.55
Batch: 680; loss: 1.06; acc: 0.66
Batch: 700; loss: 0.98; acc: 0.75
Batch: 720; loss: 1.31; acc: 0.55
Batch: 740; loss: 1.05; acc: 0.66
Batch: 760; loss: 0.98; acc: 0.72
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.74; acc: 0.77
Val Epoch over. val_loss: 1.0557832896329795; val_accuracy: 0.6511743630573248 

The current subspace-distance is: 0.0001038117115967907 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.94; acc: 0.69
Batch: 20; loss: 1.35; acc: 0.67
Batch: 40; loss: 1.2; acc: 0.62
Batch: 60; loss: 1.18; acc: 0.62
Batch: 80; loss: 0.95; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.52
Batch: 120; loss: 1.29; acc: 0.52
Batch: 140; loss: 1.2; acc: 0.62
Batch: 160; loss: 0.98; acc: 0.64
Batch: 180; loss: 1.01; acc: 0.66
Batch: 200; loss: 1.27; acc: 0.58
Batch: 220; loss: 1.02; acc: 0.67
Batch: 240; loss: 1.35; acc: 0.5
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 1.12; acc: 0.62
Batch: 300; loss: 0.82; acc: 0.66
Batch: 320; loss: 1.28; acc: 0.53
Batch: 340; loss: 1.01; acc: 0.77
Batch: 360; loss: 1.26; acc: 0.58
Batch: 380; loss: 1.3; acc: 0.56
Batch: 400; loss: 1.03; acc: 0.61
Batch: 420; loss: 1.06; acc: 0.66
Batch: 440; loss: 1.05; acc: 0.64
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 1.28; acc: 0.55
Batch: 500; loss: 1.12; acc: 0.56
Batch: 520; loss: 0.93; acc: 0.72
Batch: 540; loss: 1.06; acc: 0.61
Batch: 560; loss: 1.09; acc: 0.66
Batch: 580; loss: 1.27; acc: 0.56
Batch: 600; loss: 1.18; acc: 0.59
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 1.02; acc: 0.62
Batch: 660; loss: 1.07; acc: 0.66
Batch: 680; loss: 0.94; acc: 0.69
Batch: 700; loss: 0.92; acc: 0.67
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 1.32; acc: 0.62
Batch: 760; loss: 1.03; acc: 0.62
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 0.74; acc: 0.78
Val Epoch over. val_loss: 1.0555485061779144; val_accuracy: 0.6525676751592356 

The current subspace-distance is: 0.00010407486115582287 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.04; acc: 0.64
Batch: 40; loss: 1.0; acc: 0.66
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.08; acc: 0.61
Batch: 100; loss: 1.16; acc: 0.61
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.07; acc: 0.64
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 1.27; acc: 0.55
Batch: 200; loss: 1.18; acc: 0.62
Batch: 220; loss: 1.02; acc: 0.67
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.1; acc: 0.61
Batch: 280; loss: 1.26; acc: 0.58
Batch: 300; loss: 1.0; acc: 0.67
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 1.21; acc: 0.62
Batch: 380; loss: 1.06; acc: 0.62
Batch: 400; loss: 1.24; acc: 0.61
Batch: 420; loss: 0.91; acc: 0.8
Batch: 440; loss: 1.06; acc: 0.62
Batch: 460; loss: 1.29; acc: 0.55
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 0.99; acc: 0.72
Batch: 540; loss: 1.14; acc: 0.61
Batch: 560; loss: 1.02; acc: 0.64
Batch: 580; loss: 0.98; acc: 0.64
Batch: 600; loss: 1.12; acc: 0.61
Batch: 620; loss: 1.15; acc: 0.59
Batch: 640; loss: 1.28; acc: 0.58
Batch: 660; loss: 0.85; acc: 0.75
Batch: 680; loss: 1.39; acc: 0.58
Batch: 700; loss: 0.92; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.62
Batch: 740; loss: 0.99; acc: 0.78
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.13; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.74; acc: 0.77
Val Epoch over. val_loss: 1.0550008317467514; val_accuracy: 0.6517714968152867 

The current subspace-distance is: 0.00010694311640691012 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.93; acc: 0.75
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 1.12; acc: 0.56
Batch: 100; loss: 1.14; acc: 0.55
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 1.25; acc: 0.59
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 1.31; acc: 0.61
Batch: 200; loss: 1.36; acc: 0.55
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 1.1; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.55
Batch: 280; loss: 1.16; acc: 0.64
Batch: 300; loss: 1.15; acc: 0.58
Batch: 320; loss: 1.17; acc: 0.59
Batch: 340; loss: 0.9; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.58
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.12; acc: 0.62
Batch: 420; loss: 1.15; acc: 0.66
Batch: 440; loss: 1.11; acc: 0.64
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.38; acc: 0.58
Batch: 500; loss: 0.95; acc: 0.72
Batch: 520; loss: 1.38; acc: 0.45
Batch: 540; loss: 1.14; acc: 0.61
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 0.99; acc: 0.69
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 1.43; acc: 0.62
Batch: 660; loss: 0.97; acc: 0.67
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 1.04; acc: 0.67
Batch: 720; loss: 0.93; acc: 0.66
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 1.11; acc: 0.62
Batch: 780; loss: 1.24; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.74; acc: 0.77
Val Epoch over. val_loss: 1.0551145585479251; val_accuracy: 0.6515724522292994 

The current subspace-distance is: 0.00010795704292831942 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.69
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.91; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.59
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 1.13; acc: 0.66
Batch: 160; loss: 1.22; acc: 0.59
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.29; acc: 0.58
Batch: 220; loss: 1.09; acc: 0.59
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 0.91; acc: 0.69
Batch: 280; loss: 1.01; acc: 0.64
Batch: 300; loss: 1.13; acc: 0.61
Batch: 320; loss: 0.99; acc: 0.66
Batch: 340; loss: 1.29; acc: 0.59
Batch: 360; loss: 1.35; acc: 0.48
Batch: 380; loss: 0.91; acc: 0.77
Batch: 400; loss: 0.93; acc: 0.61
Batch: 420; loss: 1.01; acc: 0.66
Batch: 440; loss: 1.27; acc: 0.58
Batch: 460; loss: 1.13; acc: 0.59
Batch: 480; loss: 1.17; acc: 0.56
Batch: 500; loss: 1.08; acc: 0.56
Batch: 520; loss: 1.36; acc: 0.52
Batch: 540; loss: 1.22; acc: 0.62
Batch: 560; loss: 1.3; acc: 0.58
Batch: 580; loss: 1.36; acc: 0.58
Batch: 600; loss: 1.01; acc: 0.7
Batch: 620; loss: 1.2; acc: 0.58
Batch: 640; loss: 0.98; acc: 0.67
Batch: 660; loss: 0.97; acc: 0.73
Batch: 680; loss: 1.28; acc: 0.59
Batch: 700; loss: 1.09; acc: 0.64
Batch: 720; loss: 1.0; acc: 0.64
Batch: 740; loss: 1.06; acc: 0.59
Batch: 760; loss: 1.23; acc: 0.56
Batch: 780; loss: 1.05; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.78
Val Epoch over. val_loss: 1.0554617434550242; val_accuracy: 0.6515724522292994 

The current subspace-distance is: 0.00010866738739423454 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.05; acc: 0.66
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.52
Batch: 120; loss: 1.25; acc: 0.53
Batch: 140; loss: 1.05; acc: 0.62
Batch: 160; loss: 1.15; acc: 0.58
Batch: 180; loss: 1.0; acc: 0.59
Batch: 200; loss: 0.96; acc: 0.67
Batch: 220; loss: 1.12; acc: 0.58
Batch: 240; loss: 1.05; acc: 0.61
Batch: 260; loss: 1.13; acc: 0.62
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 1.22; acc: 0.62
Batch: 320; loss: 0.99; acc: 0.69
Batch: 340; loss: 1.4; acc: 0.55
Batch: 360; loss: 1.06; acc: 0.64
Batch: 380; loss: 1.06; acc: 0.62
Batch: 400; loss: 1.09; acc: 0.62
Batch: 420; loss: 1.23; acc: 0.58
Batch: 440; loss: 1.26; acc: 0.56
Batch: 460; loss: 1.09; acc: 0.59
Batch: 480; loss: 1.15; acc: 0.55
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 1.16; acc: 0.64
Batch: 560; loss: 1.16; acc: 0.56
Batch: 580; loss: 1.17; acc: 0.66
Batch: 600; loss: 1.31; acc: 0.58
Batch: 620; loss: 1.28; acc: 0.58
Batch: 640; loss: 1.16; acc: 0.56
Batch: 660; loss: 1.13; acc: 0.66
Batch: 680; loss: 1.12; acc: 0.62
Batch: 700; loss: 1.19; acc: 0.62
Batch: 720; loss: 1.58; acc: 0.45
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 1.11; acc: 0.59
Batch: 780; loss: 1.27; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.74; acc: 0.77
Val Epoch over. val_loss: 1.0550684845371612; val_accuracy: 0.6511743630573248 

The current subspace-distance is: 0.00011078968236688524 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.59
Batch: 40; loss: 0.76; acc: 0.73
Batch: 60; loss: 1.35; acc: 0.56
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.01; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.7
Batch: 160; loss: 1.11; acc: 0.61
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.21; acc: 0.58
Batch: 220; loss: 1.26; acc: 0.58
Batch: 240; loss: 1.15; acc: 0.66
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.66
Batch: 300; loss: 0.93; acc: 0.73
Batch: 320; loss: 1.08; acc: 0.66
Batch: 340; loss: 1.3; acc: 0.61
Batch: 360; loss: 1.28; acc: 0.56
Batch: 380; loss: 1.04; acc: 0.64
Batch: 400; loss: 1.11; acc: 0.59
Batch: 420; loss: 1.16; acc: 0.64
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.35; acc: 0.5
Batch: 480; loss: 1.19; acc: 0.59
Batch: 500; loss: 1.4; acc: 0.58
Batch: 520; loss: 0.99; acc: 0.67
Batch: 540; loss: 1.39; acc: 0.5
Batch: 560; loss: 1.22; acc: 0.62
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.0; acc: 0.59
Batch: 620; loss: 1.3; acc: 0.56
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.27; acc: 0.58
Batch: 680; loss: 1.55; acc: 0.48
Batch: 700; loss: 0.96; acc: 0.73
Batch: 720; loss: 1.02; acc: 0.67
Batch: 740; loss: 1.35; acc: 0.61
Batch: 760; loss: 1.14; acc: 0.67
Batch: 780; loss: 1.2; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0553754458002225; val_accuracy: 0.6508757961783439 

The current subspace-distance is: 0.00011225148773519322 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.1; acc: 0.58
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 1.11; acc: 0.61
Batch: 60; loss: 0.99; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.61
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.61
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 1.25; acc: 0.58
Batch: 200; loss: 1.36; acc: 0.52
Batch: 220; loss: 1.1; acc: 0.66
Batch: 240; loss: 1.32; acc: 0.5
Batch: 260; loss: 1.01; acc: 0.59
Batch: 280; loss: 1.28; acc: 0.58
Batch: 300; loss: 1.06; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.58
Batch: 340; loss: 1.34; acc: 0.52
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 0.98; acc: 0.69
Batch: 400; loss: 0.95; acc: 0.78
Batch: 420; loss: 1.29; acc: 0.62
Batch: 440; loss: 1.14; acc: 0.67
Batch: 460; loss: 1.02; acc: 0.69
Batch: 480; loss: 1.05; acc: 0.56
Batch: 500; loss: 0.94; acc: 0.7
Batch: 520; loss: 1.17; acc: 0.59
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.56
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.61
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 0.98; acc: 0.67
Batch: 660; loss: 1.34; acc: 0.58
Batch: 680; loss: 1.04; acc: 0.61
Batch: 700; loss: 0.8; acc: 0.73
Batch: 720; loss: 1.07; acc: 0.64
Batch: 740; loss: 1.31; acc: 0.5
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.25; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550173281863997; val_accuracy: 0.6506767515923567 

The current subspace-distance is: 0.00011389396240701899 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 1.22; acc: 0.72
Batch: 40; loss: 0.93; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.58
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.42; acc: 0.55
Batch: 140; loss: 1.17; acc: 0.69
Batch: 160; loss: 1.08; acc: 0.66
Batch: 180; loss: 0.92; acc: 0.67
Batch: 200; loss: 1.19; acc: 0.62
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 1.08; acc: 0.59
Batch: 260; loss: 1.02; acc: 0.64
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.1; acc: 0.62
Batch: 320; loss: 1.25; acc: 0.55
Batch: 340; loss: 1.15; acc: 0.7
Batch: 360; loss: 1.18; acc: 0.56
Batch: 380; loss: 1.17; acc: 0.62
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.14; acc: 0.58
Batch: 440; loss: 1.09; acc: 0.67
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.17; acc: 0.56
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.3; acc: 0.59
Batch: 540; loss: 1.0; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.58
Batch: 580; loss: 1.14; acc: 0.58
Batch: 600; loss: 1.12; acc: 0.56
Batch: 620; loss: 1.03; acc: 0.59
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.09; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.5
Batch: 700; loss: 1.14; acc: 0.61
Batch: 720; loss: 1.45; acc: 0.5
Batch: 740; loss: 1.27; acc: 0.62
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.2; acc: 0.55
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0551432150944022; val_accuracy: 0.6506767515923567 

The current subspace-distance is: 0.00011563621228560805 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 1.33; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 0.98; acc: 0.69
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.04; acc: 0.67
Batch: 160; loss: 1.12; acc: 0.61
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 0.81; acc: 0.75
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.24; acc: 0.56
Batch: 260; loss: 1.14; acc: 0.62
Batch: 280; loss: 1.13; acc: 0.53
Batch: 300; loss: 1.05; acc: 0.59
Batch: 320; loss: 1.19; acc: 0.66
Batch: 340; loss: 1.27; acc: 0.58
Batch: 360; loss: 1.16; acc: 0.61
Batch: 380; loss: 0.93; acc: 0.66
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 1.25; acc: 0.62
Batch: 440; loss: 1.24; acc: 0.56
Batch: 460; loss: 1.14; acc: 0.61
Batch: 480; loss: 1.16; acc: 0.66
Batch: 500; loss: 1.18; acc: 0.61
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 1.26; acc: 0.56
Batch: 560; loss: 1.18; acc: 0.59
Batch: 580; loss: 1.08; acc: 0.62
Batch: 600; loss: 1.23; acc: 0.58
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.12; acc: 0.53
Batch: 660; loss: 1.26; acc: 0.59
Batch: 680; loss: 1.23; acc: 0.58
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.19; acc: 0.58
Batch: 740; loss: 0.99; acc: 0.69
Batch: 760; loss: 1.13; acc: 0.64
Batch: 780; loss: 0.89; acc: 0.75
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0551693082615068; val_accuracy: 0.6510748407643312 

The current subspace-distance is: 0.00011643015750451013 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.0; acc: 0.7
Batch: 20; loss: 1.45; acc: 0.48
Batch: 40; loss: 1.02; acc: 0.59
Batch: 60; loss: 1.14; acc: 0.59
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 1.11; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.66
Batch: 160; loss: 1.32; acc: 0.58
Batch: 180; loss: 1.35; acc: 0.59
Batch: 200; loss: 1.1; acc: 0.66
Batch: 220; loss: 0.89; acc: 0.72
Batch: 240; loss: 1.47; acc: 0.55
Batch: 260; loss: 1.17; acc: 0.62
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 1.29; acc: 0.56
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.04; acc: 0.64
Batch: 360; loss: 1.1; acc: 0.62
Batch: 380; loss: 1.11; acc: 0.62
Batch: 400; loss: 1.27; acc: 0.64
Batch: 420; loss: 1.06; acc: 0.64
Batch: 440; loss: 1.15; acc: 0.61
Batch: 460; loss: 1.05; acc: 0.67
Batch: 480; loss: 1.12; acc: 0.61
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.1; acc: 0.61
Batch: 540; loss: 1.01; acc: 0.61
Batch: 560; loss: 1.01; acc: 0.7
Batch: 580; loss: 1.15; acc: 0.5
Batch: 600; loss: 1.38; acc: 0.59
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 1.22; acc: 0.56
Batch: 660; loss: 1.18; acc: 0.61
Batch: 680; loss: 1.13; acc: 0.64
Batch: 700; loss: 1.08; acc: 0.62
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.17; acc: 0.56
Batch: 760; loss: 1.11; acc: 0.62
Batch: 780; loss: 1.07; acc: 0.64
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.55
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0551872169895538; val_accuracy: 0.6510748407643312 

The current subspace-distance is: 0.00011904053098987788 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.4; acc: 0.58
Batch: 20; loss: 1.29; acc: 0.66
Batch: 40; loss: 1.04; acc: 0.56
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.62
Batch: 120; loss: 1.02; acc: 0.62
Batch: 140; loss: 0.85; acc: 0.69
Batch: 160; loss: 1.22; acc: 0.56
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.21; acc: 0.56
Batch: 220; loss: 1.32; acc: 0.55
Batch: 240; loss: 1.14; acc: 0.55
Batch: 260; loss: 1.1; acc: 0.59
Batch: 280; loss: 1.42; acc: 0.55
Batch: 300; loss: 0.96; acc: 0.69
Batch: 320; loss: 1.14; acc: 0.61
Batch: 340; loss: 1.12; acc: 0.59
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 1.18; acc: 0.59
Batch: 400; loss: 1.23; acc: 0.62
Batch: 420; loss: 1.13; acc: 0.59
Batch: 440; loss: 1.17; acc: 0.61
Batch: 460; loss: 0.94; acc: 0.72
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 1.24; acc: 0.56
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.16; acc: 0.64
Batch: 600; loss: 1.4; acc: 0.44
Batch: 620; loss: 1.17; acc: 0.62
Batch: 640; loss: 1.29; acc: 0.58
Batch: 660; loss: 1.19; acc: 0.59
Batch: 680; loss: 1.23; acc: 0.53
Batch: 700; loss: 1.18; acc: 0.61
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.13; acc: 0.58
Batch: 760; loss: 1.03; acc: 0.72
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550083814153246; val_accuracy: 0.6501791401273885 

The current subspace-distance is: 0.00012124160275561735 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.35; acc: 0.59
Batch: 20; loss: 1.1; acc: 0.58
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.09; acc: 0.61
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 0.77; acc: 0.7
Batch: 120; loss: 1.05; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.61
Batch: 160; loss: 1.04; acc: 0.69
Batch: 180; loss: 1.31; acc: 0.56
Batch: 200; loss: 0.98; acc: 0.69
Batch: 220; loss: 1.08; acc: 0.62
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 1.0; acc: 0.66
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 1.1; acc: 0.66
Batch: 360; loss: 1.23; acc: 0.64
Batch: 380; loss: 1.21; acc: 0.58
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 0.86; acc: 0.72
Batch: 440; loss: 1.03; acc: 0.66
Batch: 460; loss: 1.17; acc: 0.62
Batch: 480; loss: 1.01; acc: 0.64
Batch: 500; loss: 1.18; acc: 0.69
Batch: 520; loss: 1.16; acc: 0.62
Batch: 540; loss: 1.1; acc: 0.53
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.07; acc: 0.64
Batch: 600; loss: 1.16; acc: 0.59
Batch: 620; loss: 1.16; acc: 0.7
Batch: 640; loss: 1.22; acc: 0.66
Batch: 660; loss: 1.27; acc: 0.59
Batch: 680; loss: 1.21; acc: 0.56
Batch: 700; loss: 1.04; acc: 0.64
Batch: 720; loss: 0.99; acc: 0.67
Batch: 740; loss: 1.4; acc: 0.53
Batch: 760; loss: 1.19; acc: 0.55
Batch: 780; loss: 1.24; acc: 0.48
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550315979939358; val_accuracy: 0.6509753184713376 

The current subspace-distance is: 0.00012181288911961019 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.93; acc: 0.64
Batch: 20; loss: 0.95; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.5
Batch: 60; loss: 1.22; acc: 0.61
Batch: 80; loss: 1.15; acc: 0.55
Batch: 100; loss: 1.4; acc: 0.52
Batch: 120; loss: 0.93; acc: 0.64
Batch: 140; loss: 1.26; acc: 0.66
Batch: 160; loss: 1.02; acc: 0.72
Batch: 180; loss: 0.94; acc: 0.72
Batch: 200; loss: 1.13; acc: 0.58
Batch: 220; loss: 1.18; acc: 0.64
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 1.12; acc: 0.69
Batch: 280; loss: 1.21; acc: 0.58
Batch: 300; loss: 1.15; acc: 0.61
Batch: 320; loss: 1.15; acc: 0.61
Batch: 340; loss: 1.05; acc: 0.7
Batch: 360; loss: 0.91; acc: 0.72
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 0.96; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.61
Batch: 440; loss: 1.09; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.55
Batch: 480; loss: 1.1; acc: 0.62
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 1.06; acc: 0.66
Batch: 540; loss: 1.11; acc: 0.59
Batch: 560; loss: 1.09; acc: 0.66
Batch: 580; loss: 1.2; acc: 0.62
Batch: 600; loss: 1.24; acc: 0.62
Batch: 620; loss: 1.03; acc: 0.67
Batch: 640; loss: 0.96; acc: 0.7
Batch: 660; loss: 1.17; acc: 0.67
Batch: 680; loss: 0.89; acc: 0.69
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 0.96; acc: 0.7
Batch: 740; loss: 1.15; acc: 0.62
Batch: 760; loss: 0.73; acc: 0.75
Batch: 780; loss: 1.28; acc: 0.56
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0552531412452648; val_accuracy: 0.6509753184713376 

The current subspace-distance is: 0.0001233519142260775 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.56
Batch: 100; loss: 1.1; acc: 0.62
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 1.5; acc: 0.56
Batch: 180; loss: 1.08; acc: 0.64
Batch: 200; loss: 1.07; acc: 0.66
Batch: 220; loss: 1.35; acc: 0.56
Batch: 240; loss: 1.18; acc: 0.67
Batch: 260; loss: 0.94; acc: 0.7
Batch: 280; loss: 1.15; acc: 0.59
Batch: 300; loss: 1.18; acc: 0.62
Batch: 320; loss: 1.19; acc: 0.66
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 0.98; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.7
Batch: 400; loss: 0.93; acc: 0.67
Batch: 420; loss: 0.95; acc: 0.67
Batch: 440; loss: 1.05; acc: 0.69
Batch: 460; loss: 0.97; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 1.24; acc: 0.66
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 0.98; acc: 0.64
Batch: 580; loss: 1.41; acc: 0.58
Batch: 600; loss: 1.28; acc: 0.64
Batch: 620; loss: 1.1; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.62
Batch: 660; loss: 1.17; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.67
Batch: 700; loss: 1.34; acc: 0.59
Batch: 720; loss: 1.17; acc: 0.66
Batch: 740; loss: 1.32; acc: 0.52
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.14; acc: 0.61
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0551833707815523; val_accuracy: 0.6513734076433121 

The current subspace-distance is: 0.0001251870853593573 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.24; acc: 0.55
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 1.19; acc: 0.66
Batch: 80; loss: 1.01; acc: 0.64
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.08; acc: 0.61
Batch: 160; loss: 1.05; acc: 0.59
Batch: 180; loss: 1.17; acc: 0.66
Batch: 200; loss: 1.12; acc: 0.62
Batch: 220; loss: 0.99; acc: 0.67
Batch: 240; loss: 0.95; acc: 0.72
Batch: 260; loss: 1.43; acc: 0.53
Batch: 280; loss: 1.15; acc: 0.62
Batch: 300; loss: 1.28; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.48
Batch: 340; loss: 1.48; acc: 0.44
Batch: 360; loss: 1.24; acc: 0.62
Batch: 380; loss: 1.05; acc: 0.73
Batch: 400; loss: 1.03; acc: 0.67
Batch: 420; loss: 1.24; acc: 0.56
Batch: 440; loss: 1.43; acc: 0.52
Batch: 460; loss: 1.2; acc: 0.62
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.84; acc: 0.72
Batch: 520; loss: 1.22; acc: 0.56
Batch: 540; loss: 1.05; acc: 0.67
Batch: 560; loss: 1.23; acc: 0.66
Batch: 580; loss: 1.41; acc: 0.55
Batch: 600; loss: 1.46; acc: 0.56
Batch: 620; loss: 1.07; acc: 0.66
Batch: 640; loss: 0.98; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.05; acc: 0.64
Batch: 700; loss: 1.31; acc: 0.53
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 0.9; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550985548906266; val_accuracy: 0.6504777070063694 

The current subspace-distance is: 0.00012735463678836823 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 1.33; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.56
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.62
Batch: 160; loss: 1.03; acc: 0.7
Batch: 180; loss: 1.03; acc: 0.61
Batch: 200; loss: 1.19; acc: 0.62
Batch: 220; loss: 0.98; acc: 0.7
Batch: 240; loss: 1.09; acc: 0.62
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.93; acc: 0.67
Batch: 300; loss: 1.0; acc: 0.64
Batch: 320; loss: 1.05; acc: 0.7
Batch: 340; loss: 1.2; acc: 0.58
Batch: 360; loss: 1.17; acc: 0.58
Batch: 380; loss: 1.42; acc: 0.52
Batch: 400; loss: 1.08; acc: 0.66
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 0.98; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.66
Batch: 480; loss: 1.02; acc: 0.64
Batch: 500; loss: 1.34; acc: 0.64
Batch: 520; loss: 1.06; acc: 0.62
Batch: 540; loss: 1.12; acc: 0.59
Batch: 560; loss: 1.14; acc: 0.56
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 0.99; acc: 0.62
Batch: 620; loss: 1.29; acc: 0.55
Batch: 640; loss: 1.06; acc: 0.56
Batch: 660; loss: 1.14; acc: 0.64
Batch: 680; loss: 1.23; acc: 0.69
Batch: 700; loss: 0.98; acc: 0.61
Batch: 720; loss: 0.84; acc: 0.75
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 1.13; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550202658981274; val_accuracy: 0.6500796178343949 

The current subspace-distance is: 0.00012950370728503913 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 1.09; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.61
Batch: 160; loss: 0.93; acc: 0.67
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.15; acc: 0.64
Batch: 220; loss: 1.0; acc: 0.66
Batch: 240; loss: 1.31; acc: 0.62
Batch: 260; loss: 0.93; acc: 0.67
Batch: 280; loss: 0.98; acc: 0.72
Batch: 300; loss: 1.0; acc: 0.64
Batch: 320; loss: 1.1; acc: 0.62
Batch: 340; loss: 1.0; acc: 0.64
Batch: 360; loss: 1.23; acc: 0.53
Batch: 380; loss: 1.37; acc: 0.53
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 1.31; acc: 0.53
Batch: 440; loss: 0.87; acc: 0.77
Batch: 460; loss: 1.13; acc: 0.62
Batch: 480; loss: 1.34; acc: 0.48
Batch: 500; loss: 1.36; acc: 0.59
Batch: 520; loss: 1.01; acc: 0.64
Batch: 540; loss: 1.37; acc: 0.56
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.2; acc: 0.59
Batch: 600; loss: 0.97; acc: 0.66
Batch: 620; loss: 1.16; acc: 0.69
Batch: 640; loss: 1.27; acc: 0.58
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.29; acc: 0.61
Batch: 700; loss: 1.09; acc: 0.62
Batch: 720; loss: 1.4; acc: 0.56
Batch: 740; loss: 1.21; acc: 0.58
Batch: 760; loss: 1.63; acc: 0.53
Batch: 780; loss: 1.17; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0548913106796847; val_accuracy: 0.6496815286624203 

The current subspace-distance is: 0.0001327578502241522 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.34; acc: 0.48
Batch: 20; loss: 1.07; acc: 0.66
Batch: 40; loss: 1.28; acc: 0.59
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.53
Batch: 100; loss: 1.14; acc: 0.53
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 1.15; acc: 0.64
Batch: 160; loss: 0.99; acc: 0.66
Batch: 180; loss: 1.07; acc: 0.67
Batch: 200; loss: 1.23; acc: 0.58
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 1.25; acc: 0.58
Batch: 260; loss: 1.03; acc: 0.62
Batch: 280; loss: 1.29; acc: 0.47
Batch: 300; loss: 1.17; acc: 0.64
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 0.97; acc: 0.72
Batch: 360; loss: 1.34; acc: 0.53
Batch: 380; loss: 0.99; acc: 0.62
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 0.94; acc: 0.64
Batch: 440; loss: 1.11; acc: 0.62
Batch: 460; loss: 1.22; acc: 0.56
Batch: 480; loss: 1.1; acc: 0.62
Batch: 500; loss: 0.84; acc: 0.73
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 1.28; acc: 0.52
Batch: 560; loss: 1.33; acc: 0.53
Batch: 580; loss: 1.31; acc: 0.61
Batch: 600; loss: 1.1; acc: 0.64
Batch: 620; loss: 1.04; acc: 0.62
Batch: 640; loss: 1.06; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.59
Batch: 680; loss: 1.25; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.12; acc: 0.61
Batch: 740; loss: 1.0; acc: 0.67
Batch: 760; loss: 1.37; acc: 0.56
Batch: 780; loss: 1.08; acc: 0.66
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.15; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0551257646007903; val_accuracy: 0.6498805732484076 

The current subspace-distance is: 0.0001350025413557887 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 1.02; acc: 0.62
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.19; acc: 0.64
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 0.97; acc: 0.69
Batch: 160; loss: 1.27; acc: 0.58
Batch: 180; loss: 0.92; acc: 0.64
Batch: 200; loss: 0.95; acc: 0.67
Batch: 220; loss: 0.93; acc: 0.69
Batch: 240; loss: 1.24; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.62
Batch: 280; loss: 1.1; acc: 0.58
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 1.14; acc: 0.52
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.56
Batch: 380; loss: 1.04; acc: 0.64
Batch: 400; loss: 1.31; acc: 0.56
Batch: 420; loss: 0.9; acc: 0.69
Batch: 440; loss: 1.08; acc: 0.61
Batch: 460; loss: 1.06; acc: 0.67
Batch: 480; loss: 0.99; acc: 0.64
Batch: 500; loss: 1.34; acc: 0.58
Batch: 520; loss: 1.14; acc: 0.55
Batch: 540; loss: 1.15; acc: 0.66
Batch: 560; loss: 0.88; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.7
Batch: 600; loss: 1.13; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.21; acc: 0.52
Batch: 660; loss: 1.34; acc: 0.47
Batch: 680; loss: 0.96; acc: 0.66
Batch: 700; loss: 1.44; acc: 0.52
Batch: 720; loss: 1.12; acc: 0.55
Batch: 740; loss: 1.16; acc: 0.58
Batch: 760; loss: 1.08; acc: 0.59
Batch: 780; loss: 1.3; acc: 0.52
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550863549208185; val_accuracy: 0.649781050955414 

The current subspace-distance is: 0.00013511721044778824 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 0.96; acc: 0.61
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 1.1; acc: 0.59
Batch: 100; loss: 0.88; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.69
Batch: 140; loss: 1.33; acc: 0.62
Batch: 160; loss: 1.03; acc: 0.69
Batch: 180; loss: 1.1; acc: 0.64
Batch: 200; loss: 1.28; acc: 0.53
Batch: 220; loss: 1.05; acc: 0.73
Batch: 240; loss: 1.04; acc: 0.67
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 0.99; acc: 0.69
Batch: 300; loss: 1.06; acc: 0.61
Batch: 320; loss: 1.17; acc: 0.62
Batch: 340; loss: 1.14; acc: 0.64
Batch: 360; loss: 1.19; acc: 0.55
Batch: 380; loss: 1.17; acc: 0.61
Batch: 400; loss: 1.22; acc: 0.5
Batch: 420; loss: 1.02; acc: 0.67
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 0.79; acc: 0.73
Batch: 480; loss: 0.99; acc: 0.66
Batch: 500; loss: 1.1; acc: 0.69
Batch: 520; loss: 1.29; acc: 0.53
Batch: 540; loss: 1.04; acc: 0.64
Batch: 560; loss: 1.04; acc: 0.62
Batch: 580; loss: 1.19; acc: 0.56
Batch: 600; loss: 1.26; acc: 0.61
Batch: 620; loss: 1.08; acc: 0.64
Batch: 640; loss: 1.36; acc: 0.62
Batch: 660; loss: 1.09; acc: 0.58
Batch: 680; loss: 1.16; acc: 0.69
Batch: 700; loss: 1.25; acc: 0.58
Batch: 720; loss: 1.17; acc: 0.64
Batch: 740; loss: 1.16; acc: 0.72
Batch: 760; loss: 1.26; acc: 0.61
Batch: 780; loss: 1.14; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0549935132834563; val_accuracy: 0.6499800955414012 

The current subspace-distance is: 0.00013710568600799888 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.23; acc: 0.56
Batch: 40; loss: 1.11; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.62
Batch: 80; loss: 1.52; acc: 0.48
Batch: 100; loss: 0.98; acc: 0.69
Batch: 120; loss: 1.11; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.61
Batch: 160; loss: 1.12; acc: 0.64
Batch: 180; loss: 1.1; acc: 0.59
Batch: 200; loss: 1.23; acc: 0.53
Batch: 220; loss: 1.11; acc: 0.69
Batch: 240; loss: 0.89; acc: 0.7
Batch: 260; loss: 1.31; acc: 0.52
Batch: 280; loss: 1.39; acc: 0.53
Batch: 300; loss: 1.27; acc: 0.59
Batch: 320; loss: 1.03; acc: 0.56
Batch: 340; loss: 1.08; acc: 0.62
Batch: 360; loss: 0.98; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.56
Batch: 400; loss: 1.07; acc: 0.61
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.12; acc: 0.58
Batch: 460; loss: 1.09; acc: 0.62
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 1.0; acc: 0.7
Batch: 540; loss: 1.0; acc: 0.61
Batch: 560; loss: 1.01; acc: 0.7
Batch: 580; loss: 0.93; acc: 0.67
Batch: 600; loss: 1.08; acc: 0.62
Batch: 620; loss: 1.04; acc: 0.62
Batch: 640; loss: 1.14; acc: 0.59
Batch: 660; loss: 1.08; acc: 0.62
Batch: 680; loss: 1.03; acc: 0.72
Batch: 700; loss: 1.08; acc: 0.69
Batch: 720; loss: 1.2; acc: 0.56
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.92; acc: 0.64
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0549827362321744; val_accuracy: 0.6499800955414012 

The current subspace-distance is: 0.00013791007222607732 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 1.0; acc: 0.67
Batch: 60; loss: 1.11; acc: 0.59
Batch: 80; loss: 1.28; acc: 0.52
Batch: 100; loss: 1.17; acc: 0.62
Batch: 120; loss: 1.35; acc: 0.53
Batch: 140; loss: 1.15; acc: 0.67
Batch: 160; loss: 1.03; acc: 0.67
Batch: 180; loss: 1.01; acc: 0.61
Batch: 200; loss: 1.24; acc: 0.58
Batch: 220; loss: 0.89; acc: 0.72
Batch: 240; loss: 1.31; acc: 0.59
Batch: 260; loss: 0.88; acc: 0.72
Batch: 280; loss: 0.91; acc: 0.72
Batch: 300; loss: 1.25; acc: 0.55
Batch: 320; loss: 1.0; acc: 0.67
Batch: 340; loss: 0.96; acc: 0.77
Batch: 360; loss: 1.05; acc: 0.64
Batch: 380; loss: 1.06; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.66
Batch: 420; loss: 1.22; acc: 0.66
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.18; acc: 0.59
Batch: 480; loss: 1.08; acc: 0.69
Batch: 500; loss: 1.36; acc: 0.58
Batch: 520; loss: 1.12; acc: 0.61
Batch: 540; loss: 1.23; acc: 0.64
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 0.91; acc: 0.72
Batch: 600; loss: 1.13; acc: 0.61
Batch: 620; loss: 1.31; acc: 0.58
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 1.31; acc: 0.55
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.93; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.67
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 0.94; acc: 0.66
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.055091814250703; val_accuracy: 0.6493829617834395 

The current subspace-distance is: 0.00013827320071868598 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.59
Batch: 40; loss: 1.24; acc: 0.56
Batch: 60; loss: 1.09; acc: 0.59
Batch: 80; loss: 1.46; acc: 0.55
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 1.41; acc: 0.52
Batch: 140; loss: 1.23; acc: 0.58
Batch: 160; loss: 1.32; acc: 0.56
Batch: 180; loss: 1.0; acc: 0.64
Batch: 200; loss: 1.09; acc: 0.58
Batch: 220; loss: 1.02; acc: 0.67
Batch: 240; loss: 1.03; acc: 0.61
Batch: 260; loss: 1.2; acc: 0.61
Batch: 280; loss: 1.24; acc: 0.58
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.02; acc: 0.67
Batch: 360; loss: 1.18; acc: 0.59
Batch: 380; loss: 1.1; acc: 0.59
Batch: 400; loss: 1.13; acc: 0.61
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 0.92; acc: 0.73
Batch: 460; loss: 1.09; acc: 0.66
Batch: 480; loss: 1.04; acc: 0.67
Batch: 500; loss: 1.04; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 1.01; acc: 0.67
Batch: 560; loss: 1.23; acc: 0.53
Batch: 580; loss: 1.35; acc: 0.62
Batch: 600; loss: 0.98; acc: 0.69
Batch: 620; loss: 1.09; acc: 0.59
Batch: 640; loss: 1.06; acc: 0.61
Batch: 660; loss: 0.89; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.69
Batch: 700; loss: 0.86; acc: 0.61
Batch: 720; loss: 1.18; acc: 0.59
Batch: 740; loss: 0.84; acc: 0.7
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 1.29; acc: 0.55
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550141501578556; val_accuracy: 0.6495820063694268 

The current subspace-distance is: 0.00014051886682864279 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.59
Batch: 20; loss: 0.93; acc: 0.62
Batch: 40; loss: 1.07; acc: 0.62
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.62
Batch: 100; loss: 0.99; acc: 0.64
Batch: 120; loss: 1.33; acc: 0.5
Batch: 140; loss: 1.18; acc: 0.58
Batch: 160; loss: 1.32; acc: 0.56
Batch: 180; loss: 1.26; acc: 0.56
Batch: 200; loss: 1.03; acc: 0.64
Batch: 220; loss: 0.94; acc: 0.73
Batch: 240; loss: 1.12; acc: 0.62
Batch: 260; loss: 1.34; acc: 0.56
Batch: 280; loss: 0.91; acc: 0.67
Batch: 300; loss: 1.3; acc: 0.59
Batch: 320; loss: 1.12; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.59
Batch: 360; loss: 1.1; acc: 0.66
Batch: 380; loss: 1.3; acc: 0.58
Batch: 400; loss: 1.33; acc: 0.52
Batch: 420; loss: 1.14; acc: 0.62
Batch: 440; loss: 1.1; acc: 0.62
Batch: 460; loss: 1.06; acc: 0.59
Batch: 480; loss: 1.3; acc: 0.64
Batch: 500; loss: 1.14; acc: 0.58
Batch: 520; loss: 0.98; acc: 0.69
Batch: 540; loss: 1.07; acc: 0.64
Batch: 560; loss: 1.09; acc: 0.72
Batch: 580; loss: 1.07; acc: 0.64
Batch: 600; loss: 0.96; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.61
Batch: 640; loss: 0.87; acc: 0.7
Batch: 660; loss: 1.05; acc: 0.64
Batch: 680; loss: 0.95; acc: 0.72
Batch: 700; loss: 1.3; acc: 0.53
Batch: 720; loss: 1.25; acc: 0.52
Batch: 740; loss: 1.07; acc: 0.59
Batch: 760; loss: 1.29; acc: 0.61
Batch: 780; loss: 0.99; acc: 0.72
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550203129744073; val_accuracy: 0.6495820063694268 

The current subspace-distance is: 0.00013984297402203083 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 0.92; acc: 0.7
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 1.26; acc: 0.58
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 0.98; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 1.45; acc: 0.58
Batch: 160; loss: 1.1; acc: 0.62
Batch: 180; loss: 0.91; acc: 0.77
Batch: 200; loss: 1.19; acc: 0.58
Batch: 220; loss: 0.87; acc: 0.72
Batch: 240; loss: 1.13; acc: 0.64
Batch: 260; loss: 1.1; acc: 0.64
Batch: 280; loss: 1.41; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.55
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.06; acc: 0.7
Batch: 360; loss: 0.88; acc: 0.64
Batch: 380; loss: 1.19; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.59
Batch: 420; loss: 1.26; acc: 0.66
Batch: 440; loss: 1.07; acc: 0.64
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 1.34; acc: 0.58
Batch: 500; loss: 1.03; acc: 0.69
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 1.03; acc: 0.66
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.22; acc: 0.61
Batch: 600; loss: 1.21; acc: 0.61
Batch: 620; loss: 1.21; acc: 0.61
Batch: 640; loss: 0.96; acc: 0.69
Batch: 660; loss: 1.03; acc: 0.69
Batch: 680; loss: 1.09; acc: 0.58
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.58
Batch: 760; loss: 0.96; acc: 0.73
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0550056433981392; val_accuracy: 0.6495820063694268 

The current subspace-distance is: 0.00014053244376555085 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.22; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 1.09; acc: 0.59
Batch: 60; loss: 1.17; acc: 0.61
Batch: 80; loss: 1.33; acc: 0.55
Batch: 100; loss: 0.94; acc: 0.67
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 0.89; acc: 0.72
Batch: 160; loss: 1.05; acc: 0.62
Batch: 180; loss: 1.08; acc: 0.66
Batch: 200; loss: 1.15; acc: 0.58
Batch: 220; loss: 1.3; acc: 0.62
Batch: 240; loss: 0.92; acc: 0.69
Batch: 260; loss: 1.19; acc: 0.61
Batch: 280; loss: 0.89; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.21; acc: 0.61
Batch: 340; loss: 1.01; acc: 0.62
Batch: 360; loss: 1.1; acc: 0.56
Batch: 380; loss: 1.16; acc: 0.55
Batch: 400; loss: 1.04; acc: 0.59
Batch: 420; loss: 1.16; acc: 0.58
Batch: 440; loss: 1.27; acc: 0.56
Batch: 460; loss: 1.18; acc: 0.56
Batch: 480; loss: 1.19; acc: 0.58
Batch: 500; loss: 0.99; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.69
Batch: 540; loss: 0.92; acc: 0.75
Batch: 560; loss: 1.25; acc: 0.53
Batch: 580; loss: 1.35; acc: 0.55
Batch: 600; loss: 1.09; acc: 0.66
Batch: 620; loss: 1.11; acc: 0.72
Batch: 640; loss: 1.38; acc: 0.45
Batch: 660; loss: 0.91; acc: 0.72
Batch: 680; loss: 0.95; acc: 0.72
Batch: 700; loss: 1.44; acc: 0.5
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.17; acc: 0.59
Batch: 760; loss: 1.18; acc: 0.59
Batch: 780; loss: 1.13; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0549451803705494; val_accuracy: 0.6503781847133758 

The current subspace-distance is: 0.00014318186731543392 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.56
Batch: 40; loss: 0.9; acc: 0.7
Batch: 60; loss: 0.91; acc: 0.67
Batch: 80; loss: 1.03; acc: 0.62
Batch: 100; loss: 0.8; acc: 0.73
Batch: 120; loss: 1.1; acc: 0.62
Batch: 140; loss: 1.48; acc: 0.5
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.36; acc: 0.55
Batch: 200; loss: 1.44; acc: 0.53
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.3; acc: 0.64
Batch: 260; loss: 1.34; acc: 0.59
Batch: 280; loss: 1.25; acc: 0.67
Batch: 300; loss: 1.04; acc: 0.62
Batch: 320; loss: 1.18; acc: 0.67
Batch: 340; loss: 1.2; acc: 0.59
Batch: 360; loss: 0.97; acc: 0.67
Batch: 380; loss: 1.23; acc: 0.67
Batch: 400; loss: 1.1; acc: 0.62
Batch: 420; loss: 1.28; acc: 0.56
Batch: 440; loss: 0.94; acc: 0.66
Batch: 460; loss: 0.92; acc: 0.73
Batch: 480; loss: 0.97; acc: 0.59
Batch: 500; loss: 1.02; acc: 0.59
Batch: 520; loss: 1.1; acc: 0.64
Batch: 540; loss: 0.95; acc: 0.67
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 1.02; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.55
Batch: 620; loss: 1.14; acc: 0.62
Batch: 640; loss: 1.11; acc: 0.59
Batch: 660; loss: 1.0; acc: 0.67
Batch: 680; loss: 1.1; acc: 0.62
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.08; acc: 0.62
Batch: 740; loss: 1.11; acc: 0.58
Batch: 760; loss: 1.18; acc: 0.58
Batch: 780; loss: 1.22; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0549990910633353; val_accuracy: 0.649781050955414 

The current subspace-distance is: 0.00014675459533464164 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 1.26; acc: 0.61
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 1.03; acc: 0.7
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 1.4; acc: 0.56
Batch: 160; loss: 1.01; acc: 0.72
Batch: 180; loss: 1.01; acc: 0.66
Batch: 200; loss: 1.44; acc: 0.61
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.19; acc: 0.62
Batch: 280; loss: 0.86; acc: 0.72
Batch: 300; loss: 1.06; acc: 0.62
Batch: 320; loss: 1.17; acc: 0.59
Batch: 340; loss: 1.24; acc: 0.61
Batch: 360; loss: 1.12; acc: 0.64
Batch: 380; loss: 1.18; acc: 0.72
Batch: 400; loss: 1.29; acc: 0.55
Batch: 420; loss: 1.07; acc: 0.62
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 1.32; acc: 0.48
Batch: 480; loss: 1.27; acc: 0.58
Batch: 500; loss: 1.11; acc: 0.7
Batch: 520; loss: 0.94; acc: 0.64
Batch: 540; loss: 0.95; acc: 0.66
Batch: 560; loss: 1.18; acc: 0.58
Batch: 580; loss: 1.09; acc: 0.62
Batch: 600; loss: 0.93; acc: 0.66
Batch: 620; loss: 1.4; acc: 0.56
Batch: 640; loss: 1.12; acc: 0.67
Batch: 660; loss: 0.91; acc: 0.72
Batch: 680; loss: 1.0; acc: 0.73
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.59
Batch: 740; loss: 1.37; acc: 0.53
Batch: 760; loss: 1.39; acc: 0.58
Batch: 780; loss: 1.04; acc: 0.62
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.3; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.0549864389334516; val_accuracy: 0.6495820063694268 

The current subspace-distance is: 0.0001466664980398491 

plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 89057
elements in E: 39842000
fraction nonzero: 0.0022352542543045027
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.23
Batch: 140; loss: 2.27; acc: 0.19
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.26; acc: 0.2
Batch: 200; loss: 2.26; acc: 0.22
Batch: 220; loss: 2.24; acc: 0.3
Batch: 240; loss: 2.24; acc: 0.23
Batch: 260; loss: 2.23; acc: 0.33
Batch: 280; loss: 2.23; acc: 0.31
Batch: 300; loss: 2.21; acc: 0.38
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.48
Batch: 360; loss: 2.21; acc: 0.31
Batch: 380; loss: 2.18; acc: 0.42
Batch: 400; loss: 2.17; acc: 0.42
Batch: 420; loss: 2.17; acc: 0.34
Batch: 440; loss: 2.16; acc: 0.44
Batch: 460; loss: 2.16; acc: 0.33
Batch: 480; loss: 2.13; acc: 0.39
Batch: 500; loss: 2.1; acc: 0.38
Batch: 520; loss: 2.16; acc: 0.33
Batch: 540; loss: 2.15; acc: 0.27
Batch: 560; loss: 2.07; acc: 0.47
Batch: 580; loss: 2.12; acc: 0.42
Batch: 600; loss: 2.09; acc: 0.36
Batch: 620; loss: 2.07; acc: 0.42
Batch: 640; loss: 2.04; acc: 0.39
Batch: 660; loss: 2.03; acc: 0.39
Batch: 680; loss: 1.96; acc: 0.52
Batch: 700; loss: 1.98; acc: 0.39
Batch: 720; loss: 1.93; acc: 0.53
Batch: 740; loss: 1.96; acc: 0.45
Batch: 760; loss: 1.81; acc: 0.61
Batch: 780; loss: 1.91; acc: 0.5
Train Epoch over. train_loss: 2.16; train_accuracy: 0.34 

Batch: 0; loss: 1.88; acc: 0.47
Batch: 20; loss: 1.91; acc: 0.39
Batch: 40; loss: 1.71; acc: 0.66
Batch: 60; loss: 1.81; acc: 0.59
Batch: 80; loss: 1.82; acc: 0.55
Batch: 100; loss: 1.84; acc: 0.56
Batch: 120; loss: 1.92; acc: 0.56
Batch: 140; loss: 1.78; acc: 0.66
Val Epoch over. val_loss: 1.859067790827174; val_accuracy: 0.5284633757961783 

The current subspace-distance is: 9.207725270243827e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.89; acc: 0.5
Batch: 20; loss: 1.92; acc: 0.45
Batch: 40; loss: 1.85; acc: 0.5
Batch: 60; loss: 1.79; acc: 0.47
Batch: 80; loss: 1.72; acc: 0.56
Batch: 100; loss: 1.69; acc: 0.53
Batch: 120; loss: 1.71; acc: 0.59
Batch: 140; loss: 1.64; acc: 0.59
Batch: 160; loss: 1.68; acc: 0.62
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.61
Batch: 220; loss: 1.52; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.58
Batch: 260; loss: 1.54; acc: 0.53
Batch: 280; loss: 1.42; acc: 0.56
Batch: 300; loss: 1.58; acc: 0.45
Batch: 320; loss: 1.44; acc: 0.52
Batch: 340; loss: 1.35; acc: 0.58
Batch: 360; loss: 1.36; acc: 0.64
Batch: 380; loss: 1.26; acc: 0.64
Batch: 400; loss: 1.41; acc: 0.52
Batch: 420; loss: 1.3; acc: 0.64
Batch: 440; loss: 1.29; acc: 0.61
Batch: 460; loss: 1.2; acc: 0.62
Batch: 480; loss: 1.16; acc: 0.64
Batch: 500; loss: 1.32; acc: 0.53
Batch: 520; loss: 1.03; acc: 0.73
Batch: 540; loss: 1.04; acc: 0.73
Batch: 560; loss: 1.22; acc: 0.58
Batch: 580; loss: 0.97; acc: 0.73
Batch: 600; loss: 1.29; acc: 0.62
Batch: 620; loss: 1.2; acc: 0.72
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 1.08; acc: 0.58
Batch: 680; loss: 1.09; acc: 0.64
Batch: 700; loss: 1.07; acc: 0.64
Batch: 720; loss: 1.31; acc: 0.58
Batch: 740; loss: 1.38; acc: 0.52
Batch: 760; loss: 1.04; acc: 0.62
Batch: 780; loss: 1.05; acc: 0.66
Train Epoch over. train_loss: 1.37; train_accuracy: 0.6 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.56
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.7
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.8; acc: 0.7
Val Epoch over. val_loss: 1.0070363503352853; val_accuracy: 0.6762539808917197 

The current subspace-distance is: 2.3113459974410944e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.59
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 1.04; acc: 0.56
Batch: 60; loss: 1.12; acc: 0.69
Batch: 80; loss: 1.08; acc: 0.62
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 1.02; acc: 0.69
Batch: 160; loss: 1.11; acc: 0.59
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.98; acc: 0.72
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.13; acc: 0.66
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 1.13; acc: 0.61
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 1.28; acc: 0.62
Batch: 360; loss: 0.86; acc: 0.73
Batch: 380; loss: 0.92; acc: 0.77
Batch: 400; loss: 1.11; acc: 0.61
Batch: 420; loss: 0.81; acc: 0.73
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.94; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.66
Batch: 500; loss: 0.94; acc: 0.73
Batch: 520; loss: 0.98; acc: 0.64
Batch: 540; loss: 0.92; acc: 0.73
Batch: 560; loss: 1.09; acc: 0.62
Batch: 580; loss: 1.13; acc: 0.59
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.99; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.69
Batch: 660; loss: 1.06; acc: 0.64
Batch: 680; loss: 0.98; acc: 0.67
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 1.13; acc: 0.58
Batch: 740; loss: 1.16; acc: 0.66
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 0.97; acc: 0.64
Train Epoch over. train_loss: 0.99; train_accuracy: 0.68 

Batch: 0; loss: 1.06; acc: 0.62
Batch: 20; loss: 0.95; acc: 0.64
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.8740884407310728; val_accuracy: 0.7180533439490446 

The current subspace-distance is: 3.394497980480082e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.91; acc: 0.67
Batch: 40; loss: 1.11; acc: 0.66
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 1.08; acc: 0.62
Batch: 160; loss: 0.94; acc: 0.72
Batch: 180; loss: 0.87; acc: 0.77
Batch: 200; loss: 0.8; acc: 0.77
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 0.86; acc: 0.72
Batch: 260; loss: 1.0; acc: 0.67
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 1.05; acc: 0.67
Batch: 320; loss: 0.96; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.61
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 1.0; acc: 0.7
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 0.99; acc: 0.67
Batch: 440; loss: 0.83; acc: 0.78
Batch: 460; loss: 1.07; acc: 0.66
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 1.0; acc: 0.67
Batch: 520; loss: 0.83; acc: 0.73
Batch: 540; loss: 1.03; acc: 0.67
Batch: 560; loss: 0.86; acc: 0.77
Batch: 580; loss: 0.99; acc: 0.62
Batch: 600; loss: 0.99; acc: 0.67
Batch: 620; loss: 1.1; acc: 0.66
Batch: 640; loss: 0.85; acc: 0.64
Batch: 660; loss: 0.88; acc: 0.67
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.66
Train Epoch over. train_loss: 0.91; train_accuracy: 0.7 

Batch: 0; loss: 1.02; acc: 0.66
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.78
Batch: 60; loss: 0.91; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 1.04; acc: 0.66
Batch: 140; loss: 0.53; acc: 0.81
Val Epoch over. val_loss: 0.8194641216545347; val_accuracy: 0.7347730891719745 

The current subspace-distance is: 4.297310078982264e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.67
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.96; acc: 0.69
Batch: 160; loss: 0.96; acc: 0.73
Batch: 180; loss: 0.95; acc: 0.69
Batch: 200; loss: 1.06; acc: 0.64
Batch: 220; loss: 1.01; acc: 0.7
Batch: 240; loss: 0.96; acc: 0.72
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.85; acc: 0.81
Batch: 320; loss: 1.15; acc: 0.59
Batch: 340; loss: 0.98; acc: 0.66
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.97; acc: 0.66
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.93; acc: 0.77
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 1.06; acc: 0.69
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 0.71; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.62
Batch: 540; loss: 0.81; acc: 0.7
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.85; acc: 0.7
Batch: 600; loss: 0.84; acc: 0.69
Batch: 620; loss: 0.87; acc: 0.7
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.83; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.64
Batch: 700; loss: 0.93; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.66
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 1.02; acc: 0.69
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 1.01; acc: 0.7
Batch: 140; loss: 0.5; acc: 0.78
Val Epoch over. val_loss: 0.7887588050335076; val_accuracy: 0.7453224522292994 

The current subspace-distance is: 4.9968191888183355e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.72
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.78; acc: 0.67
Batch: 120; loss: 0.8; acc: 0.69
Batch: 140; loss: 0.8; acc: 0.73
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.85; acc: 0.67
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.95; acc: 0.7
Batch: 260; loss: 0.87; acc: 0.7
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.75
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.89; acc: 0.73
Batch: 380; loss: 0.69; acc: 0.7
Batch: 400; loss: 0.88; acc: 0.66
Batch: 420; loss: 0.84; acc: 0.73
Batch: 440; loss: 0.98; acc: 0.64
Batch: 460; loss: 1.01; acc: 0.64
Batch: 480; loss: 0.8; acc: 0.78
Batch: 500; loss: 0.74; acc: 0.77
Batch: 520; loss: 0.97; acc: 0.73
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 0.79; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.87; acc: 0.66
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 0.71; acc: 0.75
Batch: 740; loss: 1.05; acc: 0.62
Batch: 760; loss: 0.84; acc: 0.73
Batch: 780; loss: 0.81; acc: 0.78
Train Epoch over. train_loss: 0.85; train_accuracy: 0.72 

Batch: 0; loss: 0.96; acc: 0.64
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.78
Val Epoch over. val_loss: 0.7667677275314453; val_accuracy: 0.7525875796178344 

The current subspace-distance is: 5.5838681873865426e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.73
Batch: 200; loss: 1.09; acc: 0.62
Batch: 220; loss: 0.76; acc: 0.7
Batch: 240; loss: 0.86; acc: 0.72
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 0.93; acc: 0.7
Batch: 320; loss: 0.91; acc: 0.67
Batch: 340; loss: 0.84; acc: 0.69
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.72
Batch: 400; loss: 1.01; acc: 0.67
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.75
Batch: 480; loss: 0.73; acc: 0.75
Batch: 500; loss: 0.86; acc: 0.67
Batch: 520; loss: 0.73; acc: 0.77
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.85; acc: 0.69
Batch: 600; loss: 0.68; acc: 0.73
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.94; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.72
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.67; acc: 0.75
Batch: 720; loss: 0.87; acc: 0.69
Batch: 740; loss: 0.96; acc: 0.73
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 0.87; acc: 0.67
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.69
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.7
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.7472069257763541; val_accuracy: 0.7543789808917197 

The current subspace-distance is: 6.0662645410047844e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.7
Batch: 40; loss: 0.87; acc: 0.7
Batch: 60; loss: 0.81; acc: 0.69
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.89; acc: 0.67
Batch: 160; loss: 0.86; acc: 0.73
Batch: 180; loss: 0.83; acc: 0.75
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 1.05; acc: 0.69
Batch: 240; loss: 0.96; acc: 0.67
Batch: 260; loss: 1.04; acc: 0.67
Batch: 280; loss: 0.77; acc: 0.7
Batch: 300; loss: 0.72; acc: 0.73
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.82; acc: 0.73
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.91; acc: 0.75
Batch: 400; loss: 0.9; acc: 0.66
Batch: 420; loss: 0.74; acc: 0.72
Batch: 440; loss: 0.68; acc: 0.77
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.85; acc: 0.77
Batch: 500; loss: 0.92; acc: 0.77
Batch: 520; loss: 0.73; acc: 0.77
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.87; acc: 0.69
Batch: 580; loss: 0.93; acc: 0.66
Batch: 600; loss: 1.02; acc: 0.7
Batch: 620; loss: 0.99; acc: 0.7
Batch: 640; loss: 1.0; acc: 0.7
Batch: 660; loss: 0.77; acc: 0.73
Batch: 680; loss: 0.76; acc: 0.77
Batch: 700; loss: 0.7; acc: 0.75
Batch: 720; loss: 0.73; acc: 0.75
Batch: 740; loss: 0.89; acc: 0.69
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.81; acc: 0.72
Train Epoch over. train_loss: 0.81; train_accuracy: 0.74 

Batch: 0; loss: 0.91; acc: 0.69
Batch: 20; loss: 0.86; acc: 0.69
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.46; acc: 0.81
Val Epoch over. val_loss: 0.7277525733610627; val_accuracy: 0.7674164012738853 

The current subspace-distance is: 6.55427502351813e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.88; acc: 0.69
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.85; acc: 0.8
Batch: 360; loss: 0.85; acc: 0.7
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.71; acc: 0.75
Batch: 420; loss: 1.01; acc: 0.67
Batch: 440; loss: 0.87; acc: 0.69
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.83; acc: 0.75
Batch: 500; loss: 0.79; acc: 0.7
Batch: 520; loss: 0.92; acc: 0.73
Batch: 540; loss: 0.85; acc: 0.75
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.84; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.67
Batch: 640; loss: 0.69; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.72
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.96; acc: 0.72
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 1.05; acc: 0.66
Batch: 760; loss: 0.84; acc: 0.75
Batch: 780; loss: 0.82; acc: 0.75
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 0.91; acc: 0.67
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.96; acc: 0.7
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.78
Val Epoch over. val_loss: 0.7175812028395901; val_accuracy: 0.7714968152866242 

The current subspace-distance is: 7.188267773017287e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.67
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.7
Batch: 160; loss: 0.88; acc: 0.72
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.61
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 1.06; acc: 0.66
Batch: 320; loss: 0.59; acc: 0.8
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.78; acc: 0.69
Batch: 380; loss: 1.09; acc: 0.62
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 0.88; acc: 0.69
Batch: 480; loss: 1.1; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 0.69; acc: 0.75
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.86; acc: 0.72
Batch: 580; loss: 0.66; acc: 0.78
Batch: 600; loss: 0.78; acc: 0.77
Batch: 620; loss: 0.93; acc: 0.62
Batch: 640; loss: 0.74; acc: 0.72
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.72; acc: 0.75
Batch: 720; loss: 0.82; acc: 0.73
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.97; acc: 0.7
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 0.89; acc: 0.64
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.704516209424681; val_accuracy: 0.7775676751592356 

The current subspace-distance is: 7.640858530066907e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.86; acc: 0.78
Batch: 100; loss: 1.18; acc: 0.56
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.93; acc: 0.7
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.67; acc: 0.77
Batch: 200; loss: 0.91; acc: 0.73
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.78; acc: 0.7
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.88; acc: 0.72
Batch: 320; loss: 0.81; acc: 0.77
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.95; acc: 0.75
Batch: 380; loss: 0.71; acc: 0.77
Batch: 400; loss: 0.9; acc: 0.7
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.97; acc: 0.72
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 1.0; acc: 0.61
Batch: 520; loss: 0.96; acc: 0.69
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 1.06; acc: 0.73
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.71; acc: 0.75
Batch: 620; loss: 0.68; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.89; acc: 0.73
Batch: 680; loss: 0.82; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 0.77; train_accuracy: 0.75 

Batch: 0; loss: 0.87; acc: 0.66
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.84
Val Epoch over. val_loss: 0.6999208541812411; val_accuracy: 0.7778662420382165 

The current subspace-distance is: 7.953509339131415e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.73
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.72
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.93; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.94; acc: 0.78
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.77; acc: 0.77
Batch: 400; loss: 0.74; acc: 0.75
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.88; acc: 0.73
Batch: 460; loss: 0.77; acc: 0.72
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.53; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.75
Batch: 620; loss: 0.81; acc: 0.72
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.75
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.62; acc: 0.77
Batch: 740; loss: 0.74; acc: 0.73
Batch: 760; loss: 0.75; acc: 0.75
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.85; acc: 0.66
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.39; acc: 0.83
Val Epoch over. val_loss: 0.6973610905704984; val_accuracy: 0.7792595541401274 

The current subspace-distance is: 8.427595457760617e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.68; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.73
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.52; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.72
Batch: 280; loss: 0.71; acc: 0.77
Batch: 300; loss: 0.9; acc: 0.75
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.99; acc: 0.64
Batch: 360; loss: 0.67; acc: 0.75
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.77; acc: 0.77
Batch: 500; loss: 0.61; acc: 0.77
Batch: 520; loss: 0.89; acc: 0.75
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.77
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.75
Batch: 620; loss: 0.95; acc: 0.67
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.79; acc: 0.69
Batch: 680; loss: 0.94; acc: 0.66
Batch: 700; loss: 0.92; acc: 0.7
Batch: 720; loss: 0.79; acc: 0.75
Batch: 740; loss: 1.23; acc: 0.62
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.86; acc: 0.66
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.84; acc: 0.67
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.83
Val Epoch over. val_loss: 0.693699374130577; val_accuracy: 0.7782643312101911 

The current subspace-distance is: 8.908141171559691e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.96; acc: 0.62
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.76; acc: 0.72
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.93; acc: 0.77
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.72
Batch: 260; loss: 0.85; acc: 0.7
Batch: 280; loss: 0.85; acc: 0.77
Batch: 300; loss: 0.82; acc: 0.67
Batch: 320; loss: 0.73; acc: 0.72
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.9; acc: 0.69
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 1.04; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.77
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.76; acc: 0.75
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.81
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.69
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.67; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.62; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.82; acc: 0.69
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.88
Val Epoch over. val_loss: 0.688533170967345; val_accuracy: 0.7837380573248408 

The current subspace-distance is: 9.121208131546155e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.62; acc: 0.75
Batch: 180; loss: 0.89; acc: 0.72
Batch: 200; loss: 0.56; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 1.02; acc: 0.72
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.93; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.69
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.77; acc: 0.75
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.99; acc: 0.67
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.85; acc: 0.64
Batch: 480; loss: 0.75; acc: 0.75
Batch: 500; loss: 0.81; acc: 0.7
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 1.02; acc: 0.66
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.67
Batch: 600; loss: 0.93; acc: 0.7
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.72; acc: 0.7
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 0.88; acc: 0.75
Batch: 720; loss: 0.82; acc: 0.7
Batch: 740; loss: 0.91; acc: 0.62
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 1.01; acc: 0.64
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.6869769870855247; val_accuracy: 0.783140923566879 

The current subspace-distance is: 9.434751700609922e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.85; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.87; acc: 0.8
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.64; acc: 0.77
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.97; acc: 0.8
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 0.82; acc: 0.7
Batch: 360; loss: 0.92; acc: 0.69
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.8; acc: 0.78
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.69; acc: 0.77
Batch: 560; loss: 0.94; acc: 0.72
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.75; acc: 0.75
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 0.82; acc: 0.77
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.75
Batch: 780; loss: 0.82; acc: 0.73
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.81; acc: 0.69
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.88
Val Epoch over. val_loss: 0.6829213989768058; val_accuracy: 0.7840366242038217 

The current subspace-distance is: 9.914541442412883e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.89; acc: 0.73
Batch: 180; loss: 0.68; acc: 0.75
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.78; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.85; acc: 0.66
Batch: 320; loss: 0.87; acc: 0.77
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.77; acc: 0.77
Batch: 380; loss: 0.58; acc: 0.8
Batch: 400; loss: 0.69; acc: 0.75
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 0.9; acc: 0.67
Batch: 480; loss: 0.85; acc: 0.72
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.93; acc: 0.66
Batch: 540; loss: 0.54; acc: 0.73
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.92; acc: 0.8
Batch: 620; loss: 1.03; acc: 0.64
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.7; acc: 0.75
Batch: 700; loss: 0.82; acc: 0.72
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.86; acc: 0.75
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.72
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.88
Val Epoch over. val_loss: 0.6791186947731456; val_accuracy: 0.7857285031847133 

The current subspace-distance is: 0.00010302024020347744 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.81; acc: 0.72
Batch: 160; loss: 0.72; acc: 0.7
Batch: 180; loss: 0.65; acc: 0.75
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.72
Batch: 240; loss: 0.77; acc: 0.75
Batch: 260; loss: 0.77; acc: 0.77
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.69
Batch: 360; loss: 0.81; acc: 0.78
Batch: 380; loss: 0.71; acc: 0.77
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.96; acc: 0.75
Batch: 440; loss: 0.75; acc: 0.72
Batch: 460; loss: 0.76; acc: 0.73
Batch: 480; loss: 0.94; acc: 0.69
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.74; acc: 0.72
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.79; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.72
Batch: 680; loss: 0.7; acc: 0.73
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.77
Batch: 740; loss: 0.9; acc: 0.75
Batch: 760; loss: 0.71; acc: 0.7
Batch: 780; loss: 0.71; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.6776994421223926; val_accuracy: 0.7872213375796179 

The current subspace-distance is: 0.0001063540403265506 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.69
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.69
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.83; acc: 0.61
Batch: 160; loss: 0.84; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.78
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.8
Batch: 260; loss: 0.74; acc: 0.73
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 0.72; acc: 0.8
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.72; acc: 0.77
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 0.72; acc: 0.77
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 0.86; acc: 0.75
Batch: 520; loss: 0.91; acc: 0.73
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.8; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.72
Batch: 620; loss: 0.65; acc: 0.75
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.86; acc: 0.75
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.96; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.72
Batch: 740; loss: 0.74; acc: 0.73
Batch: 760; loss: 0.82; acc: 0.75
Batch: 780; loss: 0.72; acc: 0.75
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.78; acc: 0.72
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.676353860812582; val_accuracy: 0.788515127388535 

The current subspace-distance is: 0.00010990346345352009 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.89; acc: 0.7
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.73
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.74; acc: 0.73
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 0.66; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.72
Batch: 280; loss: 1.06; acc: 0.58
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.53; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.8; acc: 0.69
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.85; acc: 0.73
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.78; acc: 0.73
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.97; acc: 0.7
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.675076129140368; val_accuracy: 0.7884156050955414 

The current subspace-distance is: 0.00011436341446824372 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.78; acc: 0.72
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 0.8; acc: 0.7
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.73
Batch: 160; loss: 0.74; acc: 0.77
Batch: 180; loss: 0.86; acc: 0.73
Batch: 200; loss: 0.84; acc: 0.75
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.9; acc: 0.69
Batch: 260; loss: 0.77; acc: 0.75
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.89; acc: 0.72
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.77
Batch: 440; loss: 0.64; acc: 0.77
Batch: 460; loss: 0.59; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.99; acc: 0.67
Batch: 520; loss: 0.82; acc: 0.73
Batch: 540; loss: 0.68; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 0.67; acc: 0.72
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.94; acc: 0.67
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.76; acc: 0.72
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.81; acc: 0.72
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.6728889976337458; val_accuracy: 0.7886146496815286 

The current subspace-distance is: 0.00011684469063766301 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 1.04; acc: 0.61
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.78; acc: 0.72
Batch: 160; loss: 0.64; acc: 0.77
Batch: 180; loss: 0.58; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.75
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.61; acc: 0.78
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.78
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.75
Batch: 400; loss: 0.74; acc: 0.72
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.92; acc: 0.73
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.87; acc: 0.7
Batch: 540; loss: 0.61; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.75
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.9; acc: 0.72
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.68; acc: 0.73
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.97; acc: 0.66
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.6720346514206783; val_accuracy: 0.7888136942675159 

The current subspace-distance is: 0.00012027595948893577 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.87; acc: 0.72
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.95; acc: 0.7
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.66; acc: 0.77
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.8; acc: 0.75
Batch: 260; loss: 1.02; acc: 0.67
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.77
Batch: 320; loss: 0.92; acc: 0.66
Batch: 340; loss: 0.81; acc: 0.69
Batch: 360; loss: 0.9; acc: 0.69
Batch: 380; loss: 1.01; acc: 0.66
Batch: 400; loss: 0.76; acc: 0.75
Batch: 420; loss: 0.85; acc: 0.72
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.8; acc: 0.78
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.77; acc: 0.69
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.72
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.8; acc: 0.77
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.75; acc: 0.78
Batch: 780; loss: 0.69; acc: 0.75
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.6715210254784603; val_accuracy: 0.7876194267515924 

The current subspace-distance is: 0.00012600267655216157 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.76; acc: 0.72
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.73
Batch: 60; loss: 0.94; acc: 0.66
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.95; acc: 0.73
Batch: 200; loss: 0.6; acc: 0.78
Batch: 220; loss: 0.64; acc: 0.81
Batch: 240; loss: 0.82; acc: 0.75
Batch: 260; loss: 0.9; acc: 0.72
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.68; acc: 0.77
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.7; acc: 0.72
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.81; acc: 0.72
Batch: 560; loss: 0.88; acc: 0.7
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.73; acc: 0.7
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.78
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.91; acc: 0.7
Batch: 780; loss: 0.77; acc: 0.7
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6706229154091732; val_accuracy: 0.7887141719745223 

The current subspace-distance is: 0.00012863676238339394 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.7
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.77
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 1.02; acc: 0.62
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.81; acc: 0.73
Batch: 260; loss: 0.81; acc: 0.69
Batch: 280; loss: 0.73; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.7
Batch: 320; loss: 0.79; acc: 0.7
Batch: 340; loss: 0.69; acc: 0.77
Batch: 360; loss: 0.81; acc: 0.73
Batch: 380; loss: 0.66; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.69
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.74; acc: 0.77
Batch: 560; loss: 0.77; acc: 0.69
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.83; acc: 0.73
Batch: 620; loss: 0.7; acc: 0.7
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.78; acc: 0.73
Batch: 680; loss: 0.87; acc: 0.75
Batch: 700; loss: 0.89; acc: 0.7
Batch: 720; loss: 0.73; acc: 0.73
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6699759039529569; val_accuracy: 0.7876194267515924 

The current subspace-distance is: 0.00013294487143866718 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.73
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 0.76; acc: 0.69
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.9; acc: 0.67
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.73; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.77
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 1.13; acc: 0.69
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.72
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.86; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 1.03; acc: 0.67
Batch: 600; loss: 0.7; acc: 0.75
Batch: 620; loss: 0.57; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.75
Batch: 660; loss: 0.83; acc: 0.7
Batch: 680; loss: 0.75; acc: 0.7
Batch: 700; loss: 0.86; acc: 0.73
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6699989084984846; val_accuracy: 0.7881170382165605 

The current subspace-distance is: 0.00013531999138649553 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.53; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.75
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.73
Batch: 240; loss: 0.76; acc: 0.73
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.8; acc: 0.72
Batch: 360; loss: 0.9; acc: 0.69
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.71; acc: 0.73
Batch: 420; loss: 0.81; acc: 0.73
Batch: 440; loss: 0.83; acc: 0.73
Batch: 460; loss: 0.88; acc: 0.75
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 1.03; acc: 0.66
Batch: 560; loss: 0.8; acc: 0.83
Batch: 580; loss: 0.91; acc: 0.72
Batch: 600; loss: 0.69; acc: 0.7
Batch: 620; loss: 0.62; acc: 0.77
Batch: 640; loss: 0.87; acc: 0.7
Batch: 660; loss: 0.71; acc: 0.78
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.83; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.66
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.57; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6693300346660006; val_accuracy: 0.7887141719745223 

The current subspace-distance is: 0.00013766453776042908 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.67
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.8
Batch: 160; loss: 0.7; acc: 0.77
Batch: 180; loss: 0.86; acc: 0.72
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.78
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.98; acc: 0.72
Batch: 460; loss: 0.96; acc: 0.69
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.95; acc: 0.7
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.8; acc: 0.77
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.86; acc: 0.69
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 1.04; acc: 0.73
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.91
Val Epoch over. val_loss: 0.6685680585682012; val_accuracy: 0.7894108280254777 

The current subspace-distance is: 0.00013970673899166286 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.93; acc: 0.7
Batch: 160; loss: 0.68; acc: 0.8
Batch: 180; loss: 0.86; acc: 0.7
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 1.13; acc: 0.64
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 0.84; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.77
Batch: 320; loss: 0.92; acc: 0.72
Batch: 340; loss: 0.9; acc: 0.75
Batch: 360; loss: 0.78; acc: 0.7
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.73
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.84; acc: 0.73
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.63; acc: 0.73
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.96; acc: 0.66
Batch: 580; loss: 0.8; acc: 0.75
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.69; acc: 0.73
Batch: 660; loss: 0.86; acc: 0.75
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.8; acc: 0.72
Batch: 740; loss: 0.86; acc: 0.72
Batch: 760; loss: 0.76; acc: 0.72
Batch: 780; loss: 0.84; acc: 0.66
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.91
Val Epoch over. val_loss: 0.6680168397487349; val_accuracy: 0.7889132165605095 

The current subspace-distance is: 0.00014260118769016117 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.69
Batch: 100; loss: 0.82; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.69
Batch: 140; loss: 0.81; acc: 0.73
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.72
Batch: 240; loss: 0.82; acc: 0.77
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.7
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.89; acc: 0.72
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.84; acc: 0.69
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.66
Batch: 520; loss: 0.87; acc: 0.7
Batch: 540; loss: 0.84; acc: 0.73
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.77; acc: 0.75
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.98; acc: 0.72
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.72; acc: 0.73
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6678343637354055; val_accuracy: 0.7892117834394905 

The current subspace-distance is: 0.0001452620344934985 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.9; acc: 0.7
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.88; acc: 0.73
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.78
Batch: 200; loss: 0.78; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.86; acc: 0.7
Batch: 260; loss: 0.74; acc: 0.72
Batch: 280; loss: 0.86; acc: 0.77
Batch: 300; loss: 0.58; acc: 0.77
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.94; acc: 0.75
Batch: 420; loss: 0.72; acc: 0.72
Batch: 440; loss: 0.84; acc: 0.75
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.72; acc: 0.77
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.73
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.73; acc: 0.72
Batch: 620; loss: 0.82; acc: 0.75
Batch: 640; loss: 0.68; acc: 0.73
Batch: 660; loss: 0.73; acc: 0.73
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.98; acc: 0.69
Batch: 720; loss: 0.74; acc: 0.73
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.81; acc: 0.77
Batch: 780; loss: 0.83; acc: 0.8
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.667223925613294; val_accuracy: 0.7897093949044586 

The current subspace-distance is: 0.00014845219266135246 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.77
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.91; acc: 0.73
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.74; acc: 0.77
Batch: 300; loss: 0.82; acc: 0.73
Batch: 320; loss: 0.73; acc: 0.73
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.72
Batch: 400; loss: 0.79; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.77
Batch: 440; loss: 0.99; acc: 0.7
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 1.19; acc: 0.69
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.78
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.75
Batch: 600; loss: 0.65; acc: 0.77
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.91; acc: 0.73
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.77; acc: 0.72
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.69
Batch: 780; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.89
Val Epoch over. val_loss: 0.6669646176942594; val_accuracy: 0.7894108280254777 

The current subspace-distance is: 0.00014831757289357483 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.75
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.96; acc: 0.75
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.99; acc: 0.64
Batch: 280; loss: 1.06; acc: 0.66
Batch: 300; loss: 0.64; acc: 0.77
Batch: 320; loss: 0.68; acc: 0.73
Batch: 340; loss: 0.86; acc: 0.72
Batch: 360; loss: 0.89; acc: 0.72
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.69; acc: 0.72
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.83; acc: 0.73
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 1.0; acc: 0.64
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.98; acc: 0.7
Batch: 660; loss: 0.91; acc: 0.72
Batch: 680; loss: 0.8; acc: 0.78
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.73
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.91; acc: 0.77
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6669346221313355; val_accuracy: 0.7897093949044586 

The current subspace-distance is: 0.00014996883692219853 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.9; acc: 0.7
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 1.04; acc: 0.7
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.73; acc: 0.72
Batch: 220; loss: 0.8; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.8; acc: 0.67
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.77; acc: 0.75
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.68; acc: 0.77
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 1.02; acc: 0.64
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.75
Batch: 520; loss: 0.65; acc: 0.75
Batch: 540; loss: 0.96; acc: 0.77
Batch: 560; loss: 1.0; acc: 0.72
Batch: 580; loss: 0.89; acc: 0.78
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.76; acc: 0.73
Batch: 660; loss: 1.06; acc: 0.67
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.75
Batch: 720; loss: 0.66; acc: 0.7
Batch: 740; loss: 0.81; acc: 0.66
Batch: 760; loss: 0.88; acc: 0.7
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6667233726401238; val_accuracy: 0.7903065286624203 

The current subspace-distance is: 0.00015369734319392592 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.94; acc: 0.7
Batch: 120; loss: 0.82; acc: 0.72
Batch: 140; loss: 0.83; acc: 0.73
Batch: 160; loss: 0.76; acc: 0.75
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.83; acc: 0.73
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.84; acc: 0.75
Batch: 300; loss: 0.81; acc: 0.75
Batch: 320; loss: 0.97; acc: 0.72
Batch: 340; loss: 0.8; acc: 0.77
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.83; acc: 0.7
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.91; acc: 0.69
Batch: 440; loss: 0.83; acc: 0.72
Batch: 460; loss: 0.99; acc: 0.77
Batch: 480; loss: 0.55; acc: 0.77
Batch: 500; loss: 0.5; acc: 0.81
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.8
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.77
Batch: 720; loss: 0.68; acc: 0.77
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.91; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6666153555463075; val_accuracy: 0.7900079617834395 

The current subspace-distance is: 0.00015601972700096667 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.7
Batch: 160; loss: 0.76; acc: 0.75
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.75
Batch: 220; loss: 0.78; acc: 0.72
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.87; acc: 0.72
Batch: 280; loss: 0.8; acc: 0.72
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 0.75; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.61; acc: 0.75
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.84; acc: 0.67
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.74; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 1.13; acc: 0.72
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.99; acc: 0.7
Batch: 660; loss: 0.74; acc: 0.73
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.66; acc: 0.78
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.99; acc: 0.64
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.73
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6663583264609051; val_accuracy: 0.7898089171974523 

The current subspace-distance is: 0.00015880772843956947 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.82; acc: 0.77
Batch: 200; loss: 0.81; acc: 0.72
Batch: 220; loss: 0.82; acc: 0.73
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 1.08; acc: 0.72
Batch: 280; loss: 0.71; acc: 0.75
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 0.8; acc: 0.75
Batch: 360; loss: 0.8; acc: 0.7
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.84; acc: 0.75
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.91; acc: 0.69
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.82; acc: 0.72
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.81; acc: 0.77
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6660101315018477; val_accuracy: 0.7909036624203821 

The current subspace-distance is: 0.00016195708303712308 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 0.59; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 1.05; acc: 0.66
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.83; acc: 0.69
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 1.17; acc: 0.61
Batch: 400; loss: 0.76; acc: 0.73
Batch: 420; loss: 0.6; acc: 0.77
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.84; acc: 0.73
Batch: 480; loss: 0.47; acc: 0.81
Batch: 500; loss: 0.77; acc: 0.77
Batch: 520; loss: 0.74; acc: 0.73
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.98; acc: 0.66
Batch: 760; loss: 0.82; acc: 0.75
Batch: 780; loss: 0.75; acc: 0.77
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6658267386400016; val_accuracy: 0.7911027070063694 

The current subspace-distance is: 0.00016253814101219177 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.72
Batch: 160; loss: 0.84; acc: 0.77
Batch: 180; loss: 0.69; acc: 0.75
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.78; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.67; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.75
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.78
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.84; acc: 0.69
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.8
Batch: 480; loss: 0.99; acc: 0.7
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.75; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 0.96; acc: 0.69
Batch: 580; loss: 0.96; acc: 0.69
Batch: 600; loss: 0.6; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.69
Batch: 640; loss: 0.85; acc: 0.77
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.9; acc: 0.72
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 1.08; acc: 0.69
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6655800363440423; val_accuracy: 0.7913017515923567 

The current subspace-distance is: 0.0001622242562007159 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.59
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.79; acc: 0.7
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.69
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.86; acc: 0.7
Batch: 220; loss: 0.92; acc: 0.7
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.71; acc: 0.73
Batch: 320; loss: 0.92; acc: 0.73
Batch: 340; loss: 0.82; acc: 0.73
Batch: 360; loss: 0.84; acc: 0.73
Batch: 380; loss: 0.75; acc: 0.7
Batch: 400; loss: 0.95; acc: 0.72
Batch: 420; loss: 0.57; acc: 0.77
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.82; acc: 0.75
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 0.66; acc: 0.75
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.87; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.75
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.73
Batch: 760; loss: 0.88; acc: 0.72
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.665744557122516; val_accuracy: 0.790406050955414 

The current subspace-distance is: 0.0001649412588449195 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.69; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.73
Batch: 240; loss: 0.79; acc: 0.66
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.9; acc: 0.7
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.72; acc: 0.7
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.72
Batch: 500; loss: 0.77; acc: 0.7
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 1.19; acc: 0.67
Batch: 620; loss: 0.61; acc: 0.78
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.67
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.69; acc: 0.75
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6654955097444498; val_accuracy: 0.7911027070063694 

The current subspace-distance is: 0.00016615366621408612 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.83; acc: 0.75
Batch: 220; loss: 0.81; acc: 0.73
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.89; acc: 0.72
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.89; acc: 0.8
Batch: 340; loss: 0.71; acc: 0.75
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.88; acc: 0.7
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.79; acc: 0.75
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.68; acc: 0.75
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.77
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.89; acc: 0.72
Batch: 700; loss: 0.66; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.77
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.88; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6652874673248097; val_accuracy: 0.7909036624203821 

The current subspace-distance is: 0.000168991056852974 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.67
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.88; acc: 0.73
Batch: 160; loss: 0.88; acc: 0.7
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.94; acc: 0.7
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.7
Batch: 300; loss: 0.93; acc: 0.61
Batch: 320; loss: 0.72; acc: 0.75
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.69
Batch: 400; loss: 0.58; acc: 0.8
Batch: 420; loss: 0.97; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.67
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.69; acc: 0.72
Batch: 640; loss: 0.82; acc: 0.69
Batch: 660; loss: 0.97; acc: 0.73
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.87; acc: 0.67
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.75; acc: 0.73
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6652140418055711; val_accuracy: 0.7906050955414012 

The current subspace-distance is: 0.0001709230855340138 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 0.84; acc: 0.67
Batch: 140; loss: 1.05; acc: 0.69
Batch: 160; loss: 0.87; acc: 0.72
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.72
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.73
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.84; acc: 0.78
Batch: 440; loss: 0.88; acc: 0.7
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.74; acc: 0.78
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.73; acc: 0.78
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.74; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.84; acc: 0.78
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6652246759195996; val_accuracy: 0.791202229299363 

The current subspace-distance is: 0.00017258779553230852 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.7
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.88; acc: 0.72
Batch: 100; loss: 0.84; acc: 0.8
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.59; acc: 0.78
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.81; acc: 0.73
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.79; acc: 0.75
Batch: 420; loss: 0.85; acc: 0.7
Batch: 440; loss: 0.59; acc: 0.77
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.8; acc: 0.8
Batch: 500; loss: 1.04; acc: 0.75
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.76; acc: 0.73
Batch: 560; loss: 0.94; acc: 0.75
Batch: 580; loss: 0.86; acc: 0.75
Batch: 600; loss: 0.73; acc: 0.73
Batch: 620; loss: 0.68; acc: 0.77
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.84; acc: 0.73
Batch: 700; loss: 0.67; acc: 0.8
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.89; acc: 0.7
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6651496473391345; val_accuracy: 0.7909036624203821 

The current subspace-distance is: 0.0001760955638019368 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.77
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.73
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.73
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.77
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.89; acc: 0.73
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.91; acc: 0.8
Batch: 400; loss: 0.58; acc: 0.8
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.7
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.72
Batch: 580; loss: 0.7; acc: 0.73
Batch: 600; loss: 0.69; acc: 0.73
Batch: 620; loss: 0.83; acc: 0.72
Batch: 640; loss: 0.72; acc: 0.73
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 1.01; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.73
Batch: 720; loss: 0.75; acc: 0.73
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.84; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6650326113412335; val_accuracy: 0.7910031847133758 

The current subspace-distance is: 0.0001807171938708052 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.77; acc: 0.7
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.78; acc: 0.72
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 1.01; acc: 0.64
Batch: 320; loss: 0.69; acc: 0.77
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.77
Batch: 380; loss: 0.8; acc: 0.72
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.96; acc: 0.72
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.87; acc: 0.69
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.73
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 0.84; acc: 0.78
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.86; acc: 0.7
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.88; acc: 0.73
Batch: 760; loss: 0.78; acc: 0.72
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6649780055140234; val_accuracy: 0.791202229299363 

The current subspace-distance is: 0.0001819959725253284 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.7; acc: 0.73
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.83; acc: 0.67
Batch: 200; loss: 0.7; acc: 0.75
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.83; acc: 0.7
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.71; acc: 0.81
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.87; acc: 0.78
Batch: 380; loss: 0.76; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.7
Batch: 420; loss: 0.77; acc: 0.73
Batch: 440; loss: 0.78; acc: 0.75
Batch: 460; loss: 0.96; acc: 0.67
Batch: 480; loss: 0.78; acc: 0.78
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.78
Batch: 560; loss: 0.77; acc: 0.75
Batch: 580; loss: 0.73; acc: 0.77
Batch: 600; loss: 0.77; acc: 0.7
Batch: 620; loss: 0.92; acc: 0.66
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 0.78; acc: 0.7
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6649414285732682; val_accuracy: 0.7911027070063694 

The current subspace-distance is: 0.00018299565999768674 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.87; acc: 0.73
Batch: 160; loss: 0.91; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.78
Batch: 200; loss: 0.83; acc: 0.73
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.93; acc: 0.69
Batch: 260; loss: 0.61; acc: 0.78
Batch: 280; loss: 0.89; acc: 0.67
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.77
Batch: 360; loss: 0.72; acc: 0.75
Batch: 380; loss: 1.0; acc: 0.67
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.95; acc: 0.69
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.62; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.75
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.78
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.79; acc: 0.7
Batch: 740; loss: 0.57; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.73
Batch: 780; loss: 0.83; acc: 0.7
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6648657285863426; val_accuracy: 0.7910031847133758 

The current subspace-distance is: 0.00018554486450739205 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.83; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.8
Batch: 240; loss: 0.81; acc: 0.72
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.55; acc: 0.8
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 0.56; acc: 0.75
Batch: 360; loss: 0.71; acc: 0.72
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.7; acc: 0.81
Batch: 420; loss: 0.77; acc: 0.77
Batch: 440; loss: 0.78; acc: 0.73
Batch: 460; loss: 0.69; acc: 0.72
Batch: 480; loss: 0.8; acc: 0.78
Batch: 500; loss: 0.87; acc: 0.67
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.77
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.62; acc: 0.77
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.68; acc: 0.77
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.93; acc: 0.67
Batch: 760; loss: 1.0; acc: 0.69
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.6647163337203348; val_accuracy: 0.7911027070063694 

The current subspace-distance is: 0.00018889967759605497 

plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 177949
elements in E: 79684000
fraction nonzero: 0.0022331835751217308
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.27; acc: 0.28
Batch: 80; loss: 2.26; acc: 0.33
Batch: 100; loss: 2.27; acc: 0.23
Batch: 120; loss: 2.26; acc: 0.28
Batch: 140; loss: 2.22; acc: 0.39
Batch: 160; loss: 2.24; acc: 0.31
Batch: 180; loss: 2.2; acc: 0.42
Batch: 200; loss: 2.2; acc: 0.42
Batch: 220; loss: 2.16; acc: 0.47
Batch: 240; loss: 2.12; acc: 0.5
Batch: 260; loss: 2.12; acc: 0.52
Batch: 280; loss: 2.11; acc: 0.45
Batch: 300; loss: 2.05; acc: 0.5
Batch: 320; loss: 2.08; acc: 0.45
Batch: 340; loss: 2.0; acc: 0.59
Batch: 360; loss: 1.97; acc: 0.64
Batch: 380; loss: 1.86; acc: 0.66
Batch: 400; loss: 1.89; acc: 0.53
Batch: 420; loss: 1.81; acc: 0.62
Batch: 440; loss: 1.74; acc: 0.67
Batch: 460; loss: 1.81; acc: 0.56
Batch: 480; loss: 1.66; acc: 0.66
Batch: 500; loss: 1.6; acc: 0.59
Batch: 520; loss: 1.65; acc: 0.61
Batch: 540; loss: 1.63; acc: 0.66
Batch: 560; loss: 1.49; acc: 0.59
Batch: 580; loss: 1.47; acc: 0.64
Batch: 600; loss: 1.27; acc: 0.72
Batch: 620; loss: 1.22; acc: 0.77
Batch: 640; loss: 1.23; acc: 0.73
Batch: 660; loss: 1.21; acc: 0.67
Batch: 680; loss: 0.94; acc: 0.81
Batch: 700; loss: 1.05; acc: 0.72
Batch: 720; loss: 1.08; acc: 0.66
Batch: 740; loss: 0.89; acc: 0.81
Batch: 760; loss: 0.76; acc: 0.81
Batch: 780; loss: 1.1; acc: 0.64
Train Epoch over. train_loss: 1.77; train_accuracy: 0.53 

Batch: 0; loss: 0.97; acc: 0.73
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 0.74; acc: 0.81
Val Epoch over. val_loss: 0.883464018630374; val_accuracy: 0.7557722929936306 

The current subspace-distance is: 4.0110029658535495e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.96; acc: 0.7
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.89; acc: 0.73
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.75; acc: 0.77
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.88; acc: 0.73
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 1.0; acc: 0.73
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.89; acc: 0.7
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.77
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.77; acc: 0.77
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.7; acc: 0.77
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.72
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 0.71; train_accuracy: 0.78 

Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.73
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.5744445404619168; val_accuracy: 0.8291202229299363 

The current subspace-distance is: 5.860882083652541e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.68; acc: 0.73
Batch: 220; loss: 0.61; acc: 0.78
Batch: 240; loss: 0.58; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.89; acc: 0.72
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.9; acc: 0.72
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.69; acc: 0.77
Batch: 560; loss: 0.81; acc: 0.67
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.64; acc: 0.77
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.68; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.72
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.5241395584337271; val_accuracy: 0.8409633757961783 

The current subspace-distance is: 6.737293006153777e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.77
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.73
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.81; acc: 0.77
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.56; acc: 0.78
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.72
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.49982903774376886; val_accuracy: 0.8465366242038217 

The current subspace-distance is: 7.413650746457279e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.8
Batch: 60; loss: 0.55; acc: 0.75
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.78
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.72; acc: 0.75
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.78
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.79; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.58; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.77
Batch: 680; loss: 0.59; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.72
Batch: 760; loss: 0.71; acc: 0.75
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4841084560960721; val_accuracy: 0.8535031847133758 

The current subspace-distance is: 8.064335270319134e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.78
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.8
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.8
Batch: 640; loss: 0.5; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.6; acc: 0.77
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.89
Val Epoch over. val_loss: 0.4695374855569973; val_accuracy: 0.8589769108280255 

The current subspace-distance is: 8.697764860698953e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.77
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.44; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.75
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.73; acc: 0.77
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.46728274634309636; val_accuracy: 0.86046974522293 

The current subspace-distance is: 9.156572923529893e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.75
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 1.02; acc: 0.72
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.83
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.45275254528613607; val_accuracy: 0.865047770700637 

The current subspace-distance is: 9.804432920645922e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.78
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4471006320350489; val_accuracy: 0.8636544585987261 

The current subspace-distance is: 0.00010231524356640875 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.77
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.73
Batch: 320; loss: 0.34; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4476299589606607; val_accuracy: 0.8654458598726115 

The current subspace-distance is: 0.00010777029092423618 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.75
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.53; acc: 0.8
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.75
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.78; acc: 0.75
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4406426577431381; val_accuracy: 0.8664410828025477 

The current subspace-distance is: 0.00011177734995726496 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.72; acc: 0.73
Batch: 300; loss: 0.58; acc: 0.78
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.69; acc: 0.8
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.75
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.58; acc: 0.77
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.44220257384382233; val_accuracy: 0.8652468152866242 

The current subspace-distance is: 0.00011553536751307547 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.68; acc: 0.77
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.71; acc: 0.75
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.78; acc: 0.78
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.72; acc: 0.75
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.44155471017406245; val_accuracy: 0.865047770700637 

The current subspace-distance is: 0.00012086045171599835 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.95
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.43782817026612103; val_accuracy: 0.8674363057324841 

The current subspace-distance is: 0.00012569042155519128 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.75
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.8
Batch: 540; loss: 0.55; acc: 0.78
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.74; acc: 0.77
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.77
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4364543281922674; val_accuracy: 0.8661425159235668 

The current subspace-distance is: 0.00012812147906515747 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.8
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 1.03; acc: 0.77
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 1.01; acc: 0.78
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.81
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.77
Batch: 720; loss: 0.49; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4359624990422255; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.0001322403404628858 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.8
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.43450871840783745; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.00013653197675012052 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.8
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.43515428445141785; val_accuracy: 0.8671377388535032 

The current subspace-distance is: 0.0001397265587002039 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.68; acc: 0.78
Batch: 300; loss: 0.78; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.48; acc: 0.8
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4331646018726811; val_accuracy: 0.8681329617834395 

The current subspace-distance is: 0.00014235936396289617 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.8
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.65; acc: 0.8
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.77
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4328147682604516; val_accuracy: 0.8689291401273885 

The current subspace-distance is: 0.00014584751625079662 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.78
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.43146129037923875; val_accuracy: 0.8686305732484076 

The current subspace-distance is: 0.00014841208758298308 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.78
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.81
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.71; acc: 0.73
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4310159776241157; val_accuracy: 0.8682324840764332 

The current subspace-distance is: 0.00015176630404312164 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.65; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.43103657425588865; val_accuracy: 0.868531050955414 

The current subspace-distance is: 0.00015698019706178457 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.78
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.43034429373634847; val_accuracy: 0.8696257961783439 

The current subspace-distance is: 0.0001606600999366492 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.77
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.77
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.77
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.43008024231263786; val_accuracy: 0.8689291401273885 

The current subspace-distance is: 0.00016352224338334054 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.78
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.43056408064380575; val_accuracy: 0.8697253184713376 

The current subspace-distance is: 0.00016562058590352535 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.75
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.78
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4298920472905894; val_accuracy: 0.8699243630573248 

The current subspace-distance is: 0.00016944635717663914 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.8
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.8
Batch: 680; loss: 0.74; acc: 0.77
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4300380541830306; val_accuracy: 0.868531050955414 

The current subspace-distance is: 0.00017235279665328562 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.77
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.65; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.42921420922324915; val_accuracy: 0.8696257961783439 

The current subspace-distance is: 0.00017565523739904165 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.42901601874904266; val_accuracy: 0.8696257961783439 

The current subspace-distance is: 0.0001792454713722691 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.75
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.83; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.78
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.81
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.78
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42866853999484117; val_accuracy: 0.8691281847133758 

The current subspace-distance is: 0.00018016522517427802 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.68; acc: 0.77
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 1.01; acc: 0.73
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.77
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42821088661054135; val_accuracy: 0.8701234076433121 

The current subspace-distance is: 0.000183704643859528 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.8
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.64; acc: 0.78
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.42796149754979806; val_accuracy: 0.8695262738853503 

The current subspace-distance is: 0.00018498633289709687 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.72
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.77
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.42770496874478214; val_accuracy: 0.8703224522292994 

The current subspace-distance is: 0.00018761925457511097 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.77
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4279874965643427; val_accuracy: 0.8695262738853503 

The current subspace-distance is: 0.00019118028285447508 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.78
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.57; acc: 0.78
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.71; acc: 0.77
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4276116171460243; val_accuracy: 0.8706210191082803 

The current subspace-distance is: 0.00019293047080282122 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.79; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.63; acc: 0.77
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.8
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.427549325736465; val_accuracy: 0.8700238853503185 

The current subspace-distance is: 0.00019298071856610477 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.79; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4273195634981629; val_accuracy: 0.8695262738853503 

The current subspace-distance is: 0.00019570201402530074 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4271518272484184; val_accuracy: 0.8696257961783439 

The current subspace-distance is: 0.00019817966676782817 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.8
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.84
Batch: 180; loss: 0.27; acc: 0.88
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.8
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.83; acc: 0.8
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.8
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.81
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.427209955776573; val_accuracy: 0.8703224522292994 

The current subspace-distance is: 0.00019979297940153629 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.77
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42715576665986116; val_accuracy: 0.8697253184713376 

The current subspace-distance is: 0.00020149613555986434 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.73
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42687568040030777; val_accuracy: 0.8700238853503185 

The current subspace-distance is: 0.00020427038543857634 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.73
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.83
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.48; acc: 0.78
Batch: 640; loss: 0.56; acc: 0.8
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.81
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4268114087498112; val_accuracy: 0.8699243630573248 

The current subspace-distance is: 0.00020618591224774718 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.75
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.75
Batch: 620; loss: 0.48; acc: 0.81
Batch: 640; loss: 0.5; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4268727771890391; val_accuracy: 0.8705214968152867 

The current subspace-distance is: 0.000208868965273723 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.7
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42675343578218655; val_accuracy: 0.8700238853503185 

The current subspace-distance is: 0.0002122669538948685 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.77
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.78
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.77
Batch: 760; loss: 0.49; acc: 0.81
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42673241788414634; val_accuracy: 0.8702229299363057 

The current subspace-distance is: 0.00021438747353386134 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.77
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42669334680221643; val_accuracy: 0.8699243630573248 

The current subspace-distance is: 0.00021620247571263462 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.59; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.82; acc: 0.73
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.69; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.78
Batch: 620; loss: 0.61; acc: 0.8
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4264171866664461; val_accuracy: 0.8699243630573248 

The current subspace-distance is: 0.0002177389687858522 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.8
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.75
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4264459162950516; val_accuracy: 0.870421974522293 

The current subspace-distance is: 0.00022013351554051042 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.57; acc: 0.8
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.8
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.81
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.42639198851813176; val_accuracy: 0.8702229299363057 

The current subspace-distance is: 0.00022368915961124003 

plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 267509
elements in E: 119526000
fraction nonzero: 0.0022380820909258235
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.22
Batch: 60; loss: 2.25; acc: 0.36
Batch: 80; loss: 2.24; acc: 0.39
Batch: 100; loss: 2.22; acc: 0.36
Batch: 120; loss: 2.22; acc: 0.34
Batch: 140; loss: 2.17; acc: 0.47
Batch: 160; loss: 2.17; acc: 0.42
Batch: 180; loss: 2.13; acc: 0.45
Batch: 200; loss: 2.11; acc: 0.47
Batch: 220; loss: 2.06; acc: 0.56
Batch: 240; loss: 1.99; acc: 0.56
Batch: 260; loss: 1.99; acc: 0.58
Batch: 280; loss: 2.0; acc: 0.52
Batch: 300; loss: 1.85; acc: 0.67
Batch: 320; loss: 1.79; acc: 0.64
Batch: 340; loss: 1.65; acc: 0.67
Batch: 360; loss: 1.61; acc: 0.69
Batch: 380; loss: 1.47; acc: 0.7
Batch: 400; loss: 1.51; acc: 0.67
Batch: 420; loss: 1.27; acc: 0.73
Batch: 440; loss: 1.23; acc: 0.73
Batch: 460; loss: 1.29; acc: 0.7
Batch: 480; loss: 1.17; acc: 0.61
Batch: 500; loss: 1.02; acc: 0.75
Batch: 520; loss: 1.09; acc: 0.69
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.87; acc: 0.83
Batch: 580; loss: 0.93; acc: 0.78
Batch: 600; loss: 0.93; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.78
Batch: 700; loss: 0.65; acc: 0.77
Batch: 720; loss: 0.81; acc: 0.72
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 1.5; train_accuracy: 0.6 

Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.6153477168386909; val_accuracy: 0.8255374203821656 

The current subspace-distance is: 4.352360701886937e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.89; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.75; acc: 0.73
Batch: 160; loss: 0.5; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.55; acc: 0.78
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.78
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.45197802350209776; val_accuracy: 0.8643511146496815 

The current subspace-distance is: 5.825134212500416e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.77
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.77
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.65; acc: 0.81
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.41224786871747604; val_accuracy: 0.8772890127388535 

The current subspace-distance is: 6.650641444139183e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.8
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.84
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.75
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.40167911094465075; val_accuracy: 0.8779856687898089 

The current subspace-distance is: 7.359447772614658e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.38921185791682283; val_accuracy: 0.8830613057324841 

The current subspace-distance is: 7.975709741003811e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.77
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.38025984047040057; val_accuracy: 0.8837579617834395 

The current subspace-distance is: 8.678183803567663e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.83
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.38209962659770513; val_accuracy: 0.8827627388535032 

The current subspace-distance is: 9.191307617584243e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.77
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.36913020016661113; val_accuracy: 0.8876393312101911 

The current subspace-distance is: 9.616515308152884e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.78
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.12; acc: 1.0
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.81
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3691002941055662; val_accuracy: 0.884952229299363 

The current subspace-distance is: 0.00010168283188249916 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.24; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.78; acc: 0.72
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.36719533089239886; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.00010646758892107755 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.1; acc: 1.0
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.25; acc: 0.88
Batch: 640; loss: 0.18; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.36167263168438224; val_accuracy: 0.8907245222929936 

The current subspace-distance is: 0.0001112194950110279 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.81
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3617194390315918; val_accuracy: 0.8899283439490446 

The current subspace-distance is: 0.00011596037802519277 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.8; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3589723244499249; val_accuracy: 0.8914211783439491 

The current subspace-distance is: 0.00011986270692432299 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.77; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.84
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3579846500496196; val_accuracy: 0.8929140127388535 

The current subspace-distance is: 0.0001246115571120754 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.35527874116495156; val_accuracy: 0.892515923566879 

The current subspace-distance is: 0.0001278062118217349 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.75
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.78
Batch: 580; loss: 0.52; acc: 0.8
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.58; acc: 0.8
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3551073244944879; val_accuracy: 0.8941082802547771 

The current subspace-distance is: 0.0001328738871961832 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.83
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.63; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3519254532304539; val_accuracy: 0.8951035031847133 

The current subspace-distance is: 0.0001364609197480604 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.35232272483171173; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00014041164831724018 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.77
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3520099923584112; val_accuracy: 0.8924164012738853 

The current subspace-distance is: 0.0001439458574168384 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.78
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.78
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.8
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.35138874686068033; val_accuracy: 0.8928144904458599 

The current subspace-distance is: 0.00014817413466516882 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.56; acc: 0.8
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34857627801644575; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00015188333054538816 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.3483568073078326; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00015580527542624623 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.72; acc: 0.75
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.34793646709554515; val_accuracy: 0.8945063694267515 

The current subspace-distance is: 0.0001576273498358205 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.7; acc: 0.73
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.81
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34801277262010394; val_accuracy: 0.8958001592356688 

The current subspace-distance is: 0.00016068671538960189 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3468916875067031; val_accuracy: 0.8961982484076433 

The current subspace-distance is: 0.00016376741405110806 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.8
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3480800105507966; val_accuracy: 0.8949044585987261 

The current subspace-distance is: 0.00016671163029968739 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.71; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.34761497876636543; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.0001705682516330853 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.81
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3464734649202626; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.0001738492283038795 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.84
Batch: 480; loss: 0.15; acc: 1.0
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.34622242415596727; val_accuracy: 0.8950039808917197 

The current subspace-distance is: 0.00017651257803663611 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.83
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.3462548699633331; val_accuracy: 0.8948049363057324 

The current subspace-distance is: 0.00017940477118827403 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3458730851294129; val_accuracy: 0.8951035031847133 

The current subspace-distance is: 0.00018229259876534343 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3452908904973868; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00018531699606683105 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.83
Batch: 220; loss: 0.43; acc: 0.84
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.34559231118601597; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.00018719310173764825 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.78
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3453430036545559; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00019041278574150056 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.8
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.74; acc: 0.77
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.79; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.11; acc: 1.0
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34577217641150115; val_accuracy: 0.8953025477707006 

The current subspace-distance is: 0.0001928457641042769 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.81
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34561301312249176; val_accuracy: 0.8953025477707006 

The current subspace-distance is: 0.0001950086880242452 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3451678746729899; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.0001978921063710004 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.77
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.3453305045700377; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 0.00020070951723027974 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3448943941361585; val_accuracy: 0.8959992038216561 

The current subspace-distance is: 0.00020257719734217972 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.345121232852055; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.000205305521376431 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34496101613636987; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 0.00020745676010847092 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.73; acc: 0.84
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.8
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.83
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34469627337470937; val_accuracy: 0.8966958598726115 

The current subspace-distance is: 0.0002100736746797338 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34486714404100066; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00021212681895121932 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3448854218337946; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 0.0002144446043530479 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.83
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3446976815344422; val_accuracy: 0.8961982484076433 

The current subspace-distance is: 0.0002162677701562643 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34486638142424786; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 0.00021860108245164156 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.81
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.81
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34470919467461336; val_accuracy: 0.8966958598726115 

The current subspace-distance is: 0.00022154314501676708 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.77
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34454312056872494; val_accuracy: 0.8967953821656051 

The current subspace-distance is: 0.00022319707204587758 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.8
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.07; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34466225288476154; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.00022511085262522101 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.34456449125412925; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00022736952814739197 

plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_600_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 357162
elements in E: 159368000
fraction nonzero: 0.0022411149038702874
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.26; acc: 0.25
Batch: 60; loss: 2.24; acc: 0.38
Batch: 80; loss: 2.23; acc: 0.44
Batch: 100; loss: 2.2; acc: 0.41
Batch: 120; loss: 2.19; acc: 0.36
Batch: 140; loss: 2.12; acc: 0.56
Batch: 160; loss: 2.13; acc: 0.47
Batch: 180; loss: 2.11; acc: 0.45
Batch: 200; loss: 2.04; acc: 0.53
Batch: 220; loss: 1.98; acc: 0.53
Batch: 240; loss: 1.88; acc: 0.59
Batch: 260; loss: 1.83; acc: 0.67
Batch: 280; loss: 1.78; acc: 0.59
Batch: 300; loss: 1.61; acc: 0.73
Batch: 320; loss: 1.53; acc: 0.69
Batch: 340; loss: 1.36; acc: 0.72
Batch: 360; loss: 1.21; acc: 0.78
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 1.12; acc: 0.7
Batch: 420; loss: 0.8; acc: 0.81
Batch: 440; loss: 0.82; acc: 0.86
Batch: 460; loss: 1.04; acc: 0.66
Batch: 480; loss: 0.92; acc: 0.7
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.8
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.68; acc: 0.8
Train Epoch over. train_loss: 1.31; train_accuracy: 0.66 

Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.72
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.51872680568771; val_accuracy: 0.8402667197452229 

The current subspace-distance is: 4.4129163143225014e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.77
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.39910605098981006; val_accuracy: 0.8774880573248408 

The current subspace-distance is: 5.643045005854219e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.78
Batch: 660; loss: 0.45; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.36422499373650097; val_accuracy: 0.8894307324840764 

The current subspace-distance is: 6.529058009618893e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.77
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.35469776643499445; val_accuracy: 0.8912221337579618 

The current subspace-distance is: 7.320030272239819e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.25; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3425868376614941; val_accuracy: 0.8965963375796179 

The current subspace-distance is: 7.963919779285789e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.34242517842798476; val_accuracy: 0.8940087579617835 

The current subspace-distance is: 8.601420267950743e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3405792260435736; val_accuracy: 0.8959992038216561 

The current subspace-distance is: 9.20358143048361e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.11; acc: 1.0
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31852304828679484; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 9.737230720929801e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.317230452635106; val_accuracy: 0.9039609872611465 

The current subspace-distance is: 0.00010231651685899124 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.31603365104384484; val_accuracy: 0.9042595541401274 

The current subspace-distance is: 0.00010816368740051985 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3103068742877359; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.0001121239474741742 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.84
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.31167722557475613; val_accuracy: 0.9059514331210191 

The current subspace-distance is: 0.00011694435670506209 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.30895563438060175; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.00012108150258427486 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.21; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.31046270844852847; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00012488495849538594 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30887560669783576; val_accuracy: 0.90625 

The current subspace-distance is: 0.0001291822991333902 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.74; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3100270802143273; val_accuracy: 0.9058519108280255 

The current subspace-distance is: 0.00013280731218401343 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.98
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3055487188991088; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00013704555749427527 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.23; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3093365033151238; val_accuracy: 0.9076433121019108 

The current subspace-distance is: 0.00014084107533562928 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.46; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3080372860667053; val_accuracy: 0.9077428343949044 

The current subspace-distance is: 0.00014455089694820344 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.81
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3060366017565985; val_accuracy: 0.9088375796178344 

The current subspace-distance is: 0.00014814110181760043 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30563521774331476; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 0.00015146915393415838 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30539414718462404; val_accuracy: 0.9093351910828026 

The current subspace-distance is: 0.00015434531087521464 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30428095626982915; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 0.00015755863569211215 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.8
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30553872385032615; val_accuracy: 0.9104299363057324 

The current subspace-distance is: 0.00016121056978590786 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.83
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3051956642395372; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 0.00016400183085352182 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30533727761476664; val_accuracy: 0.9091361464968153 

The current subspace-distance is: 0.0001671870704740286 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.1; acc: 1.0
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30669242587343903; val_accuracy: 0.9088375796178344 

The current subspace-distance is: 0.0001707772898953408 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3045936311079059; val_accuracy: 0.910031847133758 

The current subspace-distance is: 0.00017416427726857364 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3046699412119616; val_accuracy: 0.9095342356687898 

The current subspace-distance is: 0.00017773843137547374 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.305954299962065; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 0.0001805679639801383 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.62; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.8
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3042329457962209; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00018366500444244593 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3040861976896502; val_accuracy: 0.9101313694267515 

The current subspace-distance is: 0.00018658711633179337 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.304288768867968; val_accuracy: 0.9096337579617835 

The current subspace-distance is: 0.0001897003094200045 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3047729924226263; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 0.00019215817155782133 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.81
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.72; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.30392162200466843; val_accuracy: 0.9094347133757962 

The current subspace-distance is: 0.00019497222092468292 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.3040068024994841; val_accuracy: 0.910031847133758 

The current subspace-distance is: 0.00019751036597881466 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.75
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.30372614311469587; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 0.00019994408648926765 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.77
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.98
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.83
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3041531770329946; val_accuracy: 0.9099323248407644 

The current subspace-distance is: 0.00020275717542972416 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3039606943441804; val_accuracy: 0.9095342356687898 

The current subspace-distance is: 0.00020532007329165936 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.83
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3039165293667347; val_accuracy: 0.910031847133758 

The current subspace-distance is: 0.00020773839787580073 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.1; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.84
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.30381891055471577; val_accuracy: 0.9096337579617835 

The current subspace-distance is: 0.0002104906743625179 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3036874192915145; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00021287165873218328 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.83
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.30402742817428463; val_accuracy: 0.9090366242038217 

The current subspace-distance is: 0.00021569330419879407 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3038311684682111; val_accuracy: 0.9096337579617835 

The current subspace-distance is: 0.00021813093917444348 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3037198525230596; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00022065143275540322 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3040164967488711; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00022335382527671754 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3038254654284116; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00022596617054659873 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.84
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.83
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3038629862438342; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00022886405349709094 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.303673254266666; val_accuracy: 0.9095342356687898 

The current subspace-distance is: 0.00023137417156249285 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.09; acc: 1.0
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.81
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3037930513216052; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 0.00023419637000188231 

plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_800_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 446148
elements in E: 199210000
fraction nonzero: 0.002239586366146278
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.28; acc: 0.14
Batch: 40; loss: 2.25; acc: 0.25
Batch: 60; loss: 2.22; acc: 0.47
Batch: 80; loss: 2.2; acc: 0.47
Batch: 100; loss: 2.16; acc: 0.39
Batch: 120; loss: 2.14; acc: 0.44
Batch: 140; loss: 2.02; acc: 0.58
Batch: 160; loss: 2.02; acc: 0.48
Batch: 180; loss: 1.95; acc: 0.53
Batch: 200; loss: 1.86; acc: 0.53
Batch: 220; loss: 1.73; acc: 0.64
Batch: 240; loss: 1.55; acc: 0.69
Batch: 260; loss: 1.51; acc: 0.66
Batch: 280; loss: 1.46; acc: 0.62
Batch: 300; loss: 1.17; acc: 0.73
Batch: 320; loss: 1.07; acc: 0.78
Batch: 340; loss: 0.91; acc: 0.75
Batch: 360; loss: 0.9; acc: 0.75
Batch: 380; loss: 0.81; acc: 0.77
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.8
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.63; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.78
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.77
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 1.16; train_accuracy: 0.69 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.45019180275452364; val_accuracy: 0.8691281847133758 

The current subspace-distance is: 4.2506406316533685e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.8
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.77
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.77
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.35991944875686793; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 5.45163857168518e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.81
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.33292801160911084; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 6.331459735520184e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.342569372788736; val_accuracy: 0.8943073248407644 

The current subspace-distance is: 7.026280218269676e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.72
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.32919741478885056; val_accuracy: 0.9023686305732485 

The current subspace-distance is: 7.667096360819414e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31721874326467514; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 8.34775491966866e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.86
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.77
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3229222289600949; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 8.877072832547128e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.29051564201997343; val_accuracy: 0.9140127388535032 

The current subspace-distance is: 9.444930037716404e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.86
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.29147731023988904; val_accuracy: 0.9131170382165605 

The current subspace-distance is: 0.00010024081711890176 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2918910574001871; val_accuracy: 0.913515127388535 

The current subspace-distance is: 0.0001055110406014137 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.09; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.28003017936542535; val_accuracy: 0.9184912420382165 

The current subspace-distance is: 0.00010996423952747136 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.27700052367653816; val_accuracy: 0.9185907643312102 

The current subspace-distance is: 0.00011474303028080612 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.27361417542787114; val_accuracy: 0.9193869426751592 

The current subspace-distance is: 0.0001186135777970776 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2745718611938179; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 0.0001232982613146305 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2739359966603814; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 0.00012780932593159378 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.27716962836540426; val_accuracy: 0.9189888535031847 

The current subspace-distance is: 0.00013191155449021608 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2688676595782778; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 0.00013589966692961752 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2687675243919822; val_accuracy: 0.9211783439490446 

The current subspace-distance is: 0.0001396087754983455 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.27251418385725873; val_accuracy: 0.9211783439490446 

The current subspace-distance is: 0.000143659402965568 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2714298661727055; val_accuracy: 0.9221735668789809 

The current subspace-distance is: 0.0001471332652727142 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.84
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2665359390199564; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 0.00015055423136800528 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26687828332755215; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 0.00015408000035677105 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26619043618820276; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 0.000157445902004838 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26653535165794334; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 0.00016099547792691737 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26611510830320373; val_accuracy: 0.9219745222929936 

The current subspace-distance is: 0.000163775694090873 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.32; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26535496699392414; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 0.00016743956075515598 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26765700662212005; val_accuracy: 0.921875 

The current subspace-distance is: 0.0001701504661468789 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2648597661951545; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 0.00017368054250255227 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.86
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26624563383828304; val_accuracy: 0.92296974522293 

The current subspace-distance is: 0.00017677723371889442 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2661881355722998; val_accuracy: 0.9220740445859873 

The current subspace-distance is: 0.00017994694644585252 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2649708927911558; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 0.00018312472093384713 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.11; acc: 1.0
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26437441710453885; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 0.00018541375175118446 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26469093788960935; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00018868887855205685 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2643979399637052; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 0.00019180115486960858 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.97
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2648095215676696; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 0.0001944002287928015 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26441512161948877; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00019767308549489826 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26415967983994515; val_accuracy: 0.9231687898089171 

The current subspace-distance is: 0.0002005559508688748 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26405436887293104; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00020327094534877688 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26383494732865864; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00020634199609048665 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26406828053058334; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 0.00020905329438392073 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26407839319888193; val_accuracy: 0.9232683121019108 

The current subspace-distance is: 0.00021185378136578947 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26397907197665255; val_accuracy: 0.923765923566879 

The current subspace-distance is: 0.0002141365548595786 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2640143220496785; val_accuracy: 0.923765923566879 

The current subspace-distance is: 0.00021685523097403347 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2639399529167801; val_accuracy: 0.9234673566878981 

The current subspace-distance is: 0.00021904290770180523 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.12; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.84
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26392049968812115; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 0.0002212558756582439 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2639721260898432; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 0.00022343320597428828 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.11; acc: 1.0
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26382412732976257; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 0.00022574953618459404 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2637552167199979; val_accuracy: 0.9234673566878981 

The current subspace-distance is: 0.0002279483014717698 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.81
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.26383717305910814; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 0.000230281992116943 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2638581267018227; val_accuracy: 0.9231687898089171 

The current subspace-distance is: 0.00023215542023535818 

plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_1000_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/MLP/2020-01-22 14:34:24/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
