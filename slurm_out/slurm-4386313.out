model : reg_lenet_3
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : False
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-19 22:01:20
nonzero elements in E: 2197
elements in E: 449900
fraction nonzero: 0.00488330740164481
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.16
Batch: 160; loss: 2.32; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.08
Batch: 380; loss: 2.31; acc: 0.08
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.31; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.16
Batch: 600; loss: 2.29; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.3; acc: 0.09
Batch: 700; loss: 2.3; acc: 0.06
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.31; acc: 0.08
Batch: 760; loss: 2.3; acc: 0.06
Batch: 780; loss: 2.28; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.11
Val Epoch over. val_loss: 2.2997442309264167; val_accuracy: 0.09623805732484077 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.31; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.31; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.2
Batch: 300; loss: 2.27; acc: 0.19
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.29; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.28; acc: 0.12
Batch: 440; loss: 2.31; acc: 0.02
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.31; acc: 0.02
Batch: 560; loss: 2.29; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.31; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.09
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.27; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.29; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.28; acc: 0.17
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.11
Val Epoch over. val_loss: 2.2944825788971723; val_accuracy: 0.09623805732484077 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.32; acc: 0.03
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.28; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.08
Batch: 240; loss: 2.29; acc: 0.03
Batch: 260; loss: 2.3; acc: 0.05
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.17
Batch: 340; loss: 2.29; acc: 0.12
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.28; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.08
Batch: 440; loss: 2.29; acc: 0.08
Batch: 460; loss: 2.3; acc: 0.05
Batch: 480; loss: 2.27; acc: 0.16
Batch: 500; loss: 2.29; acc: 0.11
Batch: 520; loss: 2.29; acc: 0.09
Batch: 540; loss: 2.29; acc: 0.06
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.29; acc: 0.03
Batch: 640; loss: 2.27; acc: 0.16
Batch: 660; loss: 2.27; acc: 0.17
Batch: 680; loss: 2.29; acc: 0.16
Batch: 700; loss: 2.3; acc: 0.03
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.29; acc: 0.05
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.2897961580069963; val_accuracy: 0.09623805732484077 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.06
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.27; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.06
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.29; acc: 0.09
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.05
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.03
Batch: 340; loss: 2.29; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.06
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.29; acc: 0.06
Batch: 420; loss: 2.29; acc: 0.12
Batch: 440; loss: 2.28; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.28; acc: 0.09
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.27; acc: 0.17
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.27; acc: 0.11
Batch: 620; loss: 2.29; acc: 0.09
Batch: 640; loss: 2.29; acc: 0.08
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.05
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.3; acc: 0.06
Batch: 760; loss: 2.29; acc: 0.03
Batch: 780; loss: 2.29; acc: 0.05
Train Epoch over. train_loss: 2.29; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.09
Batch: 60; loss: 2.27; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.27; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.28500190510112; val_accuracy: 0.0963375796178344 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.09
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.29; acc: 0.08
Batch: 200; loss: 2.28; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.29; acc: 0.08
Batch: 260; loss: 2.27; acc: 0.12
Batch: 280; loss: 2.27; acc: 0.17
Batch: 300; loss: 2.3; acc: 0.06
Batch: 320; loss: 2.28; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.05
Batch: 380; loss: 2.28; acc: 0.06
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.28; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.08
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.27; acc: 0.08
Batch: 520; loss: 2.27; acc: 0.09
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.29; acc: 0.08
Batch: 580; loss: 2.27; acc: 0.12
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.27; acc: 0.16
Batch: 640; loss: 2.29; acc: 0.05
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.28; acc: 0.08
Batch: 700; loss: 2.28; acc: 0.03
Batch: 720; loss: 2.29; acc: 0.09
Batch: 740; loss: 2.27; acc: 0.08
Batch: 760; loss: 2.25; acc: 0.11
Batch: 780; loss: 2.28; acc: 0.06
Train Epoch over. train_loss: 2.28; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.27; acc: 0.16
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.26; acc: 0.12
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.25; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.11
Val Epoch over. val_loss: 2.277631205358323; val_accuracy: 0.10499601910828026 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.06
Batch: 20; loss: 2.27; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.05
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.03
Batch: 140; loss: 2.28; acc: 0.12
Batch: 160; loss: 2.28; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.26; acc: 0.17
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.12
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.12
Batch: 340; loss: 2.27; acc: 0.12
Batch: 360; loss: 2.31; acc: 0.12
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.08
Batch: 420; loss: 2.27; acc: 0.09
Batch: 440; loss: 2.26; acc: 0.12
Batch: 460; loss: 2.26; acc: 0.11
Batch: 480; loss: 2.27; acc: 0.11
Batch: 500; loss: 2.26; acc: 0.17
Batch: 520; loss: 2.28; acc: 0.08
Batch: 540; loss: 2.29; acc: 0.16
Batch: 560; loss: 2.25; acc: 0.16
Batch: 580; loss: 2.28; acc: 0.12
Batch: 600; loss: 2.27; acc: 0.11
Batch: 620; loss: 2.29; acc: 0.08
Batch: 640; loss: 2.28; acc: 0.08
Batch: 660; loss: 2.3; acc: 0.05
Batch: 680; loss: 2.26; acc: 0.14
Batch: 700; loss: 2.26; acc: 0.19
Batch: 720; loss: 2.27; acc: 0.08
Batch: 740; loss: 2.27; acc: 0.19
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.26; acc: 0.08
Train Epoch over. train_loss: 2.27; train_accuracy: 0.11 

Batch: 0; loss: 2.27; acc: 0.12
Batch: 20; loss: 2.26; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.27; acc: 0.08
Val Epoch over. val_loss: 2.270674579462428; val_accuracy: 0.10917595541401273 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.28; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.31; acc: 0.09
Batch: 120; loss: 2.25; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.26; acc: 0.12
Batch: 200; loss: 2.27; acc: 0.12
Batch: 220; loss: 2.25; acc: 0.11
Batch: 240; loss: 2.27; acc: 0.05
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.08
Batch: 320; loss: 2.25; acc: 0.14
Batch: 340; loss: 2.26; acc: 0.12
Batch: 360; loss: 2.29; acc: 0.14
Batch: 380; loss: 2.25; acc: 0.14
Batch: 400; loss: 2.25; acc: 0.09
Batch: 420; loss: 2.27; acc: 0.08
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.29; acc: 0.03
Batch: 480; loss: 2.26; acc: 0.16
Batch: 500; loss: 2.27; acc: 0.06
Batch: 520; loss: 2.25; acc: 0.14
Batch: 540; loss: 2.26; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.12
Batch: 580; loss: 2.29; acc: 0.03
Batch: 600; loss: 2.28; acc: 0.08
Batch: 620; loss: 2.26; acc: 0.06
Batch: 640; loss: 2.26; acc: 0.09
Batch: 660; loss: 2.27; acc: 0.11
Batch: 680; loss: 2.28; acc: 0.06
Batch: 700; loss: 2.27; acc: 0.09
Batch: 720; loss: 2.26; acc: 0.11
Batch: 740; loss: 2.28; acc: 0.12
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.3; acc: 0.05
Train Epoch over. train_loss: 2.27; train_accuracy: 0.11 

Batch: 0; loss: 2.27; acc: 0.11
Batch: 20; loss: 2.26; acc: 0.09
Batch: 40; loss: 2.26; acc: 0.16
Batch: 60; loss: 2.25; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.14
Batch: 100; loss: 2.26; acc: 0.14
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.27; acc: 0.11
Val Epoch over. val_loss: 2.26467373112964; val_accuracy: 0.10947452229299363 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.02
Batch: 20; loss: 2.25; acc: 0.14
Batch: 40; loss: 2.26; acc: 0.12
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.14
Batch: 100; loss: 2.26; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.26; acc: 0.14
Batch: 160; loss: 2.24; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.26; acc: 0.12
Batch: 220; loss: 2.26; acc: 0.16
Batch: 240; loss: 2.24; acc: 0.17
Batch: 260; loss: 2.28; acc: 0.05
Batch: 280; loss: 2.23; acc: 0.06
Batch: 300; loss: 2.23; acc: 0.16
Batch: 320; loss: 2.27; acc: 0.17
Batch: 340; loss: 2.28; acc: 0.09
Batch: 360; loss: 2.25; acc: 0.11
Batch: 380; loss: 2.26; acc: 0.06
Batch: 400; loss: 2.26; acc: 0.08
Batch: 420; loss: 2.27; acc: 0.08
Batch: 440; loss: 2.27; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.09
Batch: 500; loss: 2.28; acc: 0.08
Batch: 520; loss: 2.28; acc: 0.11
Batch: 540; loss: 2.23; acc: 0.12
Batch: 560; loss: 2.24; acc: 0.22
Batch: 580; loss: 2.25; acc: 0.09
Batch: 600; loss: 2.27; acc: 0.03
Batch: 620; loss: 2.25; acc: 0.06
Batch: 640; loss: 2.25; acc: 0.09
Batch: 660; loss: 2.25; acc: 0.11
Batch: 680; loss: 2.27; acc: 0.09
Batch: 700; loss: 2.28; acc: 0.08
Batch: 720; loss: 2.23; acc: 0.12
Batch: 740; loss: 2.24; acc: 0.11
Batch: 760; loss: 2.24; acc: 0.17
Batch: 780; loss: 2.26; acc: 0.11
Train Epoch over. train_loss: 2.26; train_accuracy: 0.11 

Batch: 0; loss: 2.26; acc: 0.11
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.24; acc: 0.14
Batch: 60; loss: 2.24; acc: 0.19
Batch: 80; loss: 2.26; acc: 0.12
Batch: 100; loss: 2.25; acc: 0.12
Batch: 120; loss: 2.22; acc: 0.19
Batch: 140; loss: 2.26; acc: 0.11
Val Epoch over. val_loss: 2.251047289295561; val_accuracy: 0.10997213375796179 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.05
Batch: 20; loss: 2.22; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.16
Batch: 60; loss: 2.23; acc: 0.08
Batch: 80; loss: 2.25; acc: 0.12
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.23; acc: 0.09
Batch: 140; loss: 2.25; acc: 0.06
Batch: 160; loss: 2.21; acc: 0.14
Batch: 180; loss: 2.25; acc: 0.12
Batch: 200; loss: 2.24; acc: 0.06
Batch: 220; loss: 2.25; acc: 0.08
Batch: 240; loss: 2.23; acc: 0.09
Batch: 260; loss: 2.23; acc: 0.11
Batch: 280; loss: 2.24; acc: 0.12
Batch: 300; loss: 2.22; acc: 0.23
Batch: 320; loss: 2.26; acc: 0.09
Batch: 340; loss: 2.24; acc: 0.14
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.25; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.23; acc: 0.12
Batch: 440; loss: 2.23; acc: 0.11
Batch: 460; loss: 2.22; acc: 0.09
Batch: 480; loss: 2.23; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.06
Batch: 520; loss: 2.28; acc: 0.06
Batch: 540; loss: 2.24; acc: 0.12
Batch: 560; loss: 2.22; acc: 0.17
Batch: 580; loss: 2.19; acc: 0.12
Batch: 600; loss: 2.25; acc: 0.08
Batch: 620; loss: 2.28; acc: 0.03
Batch: 640; loss: 2.22; acc: 0.16
Batch: 660; loss: 2.23; acc: 0.09
Batch: 680; loss: 2.22; acc: 0.09
Batch: 700; loss: 2.24; acc: 0.09
Batch: 720; loss: 2.29; acc: 0.03
Batch: 740; loss: 2.24; acc: 0.08
Batch: 760; loss: 2.24; acc: 0.06
Batch: 780; loss: 2.23; acc: 0.12
Train Epoch over. train_loss: 2.24; train_accuracy: 0.11 

Batch: 0; loss: 2.24; acc: 0.09
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.22; acc: 0.17
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.24; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.08
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.22
Val Epoch over. val_loss: 2.22562669037254; val_accuracy: 0.11415207006369427 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.2; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.09
Batch: 40; loss: 2.24; acc: 0.12
Batch: 60; loss: 2.24; acc: 0.08
Batch: 80; loss: 2.22; acc: 0.09
Batch: 100; loss: 2.24; acc: 0.11
Batch: 120; loss: 2.24; acc: 0.17
Batch: 140; loss: 2.21; acc: 0.12
Batch: 160; loss: 2.21; acc: 0.12
Batch: 180; loss: 2.24; acc: 0.11
Batch: 200; loss: 2.19; acc: 0.08
Batch: 220; loss: 2.2; acc: 0.06
Batch: 240; loss: 2.26; acc: 0.08
Batch: 260; loss: 2.21; acc: 0.09
Batch: 280; loss: 2.24; acc: 0.09
Batch: 300; loss: 2.18; acc: 0.09
Batch: 320; loss: 2.19; acc: 0.12
Batch: 340; loss: 2.19; acc: 0.08
Batch: 360; loss: 2.19; acc: 0.08
Batch: 380; loss: 2.21; acc: 0.06
Batch: 400; loss: 2.21; acc: 0.11
Batch: 420; loss: 2.25; acc: 0.08
Batch: 440; loss: 2.25; acc: 0.06
Batch: 460; loss: 2.2; acc: 0.17
Batch: 480; loss: 2.2; acc: 0.12
Batch: 500; loss: 2.17; acc: 0.08
Batch: 520; loss: 2.25; acc: 0.06
Batch: 540; loss: 2.23; acc: 0.05
Batch: 560; loss: 2.17; acc: 0.19
Batch: 580; loss: 2.23; acc: 0.08
Batch: 600; loss: 2.21; acc: 0.19
Batch: 620; loss: 2.25; acc: 0.17
Batch: 640; loss: 2.25; acc: 0.12
Batch: 660; loss: 2.21; acc: 0.12
Batch: 680; loss: 2.19; acc: 0.16
Batch: 700; loss: 2.22; acc: 0.17
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.21; acc: 0.12
Batch: 760; loss: 2.15; acc: 0.25
Batch: 780; loss: 2.21; acc: 0.09
Train Epoch over. train_loss: 2.21; train_accuracy: 0.12 

Batch: 0; loss: 2.22; acc: 0.11
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.21; acc: 0.09
Batch: 60; loss: 2.2; acc: 0.12
Batch: 80; loss: 2.2; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.08
Batch: 120; loss: 2.2; acc: 0.12
Batch: 140; loss: 2.2; acc: 0.17
Val Epoch over. val_loss: 2.1969202460756727; val_accuracy: 0.13425557324840764 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.22; acc: 0.11
Batch: 20; loss: 2.21; acc: 0.06
Batch: 40; loss: 2.2; acc: 0.09
Batch: 60; loss: 2.18; acc: 0.09
Batch: 80; loss: 2.2; acc: 0.16
Batch: 100; loss: 2.21; acc: 0.08
Batch: 120; loss: 2.27; acc: 0.06
Batch: 140; loss: 2.2; acc: 0.14
Batch: 160; loss: 2.18; acc: 0.12
Batch: 180; loss: 2.13; acc: 0.17
Batch: 200; loss: 2.16; acc: 0.19
Batch: 220; loss: 2.21; acc: 0.11
Batch: 240; loss: 2.12; acc: 0.2
Batch: 260; loss: 2.24; acc: 0.17
Batch: 280; loss: 2.14; acc: 0.19
Batch: 300; loss: 2.18; acc: 0.2
Batch: 320; loss: 2.16; acc: 0.2
Batch: 340; loss: 2.22; acc: 0.14
Batch: 360; loss: 2.15; acc: 0.19
Batch: 380; loss: 2.24; acc: 0.14
Batch: 400; loss: 2.15; acc: 0.23
Batch: 420; loss: 2.16; acc: 0.19
Batch: 440; loss: 2.25; acc: 0.09
Batch: 460; loss: 2.19; acc: 0.16
Batch: 480; loss: 2.18; acc: 0.22
Batch: 500; loss: 2.26; acc: 0.12
Batch: 520; loss: 2.2; acc: 0.17
Batch: 540; loss: 2.25; acc: 0.09
Batch: 560; loss: 2.27; acc: 0.14
Batch: 580; loss: 2.15; acc: 0.19
Batch: 600; loss: 2.24; acc: 0.12
Batch: 620; loss: 2.2; acc: 0.11
Batch: 640; loss: 2.25; acc: 0.09
Batch: 660; loss: 2.27; acc: 0.08
Batch: 680; loss: 2.17; acc: 0.12
Batch: 700; loss: 2.21; acc: 0.11
Batch: 720; loss: 2.23; acc: 0.12
Batch: 740; loss: 2.16; acc: 0.16
Batch: 760; loss: 2.19; acc: 0.14
Batch: 780; loss: 2.18; acc: 0.25
Train Epoch over. train_loss: 2.2; train_accuracy: 0.14 

Batch: 0; loss: 2.21; acc: 0.09
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.21; acc: 0.14
Batch: 60; loss: 2.18; acc: 0.16
Batch: 80; loss: 2.18; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.12
Batch: 120; loss: 2.19; acc: 0.16
Batch: 140; loss: 2.18; acc: 0.16
Val Epoch over. val_loss: 2.186155747456156; val_accuracy: 0.16411226114649682 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.15; acc: 0.2
Batch: 20; loss: 2.17; acc: 0.16
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.17; acc: 0.19
Batch: 100; loss: 2.22; acc: 0.12
Batch: 120; loss: 2.15; acc: 0.17
Batch: 140; loss: 2.18; acc: 0.14
Batch: 160; loss: 2.15; acc: 0.17
Batch: 180; loss: 2.27; acc: 0.08
Batch: 200; loss: 2.19; acc: 0.2
Batch: 220; loss: 2.23; acc: 0.14
Batch: 240; loss: 2.17; acc: 0.19
Batch: 260; loss: 2.17; acc: 0.19
Batch: 280; loss: 2.1; acc: 0.28
Batch: 300; loss: 2.23; acc: 0.11
Batch: 320; loss: 2.16; acc: 0.14
Batch: 340; loss: 2.2; acc: 0.09
Batch: 360; loss: 2.23; acc: 0.14
Batch: 380; loss: 2.2; acc: 0.06
Batch: 400; loss: 2.24; acc: 0.17
Batch: 420; loss: 2.11; acc: 0.3
Batch: 440; loss: 2.19; acc: 0.16
Batch: 460; loss: 2.17; acc: 0.19
Batch: 480; loss: 2.18; acc: 0.17
Batch: 500; loss: 2.17; acc: 0.25
Batch: 520; loss: 2.15; acc: 0.19
Batch: 540; loss: 2.17; acc: 0.19
Batch: 560; loss: 2.18; acc: 0.19
Batch: 580; loss: 2.14; acc: 0.2
Batch: 600; loss: 2.22; acc: 0.14
Batch: 620; loss: 2.19; acc: 0.3
Batch: 640; loss: 2.19; acc: 0.16
Batch: 660; loss: 2.13; acc: 0.28
Batch: 680; loss: 2.14; acc: 0.25
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.19; acc: 0.22
Batch: 740; loss: 2.18; acc: 0.12
Batch: 760; loss: 2.14; acc: 0.22
Batch: 780; loss: 2.19; acc: 0.23
Train Epoch over. train_loss: 2.19; train_accuracy: 0.18 

Batch: 0; loss: 2.19; acc: 0.17
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.21; acc: 0.19
Batch: 60; loss: 2.17; acc: 0.16
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.12
Batch: 120; loss: 2.18; acc: 0.17
Batch: 140; loss: 2.17; acc: 0.23
Val Epoch over. val_loss: 2.1766657753355183; val_accuracy: 0.19317277070063693 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.19; acc: 0.16
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 2.17; acc: 0.23
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.19; acc: 0.2
Batch: 100; loss: 2.23; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.22; acc: 0.12
Batch: 160; loss: 2.14; acc: 0.23
Batch: 180; loss: 2.15; acc: 0.2
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.11; acc: 0.23
Batch: 240; loss: 2.15; acc: 0.22
Batch: 260; loss: 2.18; acc: 0.23
Batch: 280; loss: 2.2; acc: 0.16
Batch: 300; loss: 2.22; acc: 0.17
Batch: 320; loss: 2.25; acc: 0.16
Batch: 340; loss: 2.18; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.19
Batch: 380; loss: 2.18; acc: 0.14
Batch: 400; loss: 2.21; acc: 0.12
Batch: 420; loss: 2.16; acc: 0.2
Batch: 440; loss: 2.19; acc: 0.12
Batch: 460; loss: 2.2; acc: 0.19
Batch: 480; loss: 2.18; acc: 0.16
Batch: 500; loss: 2.18; acc: 0.25
Batch: 520; loss: 2.13; acc: 0.23
Batch: 540; loss: 2.18; acc: 0.22
Batch: 560; loss: 2.15; acc: 0.23
Batch: 580; loss: 2.16; acc: 0.25
Batch: 600; loss: 2.14; acc: 0.28
Batch: 620; loss: 2.21; acc: 0.12
Batch: 640; loss: 2.23; acc: 0.14
Batch: 660; loss: 2.26; acc: 0.09
Batch: 680; loss: 2.15; acc: 0.17
Batch: 700; loss: 2.21; acc: 0.17
Batch: 720; loss: 2.15; acc: 0.25
Batch: 740; loss: 2.22; acc: 0.16
Batch: 760; loss: 2.15; acc: 0.27
Batch: 780; loss: 2.16; acc: 0.19
Train Epoch over. train_loss: 2.18; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 2.2; acc: 0.19
Batch: 60; loss: 2.16; acc: 0.17
Batch: 80; loss: 2.16; acc: 0.22
Batch: 100; loss: 2.22; acc: 0.14
Batch: 120; loss: 2.18; acc: 0.19
Batch: 140; loss: 2.16; acc: 0.27
Val Epoch over. val_loss: 2.172119717689077; val_accuracy: 0.20650875796178345 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.11; acc: 0.23
Batch: 20; loss: 2.14; acc: 0.27
Batch: 40; loss: 2.15; acc: 0.19
Batch: 60; loss: 2.19; acc: 0.19
Batch: 80; loss: 2.14; acc: 0.16
Batch: 100; loss: 2.15; acc: 0.23
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.12; acc: 0.17
Batch: 160; loss: 2.14; acc: 0.23
Batch: 180; loss: 2.2; acc: 0.11
Batch: 200; loss: 2.18; acc: 0.17
Batch: 220; loss: 2.21; acc: 0.14
Batch: 240; loss: 2.17; acc: 0.23
Batch: 260; loss: 2.24; acc: 0.14
Batch: 280; loss: 2.17; acc: 0.19
Batch: 300; loss: 2.2; acc: 0.17
Batch: 320; loss: 2.22; acc: 0.17
Batch: 340; loss: 2.22; acc: 0.16
Batch: 360; loss: 2.16; acc: 0.16
Batch: 380; loss: 2.21; acc: 0.2
Batch: 400; loss: 2.22; acc: 0.17
Batch: 420; loss: 2.14; acc: 0.27
Batch: 440; loss: 2.22; acc: 0.08
Batch: 460; loss: 2.15; acc: 0.23
Batch: 480; loss: 2.21; acc: 0.17
Batch: 500; loss: 2.16; acc: 0.22
Batch: 520; loss: 2.21; acc: 0.22
Batch: 540; loss: 2.13; acc: 0.2
Batch: 560; loss: 2.18; acc: 0.17
Batch: 580; loss: 2.14; acc: 0.31
Batch: 600; loss: 2.14; acc: 0.28
Batch: 620; loss: 2.17; acc: 0.28
Batch: 640; loss: 2.16; acc: 0.19
Batch: 660; loss: 2.21; acc: 0.25
Batch: 680; loss: 2.17; acc: 0.17
Batch: 700; loss: 2.14; acc: 0.23
Batch: 720; loss: 2.16; acc: 0.19
Batch: 740; loss: 2.23; acc: 0.2
Batch: 760; loss: 2.23; acc: 0.2
Batch: 780; loss: 2.29; acc: 0.09
Train Epoch over. train_loss: 2.18; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 2.2; acc: 0.19
Batch: 60; loss: 2.16; acc: 0.22
Batch: 80; loss: 2.15; acc: 0.27
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.18; acc: 0.19
Batch: 140; loss: 2.16; acc: 0.27
Val Epoch over. val_loss: 2.1701667126576614; val_accuracy: 0.21566480891719744 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.25
Batch: 20; loss: 2.18; acc: 0.16
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.18; acc: 0.19
Batch: 80; loss: 2.16; acc: 0.22
Batch: 100; loss: 2.13; acc: 0.22
Batch: 120; loss: 2.17; acc: 0.25
Batch: 140; loss: 2.25; acc: 0.11
Batch: 160; loss: 2.17; acc: 0.23
Batch: 180; loss: 2.14; acc: 0.25
Batch: 200; loss: 2.2; acc: 0.23
Batch: 220; loss: 2.18; acc: 0.2
Batch: 240; loss: 2.09; acc: 0.23
Batch: 260; loss: 2.18; acc: 0.14
Batch: 280; loss: 2.18; acc: 0.19
Batch: 300; loss: 2.12; acc: 0.22
Batch: 320; loss: 2.17; acc: 0.19
Batch: 340; loss: 2.25; acc: 0.2
Batch: 360; loss: 2.12; acc: 0.23
Batch: 380; loss: 2.15; acc: 0.25
Batch: 400; loss: 2.11; acc: 0.33
Batch: 420; loss: 2.14; acc: 0.23
Batch: 440; loss: 2.18; acc: 0.19
Batch: 460; loss: 2.24; acc: 0.17
Batch: 480; loss: 2.15; acc: 0.23
Batch: 500; loss: 2.17; acc: 0.2
Batch: 520; loss: 2.2; acc: 0.22
Batch: 540; loss: 2.08; acc: 0.27
Batch: 560; loss: 2.19; acc: 0.25
Batch: 580; loss: 2.13; acc: 0.19
Batch: 600; loss: 2.17; acc: 0.25
Batch: 620; loss: 2.06; acc: 0.25
Batch: 640; loss: 2.24; acc: 0.19
Batch: 660; loss: 2.2; acc: 0.17
Batch: 680; loss: 2.25; acc: 0.14
Batch: 700; loss: 2.19; acc: 0.2
Batch: 720; loss: 2.22; acc: 0.23
Batch: 740; loss: 2.2; acc: 0.28
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.13; acc: 0.31
Train Epoch over. train_loss: 2.18; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.24; acc: 0.16
Batch: 40; loss: 2.2; acc: 0.19
Batch: 60; loss: 2.15; acc: 0.23
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 2.22; acc: 0.2
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.28
Val Epoch over. val_loss: 2.169234410972352; val_accuracy: 0.2216361464968153 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.19; acc: 0.25
Batch: 20; loss: 2.1; acc: 0.25
Batch: 40; loss: 2.14; acc: 0.25
Batch: 60; loss: 2.2; acc: 0.22
Batch: 80; loss: 2.2; acc: 0.23
Batch: 100; loss: 2.2; acc: 0.12
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.17
Batch: 160; loss: 2.14; acc: 0.27
Batch: 180; loss: 2.09; acc: 0.31
Batch: 200; loss: 2.08; acc: 0.27
Batch: 220; loss: 2.21; acc: 0.2
Batch: 240; loss: 2.19; acc: 0.22
Batch: 260; loss: 2.18; acc: 0.25
Batch: 280; loss: 2.2; acc: 0.23
Batch: 300; loss: 2.17; acc: 0.23
Batch: 320; loss: 2.19; acc: 0.22
Batch: 340; loss: 2.18; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.12
Batch: 380; loss: 2.14; acc: 0.25
Batch: 400; loss: 2.14; acc: 0.2
Batch: 420; loss: 2.21; acc: 0.14
Batch: 440; loss: 2.15; acc: 0.25
Batch: 460; loss: 2.23; acc: 0.17
Batch: 480; loss: 2.21; acc: 0.14
Batch: 500; loss: 2.17; acc: 0.16
Batch: 520; loss: 2.21; acc: 0.16
Batch: 540; loss: 2.16; acc: 0.22
Batch: 560; loss: 2.06; acc: 0.36
Batch: 580; loss: 2.17; acc: 0.25
Batch: 600; loss: 2.22; acc: 0.12
Batch: 620; loss: 2.21; acc: 0.23
Batch: 640; loss: 2.11; acc: 0.3
Batch: 660; loss: 2.12; acc: 0.23
Batch: 680; loss: 2.22; acc: 0.19
Batch: 700; loss: 2.15; acc: 0.25
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.15; acc: 0.25
Batch: 760; loss: 2.22; acc: 0.17
Batch: 780; loss: 2.14; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.22 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.24; acc: 0.16
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.15; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.27
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.3
Val Epoch over. val_loss: 2.1687024474903276; val_accuracy: 0.2239251592356688 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.25; acc: 0.23
Batch: 20; loss: 2.17; acc: 0.12
Batch: 40; loss: 2.18; acc: 0.19
Batch: 60; loss: 2.14; acc: 0.28
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.14; acc: 0.27
Batch: 120; loss: 2.13; acc: 0.23
Batch: 140; loss: 2.13; acc: 0.25
Batch: 160; loss: 2.23; acc: 0.19
Batch: 180; loss: 2.19; acc: 0.22
Batch: 200; loss: 2.21; acc: 0.19
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 2.16; acc: 0.22
Batch: 260; loss: 2.2; acc: 0.25
Batch: 280; loss: 2.21; acc: 0.3
Batch: 300; loss: 2.16; acc: 0.28
Batch: 320; loss: 2.18; acc: 0.14
Batch: 340; loss: 2.24; acc: 0.19
Batch: 360; loss: 2.13; acc: 0.28
Batch: 380; loss: 2.21; acc: 0.12
Batch: 400; loss: 2.14; acc: 0.3
Batch: 420; loss: 2.23; acc: 0.17
Batch: 440; loss: 2.16; acc: 0.2
Batch: 460; loss: 2.09; acc: 0.3
Batch: 480; loss: 2.15; acc: 0.19
Batch: 500; loss: 2.13; acc: 0.27
Batch: 520; loss: 2.27; acc: 0.11
Batch: 540; loss: 2.21; acc: 0.23
Batch: 560; loss: 2.12; acc: 0.31
Batch: 580; loss: 2.2; acc: 0.25
Batch: 600; loss: 2.18; acc: 0.22
Batch: 620; loss: 2.13; acc: 0.22
Batch: 640; loss: 2.16; acc: 0.19
Batch: 660; loss: 2.11; acc: 0.31
Batch: 680; loss: 2.2; acc: 0.19
Batch: 700; loss: 2.06; acc: 0.27
Batch: 720; loss: 2.12; acc: 0.27
Batch: 740; loss: 2.21; acc: 0.14
Batch: 760; loss: 2.15; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.22 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.24; acc: 0.16
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.15; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.27
Batch: 100; loss: 2.22; acc: 0.2
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.17; acc: 0.31
Val Epoch over. val_loss: 2.1684111060610243; val_accuracy: 0.22800557324840764 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.2; acc: 0.25
Batch: 40; loss: 2.19; acc: 0.27
Batch: 60; loss: 2.2; acc: 0.23
Batch: 80; loss: 2.2; acc: 0.19
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.17; acc: 0.23
Batch: 140; loss: 2.12; acc: 0.27
Batch: 160; loss: 2.15; acc: 0.19
Batch: 180; loss: 2.17; acc: 0.23
Batch: 200; loss: 2.12; acc: 0.28
Batch: 220; loss: 2.13; acc: 0.28
Batch: 240; loss: 2.14; acc: 0.2
Batch: 260; loss: 2.14; acc: 0.31
Batch: 280; loss: 2.22; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.22
Batch: 320; loss: 2.24; acc: 0.19
Batch: 340; loss: 2.18; acc: 0.19
Batch: 360; loss: 2.16; acc: 0.28
Batch: 380; loss: 2.16; acc: 0.27
Batch: 400; loss: 2.21; acc: 0.2
Batch: 420; loss: 2.21; acc: 0.19
Batch: 440; loss: 2.12; acc: 0.19
Batch: 460; loss: 2.21; acc: 0.2
Batch: 480; loss: 2.16; acc: 0.23
Batch: 500; loss: 2.15; acc: 0.2
Batch: 520; loss: 2.23; acc: 0.14
Batch: 540; loss: 2.2; acc: 0.27
Batch: 560; loss: 2.23; acc: 0.23
Batch: 580; loss: 2.2; acc: 0.2
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.17; acc: 0.22
Batch: 640; loss: 2.18; acc: 0.27
Batch: 660; loss: 2.16; acc: 0.23
Batch: 680; loss: 2.15; acc: 0.33
Batch: 700; loss: 2.17; acc: 0.23
Batch: 720; loss: 2.19; acc: 0.19
Batch: 740; loss: 2.24; acc: 0.16
Batch: 760; loss: 2.13; acc: 0.3
Batch: 780; loss: 2.18; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.22 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.15; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.2
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.17; acc: 0.31
Val Epoch over. val_loss: 2.1680777452553914; val_accuracy: 0.2310907643312102 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.17; acc: 0.25
Batch: 20; loss: 2.13; acc: 0.23
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.19; acc: 0.2
Batch: 80; loss: 2.15; acc: 0.31
Batch: 100; loss: 2.14; acc: 0.23
Batch: 120; loss: 2.12; acc: 0.23
Batch: 140; loss: 2.19; acc: 0.27
Batch: 160; loss: 2.18; acc: 0.23
Batch: 180; loss: 2.23; acc: 0.14
Batch: 200; loss: 2.15; acc: 0.23
Batch: 220; loss: 2.17; acc: 0.17
Batch: 240; loss: 2.14; acc: 0.25
Batch: 260; loss: 2.14; acc: 0.22
Batch: 280; loss: 2.17; acc: 0.16
Batch: 300; loss: 2.18; acc: 0.14
Batch: 320; loss: 2.14; acc: 0.3
Batch: 340; loss: 2.14; acc: 0.28
Batch: 360; loss: 2.08; acc: 0.25
Batch: 380; loss: 2.07; acc: 0.38
Batch: 400; loss: 2.22; acc: 0.14
Batch: 420; loss: 2.16; acc: 0.16
Batch: 440; loss: 2.22; acc: 0.22
Batch: 460; loss: 2.17; acc: 0.3
Batch: 480; loss: 2.21; acc: 0.16
Batch: 500; loss: 2.13; acc: 0.3
Batch: 520; loss: 2.22; acc: 0.23
Batch: 540; loss: 2.14; acc: 0.23
Batch: 560; loss: 2.19; acc: 0.27
Batch: 580; loss: 2.15; acc: 0.23
Batch: 600; loss: 2.13; acc: 0.25
Batch: 620; loss: 2.24; acc: 0.14
Batch: 640; loss: 2.19; acc: 0.3
Batch: 660; loss: 2.2; acc: 0.22
Batch: 680; loss: 2.21; acc: 0.16
Batch: 700; loss: 2.16; acc: 0.25
Batch: 720; loss: 2.17; acc: 0.22
Batch: 740; loss: 2.17; acc: 0.33
Batch: 760; loss: 2.18; acc: 0.25
Batch: 780; loss: 2.17; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.22 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.15; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.27
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.168002770964507; val_accuracy: 0.23158837579617833 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.17; acc: 0.23
Batch: 60; loss: 2.16; acc: 0.25
Batch: 80; loss: 2.13; acc: 0.3
Batch: 100; loss: 2.15; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.27
Batch: 140; loss: 2.15; acc: 0.23
Batch: 160; loss: 2.12; acc: 0.23
Batch: 180; loss: 2.17; acc: 0.22
Batch: 200; loss: 2.13; acc: 0.25
Batch: 220; loss: 2.19; acc: 0.16
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.16; acc: 0.17
Batch: 280; loss: 2.15; acc: 0.3
Batch: 300; loss: 2.22; acc: 0.14
Batch: 320; loss: 2.13; acc: 0.3
Batch: 340; loss: 2.18; acc: 0.19
Batch: 360; loss: 2.17; acc: 0.27
Batch: 380; loss: 2.14; acc: 0.19
Batch: 400; loss: 2.09; acc: 0.3
Batch: 420; loss: 2.16; acc: 0.27
Batch: 440; loss: 2.15; acc: 0.27
Batch: 460; loss: 2.12; acc: 0.31
Batch: 480; loss: 2.22; acc: 0.16
Batch: 500; loss: 2.16; acc: 0.28
Batch: 520; loss: 2.24; acc: 0.22
Batch: 540; loss: 2.19; acc: 0.23
Batch: 560; loss: 2.18; acc: 0.23
Batch: 580; loss: 2.12; acc: 0.19
Batch: 600; loss: 2.1; acc: 0.27
Batch: 620; loss: 2.1; acc: 0.28
Batch: 640; loss: 2.2; acc: 0.22
Batch: 660; loss: 2.16; acc: 0.17
Batch: 680; loss: 2.11; acc: 0.31
Batch: 700; loss: 2.13; acc: 0.27
Batch: 720; loss: 2.26; acc: 0.22
Batch: 740; loss: 2.12; acc: 0.27
Batch: 760; loss: 2.1; acc: 0.28
Batch: 780; loss: 2.14; acc: 0.22
Train Epoch over. train_loss: 2.17; train_accuracy: 0.22 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.15; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.17; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1677909930040884; val_accuracy: 0.2326831210191083 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.22
Batch: 20; loss: 2.19; acc: 0.22
Batch: 40; loss: 2.17; acc: 0.23
Batch: 60; loss: 2.21; acc: 0.28
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.19; acc: 0.25
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.15; acc: 0.31
Batch: 160; loss: 2.24; acc: 0.22
Batch: 180; loss: 2.13; acc: 0.3
Batch: 200; loss: 2.14; acc: 0.3
Batch: 220; loss: 2.18; acc: 0.25
Batch: 240; loss: 2.24; acc: 0.2
Batch: 260; loss: 2.17; acc: 0.25
Batch: 280; loss: 2.14; acc: 0.17
Batch: 300; loss: 2.2; acc: 0.25
Batch: 320; loss: 2.17; acc: 0.19
Batch: 340; loss: 2.18; acc: 0.28
Batch: 360; loss: 2.23; acc: 0.2
Batch: 380; loss: 2.14; acc: 0.17
Batch: 400; loss: 2.14; acc: 0.28
Batch: 420; loss: 2.14; acc: 0.3
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.13; acc: 0.38
Batch: 480; loss: 2.21; acc: 0.19
Batch: 500; loss: 2.14; acc: 0.22
Batch: 520; loss: 2.15; acc: 0.25
Batch: 540; loss: 2.19; acc: 0.25
Batch: 560; loss: 2.22; acc: 0.2
Batch: 580; loss: 2.19; acc: 0.16
Batch: 600; loss: 2.2; acc: 0.14
Batch: 620; loss: 2.1; acc: 0.28
Batch: 640; loss: 2.15; acc: 0.3
Batch: 660; loss: 2.15; acc: 0.17
Batch: 680; loss: 2.13; acc: 0.25
Batch: 700; loss: 2.2; acc: 0.23
Batch: 720; loss: 2.16; acc: 0.22
Batch: 740; loss: 2.21; acc: 0.22
Batch: 760; loss: 2.2; acc: 0.27
Batch: 780; loss: 2.2; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.15; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.17; acc: 0.31
Val Epoch over. val_loss: 2.167700154006861; val_accuracy: 0.23377786624203822 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.18; acc: 0.23
Batch: 20; loss: 2.16; acc: 0.19
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.2; acc: 0.2
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.16; acc: 0.17
Batch: 120; loss: 2.24; acc: 0.23
Batch: 140; loss: 2.14; acc: 0.22
Batch: 160; loss: 2.13; acc: 0.31
Batch: 180; loss: 2.25; acc: 0.19
Batch: 200; loss: 2.24; acc: 0.19
Batch: 220; loss: 2.08; acc: 0.31
Batch: 240; loss: 2.19; acc: 0.27
Batch: 260; loss: 2.18; acc: 0.23
Batch: 280; loss: 2.21; acc: 0.11
Batch: 300; loss: 2.1; acc: 0.36
Batch: 320; loss: 2.18; acc: 0.16
Batch: 340; loss: 2.17; acc: 0.28
Batch: 360; loss: 2.27; acc: 0.2
Batch: 380; loss: 2.18; acc: 0.25
Batch: 400; loss: 2.15; acc: 0.28
Batch: 420; loss: 2.25; acc: 0.16
Batch: 440; loss: 2.25; acc: 0.17
Batch: 460; loss: 2.18; acc: 0.16
Batch: 480; loss: 2.17; acc: 0.25
Batch: 500; loss: 2.16; acc: 0.25
Batch: 520; loss: 2.13; acc: 0.3
Batch: 540; loss: 2.11; acc: 0.3
Batch: 560; loss: 2.21; acc: 0.22
Batch: 580; loss: 2.22; acc: 0.16
Batch: 600; loss: 2.2; acc: 0.22
Batch: 620; loss: 2.11; acc: 0.23
Batch: 640; loss: 2.18; acc: 0.19
Batch: 660; loss: 2.23; acc: 0.2
Batch: 680; loss: 2.18; acc: 0.27
Batch: 700; loss: 2.19; acc: 0.2
Batch: 720; loss: 2.15; acc: 0.28
Batch: 740; loss: 2.17; acc: 0.19
Batch: 760; loss: 2.11; acc: 0.3
Batch: 780; loss: 2.15; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.15; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.17; acc: 0.31
Val Epoch over. val_loss: 2.1676389518057464; val_accuracy: 0.23397691082802546 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.14
Batch: 20; loss: 2.19; acc: 0.22
Batch: 40; loss: 2.17; acc: 0.22
Batch: 60; loss: 2.14; acc: 0.19
Batch: 80; loss: 2.15; acc: 0.23
Batch: 100; loss: 2.18; acc: 0.28
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.19
Batch: 160; loss: 2.17; acc: 0.2
Batch: 180; loss: 2.21; acc: 0.25
Batch: 200; loss: 2.21; acc: 0.16
Batch: 220; loss: 2.18; acc: 0.2
Batch: 240; loss: 2.02; acc: 0.33
Batch: 260; loss: 2.17; acc: 0.14
Batch: 280; loss: 2.12; acc: 0.27
Batch: 300; loss: 2.18; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.19
Batch: 340; loss: 2.22; acc: 0.23
Batch: 360; loss: 2.19; acc: 0.19
Batch: 380; loss: 2.16; acc: 0.2
Batch: 400; loss: 2.16; acc: 0.27
Batch: 420; loss: 2.17; acc: 0.23
Batch: 440; loss: 2.18; acc: 0.25
Batch: 460; loss: 2.13; acc: 0.25
Batch: 480; loss: 2.15; acc: 0.22
Batch: 500; loss: 2.12; acc: 0.22
Batch: 520; loss: 2.15; acc: 0.27
Batch: 540; loss: 2.09; acc: 0.23
Batch: 560; loss: 2.13; acc: 0.2
Batch: 580; loss: 2.12; acc: 0.25
Batch: 600; loss: 2.13; acc: 0.33
Batch: 620; loss: 2.19; acc: 0.19
Batch: 640; loss: 2.07; acc: 0.33
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.18; acc: 0.22
Batch: 700; loss: 2.15; acc: 0.33
Batch: 720; loss: 2.16; acc: 0.22
Batch: 740; loss: 2.16; acc: 0.23
Batch: 760; loss: 2.18; acc: 0.19
Batch: 780; loss: 2.23; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.22
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.17; acc: 0.31
Val Epoch over. val_loss: 2.167560389087458; val_accuracy: 0.2359673566878981 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.14; acc: 0.25
Batch: 40; loss: 2.17; acc: 0.2
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.23; acc: 0.17
Batch: 100; loss: 2.23; acc: 0.19
Batch: 120; loss: 2.06; acc: 0.33
Batch: 140; loss: 2.13; acc: 0.25
Batch: 160; loss: 2.22; acc: 0.19
Batch: 180; loss: 2.1; acc: 0.25
Batch: 200; loss: 2.16; acc: 0.25
Batch: 220; loss: 2.11; acc: 0.33
Batch: 240; loss: 2.19; acc: 0.28
Batch: 260; loss: 2.17; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.25
Batch: 300; loss: 2.22; acc: 0.19
Batch: 320; loss: 2.18; acc: 0.28
Batch: 340; loss: 2.19; acc: 0.2
Batch: 360; loss: 2.19; acc: 0.22
Batch: 380; loss: 2.26; acc: 0.2
Batch: 400; loss: 2.18; acc: 0.2
Batch: 420; loss: 2.16; acc: 0.22
Batch: 440; loss: 2.15; acc: 0.25
Batch: 460; loss: 2.08; acc: 0.33
Batch: 480; loss: 2.15; acc: 0.27
Batch: 500; loss: 2.19; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.25
Batch: 540; loss: 2.21; acc: 0.19
Batch: 560; loss: 2.18; acc: 0.19
Batch: 580; loss: 2.16; acc: 0.22
Batch: 600; loss: 2.22; acc: 0.19
Batch: 620; loss: 2.23; acc: 0.19
Batch: 640; loss: 2.15; acc: 0.23
Batch: 660; loss: 2.25; acc: 0.14
Batch: 680; loss: 2.26; acc: 0.12
Batch: 700; loss: 2.17; acc: 0.19
Batch: 720; loss: 2.26; acc: 0.19
Batch: 740; loss: 2.24; acc: 0.22
Batch: 760; loss: 2.13; acc: 0.27
Batch: 780; loss: 2.21; acc: 0.22
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1674915362315574; val_accuracy: 0.23507165605095542 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.18; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.16
Batch: 60; loss: 2.15; acc: 0.23
Batch: 80; loss: 2.22; acc: 0.16
Batch: 100; loss: 2.16; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.2
Batch: 160; loss: 2.18; acc: 0.2
Batch: 180; loss: 2.18; acc: 0.2
Batch: 200; loss: 2.17; acc: 0.22
Batch: 220; loss: 2.27; acc: 0.22
Batch: 240; loss: 2.19; acc: 0.16
Batch: 260; loss: 2.17; acc: 0.22
Batch: 280; loss: 2.17; acc: 0.28
Batch: 300; loss: 2.06; acc: 0.3
Batch: 320; loss: 2.21; acc: 0.19
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.17
Batch: 380; loss: 2.19; acc: 0.28
Batch: 400; loss: 2.11; acc: 0.23
Batch: 420; loss: 2.1; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.28
Batch: 460; loss: 2.14; acc: 0.25
Batch: 480; loss: 2.22; acc: 0.2
Batch: 500; loss: 2.12; acc: 0.3
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.17; acc: 0.23
Batch: 560; loss: 2.15; acc: 0.22
Batch: 580; loss: 2.16; acc: 0.25
Batch: 600; loss: 2.13; acc: 0.25
Batch: 620; loss: 2.14; acc: 0.34
Batch: 640; loss: 2.21; acc: 0.19
Batch: 660; loss: 2.17; acc: 0.2
Batch: 680; loss: 2.2; acc: 0.23
Batch: 700; loss: 2.14; acc: 0.22
Batch: 720; loss: 2.17; acc: 0.19
Batch: 740; loss: 2.2; acc: 0.22
Batch: 760; loss: 2.15; acc: 0.25
Batch: 780; loss: 2.08; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.28
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1674387454986572; val_accuracy: 0.2360668789808917 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.2; acc: 0.27
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.19; acc: 0.2
Batch: 60; loss: 2.13; acc: 0.22
Batch: 80; loss: 2.2; acc: 0.22
Batch: 100; loss: 2.13; acc: 0.3
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.1; acc: 0.33
Batch: 160; loss: 2.18; acc: 0.23
Batch: 180; loss: 2.2; acc: 0.25
Batch: 200; loss: 2.2; acc: 0.2
Batch: 220; loss: 2.24; acc: 0.25
Batch: 240; loss: 2.12; acc: 0.31
Batch: 260; loss: 2.2; acc: 0.27
Batch: 280; loss: 2.15; acc: 0.3
Batch: 300; loss: 2.22; acc: 0.25
Batch: 320; loss: 2.17; acc: 0.22
Batch: 340; loss: 2.21; acc: 0.17
Batch: 360; loss: 2.18; acc: 0.2
Batch: 380; loss: 2.16; acc: 0.3
Batch: 400; loss: 2.21; acc: 0.2
Batch: 420; loss: 2.19; acc: 0.23
Batch: 440; loss: 2.13; acc: 0.27
Batch: 460; loss: 2.24; acc: 0.22
Batch: 480; loss: 2.08; acc: 0.38
Batch: 500; loss: 2.25; acc: 0.22
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.16; acc: 0.22
Batch: 560; loss: 2.12; acc: 0.3
Batch: 580; loss: 2.25; acc: 0.16
Batch: 600; loss: 2.27; acc: 0.19
Batch: 620; loss: 2.17; acc: 0.25
Batch: 640; loss: 2.17; acc: 0.27
Batch: 660; loss: 2.11; acc: 0.2
Batch: 680; loss: 2.21; acc: 0.22
Batch: 700; loss: 2.16; acc: 0.19
Batch: 720; loss: 2.21; acc: 0.23
Batch: 740; loss: 2.14; acc: 0.31
Batch: 760; loss: 2.14; acc: 0.23
Batch: 780; loss: 2.12; acc: 0.31
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1673658926775503; val_accuracy: 0.23477308917197454 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.19; acc: 0.19
Batch: 20; loss: 2.21; acc: 0.3
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.17; acc: 0.23
Batch: 80; loss: 2.21; acc: 0.2
Batch: 100; loss: 2.24; acc: 0.11
Batch: 120; loss: 2.11; acc: 0.23
Batch: 140; loss: 2.2; acc: 0.22
Batch: 160; loss: 2.22; acc: 0.2
Batch: 180; loss: 2.12; acc: 0.22
Batch: 200; loss: 2.1; acc: 0.31
Batch: 220; loss: 2.09; acc: 0.27
Batch: 240; loss: 2.15; acc: 0.27
Batch: 260; loss: 2.23; acc: 0.27
Batch: 280; loss: 2.16; acc: 0.17
Batch: 300; loss: 2.14; acc: 0.19
Batch: 320; loss: 2.14; acc: 0.22
Batch: 340; loss: 2.17; acc: 0.19
Batch: 360; loss: 2.19; acc: 0.25
Batch: 380; loss: 2.16; acc: 0.27
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.22; acc: 0.16
Batch: 440; loss: 2.11; acc: 0.28
Batch: 460; loss: 2.25; acc: 0.2
Batch: 480; loss: 2.11; acc: 0.31
Batch: 500; loss: 2.14; acc: 0.28
Batch: 520; loss: 2.18; acc: 0.28
Batch: 540; loss: 2.15; acc: 0.28
Batch: 560; loss: 2.2; acc: 0.19
Batch: 580; loss: 2.11; acc: 0.3
Batch: 600; loss: 2.11; acc: 0.28
Batch: 620; loss: 2.18; acc: 0.14
Batch: 640; loss: 2.15; acc: 0.31
Batch: 660; loss: 2.16; acc: 0.19
Batch: 680; loss: 2.22; acc: 0.19
Batch: 700; loss: 2.14; acc: 0.25
Batch: 720; loss: 2.12; acc: 0.23
Batch: 740; loss: 2.15; acc: 0.2
Batch: 760; loss: 2.14; acc: 0.11
Batch: 780; loss: 2.23; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.16733038805093; val_accuracy: 0.23676353503184713 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.23
Batch: 40; loss: 2.25; acc: 0.17
Batch: 60; loss: 2.13; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.23
Batch: 100; loss: 2.16; acc: 0.3
Batch: 120; loss: 2.19; acc: 0.23
Batch: 140; loss: 2.18; acc: 0.2
Batch: 160; loss: 2.2; acc: 0.22
Batch: 180; loss: 2.15; acc: 0.28
Batch: 200; loss: 2.1; acc: 0.25
Batch: 220; loss: 2.18; acc: 0.16
Batch: 240; loss: 2.19; acc: 0.17
Batch: 260; loss: 2.13; acc: 0.28
Batch: 280; loss: 2.25; acc: 0.19
Batch: 300; loss: 2.16; acc: 0.17
Batch: 320; loss: 2.11; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.22
Batch: 360; loss: 2.16; acc: 0.28
Batch: 380; loss: 2.1; acc: 0.27
Batch: 400; loss: 2.18; acc: 0.25
Batch: 420; loss: 2.21; acc: 0.19
Batch: 440; loss: 2.22; acc: 0.17
Batch: 460; loss: 2.2; acc: 0.2
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.13; acc: 0.33
Batch: 520; loss: 2.17; acc: 0.2
Batch: 540; loss: 2.23; acc: 0.22
Batch: 560; loss: 2.19; acc: 0.16
Batch: 580; loss: 2.2; acc: 0.2
Batch: 600; loss: 2.22; acc: 0.2
Batch: 620; loss: 2.25; acc: 0.14
Batch: 640; loss: 2.16; acc: 0.23
Batch: 660; loss: 2.22; acc: 0.27
Batch: 680; loss: 2.14; acc: 0.27
Batch: 700; loss: 2.2; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.2
Batch: 740; loss: 2.11; acc: 0.22
Batch: 760; loss: 2.22; acc: 0.2
Batch: 780; loss: 2.15; acc: 0.28
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1672682944376755; val_accuracy: 0.23636544585987262 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.24; acc: 0.14
Batch: 20; loss: 2.13; acc: 0.25
Batch: 40; loss: 2.14; acc: 0.25
Batch: 60; loss: 2.21; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.25
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.11; acc: 0.25
Batch: 140; loss: 2.16; acc: 0.23
Batch: 160; loss: 2.15; acc: 0.19
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.15; acc: 0.27
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.21; acc: 0.25
Batch: 260; loss: 2.2; acc: 0.16
Batch: 280; loss: 2.15; acc: 0.22
Batch: 300; loss: 2.17; acc: 0.22
Batch: 320; loss: 2.14; acc: 0.25
Batch: 340; loss: 2.19; acc: 0.17
Batch: 360; loss: 2.19; acc: 0.23
Batch: 380; loss: 2.13; acc: 0.25
Batch: 400; loss: 2.15; acc: 0.22
Batch: 420; loss: 2.22; acc: 0.16
Batch: 440; loss: 2.22; acc: 0.23
Batch: 460; loss: 2.21; acc: 0.2
Batch: 480; loss: 2.1; acc: 0.3
Batch: 500; loss: 2.21; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.19
Batch: 540; loss: 2.18; acc: 0.25
Batch: 560; loss: 2.21; acc: 0.22
Batch: 580; loss: 2.08; acc: 0.31
Batch: 600; loss: 2.22; acc: 0.17
Batch: 620; loss: 2.25; acc: 0.17
Batch: 640; loss: 2.16; acc: 0.23
Batch: 660; loss: 2.27; acc: 0.16
Batch: 680; loss: 2.24; acc: 0.17
Batch: 700; loss: 2.26; acc: 0.11
Batch: 720; loss: 2.19; acc: 0.23
Batch: 740; loss: 2.25; acc: 0.11
Batch: 760; loss: 2.18; acc: 0.17
Batch: 780; loss: 2.16; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.167214158234323; val_accuracy: 0.23646496815286625 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.2; acc: 0.17
Batch: 20; loss: 2.15; acc: 0.25
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.18; acc: 0.16
Batch: 80; loss: 2.16; acc: 0.23
Batch: 100; loss: 2.29; acc: 0.12
Batch: 120; loss: 2.18; acc: 0.19
Batch: 140; loss: 2.16; acc: 0.3
Batch: 160; loss: 2.26; acc: 0.14
Batch: 180; loss: 2.07; acc: 0.36
Batch: 200; loss: 2.27; acc: 0.12
Batch: 220; loss: 2.21; acc: 0.17
Batch: 240; loss: 2.13; acc: 0.28
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.14; acc: 0.22
Batch: 300; loss: 2.16; acc: 0.25
Batch: 320; loss: 2.19; acc: 0.28
Batch: 340; loss: 2.18; acc: 0.2
Batch: 360; loss: 2.12; acc: 0.23
Batch: 380; loss: 2.18; acc: 0.23
Batch: 400; loss: 2.16; acc: 0.19
Batch: 420; loss: 2.1; acc: 0.36
Batch: 440; loss: 2.19; acc: 0.17
Batch: 460; loss: 2.25; acc: 0.19
Batch: 480; loss: 2.19; acc: 0.31
Batch: 500; loss: 2.16; acc: 0.22
Batch: 520; loss: 2.13; acc: 0.27
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.24; acc: 0.17
Batch: 580; loss: 2.14; acc: 0.3
Batch: 600; loss: 2.22; acc: 0.22
Batch: 620; loss: 2.19; acc: 0.23
Batch: 640; loss: 2.23; acc: 0.22
Batch: 660; loss: 2.19; acc: 0.22
Batch: 680; loss: 2.18; acc: 0.28
Batch: 700; loss: 2.17; acc: 0.25
Batch: 720; loss: 2.24; acc: 0.22
Batch: 740; loss: 2.14; acc: 0.25
Batch: 760; loss: 2.13; acc: 0.27
Batch: 780; loss: 2.21; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.167176852560347; val_accuracy: 0.23686305732484075 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.2; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.28
Batch: 40; loss: 2.14; acc: 0.28
Batch: 60; loss: 2.13; acc: 0.25
Batch: 80; loss: 2.2; acc: 0.22
Batch: 100; loss: 2.19; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.22; acc: 0.2
Batch: 160; loss: 2.19; acc: 0.22
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.11; acc: 0.25
Batch: 220; loss: 2.15; acc: 0.19
Batch: 240; loss: 2.15; acc: 0.19
Batch: 260; loss: 2.15; acc: 0.17
Batch: 280; loss: 2.08; acc: 0.3
Batch: 300; loss: 2.27; acc: 0.16
Batch: 320; loss: 2.14; acc: 0.34
Batch: 340; loss: 2.18; acc: 0.17
Batch: 360; loss: 2.17; acc: 0.19
Batch: 380; loss: 2.22; acc: 0.2
Batch: 400; loss: 2.14; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.17
Batch: 440; loss: 2.18; acc: 0.27
Batch: 460; loss: 2.24; acc: 0.23
Batch: 480; loss: 2.13; acc: 0.27
Batch: 500; loss: 2.18; acc: 0.19
Batch: 520; loss: 2.28; acc: 0.16
Batch: 540; loss: 2.23; acc: 0.16
Batch: 560; loss: 2.17; acc: 0.27
Batch: 580; loss: 2.21; acc: 0.25
Batch: 600; loss: 2.17; acc: 0.28
Batch: 620; loss: 2.06; acc: 0.34
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 2.22; acc: 0.28
Batch: 680; loss: 2.25; acc: 0.25
Batch: 700; loss: 2.16; acc: 0.25
Batch: 720; loss: 2.21; acc: 0.27
Batch: 740; loss: 2.1; acc: 0.23
Batch: 760; loss: 2.24; acc: 0.22
Batch: 780; loss: 2.15; acc: 0.31
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.167152533865279; val_accuracy: 0.2369625796178344 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.11; acc: 0.31
Batch: 40; loss: 2.23; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.17
Batch: 80; loss: 2.18; acc: 0.22
Batch: 100; loss: 2.18; acc: 0.22
Batch: 120; loss: 2.27; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.17
Batch: 160; loss: 2.26; acc: 0.17
Batch: 180; loss: 2.13; acc: 0.22
Batch: 200; loss: 2.17; acc: 0.17
Batch: 220; loss: 2.22; acc: 0.2
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.24; acc: 0.27
Batch: 280; loss: 2.19; acc: 0.2
Batch: 300; loss: 2.17; acc: 0.23
Batch: 320; loss: 2.18; acc: 0.2
Batch: 340; loss: 2.26; acc: 0.17
Batch: 360; loss: 2.2; acc: 0.27
Batch: 380; loss: 2.14; acc: 0.2
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.22; acc: 0.19
Batch: 440; loss: 2.12; acc: 0.39
Batch: 460; loss: 2.13; acc: 0.25
Batch: 480; loss: 2.23; acc: 0.16
Batch: 500; loss: 2.13; acc: 0.27
Batch: 520; loss: 2.14; acc: 0.23
Batch: 540; loss: 2.2; acc: 0.22
Batch: 560; loss: 2.2; acc: 0.19
Batch: 580; loss: 2.1; acc: 0.27
Batch: 600; loss: 2.17; acc: 0.3
Batch: 620; loss: 2.28; acc: 0.14
Batch: 640; loss: 2.18; acc: 0.22
Batch: 660; loss: 2.14; acc: 0.3
Batch: 680; loss: 2.16; acc: 0.3
Batch: 700; loss: 2.15; acc: 0.28
Batch: 720; loss: 2.13; acc: 0.3
Batch: 740; loss: 2.18; acc: 0.2
Batch: 760; loss: 2.15; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1671306342835637; val_accuracy: 0.23706210191082802 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.25; acc: 0.14
Batch: 20; loss: 2.16; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.21; acc: 0.28
Batch: 80; loss: 2.23; acc: 0.22
Batch: 100; loss: 2.23; acc: 0.2
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.14; acc: 0.25
Batch: 160; loss: 2.21; acc: 0.19
Batch: 180; loss: 2.18; acc: 0.23
Batch: 200; loss: 2.22; acc: 0.17
Batch: 220; loss: 2.18; acc: 0.19
Batch: 240; loss: 2.19; acc: 0.16
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.1; acc: 0.31
Batch: 300; loss: 2.21; acc: 0.22
Batch: 320; loss: 2.15; acc: 0.27
Batch: 340; loss: 2.13; acc: 0.28
Batch: 360; loss: 2.18; acc: 0.2
Batch: 380; loss: 2.17; acc: 0.19
Batch: 400; loss: 2.16; acc: 0.23
Batch: 420; loss: 2.24; acc: 0.17
Batch: 440; loss: 2.17; acc: 0.31
Batch: 460; loss: 2.17; acc: 0.23
Batch: 480; loss: 2.27; acc: 0.12
Batch: 500; loss: 2.23; acc: 0.19
Batch: 520; loss: 2.17; acc: 0.27
Batch: 540; loss: 2.14; acc: 0.31
Batch: 560; loss: 2.24; acc: 0.22
Batch: 580; loss: 2.21; acc: 0.22
Batch: 600; loss: 2.15; acc: 0.22
Batch: 620; loss: 2.17; acc: 0.23
Batch: 640; loss: 2.19; acc: 0.16
Batch: 660; loss: 2.2; acc: 0.2
Batch: 680; loss: 2.15; acc: 0.27
Batch: 700; loss: 2.13; acc: 0.33
Batch: 720; loss: 2.19; acc: 0.25
Batch: 740; loss: 2.18; acc: 0.3
Batch: 760; loss: 2.1; acc: 0.31
Batch: 780; loss: 2.17; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1671051356443174; val_accuracy: 0.2373606687898089 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.21; acc: 0.22
Batch: 20; loss: 2.2; acc: 0.17
Batch: 40; loss: 2.19; acc: 0.14
Batch: 60; loss: 2.2; acc: 0.27
Batch: 80; loss: 2.17; acc: 0.28
Batch: 100; loss: 2.1; acc: 0.28
Batch: 120; loss: 2.14; acc: 0.23
Batch: 140; loss: 2.13; acc: 0.3
Batch: 160; loss: 2.13; acc: 0.23
Batch: 180; loss: 2.24; acc: 0.16
Batch: 200; loss: 2.11; acc: 0.31
Batch: 220; loss: 2.2; acc: 0.25
Batch: 240; loss: 2.15; acc: 0.2
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.18; acc: 0.28
Batch: 300; loss: 2.15; acc: 0.31
Batch: 320; loss: 2.13; acc: 0.33
Batch: 340; loss: 2.18; acc: 0.19
Batch: 360; loss: 2.17; acc: 0.3
Batch: 380; loss: 2.18; acc: 0.25
Batch: 400; loss: 2.2; acc: 0.22
Batch: 420; loss: 2.11; acc: 0.25
Batch: 440; loss: 2.18; acc: 0.22
Batch: 460; loss: 2.21; acc: 0.17
Batch: 480; loss: 2.17; acc: 0.28
Batch: 500; loss: 2.22; acc: 0.16
Batch: 520; loss: 2.15; acc: 0.28
Batch: 540; loss: 2.18; acc: 0.2
Batch: 560; loss: 2.13; acc: 0.3
Batch: 580; loss: 2.2; acc: 0.23
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.22; acc: 0.23
Batch: 640; loss: 2.18; acc: 0.28
Batch: 660; loss: 2.18; acc: 0.25
Batch: 680; loss: 2.11; acc: 0.3
Batch: 700; loss: 2.18; acc: 0.22
Batch: 720; loss: 2.2; acc: 0.17
Batch: 740; loss: 2.17; acc: 0.2
Batch: 760; loss: 2.12; acc: 0.3
Batch: 780; loss: 2.24; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.167085814627872; val_accuracy: 0.23755971337579618 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.16
Batch: 20; loss: 2.13; acc: 0.34
Batch: 40; loss: 2.13; acc: 0.23
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.18; acc: 0.25
Batch: 100; loss: 2.21; acc: 0.17
Batch: 120; loss: 2.17; acc: 0.19
Batch: 140; loss: 2.1; acc: 0.28
Batch: 160; loss: 2.22; acc: 0.2
Batch: 180; loss: 2.21; acc: 0.12
Batch: 200; loss: 2.2; acc: 0.22
Batch: 220; loss: 2.17; acc: 0.2
Batch: 240; loss: 2.19; acc: 0.2
Batch: 260; loss: 2.11; acc: 0.28
Batch: 280; loss: 2.13; acc: 0.28
Batch: 300; loss: 2.3; acc: 0.12
Batch: 320; loss: 2.16; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.22
Batch: 360; loss: 2.14; acc: 0.27
Batch: 380; loss: 2.21; acc: 0.23
Batch: 400; loss: 2.18; acc: 0.27
Batch: 420; loss: 2.09; acc: 0.23
Batch: 440; loss: 2.16; acc: 0.25
Batch: 460; loss: 2.21; acc: 0.2
Batch: 480; loss: 2.11; acc: 0.23
Batch: 500; loss: 2.15; acc: 0.25
Batch: 520; loss: 2.16; acc: 0.16
Batch: 540; loss: 2.26; acc: 0.16
Batch: 560; loss: 2.16; acc: 0.23
Batch: 580; loss: 2.1; acc: 0.33
Batch: 600; loss: 2.18; acc: 0.2
Batch: 620; loss: 2.16; acc: 0.22
Batch: 640; loss: 2.12; acc: 0.31
Batch: 660; loss: 2.19; acc: 0.28
Batch: 680; loss: 2.1; acc: 0.34
Batch: 700; loss: 2.18; acc: 0.23
Batch: 720; loss: 2.22; acc: 0.22
Batch: 740; loss: 2.1; acc: 0.28
Batch: 760; loss: 2.16; acc: 0.25
Batch: 780; loss: 2.18; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1670655520858277; val_accuracy: 0.2376592356687898 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.16
Batch: 20; loss: 2.16; acc: 0.27
Batch: 40; loss: 2.23; acc: 0.23
Batch: 60; loss: 2.22; acc: 0.2
Batch: 80; loss: 2.14; acc: 0.22
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.1; acc: 0.31
Batch: 140; loss: 2.19; acc: 0.2
Batch: 160; loss: 2.18; acc: 0.19
Batch: 180; loss: 2.21; acc: 0.23
Batch: 200; loss: 2.12; acc: 0.27
Batch: 220; loss: 2.22; acc: 0.25
Batch: 240; loss: 2.13; acc: 0.27
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.1; acc: 0.28
Batch: 300; loss: 2.15; acc: 0.25
Batch: 320; loss: 2.16; acc: 0.25
Batch: 340; loss: 2.17; acc: 0.16
Batch: 360; loss: 2.22; acc: 0.16
Batch: 380; loss: 2.2; acc: 0.23
Batch: 400; loss: 2.19; acc: 0.19
Batch: 420; loss: 2.18; acc: 0.22
Batch: 440; loss: 2.24; acc: 0.23
Batch: 460; loss: 2.1; acc: 0.25
Batch: 480; loss: 2.21; acc: 0.25
Batch: 500; loss: 2.16; acc: 0.23
Batch: 520; loss: 2.27; acc: 0.25
Batch: 540; loss: 2.19; acc: 0.19
Batch: 560; loss: 2.13; acc: 0.22
Batch: 580; loss: 2.16; acc: 0.25
Batch: 600; loss: 2.17; acc: 0.2
Batch: 620; loss: 2.22; acc: 0.19
Batch: 640; loss: 2.15; acc: 0.2
Batch: 660; loss: 2.18; acc: 0.23
Batch: 680; loss: 2.22; acc: 0.14
Batch: 700; loss: 2.19; acc: 0.27
Batch: 720; loss: 2.24; acc: 0.17
Batch: 740; loss: 2.17; acc: 0.2
Batch: 760; loss: 2.18; acc: 0.19
Batch: 780; loss: 2.17; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.167046820282177; val_accuracy: 0.23835589171974522 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.16; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.25
Batch: 40; loss: 2.1; acc: 0.28
Batch: 60; loss: 2.21; acc: 0.23
Batch: 80; loss: 2.16; acc: 0.27
Batch: 100; loss: 2.15; acc: 0.2
Batch: 120; loss: 2.21; acc: 0.22
Batch: 140; loss: 2.14; acc: 0.28
Batch: 160; loss: 2.19; acc: 0.2
Batch: 180; loss: 2.21; acc: 0.25
Batch: 200; loss: 2.19; acc: 0.22
Batch: 220; loss: 2.16; acc: 0.27
Batch: 240; loss: 2.2; acc: 0.22
Batch: 260; loss: 2.13; acc: 0.25
Batch: 280; loss: 2.13; acc: 0.27
Batch: 300; loss: 2.1; acc: 0.28
Batch: 320; loss: 2.13; acc: 0.3
Batch: 340; loss: 2.1; acc: 0.28
Batch: 360; loss: 2.16; acc: 0.22
Batch: 380; loss: 2.21; acc: 0.17
Batch: 400; loss: 2.18; acc: 0.22
Batch: 420; loss: 2.23; acc: 0.16
Batch: 440; loss: 2.18; acc: 0.2
Batch: 460; loss: 2.2; acc: 0.3
Batch: 480; loss: 2.25; acc: 0.23
Batch: 500; loss: 2.23; acc: 0.11
Batch: 520; loss: 2.14; acc: 0.28
Batch: 540; loss: 2.19; acc: 0.22
Batch: 560; loss: 2.23; acc: 0.17
Batch: 580; loss: 2.16; acc: 0.28
Batch: 600; loss: 2.12; acc: 0.2
Batch: 620; loss: 2.15; acc: 0.28
Batch: 640; loss: 2.03; acc: 0.38
Batch: 660; loss: 2.13; acc: 0.25
Batch: 680; loss: 2.12; acc: 0.22
Batch: 700; loss: 2.16; acc: 0.2
Batch: 720; loss: 2.13; acc: 0.28
Batch: 740; loss: 2.13; acc: 0.28
Batch: 760; loss: 2.26; acc: 0.14
Batch: 780; loss: 2.2; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1670201341058037; val_accuracy: 0.2385549363057325 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.23
Batch: 20; loss: 2.23; acc: 0.22
Batch: 40; loss: 2.23; acc: 0.22
Batch: 60; loss: 2.13; acc: 0.27
Batch: 80; loss: 2.2; acc: 0.2
Batch: 100; loss: 2.2; acc: 0.17
Batch: 120; loss: 2.2; acc: 0.2
Batch: 140; loss: 2.1; acc: 0.34
Batch: 160; loss: 2.13; acc: 0.23
Batch: 180; loss: 2.2; acc: 0.14
Batch: 200; loss: 2.09; acc: 0.33
Batch: 220; loss: 2.16; acc: 0.34
Batch: 240; loss: 2.12; acc: 0.3
Batch: 260; loss: 2.13; acc: 0.22
Batch: 280; loss: 2.16; acc: 0.2
Batch: 300; loss: 2.23; acc: 0.17
Batch: 320; loss: 2.27; acc: 0.22
Batch: 340; loss: 2.21; acc: 0.19
Batch: 360; loss: 2.22; acc: 0.23
Batch: 380; loss: 2.05; acc: 0.31
Batch: 400; loss: 2.12; acc: 0.33
Batch: 420; loss: 2.13; acc: 0.27
Batch: 440; loss: 2.24; acc: 0.17
Batch: 460; loss: 2.03; acc: 0.38
Batch: 480; loss: 2.23; acc: 0.25
Batch: 500; loss: 2.29; acc: 0.17
Batch: 520; loss: 2.08; acc: 0.31
Batch: 540; loss: 2.19; acc: 0.22
Batch: 560; loss: 2.18; acc: 0.22
Batch: 580; loss: 2.2; acc: 0.25
Batch: 600; loss: 2.19; acc: 0.2
Batch: 620; loss: 2.21; acc: 0.19
Batch: 640; loss: 2.17; acc: 0.22
Batch: 660; loss: 2.14; acc: 0.23
Batch: 680; loss: 2.17; acc: 0.3
Batch: 700; loss: 2.15; acc: 0.28
Batch: 720; loss: 2.18; acc: 0.2
Batch: 740; loss: 2.25; acc: 0.16
Batch: 760; loss: 2.15; acc: 0.27
Batch: 780; loss: 2.15; acc: 0.22
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.166993980954407; val_accuracy: 0.23835589171974522 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.16; acc: 0.27
Batch: 20; loss: 2.23; acc: 0.25
Batch: 40; loss: 2.12; acc: 0.27
Batch: 60; loss: 2.09; acc: 0.25
Batch: 80; loss: 2.1; acc: 0.27
Batch: 100; loss: 2.13; acc: 0.34
Batch: 120; loss: 2.1; acc: 0.33
Batch: 140; loss: 2.13; acc: 0.23
Batch: 160; loss: 2.17; acc: 0.3
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.12; acc: 0.25
Batch: 220; loss: 2.25; acc: 0.19
Batch: 240; loss: 2.2; acc: 0.25
Batch: 260; loss: 2.15; acc: 0.19
Batch: 280; loss: 2.14; acc: 0.23
Batch: 300; loss: 2.24; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.19
Batch: 340; loss: 2.14; acc: 0.23
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.19; acc: 0.2
Batch: 400; loss: 2.26; acc: 0.17
Batch: 420; loss: 2.1; acc: 0.3
Batch: 440; loss: 2.19; acc: 0.19
Batch: 460; loss: 2.12; acc: 0.27
Batch: 480; loss: 2.19; acc: 0.25
Batch: 500; loss: 2.1; acc: 0.27
Batch: 520; loss: 2.23; acc: 0.2
Batch: 540; loss: 2.17; acc: 0.22
Batch: 560; loss: 2.16; acc: 0.28
Batch: 580; loss: 2.22; acc: 0.19
Batch: 600; loss: 2.16; acc: 0.19
Batch: 620; loss: 2.23; acc: 0.12
Batch: 640; loss: 2.21; acc: 0.16
Batch: 660; loss: 2.16; acc: 0.23
Batch: 680; loss: 2.15; acc: 0.28
Batch: 700; loss: 2.14; acc: 0.2
Batch: 720; loss: 2.23; acc: 0.19
Batch: 740; loss: 2.23; acc: 0.22
Batch: 760; loss: 2.18; acc: 0.2
Batch: 780; loss: 2.13; acc: 0.28
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1669704595189185; val_accuracy: 0.23875398089171976 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.09; acc: 0.31
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.22
Batch: 60; loss: 2.19; acc: 0.22
Batch: 80; loss: 2.12; acc: 0.28
Batch: 100; loss: 2.11; acc: 0.25
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.19; acc: 0.23
Batch: 160; loss: 2.18; acc: 0.25
Batch: 180; loss: 2.18; acc: 0.19
Batch: 200; loss: 2.18; acc: 0.23
Batch: 220; loss: 2.18; acc: 0.22
Batch: 240; loss: 2.22; acc: 0.2
Batch: 260; loss: 2.19; acc: 0.23
Batch: 280; loss: 2.16; acc: 0.25
Batch: 300; loss: 2.16; acc: 0.25
Batch: 320; loss: 2.2; acc: 0.23
Batch: 340; loss: 2.15; acc: 0.28
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.18; acc: 0.25
Batch: 400; loss: 2.23; acc: 0.25
Batch: 420; loss: 2.19; acc: 0.12
Batch: 440; loss: 2.18; acc: 0.22
Batch: 460; loss: 2.22; acc: 0.16
Batch: 480; loss: 2.15; acc: 0.22
Batch: 500; loss: 2.09; acc: 0.25
Batch: 520; loss: 2.26; acc: 0.19
Batch: 540; loss: 2.08; acc: 0.28
Batch: 560; loss: 2.17; acc: 0.22
Batch: 580; loss: 2.15; acc: 0.25
Batch: 600; loss: 2.23; acc: 0.25
Batch: 620; loss: 2.18; acc: 0.25
Batch: 640; loss: 2.12; acc: 0.28
Batch: 660; loss: 2.14; acc: 0.27
Batch: 680; loss: 2.09; acc: 0.31
Batch: 700; loss: 2.2; acc: 0.23
Batch: 720; loss: 2.15; acc: 0.23
Batch: 740; loss: 2.21; acc: 0.23
Batch: 760; loss: 2.11; acc: 0.28
Batch: 780; loss: 2.17; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1669502060884125; val_accuracy: 0.23805732484076433 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.12; acc: 0.23
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.22
Batch: 60; loss: 2.09; acc: 0.31
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.1; acc: 0.31
Batch: 120; loss: 2.16; acc: 0.19
Batch: 140; loss: 2.2; acc: 0.19
Batch: 160; loss: 2.1; acc: 0.2
Batch: 180; loss: 2.17; acc: 0.25
Batch: 200; loss: 2.19; acc: 0.22
Batch: 220; loss: 2.13; acc: 0.27
Batch: 240; loss: 2.19; acc: 0.2
Batch: 260; loss: 2.19; acc: 0.23
Batch: 280; loss: 2.19; acc: 0.23
Batch: 300; loss: 2.18; acc: 0.23
Batch: 320; loss: 2.21; acc: 0.16
Batch: 340; loss: 2.19; acc: 0.25
Batch: 360; loss: 2.25; acc: 0.17
Batch: 380; loss: 2.16; acc: 0.2
Batch: 400; loss: 2.21; acc: 0.16
Batch: 420; loss: 2.21; acc: 0.2
Batch: 440; loss: 2.21; acc: 0.2
Batch: 460; loss: 2.15; acc: 0.3
Batch: 480; loss: 2.1; acc: 0.33
Batch: 500; loss: 2.22; acc: 0.14
Batch: 520; loss: 2.16; acc: 0.25
Batch: 540; loss: 2.16; acc: 0.25
Batch: 560; loss: 2.11; acc: 0.25
Batch: 580; loss: 2.21; acc: 0.17
Batch: 600; loss: 2.14; acc: 0.25
Batch: 620; loss: 2.14; acc: 0.19
Batch: 640; loss: 2.24; acc: 0.19
Batch: 660; loss: 2.26; acc: 0.2
Batch: 680; loss: 2.18; acc: 0.28
Batch: 700; loss: 2.15; acc: 0.27
Batch: 720; loss: 2.12; acc: 0.23
Batch: 740; loss: 2.18; acc: 0.28
Batch: 760; loss: 2.12; acc: 0.25
Batch: 780; loss: 2.23; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1669400452048917; val_accuracy: 0.2379578025477707 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.1; acc: 0.27
Batch: 20; loss: 2.18; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.22
Batch: 60; loss: 2.23; acc: 0.16
Batch: 80; loss: 2.1; acc: 0.34
Batch: 100; loss: 2.07; acc: 0.33
Batch: 120; loss: 2.17; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.31
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.16; acc: 0.28
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.18; acc: 0.27
Batch: 240; loss: 2.18; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.24; acc: 0.16
Batch: 300; loss: 2.19; acc: 0.23
Batch: 320; loss: 2.13; acc: 0.31
Batch: 340; loss: 2.26; acc: 0.19
Batch: 360; loss: 2.16; acc: 0.25
Batch: 380; loss: 2.18; acc: 0.27
Batch: 400; loss: 2.24; acc: 0.17
Batch: 420; loss: 2.19; acc: 0.27
Batch: 440; loss: 2.18; acc: 0.2
Batch: 460; loss: 2.17; acc: 0.25
Batch: 480; loss: 2.2; acc: 0.25
Batch: 500; loss: 2.23; acc: 0.2
Batch: 520; loss: 2.17; acc: 0.27
Batch: 540; loss: 2.1; acc: 0.25
Batch: 560; loss: 2.1; acc: 0.23
Batch: 580; loss: 2.16; acc: 0.2
Batch: 600; loss: 2.18; acc: 0.19
Batch: 620; loss: 2.15; acc: 0.3
Batch: 640; loss: 2.17; acc: 0.25
Batch: 660; loss: 2.21; acc: 0.23
Batch: 680; loss: 2.13; acc: 0.27
Batch: 700; loss: 2.14; acc: 0.2
Batch: 720; loss: 2.16; acc: 0.28
Batch: 740; loss: 2.1; acc: 0.2
Batch: 760; loss: 2.15; acc: 0.27
Batch: 780; loss: 2.12; acc: 0.33
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.166929645902792; val_accuracy: 0.23805732484076433 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.13; acc: 0.28
Batch: 20; loss: 2.19; acc: 0.25
Batch: 40; loss: 2.15; acc: 0.25
Batch: 60; loss: 2.2; acc: 0.25
Batch: 80; loss: 2.23; acc: 0.17
Batch: 100; loss: 2.15; acc: 0.22
Batch: 120; loss: 2.11; acc: 0.25
Batch: 140; loss: 2.19; acc: 0.27
Batch: 160; loss: 2.14; acc: 0.3
Batch: 180; loss: 2.2; acc: 0.19
Batch: 200; loss: 2.13; acc: 0.23
Batch: 220; loss: 2.09; acc: 0.28
Batch: 240; loss: 2.12; acc: 0.25
Batch: 260; loss: 2.15; acc: 0.25
Batch: 280; loss: 2.12; acc: 0.3
Batch: 300; loss: 2.15; acc: 0.23
Batch: 320; loss: 1.99; acc: 0.41
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.09; acc: 0.36
Batch: 380; loss: 2.26; acc: 0.2
Batch: 400; loss: 2.12; acc: 0.3
Batch: 420; loss: 2.18; acc: 0.2
Batch: 440; loss: 2.23; acc: 0.22
Batch: 460; loss: 2.13; acc: 0.23
Batch: 480; loss: 2.21; acc: 0.23
Batch: 500; loss: 2.14; acc: 0.19
Batch: 520; loss: 2.11; acc: 0.25
Batch: 540; loss: 2.21; acc: 0.16
Batch: 560; loss: 2.22; acc: 0.22
Batch: 580; loss: 2.19; acc: 0.25
Batch: 600; loss: 2.17; acc: 0.23
Batch: 620; loss: 2.15; acc: 0.2
Batch: 640; loss: 2.17; acc: 0.2
Batch: 660; loss: 2.2; acc: 0.2
Batch: 680; loss: 2.2; acc: 0.2
Batch: 700; loss: 2.26; acc: 0.19
Batch: 720; loss: 2.19; acc: 0.19
Batch: 740; loss: 2.17; acc: 0.25
Batch: 760; loss: 2.22; acc: 0.17
Batch: 780; loss: 2.07; acc: 0.38
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1669207090025493; val_accuracy: 0.23845541401273884 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.23; acc: 0.25
Batch: 20; loss: 2.16; acc: 0.28
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.13; acc: 0.27
Batch: 80; loss: 2.2; acc: 0.19
Batch: 100; loss: 2.14; acc: 0.33
Batch: 120; loss: 2.21; acc: 0.16
Batch: 140; loss: 2.14; acc: 0.28
Batch: 160; loss: 2.21; acc: 0.19
Batch: 180; loss: 2.25; acc: 0.17
Batch: 200; loss: 2.2; acc: 0.22
Batch: 220; loss: 2.09; acc: 0.27
Batch: 240; loss: 2.19; acc: 0.23
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.19; acc: 0.23
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.12; acc: 0.31
Batch: 340; loss: 2.17; acc: 0.23
Batch: 360; loss: 2.17; acc: 0.25
Batch: 380; loss: 2.25; acc: 0.19
Batch: 400; loss: 2.12; acc: 0.3
Batch: 420; loss: 2.22; acc: 0.17
Batch: 440; loss: 2.11; acc: 0.28
Batch: 460; loss: 2.2; acc: 0.16
Batch: 480; loss: 2.31; acc: 0.11
Batch: 500; loss: 2.2; acc: 0.25
Batch: 520; loss: 2.08; acc: 0.31
Batch: 540; loss: 2.15; acc: 0.27
Batch: 560; loss: 2.2; acc: 0.22
Batch: 580; loss: 2.2; acc: 0.28
Batch: 600; loss: 2.17; acc: 0.22
Batch: 620; loss: 2.21; acc: 0.23
Batch: 640; loss: 2.16; acc: 0.27
Batch: 660; loss: 2.14; acc: 0.22
Batch: 680; loss: 2.14; acc: 0.22
Batch: 700; loss: 2.18; acc: 0.27
Batch: 720; loss: 2.2; acc: 0.23
Batch: 740; loss: 2.12; acc: 0.33
Batch: 760; loss: 2.21; acc: 0.17
Batch: 780; loss: 2.19; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.166912384093947; val_accuracy: 0.23885350318471338 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.26; acc: 0.23
Batch: 40; loss: 2.16; acc: 0.22
Batch: 60; loss: 2.17; acc: 0.27
Batch: 80; loss: 2.26; acc: 0.17
Batch: 100; loss: 2.2; acc: 0.22
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.27; acc: 0.16
Batch: 160; loss: 2.19; acc: 0.12
Batch: 180; loss: 2.19; acc: 0.23
Batch: 200; loss: 2.2; acc: 0.3
Batch: 220; loss: 2.15; acc: 0.22
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.21; acc: 0.25
Batch: 280; loss: 2.18; acc: 0.2
Batch: 300; loss: 2.21; acc: 0.17
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.25; acc: 0.17
Batch: 360; loss: 2.2; acc: 0.17
Batch: 380; loss: 2.22; acc: 0.19
Batch: 400; loss: 2.1; acc: 0.34
Batch: 420; loss: 2.23; acc: 0.17
Batch: 440; loss: 2.19; acc: 0.28
Batch: 460; loss: 2.18; acc: 0.22
Batch: 480; loss: 2.23; acc: 0.17
Batch: 500; loss: 2.13; acc: 0.27
Batch: 520; loss: 2.18; acc: 0.17
Batch: 540; loss: 2.13; acc: 0.25
Batch: 560; loss: 2.19; acc: 0.12
Batch: 580; loss: 2.24; acc: 0.19
Batch: 600; loss: 2.18; acc: 0.27
Batch: 620; loss: 2.2; acc: 0.17
Batch: 640; loss: 2.15; acc: 0.28
Batch: 660; loss: 2.12; acc: 0.23
Batch: 680; loss: 2.2; acc: 0.22
Batch: 700; loss: 2.1; acc: 0.27
Batch: 720; loss: 2.2; acc: 0.25
Batch: 740; loss: 2.13; acc: 0.25
Batch: 760; loss: 2.11; acc: 0.25
Batch: 780; loss: 2.2; acc: 0.22
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.166901880009159; val_accuracy: 0.2386544585987261 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.23; acc: 0.22
Batch: 20; loss: 2.12; acc: 0.25
Batch: 40; loss: 2.16; acc: 0.23
Batch: 60; loss: 2.25; acc: 0.23
Batch: 80; loss: 2.09; acc: 0.3
Batch: 100; loss: 2.2; acc: 0.28
Batch: 120; loss: 2.27; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.19
Batch: 160; loss: 2.19; acc: 0.19
Batch: 180; loss: 2.21; acc: 0.27
Batch: 200; loss: 2.14; acc: 0.34
Batch: 220; loss: 2.1; acc: 0.3
Batch: 240; loss: 2.15; acc: 0.23
Batch: 260; loss: 2.11; acc: 0.31
Batch: 280; loss: 2.25; acc: 0.2
Batch: 300; loss: 2.24; acc: 0.2
Batch: 320; loss: 2.18; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.19
Batch: 360; loss: 2.13; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.2
Batch: 400; loss: 2.28; acc: 0.19
Batch: 420; loss: 2.18; acc: 0.27
Batch: 440; loss: 2.2; acc: 0.12
Batch: 460; loss: 2.21; acc: 0.27
Batch: 480; loss: 2.16; acc: 0.25
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.17
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.2; acc: 0.17
Batch: 580; loss: 2.2; acc: 0.19
Batch: 600; loss: 2.14; acc: 0.22
Batch: 620; loss: 2.25; acc: 0.19
Batch: 640; loss: 2.22; acc: 0.16
Batch: 660; loss: 2.08; acc: 0.3
Batch: 680; loss: 2.23; acc: 0.27
Batch: 700; loss: 2.19; acc: 0.22
Batch: 720; loss: 2.14; acc: 0.27
Batch: 740; loss: 2.18; acc: 0.14
Batch: 760; loss: 2.2; acc: 0.16
Batch: 780; loss: 2.2; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1668925907961123; val_accuracy: 0.2386544585987261 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.12; acc: 0.25
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.17; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.23
Batch: 100; loss: 2.23; acc: 0.12
Batch: 120; loss: 2.17; acc: 0.28
Batch: 140; loss: 2.2; acc: 0.25
Batch: 160; loss: 2.2; acc: 0.2
Batch: 180; loss: 2.21; acc: 0.22
Batch: 200; loss: 2.13; acc: 0.25
Batch: 220; loss: 2.14; acc: 0.23
Batch: 240; loss: 2.18; acc: 0.28
Batch: 260; loss: 2.18; acc: 0.22
Batch: 280; loss: 2.31; acc: 0.14
Batch: 300; loss: 2.13; acc: 0.38
Batch: 320; loss: 2.05; acc: 0.33
Batch: 340; loss: 2.12; acc: 0.3
Batch: 360; loss: 2.2; acc: 0.17
Batch: 380; loss: 2.14; acc: 0.28
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.17; acc: 0.27
Batch: 440; loss: 2.17; acc: 0.31
Batch: 460; loss: 2.22; acc: 0.17
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.22; acc: 0.22
Batch: 520; loss: 2.14; acc: 0.25
Batch: 540; loss: 2.18; acc: 0.22
Batch: 560; loss: 2.19; acc: 0.17
Batch: 580; loss: 2.16; acc: 0.25
Batch: 600; loss: 2.11; acc: 0.22
Batch: 620; loss: 2.16; acc: 0.22
Batch: 640; loss: 2.14; acc: 0.2
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.24; acc: 0.14
Batch: 700; loss: 2.2; acc: 0.27
Batch: 720; loss: 2.23; acc: 0.19
Batch: 740; loss: 2.18; acc: 0.3
Batch: 760; loss: 2.2; acc: 0.23
Batch: 780; loss: 2.18; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.2; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.166883532408696; val_accuracy: 0.2386544585987261 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.08; acc: 0.33
Batch: 20; loss: 2.21; acc: 0.25
Batch: 40; loss: 2.14; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.19
Batch: 80; loss: 2.19; acc: 0.19
Batch: 100; loss: 2.17; acc: 0.19
Batch: 120; loss: 2.24; acc: 0.2
Batch: 140; loss: 2.1; acc: 0.34
Batch: 160; loss: 2.17; acc: 0.22
Batch: 180; loss: 2.18; acc: 0.14
Batch: 200; loss: 2.19; acc: 0.25
Batch: 220; loss: 2.19; acc: 0.2
Batch: 240; loss: 2.17; acc: 0.12
Batch: 260; loss: 2.09; acc: 0.3
Batch: 280; loss: 2.17; acc: 0.2
Batch: 300; loss: 2.15; acc: 0.22
Batch: 320; loss: 2.18; acc: 0.25
Batch: 340; loss: 2.16; acc: 0.25
Batch: 360; loss: 2.2; acc: 0.2
Batch: 380; loss: 2.2; acc: 0.2
Batch: 400; loss: 2.25; acc: 0.14
Batch: 420; loss: 2.14; acc: 0.28
Batch: 440; loss: 2.07; acc: 0.36
Batch: 460; loss: 2.2; acc: 0.27
Batch: 480; loss: 2.05; acc: 0.28
Batch: 500; loss: 2.14; acc: 0.28
Batch: 520; loss: 2.17; acc: 0.16
Batch: 540; loss: 2.18; acc: 0.31
Batch: 560; loss: 2.23; acc: 0.19
Batch: 580; loss: 2.18; acc: 0.3
Batch: 600; loss: 2.08; acc: 0.36
Batch: 620; loss: 2.1; acc: 0.27
Batch: 640; loss: 2.13; acc: 0.3
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.17; acc: 0.27
Batch: 700; loss: 2.17; acc: 0.25
Batch: 720; loss: 2.09; acc: 0.3
Batch: 740; loss: 2.18; acc: 0.23
Batch: 760; loss: 2.16; acc: 0.28
Batch: 780; loss: 2.21; acc: 0.12
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.2; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1668730526213436; val_accuracy: 0.23875398089171976 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.17; acc: 0.25
Batch: 20; loss: 2.19; acc: 0.27
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.24; acc: 0.16
Batch: 80; loss: 2.17; acc: 0.27
Batch: 100; loss: 2.17; acc: 0.25
Batch: 120; loss: 2.16; acc: 0.31
Batch: 140; loss: 2.12; acc: 0.23
Batch: 160; loss: 2.21; acc: 0.23
Batch: 180; loss: 2.09; acc: 0.31
Batch: 200; loss: 2.24; acc: 0.25
Batch: 220; loss: 2.16; acc: 0.16
Batch: 240; loss: 2.17; acc: 0.25
Batch: 260; loss: 2.19; acc: 0.22
Batch: 280; loss: 2.18; acc: 0.27
Batch: 300; loss: 2.17; acc: 0.27
Batch: 320; loss: 2.18; acc: 0.25
Batch: 340; loss: 2.22; acc: 0.23
Batch: 360; loss: 2.21; acc: 0.2
Batch: 380; loss: 2.17; acc: 0.3
Batch: 400; loss: 2.17; acc: 0.28
Batch: 420; loss: 2.15; acc: 0.28
Batch: 440; loss: 2.17; acc: 0.25
Batch: 460; loss: 2.18; acc: 0.23
Batch: 480; loss: 2.22; acc: 0.3
Batch: 500; loss: 2.17; acc: 0.25
Batch: 520; loss: 2.11; acc: 0.28
Batch: 540; loss: 2.18; acc: 0.2
Batch: 560; loss: 2.14; acc: 0.23
Batch: 580; loss: 2.17; acc: 0.23
Batch: 600; loss: 2.23; acc: 0.25
Batch: 620; loss: 2.16; acc: 0.2
Batch: 640; loss: 2.12; acc: 0.3
Batch: 660; loss: 2.14; acc: 0.31
Batch: 680; loss: 2.23; acc: 0.22
Batch: 700; loss: 2.21; acc: 0.19
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.21; acc: 0.22
Batch: 760; loss: 2.25; acc: 0.19
Batch: 780; loss: 2.17; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.2; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.1668632015301164; val_accuracy: 0.238953025477707 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.22; acc: 0.22
Batch: 20; loss: 2.08; acc: 0.27
Batch: 40; loss: 2.15; acc: 0.22
Batch: 60; loss: 2.17; acc: 0.25
Batch: 80; loss: 2.13; acc: 0.23
Batch: 100; loss: 2.1; acc: 0.31
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.22
Batch: 160; loss: 2.13; acc: 0.27
Batch: 180; loss: 2.16; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.19
Batch: 220; loss: 2.14; acc: 0.23
Batch: 240; loss: 2.22; acc: 0.23
Batch: 260; loss: 2.23; acc: 0.16
Batch: 280; loss: 2.21; acc: 0.19
Batch: 300; loss: 2.15; acc: 0.27
Batch: 320; loss: 2.2; acc: 0.19
Batch: 340; loss: 2.14; acc: 0.23
Batch: 360; loss: 2.18; acc: 0.17
Batch: 380; loss: 2.11; acc: 0.27
Batch: 400; loss: 2.29; acc: 0.17
Batch: 420; loss: 2.22; acc: 0.17
Batch: 440; loss: 2.2; acc: 0.19
Batch: 460; loss: 2.18; acc: 0.25
Batch: 480; loss: 2.2; acc: 0.22
Batch: 500; loss: 2.2; acc: 0.2
Batch: 520; loss: 2.12; acc: 0.2
Batch: 540; loss: 2.2; acc: 0.23
Batch: 560; loss: 2.23; acc: 0.2
Batch: 580; loss: 2.18; acc: 0.22
Batch: 600; loss: 2.08; acc: 0.28
Batch: 620; loss: 2.24; acc: 0.14
Batch: 640; loss: 2.24; acc: 0.22
Batch: 660; loss: 2.18; acc: 0.19
Batch: 680; loss: 2.16; acc: 0.31
Batch: 700; loss: 2.21; acc: 0.16
Batch: 720; loss: 2.21; acc: 0.27
Batch: 740; loss: 2.15; acc: 0.23
Batch: 760; loss: 2.21; acc: 0.23
Batch: 780; loss: 2.1; acc: 0.28
Train Epoch over. train_loss: 2.17; train_accuracy: 0.23 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.14; acc: 0.3
Batch: 100; loss: 2.2; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.23
Batch: 140; loss: 2.16; acc: 0.31
Val Epoch over. val_loss: 2.166852522807516; val_accuracy: 0.2385549363057325 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5277
elements in E: 1124750
fraction nonzero: 0.004691709268726384
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.16
Batch: 160; loss: 2.32; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.31; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.32; acc: 0.08
Batch: 380; loss: 2.31; acc: 0.08
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.31; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.12
Batch: 480; loss: 2.31; acc: 0.11
Batch: 500; loss: 2.32; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.31; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.16
Batch: 600; loss: 2.3; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.17
Batch: 680; loss: 2.3; acc: 0.09
Batch: 700; loss: 2.3; acc: 0.06
Batch: 720; loss: 2.3; acc: 0.16
Batch: 740; loss: 2.31; acc: 0.08
Batch: 760; loss: 2.3; acc: 0.06
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.11
Val Epoch over. val_loss: 2.302643154836764; val_accuracy: 0.09683519108280254 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.31; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.31; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.3; acc: 0.11
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.2
Batch: 300; loss: 2.28; acc: 0.2
Batch: 320; loss: 2.29; acc: 0.14
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.11
Batch: 440; loss: 2.31; acc: 0.03
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.31; acc: 0.08
Batch: 540; loss: 2.31; acc: 0.03
Batch: 560; loss: 2.29; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.06
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.11
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.29; acc: 0.12
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.28; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.17
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.17
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.12
Val Epoch over. val_loss: 2.2972967822080963; val_accuracy: 0.10101512738853503 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.31; acc: 0.03
Batch: 40; loss: 2.31; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.17
Batch: 180; loss: 2.3; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.17
Batch: 220; loss: 2.28; acc: 0.17
Batch: 240; loss: 2.3; acc: 0.06
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.17
Batch: 340; loss: 2.29; acc: 0.19
Batch: 360; loss: 2.29; acc: 0.17
Batch: 380; loss: 2.27; acc: 0.17
Batch: 400; loss: 2.28; acc: 0.17
Batch: 420; loss: 2.29; acc: 0.12
Batch: 440; loss: 2.29; acc: 0.14
Batch: 460; loss: 2.29; acc: 0.11
Batch: 480; loss: 2.28; acc: 0.23
Batch: 500; loss: 2.29; acc: 0.19
Batch: 520; loss: 2.28; acc: 0.19
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.14
Batch: 580; loss: 2.3; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.16
Batch: 640; loss: 2.28; acc: 0.16
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.16
Batch: 700; loss: 2.3; acc: 0.11
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.27; acc: 0.23
Batch: 760; loss: 2.29; acc: 0.16
Batch: 780; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.29; train_accuracy: 0.13 

Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.29; acc: 0.22
Batch: 40; loss: 2.29; acc: 0.2
Batch: 60; loss: 2.29; acc: 0.19
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.28; acc: 0.19
Batch: 140; loss: 2.29; acc: 0.12
Val Epoch over. val_loss: 2.292535016491155; val_accuracy: 0.15316480891719744 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.29; acc: 0.19
Batch: 140; loss: 2.29; acc: 0.22
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.28; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.17
Batch: 220; loss: 2.31; acc: 0.09
Batch: 240; loss: 2.26; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.16
Batch: 280; loss: 2.29; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.17
Batch: 340; loss: 2.29; acc: 0.17
Batch: 360; loss: 2.29; acc: 0.14
Batch: 380; loss: 2.29; acc: 0.19
Batch: 400; loss: 2.28; acc: 0.2
Batch: 420; loss: 2.3; acc: 0.05
Batch: 440; loss: 2.29; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.2
Batch: 480; loss: 2.29; acc: 0.05
Batch: 500; loss: 2.3; acc: 0.16
Batch: 520; loss: 2.29; acc: 0.11
Batch: 540; loss: 2.29; acc: 0.11
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.29; acc: 0.19
Batch: 600; loss: 2.28; acc: 0.22
Batch: 620; loss: 2.3; acc: 0.08
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.31; acc: 0.06
Batch: 680; loss: 2.3; acc: 0.08
Batch: 700; loss: 2.28; acc: 0.19
Batch: 720; loss: 2.28; acc: 0.2
Batch: 740; loss: 2.29; acc: 0.12
Batch: 760; loss: 2.29; acc: 0.23
Batch: 780; loss: 2.28; acc: 0.11
Train Epoch over. train_loss: 2.29; train_accuracy: 0.15 

Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.2
Batch: 40; loss: 2.29; acc: 0.19
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.19
Batch: 120; loss: 2.27; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.16
Val Epoch over. val_loss: 2.2880130208981266; val_accuracy: 0.14888535031847133 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.2
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.2
Batch: 60; loss: 2.29; acc: 0.2
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.22
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.29; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.19
Batch: 220; loss: 2.31; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.28; acc: 0.08
Batch: 280; loss: 2.28; acc: 0.16
Batch: 300; loss: 2.31; acc: 0.22
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.19
Batch: 360; loss: 2.3; acc: 0.14
Batch: 380; loss: 2.29; acc: 0.17
Batch: 400; loss: 2.3; acc: 0.16
Batch: 420; loss: 2.29; acc: 0.09
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.28; acc: 0.2
Batch: 480; loss: 2.3; acc: 0.14
Batch: 500; loss: 2.28; acc: 0.2
Batch: 520; loss: 2.26; acc: 0.28
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.28; acc: 0.19
Batch: 600; loss: 2.29; acc: 0.19
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.29; acc: 0.2
Batch: 660; loss: 2.31; acc: 0.09
Batch: 680; loss: 2.28; acc: 0.16
Batch: 700; loss: 2.28; acc: 0.16
Batch: 720; loss: 2.3; acc: 0.12
Batch: 740; loss: 2.25; acc: 0.2
Batch: 760; loss: 2.24; acc: 0.34
Batch: 780; loss: 2.29; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.16 

Batch: 0; loss: 2.28; acc: 0.22
Batch: 20; loss: 2.28; acc: 0.19
Batch: 40; loss: 2.28; acc: 0.19
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.28; acc: 0.19
Batch: 120; loss: 2.26; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.16
Val Epoch over. val_loss: 2.2798262994000864; val_accuracy: 0.17127786624203822 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.25; acc: 0.27
Batch: 40; loss: 2.3; acc: 0.19
Batch: 60; loss: 2.27; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.19
Batch: 100; loss: 2.28; acc: 0.16
Batch: 120; loss: 2.27; acc: 0.23
Batch: 140; loss: 2.29; acc: 0.16
Batch: 160; loss: 2.28; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.2
Batch: 200; loss: 2.27; acc: 0.16
Batch: 220; loss: 2.29; acc: 0.14
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.19
Batch: 280; loss: 2.28; acc: 0.17
Batch: 300; loss: 2.26; acc: 0.23
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.28; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.24; acc: 0.17
Batch: 400; loss: 2.26; acc: 0.16
Batch: 420; loss: 2.28; acc: 0.23
Batch: 440; loss: 2.26; acc: 0.14
Batch: 460; loss: 2.26; acc: 0.22
Batch: 480; loss: 2.27; acc: 0.11
Batch: 500; loss: 2.28; acc: 0.09
Batch: 520; loss: 2.28; acc: 0.08
Batch: 540; loss: 2.29; acc: 0.14
Batch: 560; loss: 2.26; acc: 0.19
Batch: 580; loss: 2.27; acc: 0.12
Batch: 600; loss: 2.25; acc: 0.17
Batch: 620; loss: 2.25; acc: 0.22
Batch: 640; loss: 2.24; acc: 0.2
Batch: 660; loss: 2.29; acc: 0.09
Batch: 680; loss: 2.24; acc: 0.2
Batch: 700; loss: 2.27; acc: 0.17
Batch: 720; loss: 2.25; acc: 0.19
Batch: 740; loss: 2.25; acc: 0.2
Batch: 760; loss: 2.24; acc: 0.14
Batch: 780; loss: 2.25; acc: 0.19
Train Epoch over. train_loss: 2.27; train_accuracy: 0.17 

Batch: 0; loss: 2.25; acc: 0.22
Batch: 20; loss: 2.24; acc: 0.22
Batch: 40; loss: 2.25; acc: 0.22
Batch: 60; loss: 2.25; acc: 0.19
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.24; acc: 0.16
Batch: 120; loss: 2.23; acc: 0.23
Batch: 140; loss: 2.25; acc: 0.17
Val Epoch over. val_loss: 2.2525442451428455; val_accuracy: 0.17406449044585987 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.25; acc: 0.19
Batch: 20; loss: 2.26; acc: 0.22
Batch: 40; loss: 2.25; acc: 0.19
Batch: 60; loss: 2.26; acc: 0.19
Batch: 80; loss: 2.27; acc: 0.14
Batch: 100; loss: 2.32; acc: 0.09
Batch: 120; loss: 2.24; acc: 0.23
Batch: 140; loss: 2.23; acc: 0.2
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.24; acc: 0.2
Batch: 200; loss: 2.24; acc: 0.2
Batch: 220; loss: 2.22; acc: 0.25
Batch: 240; loss: 2.24; acc: 0.19
Batch: 260; loss: 2.32; acc: 0.14
Batch: 280; loss: 2.23; acc: 0.27
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.2; acc: 0.23
Batch: 340; loss: 2.2; acc: 0.23
Batch: 360; loss: 2.25; acc: 0.2
Batch: 380; loss: 2.25; acc: 0.17
Batch: 400; loss: 2.21; acc: 0.22
Batch: 420; loss: 2.23; acc: 0.17
Batch: 440; loss: 2.22; acc: 0.17
Batch: 460; loss: 2.24; acc: 0.25
Batch: 480; loss: 2.2; acc: 0.2
Batch: 500; loss: 2.27; acc: 0.16
Batch: 520; loss: 2.18; acc: 0.17
Batch: 540; loss: 2.25; acc: 0.2
Batch: 560; loss: 2.27; acc: 0.2
Batch: 580; loss: 2.23; acc: 0.19
Batch: 600; loss: 2.24; acc: 0.3
Batch: 620; loss: 2.2; acc: 0.23
Batch: 640; loss: 2.31; acc: 0.14
Batch: 660; loss: 2.22; acc: 0.28
Batch: 680; loss: 2.24; acc: 0.23
Batch: 700; loss: 2.18; acc: 0.25
Batch: 720; loss: 2.2; acc: 0.25
Batch: 740; loss: 2.24; acc: 0.19
Batch: 760; loss: 2.17; acc: 0.3
Batch: 780; loss: 2.28; acc: 0.19
Train Epoch over. train_loss: 2.24; train_accuracy: 0.19 

Batch: 0; loss: 2.22; acc: 0.23
Batch: 20; loss: 2.23; acc: 0.17
Batch: 40; loss: 2.2; acc: 0.27
Batch: 60; loss: 2.21; acc: 0.2
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.19; acc: 0.19
Batch: 120; loss: 2.19; acc: 0.27
Batch: 140; loss: 2.22; acc: 0.19
Val Epoch over. val_loss: 2.217579721645185; val_accuracy: 0.21267914012738853 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.14
Batch: 20; loss: 2.19; acc: 0.28
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.21; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.27
Batch: 120; loss: 2.28; acc: 0.17
Batch: 140; loss: 2.16; acc: 0.2
Batch: 160; loss: 2.21; acc: 0.27
Batch: 180; loss: 2.22; acc: 0.16
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.21; acc: 0.22
Batch: 240; loss: 2.25; acc: 0.2
Batch: 260; loss: 2.23; acc: 0.12
Batch: 280; loss: 2.13; acc: 0.36
Batch: 300; loss: 2.22; acc: 0.23
Batch: 320; loss: 2.15; acc: 0.31
Batch: 340; loss: 2.33; acc: 0.11
Batch: 360; loss: 2.23; acc: 0.19
Batch: 380; loss: 2.16; acc: 0.22
Batch: 400; loss: 2.2; acc: 0.16
Batch: 420; loss: 2.24; acc: 0.14
Batch: 440; loss: 2.26; acc: 0.17
Batch: 460; loss: 2.25; acc: 0.14
Batch: 480; loss: 2.25; acc: 0.17
Batch: 500; loss: 2.2; acc: 0.22
Batch: 520; loss: 2.2; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.2
Batch: 560; loss: 2.22; acc: 0.14
Batch: 580; loss: 2.15; acc: 0.22
Batch: 600; loss: 2.16; acc: 0.19
Batch: 620; loss: 2.22; acc: 0.14
Batch: 640; loss: 2.23; acc: 0.19
Batch: 660; loss: 2.28; acc: 0.11
Batch: 680; loss: 2.25; acc: 0.19
Batch: 700; loss: 2.15; acc: 0.2
Batch: 720; loss: 2.2; acc: 0.16
Batch: 740; loss: 2.16; acc: 0.22
Batch: 760; loss: 2.18; acc: 0.25
Batch: 780; loss: 2.14; acc: 0.16
Train Epoch over. train_loss: 2.21; train_accuracy: 0.2 

Batch: 0; loss: 2.16; acc: 0.16
Batch: 20; loss: 2.22; acc: 0.17
Batch: 40; loss: 2.15; acc: 0.11
Batch: 60; loss: 2.15; acc: 0.09
Batch: 80; loss: 2.21; acc: 0.17
Batch: 100; loss: 2.12; acc: 0.11
Batch: 120; loss: 2.11; acc: 0.25
Batch: 140; loss: 2.17; acc: 0.17
Val Epoch over. val_loss: 2.1787370952071656; val_accuracy: 0.1614251592356688 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.25; acc: 0.08
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.14
Batch: 60; loss: 2.14; acc: 0.11
Batch: 80; loss: 2.12; acc: 0.14
Batch: 100; loss: 2.19; acc: 0.14
Batch: 120; loss: 2.1; acc: 0.2
Batch: 140; loss: 2.15; acc: 0.12
Batch: 160; loss: 2.11; acc: 0.2
Batch: 180; loss: 2.09; acc: 0.19
Batch: 200; loss: 2.07; acc: 0.25
Batch: 220; loss: 2.18; acc: 0.11
Batch: 240; loss: 2.19; acc: 0.09
Batch: 260; loss: 2.12; acc: 0.2
Batch: 280; loss: 2.11; acc: 0.28
Batch: 300; loss: 2.04; acc: 0.2
Batch: 320; loss: 2.16; acc: 0.23
Batch: 340; loss: 2.18; acc: 0.22
Batch: 360; loss: 2.09; acc: 0.19
Batch: 380; loss: 2.13; acc: 0.22
Batch: 400; loss: 2.21; acc: 0.12
Batch: 420; loss: 2.13; acc: 0.16
Batch: 440; loss: 2.08; acc: 0.22
Batch: 460; loss: 2.19; acc: 0.16
Batch: 480; loss: 1.99; acc: 0.16
Batch: 500; loss: 2.17; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.09
Batch: 540; loss: 2.09; acc: 0.16
Batch: 560; loss: 2.1; acc: 0.2
Batch: 580; loss: 2.16; acc: 0.25
Batch: 600; loss: 2.1; acc: 0.19
Batch: 620; loss: 2.13; acc: 0.14
Batch: 640; loss: 2.16; acc: 0.17
Batch: 660; loss: 2.12; acc: 0.17
Batch: 680; loss: 2.13; acc: 0.19
Batch: 700; loss: 2.06; acc: 0.14
Batch: 720; loss: 2.09; acc: 0.22
Batch: 740; loss: 2.07; acc: 0.19
Batch: 760; loss: 2.1; acc: 0.12
Batch: 780; loss: 2.01; acc: 0.22
Train Epoch over. train_loss: 2.13; train_accuracy: 0.17 

Batch: 0; loss: 2.0; acc: 0.19
Batch: 20; loss: 2.15; acc: 0.19
Batch: 40; loss: 1.97; acc: 0.17
Batch: 60; loss: 2.02; acc: 0.14
Batch: 80; loss: 2.11; acc: 0.19
Batch: 100; loss: 1.92; acc: 0.19
Batch: 120; loss: 2.01; acc: 0.19
Batch: 140; loss: 2.07; acc: 0.16
Val Epoch over. val_loss: 2.0703912684871892; val_accuracy: 0.1913813694267516 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.0; acc: 0.2
Batch: 20; loss: 2.11; acc: 0.23
Batch: 40; loss: 1.96; acc: 0.2
Batch: 60; loss: 2.03; acc: 0.2
Batch: 80; loss: 2.01; acc: 0.2
Batch: 100; loss: 2.14; acc: 0.19
Batch: 120; loss: 1.97; acc: 0.3
Batch: 140; loss: 2.02; acc: 0.19
Batch: 160; loss: 2.05; acc: 0.16
Batch: 180; loss: 2.1; acc: 0.16
Batch: 200; loss: 2.08; acc: 0.23
Batch: 220; loss: 2.01; acc: 0.22
Batch: 240; loss: 2.11; acc: 0.17
Batch: 260; loss: 1.9; acc: 0.2
Batch: 280; loss: 1.99; acc: 0.25
Batch: 300; loss: 2.0; acc: 0.2
Batch: 320; loss: 1.96; acc: 0.23
Batch: 340; loss: 2.07; acc: 0.25
Batch: 360; loss: 2.03; acc: 0.2
Batch: 380; loss: 1.92; acc: 0.25
Batch: 400; loss: 1.92; acc: 0.25
Batch: 420; loss: 2.02; acc: 0.19
Batch: 440; loss: 2.15; acc: 0.17
Batch: 460; loss: 2.16; acc: 0.09
Batch: 480; loss: 2.06; acc: 0.23
Batch: 500; loss: 2.08; acc: 0.16
Batch: 520; loss: 1.95; acc: 0.23
Batch: 540; loss: 2.0; acc: 0.16
Batch: 560; loss: 2.0; acc: 0.27
Batch: 580; loss: 1.94; acc: 0.34
Batch: 600; loss: 2.06; acc: 0.16
Batch: 620; loss: 2.03; acc: 0.16
Batch: 640; loss: 2.06; acc: 0.17
Batch: 660; loss: 1.92; acc: 0.27
Batch: 680; loss: 2.13; acc: 0.17
Batch: 700; loss: 2.03; acc: 0.19
Batch: 720; loss: 1.97; acc: 0.22
Batch: 740; loss: 1.96; acc: 0.22
Batch: 760; loss: 2.05; acc: 0.2
Batch: 780; loss: 2.02; acc: 0.23
Train Epoch over. train_loss: 2.04; train_accuracy: 0.2 

Batch: 0; loss: 1.94; acc: 0.19
Batch: 20; loss: 2.1; acc: 0.19
Batch: 40; loss: 1.86; acc: 0.23
Batch: 60; loss: 1.94; acc: 0.14
Batch: 80; loss: 2.04; acc: 0.12
Batch: 100; loss: 1.84; acc: 0.19
Batch: 120; loss: 2.0; acc: 0.23
Batch: 140; loss: 2.04; acc: 0.17
Val Epoch over. val_loss: 2.014309189122194; val_accuracy: 0.2070063694267516 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.07; acc: 0.22
Batch: 20; loss: 2.08; acc: 0.27
Batch: 40; loss: 2.07; acc: 0.22
Batch: 60; loss: 1.96; acc: 0.25
Batch: 80; loss: 2.01; acc: 0.17
Batch: 100; loss: 2.01; acc: 0.16
Batch: 120; loss: 1.98; acc: 0.22
Batch: 140; loss: 2.03; acc: 0.19
Batch: 160; loss: 2.0; acc: 0.2
Batch: 180; loss: 2.03; acc: 0.12
Batch: 200; loss: 2.03; acc: 0.19
Batch: 220; loss: 2.08; acc: 0.19
Batch: 240; loss: 2.08; acc: 0.23
Batch: 260; loss: 2.05; acc: 0.17
Batch: 280; loss: 1.98; acc: 0.2
Batch: 300; loss: 1.99; acc: 0.22
Batch: 320; loss: 1.98; acc: 0.22
Batch: 340; loss: 1.92; acc: 0.25
Batch: 360; loss: 1.93; acc: 0.22
Batch: 380; loss: 1.83; acc: 0.33
Batch: 400; loss: 2.14; acc: 0.17
Batch: 420; loss: 1.94; acc: 0.17
Batch: 440; loss: 1.98; acc: 0.28
Batch: 460; loss: 2.12; acc: 0.08
Batch: 480; loss: 1.95; acc: 0.28
Batch: 500; loss: 2.2; acc: 0.14
Batch: 520; loss: 2.01; acc: 0.23
Batch: 540; loss: 2.04; acc: 0.19
Batch: 560; loss: 2.1; acc: 0.2
Batch: 580; loss: 2.15; acc: 0.17
Batch: 600; loss: 2.04; acc: 0.2
Batch: 620; loss: 2.09; acc: 0.2
Batch: 640; loss: 1.94; acc: 0.28
Batch: 660; loss: 1.91; acc: 0.23
Batch: 680; loss: 2.01; acc: 0.17
Batch: 700; loss: 1.99; acc: 0.22
Batch: 720; loss: 1.99; acc: 0.25
Batch: 740; loss: 1.93; acc: 0.28
Batch: 760; loss: 1.99; acc: 0.22
Batch: 780; loss: 2.15; acc: 0.23
Train Epoch over. train_loss: 2.02; train_accuracy: 0.21 

Batch: 0; loss: 1.95; acc: 0.2
Batch: 20; loss: 2.1; acc: 0.19
Batch: 40; loss: 1.85; acc: 0.28
Batch: 60; loss: 1.93; acc: 0.2
Batch: 80; loss: 2.02; acc: 0.14
Batch: 100; loss: 1.85; acc: 0.17
Batch: 120; loss: 2.0; acc: 0.25
Batch: 140; loss: 2.04; acc: 0.14
Val Epoch over. val_loss: 2.0072466271698097; val_accuracy: 0.21297770700636942 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.99; acc: 0.23
Batch: 20; loss: 2.1; acc: 0.22
Batch: 40; loss: 1.97; acc: 0.2
Batch: 60; loss: 2.01; acc: 0.25
Batch: 80; loss: 2.05; acc: 0.23
Batch: 100; loss: 2.09; acc: 0.14
Batch: 120; loss: 2.06; acc: 0.22
Batch: 140; loss: 2.09; acc: 0.22
Batch: 160; loss: 2.07; acc: 0.17
Batch: 180; loss: 1.88; acc: 0.22
Batch: 200; loss: 2.06; acc: 0.28
Batch: 220; loss: 1.91; acc: 0.27
Batch: 240; loss: 2.03; acc: 0.17
Batch: 260; loss: 2.05; acc: 0.2
Batch: 280; loss: 2.03; acc: 0.22
Batch: 300; loss: 2.07; acc: 0.22
Batch: 320; loss: 2.0; acc: 0.27
Batch: 340; loss: 1.91; acc: 0.27
Batch: 360; loss: 2.05; acc: 0.17
Batch: 380; loss: 2.08; acc: 0.23
Batch: 400; loss: 2.11; acc: 0.22
Batch: 420; loss: 2.08; acc: 0.22
Batch: 440; loss: 1.92; acc: 0.34
Batch: 460; loss: 2.06; acc: 0.19
Batch: 480; loss: 1.95; acc: 0.23
Batch: 500; loss: 1.89; acc: 0.3
Batch: 520; loss: 1.95; acc: 0.22
Batch: 540; loss: 1.92; acc: 0.28
Batch: 560; loss: 2.05; acc: 0.2
Batch: 580; loss: 2.05; acc: 0.2
Batch: 600; loss: 1.92; acc: 0.27
Batch: 620; loss: 2.06; acc: 0.17
Batch: 640; loss: 1.94; acc: 0.23
Batch: 660; loss: 2.07; acc: 0.23
Batch: 680; loss: 1.93; acc: 0.31
Batch: 700; loss: 1.97; acc: 0.22
Batch: 720; loss: 2.04; acc: 0.2
Batch: 740; loss: 2.14; acc: 0.16
Batch: 760; loss: 1.93; acc: 0.25
Batch: 780; loss: 1.98; acc: 0.25
Train Epoch over. train_loss: 2.01; train_accuracy: 0.22 

Batch: 0; loss: 1.96; acc: 0.22
Batch: 20; loss: 2.09; acc: 0.19
Batch: 40; loss: 1.85; acc: 0.27
Batch: 60; loss: 1.93; acc: 0.25
Batch: 80; loss: 2.0; acc: 0.14
Batch: 100; loss: 1.85; acc: 0.2
Batch: 120; loss: 1.98; acc: 0.27
Batch: 140; loss: 2.04; acc: 0.17
Val Epoch over. val_loss: 2.0017523811121656; val_accuracy: 0.22472133757961785 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.96; acc: 0.27
Batch: 20; loss: 2.05; acc: 0.22
Batch: 40; loss: 2.02; acc: 0.25
Batch: 60; loss: 1.97; acc: 0.19
Batch: 80; loss: 2.05; acc: 0.12
Batch: 100; loss: 1.91; acc: 0.19
Batch: 120; loss: 2.03; acc: 0.23
Batch: 140; loss: 1.96; acc: 0.3
Batch: 160; loss: 1.94; acc: 0.22
Batch: 180; loss: 1.91; acc: 0.25
Batch: 200; loss: 2.01; acc: 0.19
Batch: 220; loss: 2.02; acc: 0.23
Batch: 240; loss: 1.95; acc: 0.3
Batch: 260; loss: 2.04; acc: 0.27
Batch: 280; loss: 2.03; acc: 0.17
Batch: 300; loss: 1.86; acc: 0.31
Batch: 320; loss: 1.91; acc: 0.25
Batch: 340; loss: 2.02; acc: 0.17
Batch: 360; loss: 2.11; acc: 0.25
Batch: 380; loss: 1.99; acc: 0.22
Batch: 400; loss: 2.08; acc: 0.23
Batch: 420; loss: 2.05; acc: 0.27
Batch: 440; loss: 1.85; acc: 0.3
Batch: 460; loss: 1.99; acc: 0.19
Batch: 480; loss: 1.94; acc: 0.33
Batch: 500; loss: 2.02; acc: 0.17
Batch: 520; loss: 2.09; acc: 0.22
Batch: 540; loss: 2.06; acc: 0.25
Batch: 560; loss: 2.05; acc: 0.22
Batch: 580; loss: 2.05; acc: 0.27
Batch: 600; loss: 2.05; acc: 0.22
Batch: 620; loss: 2.0; acc: 0.16
Batch: 640; loss: 2.01; acc: 0.22
Batch: 660; loss: 1.97; acc: 0.27
Batch: 680; loss: 1.88; acc: 0.33
Batch: 700; loss: 1.99; acc: 0.23
Batch: 720; loss: 1.9; acc: 0.28
Batch: 740; loss: 2.12; acc: 0.2
Batch: 760; loss: 1.87; acc: 0.28
Batch: 780; loss: 2.0; acc: 0.17
Train Epoch over. train_loss: 2.01; train_accuracy: 0.23 

Batch: 0; loss: 1.97; acc: 0.23
Batch: 20; loss: 2.08; acc: 0.2
Batch: 40; loss: 1.85; acc: 0.27
Batch: 60; loss: 1.92; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.19
Batch: 100; loss: 1.86; acc: 0.23
Batch: 120; loss: 1.97; acc: 0.27
Batch: 140; loss: 2.04; acc: 0.19
Val Epoch over. val_loss: 1.9980065875751958; val_accuracy: 0.23656449044585987 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.02; acc: 0.17
Batch: 20; loss: 2.03; acc: 0.27
Batch: 40; loss: 2.02; acc: 0.22
Batch: 60; loss: 1.9; acc: 0.28
Batch: 80; loss: 2.05; acc: 0.25
Batch: 100; loss: 2.09; acc: 0.25
Batch: 120; loss: 1.93; acc: 0.31
Batch: 140; loss: 2.02; acc: 0.23
Batch: 160; loss: 1.91; acc: 0.31
Batch: 180; loss: 1.91; acc: 0.23
Batch: 200; loss: 1.99; acc: 0.27
Batch: 220; loss: 1.91; acc: 0.3
Batch: 240; loss: 2.08; acc: 0.17
Batch: 260; loss: 1.91; acc: 0.33
Batch: 280; loss: 2.01; acc: 0.23
Batch: 300; loss: 1.9; acc: 0.27
Batch: 320; loss: 2.14; acc: 0.17
Batch: 340; loss: 2.05; acc: 0.2
Batch: 360; loss: 1.95; acc: 0.38
Batch: 380; loss: 2.03; acc: 0.23
Batch: 400; loss: 1.96; acc: 0.33
Batch: 420; loss: 2.09; acc: 0.19
Batch: 440; loss: 2.02; acc: 0.25
Batch: 460; loss: 2.09; acc: 0.11
Batch: 480; loss: 1.99; acc: 0.28
Batch: 500; loss: 1.94; acc: 0.27
Batch: 520; loss: 1.89; acc: 0.33
Batch: 540; loss: 1.98; acc: 0.25
Batch: 560; loss: 2.02; acc: 0.25
Batch: 580; loss: 1.97; acc: 0.27
Batch: 600; loss: 1.96; acc: 0.3
Batch: 620; loss: 2.08; acc: 0.19
Batch: 640; loss: 2.05; acc: 0.27
Batch: 660; loss: 2.08; acc: 0.25
Batch: 680; loss: 2.02; acc: 0.25
Batch: 700; loss: 1.92; acc: 0.33
Batch: 720; loss: 1.99; acc: 0.2
Batch: 740; loss: 1.99; acc: 0.22
Batch: 760; loss: 2.12; acc: 0.19
Batch: 780; loss: 2.0; acc: 0.28
Train Epoch over. train_loss: 2.0; train_accuracy: 0.24 

Batch: 0; loss: 1.98; acc: 0.23
Batch: 20; loss: 2.07; acc: 0.22
Batch: 40; loss: 1.86; acc: 0.27
Batch: 60; loss: 1.92; acc: 0.27
Batch: 80; loss: 1.98; acc: 0.22
Batch: 100; loss: 1.86; acc: 0.27
Batch: 120; loss: 1.97; acc: 0.31
Batch: 140; loss: 2.04; acc: 0.17
Val Epoch over. val_loss: 1.9946206398070998; val_accuracy: 0.2415406050955414 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.03; acc: 0.19
Batch: 20; loss: 1.97; acc: 0.31
Batch: 40; loss: 2.06; acc: 0.2
Batch: 60; loss: 2.02; acc: 0.23
Batch: 80; loss: 2.04; acc: 0.27
Batch: 100; loss: 2.04; acc: 0.22
Batch: 120; loss: 1.92; acc: 0.28
Batch: 140; loss: 1.96; acc: 0.28
Batch: 160; loss: 1.99; acc: 0.12
Batch: 180; loss: 1.91; acc: 0.23
Batch: 200; loss: 2.07; acc: 0.19
Batch: 220; loss: 1.98; acc: 0.3
Batch: 240; loss: 1.99; acc: 0.23
Batch: 260; loss: 2.09; acc: 0.19
Batch: 280; loss: 2.08; acc: 0.19
Batch: 300; loss: 2.05; acc: 0.27
Batch: 320; loss: 2.0; acc: 0.34
Batch: 340; loss: 2.17; acc: 0.23
Batch: 360; loss: 2.08; acc: 0.22
Batch: 380; loss: 1.96; acc: 0.23
Batch: 400; loss: 1.9; acc: 0.38
Batch: 420; loss: 2.09; acc: 0.2
Batch: 440; loss: 2.03; acc: 0.2
Batch: 460; loss: 2.07; acc: 0.19
Batch: 480; loss: 2.01; acc: 0.22
Batch: 500; loss: 1.95; acc: 0.28
Batch: 520; loss: 1.97; acc: 0.25
Batch: 540; loss: 1.95; acc: 0.28
Batch: 560; loss: 2.0; acc: 0.2
Batch: 580; loss: 2.03; acc: 0.3
Batch: 600; loss: 2.06; acc: 0.23
Batch: 620; loss: 1.92; acc: 0.34
Batch: 640; loss: 2.02; acc: 0.19
Batch: 660; loss: 2.06; acc: 0.22
Batch: 680; loss: 1.99; acc: 0.3
Batch: 700; loss: 2.1; acc: 0.14
Batch: 720; loss: 2.04; acc: 0.25
Batch: 740; loss: 2.06; acc: 0.23
Batch: 760; loss: 1.9; acc: 0.27
Batch: 780; loss: 1.99; acc: 0.17
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.0; acc: 0.27
Batch: 20; loss: 2.06; acc: 0.22
Batch: 40; loss: 1.86; acc: 0.27
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 1.98; acc: 0.25
Batch: 100; loss: 1.87; acc: 0.25
Batch: 120; loss: 1.96; acc: 0.34
Batch: 140; loss: 2.04; acc: 0.2
Val Epoch over. val_loss: 1.9926803787802434; val_accuracy: 0.24781050955414013 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.96; acc: 0.23
Batch: 20; loss: 2.09; acc: 0.2
Batch: 40; loss: 2.07; acc: 0.23
Batch: 60; loss: 2.02; acc: 0.25
Batch: 80; loss: 1.92; acc: 0.25
Batch: 100; loss: 2.01; acc: 0.22
Batch: 120; loss: 1.94; acc: 0.25
Batch: 140; loss: 2.01; acc: 0.25
Batch: 160; loss: 1.93; acc: 0.25
Batch: 180; loss: 2.1; acc: 0.23
Batch: 200; loss: 2.01; acc: 0.27
Batch: 220; loss: 2.0; acc: 0.25
Batch: 240; loss: 1.79; acc: 0.28
Batch: 260; loss: 1.98; acc: 0.28
Batch: 280; loss: 2.03; acc: 0.25
Batch: 300; loss: 2.17; acc: 0.14
Batch: 320; loss: 2.03; acc: 0.27
Batch: 340; loss: 2.07; acc: 0.22
Batch: 360; loss: 2.07; acc: 0.23
Batch: 380; loss: 1.88; acc: 0.3
Batch: 400; loss: 1.94; acc: 0.34
Batch: 420; loss: 1.97; acc: 0.25
Batch: 440; loss: 2.01; acc: 0.25
Batch: 460; loss: 1.91; acc: 0.3
Batch: 480; loss: 2.05; acc: 0.23
Batch: 500; loss: 1.94; acc: 0.31
Batch: 520; loss: 2.11; acc: 0.19
Batch: 540; loss: 2.06; acc: 0.25
Batch: 560; loss: 1.93; acc: 0.33
Batch: 580; loss: 1.87; acc: 0.3
Batch: 600; loss: 1.93; acc: 0.25
Batch: 620; loss: 1.87; acc: 0.3
Batch: 640; loss: 1.92; acc: 0.33
Batch: 660; loss: 1.98; acc: 0.27
Batch: 680; loss: 1.95; acc: 0.27
Batch: 700; loss: 1.86; acc: 0.28
Batch: 720; loss: 2.07; acc: 0.23
Batch: 740; loss: 2.11; acc: 0.17
Batch: 760; loss: 1.96; acc: 0.3
Batch: 780; loss: 2.0; acc: 0.27
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.0; acc: 0.25
Batch: 20; loss: 2.06; acc: 0.2
Batch: 40; loss: 1.86; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.27
Batch: 100; loss: 1.88; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.33
Batch: 140; loss: 2.04; acc: 0.2
Val Epoch over. val_loss: 1.9907317746217084; val_accuracy: 0.2538813694267516 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.05; acc: 0.25
Batch: 20; loss: 2.01; acc: 0.17
Batch: 40; loss: 2.0; acc: 0.16
Batch: 60; loss: 2.17; acc: 0.22
Batch: 80; loss: 1.99; acc: 0.23
Batch: 100; loss: 2.0; acc: 0.23
Batch: 120; loss: 1.97; acc: 0.31
Batch: 140; loss: 2.07; acc: 0.2
Batch: 160; loss: 2.13; acc: 0.22
Batch: 180; loss: 2.03; acc: 0.2
Batch: 200; loss: 1.95; acc: 0.3
Batch: 220; loss: 1.85; acc: 0.33
Batch: 240; loss: 1.98; acc: 0.3
Batch: 260; loss: 1.87; acc: 0.31
Batch: 280; loss: 2.14; acc: 0.22
Batch: 300; loss: 2.0; acc: 0.2
Batch: 320; loss: 1.99; acc: 0.25
Batch: 340; loss: 1.96; acc: 0.23
Batch: 360; loss: 1.94; acc: 0.31
Batch: 380; loss: 2.12; acc: 0.11
Batch: 400; loss: 1.91; acc: 0.28
Batch: 420; loss: 2.02; acc: 0.25
Batch: 440; loss: 2.06; acc: 0.17
Batch: 460; loss: 2.0; acc: 0.31
Batch: 480; loss: 1.81; acc: 0.38
Batch: 500; loss: 1.96; acc: 0.39
Batch: 520; loss: 2.15; acc: 0.25
Batch: 540; loss: 2.01; acc: 0.25
Batch: 560; loss: 1.97; acc: 0.31
Batch: 580; loss: 2.0; acc: 0.25
Batch: 600; loss: 2.16; acc: 0.14
Batch: 620; loss: 1.96; acc: 0.22
Batch: 640; loss: 1.96; acc: 0.25
Batch: 660; loss: 2.1; acc: 0.27
Batch: 680; loss: 2.03; acc: 0.33
Batch: 700; loss: 1.94; acc: 0.27
Batch: 720; loss: 2.0; acc: 0.25
Batch: 740; loss: 1.99; acc: 0.25
Batch: 760; loss: 2.0; acc: 0.3
Batch: 780; loss: 1.94; acc: 0.3
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.01; acc: 0.28
Batch: 20; loss: 2.05; acc: 0.19
Batch: 40; loss: 1.87; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.28
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.33
Batch: 140; loss: 2.03; acc: 0.2
Val Epoch over. val_loss: 1.9890392411286664; val_accuracy: 0.2561703821656051 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.01; acc: 0.27
Batch: 20; loss: 2.01; acc: 0.3
Batch: 40; loss: 1.98; acc: 0.28
Batch: 60; loss: 1.98; acc: 0.34
Batch: 80; loss: 1.99; acc: 0.28
Batch: 100; loss: 1.98; acc: 0.25
Batch: 120; loss: 1.84; acc: 0.36
Batch: 140; loss: 2.08; acc: 0.14
Batch: 160; loss: 1.88; acc: 0.28
Batch: 180; loss: 2.02; acc: 0.25
Batch: 200; loss: 2.06; acc: 0.22
Batch: 220; loss: 1.95; acc: 0.23
Batch: 240; loss: 2.07; acc: 0.2
Batch: 260; loss: 1.93; acc: 0.33
Batch: 280; loss: 2.12; acc: 0.16
Batch: 300; loss: 1.97; acc: 0.19
Batch: 320; loss: 2.17; acc: 0.12
Batch: 340; loss: 2.04; acc: 0.22
Batch: 360; loss: 2.1; acc: 0.27
Batch: 380; loss: 2.09; acc: 0.23
Batch: 400; loss: 2.01; acc: 0.3
Batch: 420; loss: 1.97; acc: 0.2
Batch: 440; loss: 1.87; acc: 0.36
Batch: 460; loss: 2.02; acc: 0.19
Batch: 480; loss: 1.91; acc: 0.31
Batch: 500; loss: 1.89; acc: 0.38
Batch: 520; loss: 1.91; acc: 0.3
Batch: 540; loss: 1.92; acc: 0.2
Batch: 560; loss: 2.04; acc: 0.23
Batch: 580; loss: 2.01; acc: 0.3
Batch: 600; loss: 1.9; acc: 0.25
Batch: 620; loss: 1.95; acc: 0.28
Batch: 640; loss: 1.88; acc: 0.38
Batch: 660; loss: 1.93; acc: 0.25
Batch: 680; loss: 1.93; acc: 0.28
Batch: 700; loss: 1.98; acc: 0.19
Batch: 720; loss: 1.89; acc: 0.28
Batch: 740; loss: 2.06; acc: 0.28
Batch: 760; loss: 2.13; acc: 0.19
Batch: 780; loss: 2.14; acc: 0.16
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.02; acc: 0.25
Batch: 20; loss: 2.04; acc: 0.19
Batch: 40; loss: 1.87; acc: 0.23
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.88; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.02; acc: 0.22
Val Epoch over. val_loss: 1.9869277211511212; val_accuracy: 0.2552746815286624 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.07; acc: 0.2
Batch: 20; loss: 1.93; acc: 0.25
Batch: 40; loss: 1.91; acc: 0.2
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.9; acc: 0.28
Batch: 100; loss: 2.05; acc: 0.22
Batch: 120; loss: 1.89; acc: 0.3
Batch: 140; loss: 1.85; acc: 0.33
Batch: 160; loss: 2.02; acc: 0.22
Batch: 180; loss: 2.1; acc: 0.16
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 2.0; acc: 0.25
Batch: 240; loss: 2.01; acc: 0.27
Batch: 260; loss: 1.89; acc: 0.3
Batch: 280; loss: 2.05; acc: 0.2
Batch: 300; loss: 1.92; acc: 0.33
Batch: 320; loss: 1.79; acc: 0.34
Batch: 340; loss: 1.91; acc: 0.3
Batch: 360; loss: 1.99; acc: 0.22
Batch: 380; loss: 1.93; acc: 0.23
Batch: 400; loss: 1.85; acc: 0.28
Batch: 420; loss: 1.88; acc: 0.36
Batch: 440; loss: 1.9; acc: 0.33
Batch: 460; loss: 2.14; acc: 0.25
Batch: 480; loss: 1.9; acc: 0.25
Batch: 500; loss: 2.01; acc: 0.23
Batch: 520; loss: 1.91; acc: 0.38
Batch: 540; loss: 2.05; acc: 0.3
Batch: 560; loss: 1.95; acc: 0.28
Batch: 580; loss: 1.93; acc: 0.19
Batch: 600; loss: 1.98; acc: 0.22
Batch: 620; loss: 1.96; acc: 0.23
Batch: 640; loss: 1.87; acc: 0.34
Batch: 660; loss: 2.04; acc: 0.28
Batch: 680; loss: 1.98; acc: 0.31
Batch: 700; loss: 1.99; acc: 0.25
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.01; acc: 0.31
Batch: 760; loss: 1.94; acc: 0.28
Batch: 780; loss: 2.05; acc: 0.27
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.02; acc: 0.23
Batch: 20; loss: 2.04; acc: 0.2
Batch: 40; loss: 1.87; acc: 0.23
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.88; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.02; acc: 0.22
Val Epoch over. val_loss: 1.9862910152240922; val_accuracy: 0.2541799363057325 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.17; acc: 0.2
Batch: 20; loss: 1.99; acc: 0.23
Batch: 40; loss: 1.99; acc: 0.27
Batch: 60; loss: 2.03; acc: 0.3
Batch: 80; loss: 2.03; acc: 0.28
Batch: 100; loss: 1.94; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.17
Batch: 140; loss: 1.98; acc: 0.27
Batch: 160; loss: 2.04; acc: 0.27
Batch: 180; loss: 1.92; acc: 0.3
Batch: 200; loss: 1.98; acc: 0.22
Batch: 220; loss: 2.1; acc: 0.2
Batch: 240; loss: 1.81; acc: 0.33
Batch: 260; loss: 2.04; acc: 0.17
Batch: 280; loss: 2.03; acc: 0.22
Batch: 300; loss: 1.9; acc: 0.3
Batch: 320; loss: 1.93; acc: 0.31
Batch: 340; loss: 1.92; acc: 0.31
Batch: 360; loss: 2.02; acc: 0.27
Batch: 380; loss: 2.03; acc: 0.22
Batch: 400; loss: 2.02; acc: 0.28
Batch: 420; loss: 2.03; acc: 0.17
Batch: 440; loss: 1.95; acc: 0.25
Batch: 460; loss: 2.18; acc: 0.19
Batch: 480; loss: 1.87; acc: 0.3
Batch: 500; loss: 1.94; acc: 0.33
Batch: 520; loss: 2.07; acc: 0.27
Batch: 540; loss: 1.95; acc: 0.36
Batch: 560; loss: 2.01; acc: 0.27
Batch: 580; loss: 2.11; acc: 0.23
Batch: 600; loss: 1.99; acc: 0.31
Batch: 620; loss: 2.04; acc: 0.27
Batch: 640; loss: 2.01; acc: 0.19
Batch: 660; loss: 2.03; acc: 0.19
Batch: 680; loss: 1.83; acc: 0.33
Batch: 700; loss: 1.97; acc: 0.27
Batch: 720; loss: 1.87; acc: 0.3
Batch: 740; loss: 2.11; acc: 0.2
Batch: 760; loss: 1.98; acc: 0.3
Batch: 780; loss: 2.08; acc: 0.19
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.25
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.87; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.02; acc: 0.23
Val Epoch over. val_loss: 1.9858470304756408; val_accuracy: 0.2533837579617834 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.0; acc: 0.38
Batch: 20; loss: 2.02; acc: 0.25
Batch: 40; loss: 2.08; acc: 0.28
Batch: 60; loss: 1.95; acc: 0.31
Batch: 80; loss: 2.01; acc: 0.27
Batch: 100; loss: 1.76; acc: 0.38
Batch: 120; loss: 1.97; acc: 0.28
Batch: 140; loss: 1.92; acc: 0.31
Batch: 160; loss: 1.82; acc: 0.34
Batch: 180; loss: 1.92; acc: 0.27
Batch: 200; loss: 2.07; acc: 0.22
Batch: 220; loss: 2.03; acc: 0.27
Batch: 240; loss: 2.14; acc: 0.19
Batch: 260; loss: 1.88; acc: 0.31
Batch: 280; loss: 2.07; acc: 0.22
Batch: 300; loss: 2.02; acc: 0.28
Batch: 320; loss: 2.09; acc: 0.17
Batch: 340; loss: 2.03; acc: 0.36
Batch: 360; loss: 1.97; acc: 0.22
Batch: 380; loss: 2.07; acc: 0.19
Batch: 400; loss: 2.05; acc: 0.2
Batch: 420; loss: 2.03; acc: 0.27
Batch: 440; loss: 1.93; acc: 0.31
Batch: 460; loss: 1.91; acc: 0.3
Batch: 480; loss: 2.01; acc: 0.25
Batch: 500; loss: 2.08; acc: 0.25
Batch: 520; loss: 1.91; acc: 0.34
Batch: 540; loss: 1.92; acc: 0.27
Batch: 560; loss: 1.97; acc: 0.27
Batch: 580; loss: 1.99; acc: 0.14
Batch: 600; loss: 1.97; acc: 0.23
Batch: 620; loss: 2.12; acc: 0.16
Batch: 640; loss: 1.99; acc: 0.27
Batch: 660; loss: 2.01; acc: 0.28
Batch: 680; loss: 2.03; acc: 0.25
Batch: 700; loss: 2.01; acc: 0.23
Batch: 720; loss: 2.04; acc: 0.2
Batch: 740; loss: 1.92; acc: 0.28
Batch: 760; loss: 2.08; acc: 0.23
Batch: 780; loss: 1.96; acc: 0.2
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.27
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9855084753340217; val_accuracy: 0.25308519108280253 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.07; acc: 0.25
Batch: 20; loss: 1.91; acc: 0.23
Batch: 40; loss: 1.98; acc: 0.23
Batch: 60; loss: 1.96; acc: 0.3
Batch: 80; loss: 2.11; acc: 0.17
Batch: 100; loss: 2.03; acc: 0.22
Batch: 120; loss: 2.02; acc: 0.14
Batch: 140; loss: 1.97; acc: 0.2
Batch: 160; loss: 1.93; acc: 0.27
Batch: 180; loss: 2.03; acc: 0.25
Batch: 200; loss: 1.97; acc: 0.27
Batch: 220; loss: 2.14; acc: 0.19
Batch: 240; loss: 2.03; acc: 0.31
Batch: 260; loss: 1.88; acc: 0.27
Batch: 280; loss: 2.11; acc: 0.22
Batch: 300; loss: 1.96; acc: 0.31
Batch: 320; loss: 1.97; acc: 0.28
Batch: 340; loss: 1.96; acc: 0.28
Batch: 360; loss: 2.11; acc: 0.25
Batch: 380; loss: 2.13; acc: 0.23
Batch: 400; loss: 1.98; acc: 0.25
Batch: 420; loss: 1.91; acc: 0.34
Batch: 440; loss: 2.0; acc: 0.23
Batch: 460; loss: 2.03; acc: 0.23
Batch: 480; loss: 2.04; acc: 0.33
Batch: 500; loss: 1.98; acc: 0.27
Batch: 520; loss: 2.01; acc: 0.25
Batch: 540; loss: 1.83; acc: 0.33
Batch: 560; loss: 1.82; acc: 0.33
Batch: 580; loss: 1.83; acc: 0.33
Batch: 600; loss: 1.98; acc: 0.22
Batch: 620; loss: 1.98; acc: 0.2
Batch: 640; loss: 2.01; acc: 0.2
Batch: 660; loss: 2.07; acc: 0.25
Batch: 680; loss: 2.0; acc: 0.25
Batch: 700; loss: 2.01; acc: 0.27
Batch: 720; loss: 1.96; acc: 0.25
Batch: 740; loss: 2.08; acc: 0.23
Batch: 760; loss: 1.98; acc: 0.25
Batch: 780; loss: 1.86; acc: 0.3
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.23
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.87; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.33
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9854727748093333; val_accuracy: 0.25398089171974525 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.12; acc: 0.17
Batch: 20; loss: 1.84; acc: 0.27
Batch: 40; loss: 1.85; acc: 0.3
Batch: 60; loss: 1.99; acc: 0.27
Batch: 80; loss: 1.94; acc: 0.3
Batch: 100; loss: 2.05; acc: 0.2
Batch: 120; loss: 2.02; acc: 0.23
Batch: 140; loss: 1.85; acc: 0.39
Batch: 160; loss: 1.95; acc: 0.3
Batch: 180; loss: 1.97; acc: 0.27
Batch: 200; loss: 1.87; acc: 0.31
Batch: 220; loss: 2.02; acc: 0.25
Batch: 240; loss: 2.1; acc: 0.19
Batch: 260; loss: 1.95; acc: 0.31
Batch: 280; loss: 1.95; acc: 0.27
Batch: 300; loss: 2.01; acc: 0.25
Batch: 320; loss: 1.96; acc: 0.27
Batch: 340; loss: 2.03; acc: 0.23
Batch: 360; loss: 2.0; acc: 0.27
Batch: 380; loss: 1.95; acc: 0.31
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.96; acc: 0.25
Batch: 440; loss: 1.86; acc: 0.31
Batch: 460; loss: 1.95; acc: 0.3
Batch: 480; loss: 1.92; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.22
Batch: 520; loss: 2.2; acc: 0.17
Batch: 540; loss: 1.98; acc: 0.2
Batch: 560; loss: 1.87; acc: 0.3
Batch: 580; loss: 2.14; acc: 0.2
Batch: 600; loss: 2.09; acc: 0.27
Batch: 620; loss: 2.0; acc: 0.27
Batch: 640; loss: 2.05; acc: 0.23
Batch: 660; loss: 2.02; acc: 0.23
Batch: 680; loss: 1.92; acc: 0.33
Batch: 700; loss: 2.23; acc: 0.17
Batch: 720; loss: 2.01; acc: 0.23
Batch: 740; loss: 1.88; acc: 0.31
Batch: 760; loss: 2.0; acc: 0.28
Batch: 780; loss: 2.0; acc: 0.22
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.23
Batch: 20; loss: 2.04; acc: 0.2
Batch: 40; loss: 1.87; acc: 0.23
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.36
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.985832026050349; val_accuracy: 0.25607085987261147 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.08; acc: 0.16
Batch: 20; loss: 1.93; acc: 0.3
Batch: 40; loss: 2.07; acc: 0.17
Batch: 60; loss: 2.09; acc: 0.25
Batch: 80; loss: 2.05; acc: 0.25
Batch: 100; loss: 2.17; acc: 0.14
Batch: 120; loss: 2.07; acc: 0.23
Batch: 140; loss: 2.06; acc: 0.25
Batch: 160; loss: 1.92; acc: 0.27
Batch: 180; loss: 1.95; acc: 0.33
Batch: 200; loss: 1.99; acc: 0.27
Batch: 220; loss: 1.92; acc: 0.3
Batch: 240; loss: 2.05; acc: 0.25
Batch: 260; loss: 1.98; acc: 0.23
Batch: 280; loss: 1.99; acc: 0.31
Batch: 300; loss: 2.03; acc: 0.22
Batch: 320; loss: 1.97; acc: 0.3
Batch: 340; loss: 1.98; acc: 0.25
Batch: 360; loss: 2.14; acc: 0.22
Batch: 380; loss: 2.04; acc: 0.28
Batch: 400; loss: 1.9; acc: 0.28
Batch: 420; loss: 1.83; acc: 0.36
Batch: 440; loss: 1.99; acc: 0.27
Batch: 460; loss: 1.89; acc: 0.25
Batch: 480; loss: 1.86; acc: 0.27
Batch: 500; loss: 1.98; acc: 0.28
Batch: 520; loss: 2.03; acc: 0.23
Batch: 540; loss: 1.99; acc: 0.27
Batch: 560; loss: 2.02; acc: 0.22
Batch: 580; loss: 2.08; acc: 0.3
Batch: 600; loss: 2.0; acc: 0.25
Batch: 620; loss: 2.01; acc: 0.17
Batch: 640; loss: 1.94; acc: 0.3
Batch: 660; loss: 2.0; acc: 0.22
Batch: 680; loss: 2.16; acc: 0.25
Batch: 700; loss: 2.14; acc: 0.22
Batch: 720; loss: 2.07; acc: 0.2
Batch: 740; loss: 2.08; acc: 0.17
Batch: 760; loss: 2.08; acc: 0.19
Batch: 780; loss: 1.91; acc: 0.3
Train Epoch over. train_loss: 2.0; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.23
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9851517912688528; val_accuracy: 0.25507563694267515 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.1; acc: 0.25
Batch: 20; loss: 2.18; acc: 0.17
Batch: 40; loss: 1.86; acc: 0.27
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 1.94; acc: 0.23
Batch: 100; loss: 1.83; acc: 0.33
Batch: 120; loss: 2.03; acc: 0.2
Batch: 140; loss: 2.17; acc: 0.17
Batch: 160; loss: 2.04; acc: 0.3
Batch: 180; loss: 1.99; acc: 0.36
Batch: 200; loss: 2.01; acc: 0.23
Batch: 220; loss: 1.87; acc: 0.28
Batch: 240; loss: 2.04; acc: 0.23
Batch: 260; loss: 2.03; acc: 0.28
Batch: 280; loss: 1.97; acc: 0.3
Batch: 300; loss: 2.0; acc: 0.23
Batch: 320; loss: 1.92; acc: 0.3
Batch: 340; loss: 1.89; acc: 0.28
Batch: 360; loss: 2.05; acc: 0.22
Batch: 380; loss: 2.04; acc: 0.31
Batch: 400; loss: 2.04; acc: 0.25
Batch: 420; loss: 1.87; acc: 0.36
Batch: 440; loss: 2.04; acc: 0.3
Batch: 460; loss: 2.08; acc: 0.19
Batch: 480; loss: 1.99; acc: 0.23
Batch: 500; loss: 1.91; acc: 0.27
Batch: 520; loss: 2.01; acc: 0.27
Batch: 540; loss: 2.07; acc: 0.28
Batch: 560; loss: 2.02; acc: 0.12
Batch: 580; loss: 1.95; acc: 0.27
Batch: 600; loss: 2.08; acc: 0.2
Batch: 620; loss: 1.79; acc: 0.39
Batch: 640; loss: 2.05; acc: 0.2
Batch: 660; loss: 1.91; acc: 0.3
Batch: 680; loss: 2.06; acc: 0.27
Batch: 700; loss: 1.87; acc: 0.28
Batch: 720; loss: 1.93; acc: 0.3
Batch: 740; loss: 1.93; acc: 0.25
Batch: 760; loss: 2.09; acc: 0.22
Batch: 780; loss: 1.9; acc: 0.34
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.23
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9852455581069752; val_accuracy: 0.254578025477707 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.05; acc: 0.23
Batch: 20; loss: 1.98; acc: 0.28
Batch: 40; loss: 2.1; acc: 0.17
Batch: 60; loss: 1.98; acc: 0.27
Batch: 80; loss: 2.03; acc: 0.22
Batch: 100; loss: 2.03; acc: 0.27
Batch: 120; loss: 1.97; acc: 0.36
Batch: 140; loss: 2.02; acc: 0.22
Batch: 160; loss: 1.93; acc: 0.28
Batch: 180; loss: 1.9; acc: 0.34
Batch: 200; loss: 1.94; acc: 0.27
Batch: 220; loss: 1.95; acc: 0.27
Batch: 240; loss: 1.94; acc: 0.3
Batch: 260; loss: 1.81; acc: 0.31
Batch: 280; loss: 1.99; acc: 0.27
Batch: 300; loss: 2.18; acc: 0.11
Batch: 320; loss: 2.08; acc: 0.25
Batch: 340; loss: 2.05; acc: 0.19
Batch: 360; loss: 1.98; acc: 0.31
Batch: 380; loss: 1.89; acc: 0.34
Batch: 400; loss: 2.03; acc: 0.23
Batch: 420; loss: 1.82; acc: 0.41
Batch: 440; loss: 2.03; acc: 0.36
Batch: 460; loss: 1.99; acc: 0.25
Batch: 480; loss: 2.04; acc: 0.2
Batch: 500; loss: 2.06; acc: 0.2
Batch: 520; loss: 2.04; acc: 0.23
Batch: 540; loss: 1.94; acc: 0.28
Batch: 560; loss: 2.04; acc: 0.31
Batch: 580; loss: 2.05; acc: 0.2
Batch: 600; loss: 2.17; acc: 0.12
Batch: 620; loss: 1.99; acc: 0.3
Batch: 640; loss: 2.03; acc: 0.17
Batch: 660; loss: 2.1; acc: 0.25
Batch: 680; loss: 1.85; acc: 0.39
Batch: 700; loss: 1.97; acc: 0.33
Batch: 720; loss: 2.06; acc: 0.27
Batch: 740; loss: 2.07; acc: 0.16
Batch: 760; loss: 2.05; acc: 0.28
Batch: 780; loss: 2.01; acc: 0.27
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.87; acc: 0.23
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9856667624917; val_accuracy: 0.25636942675159236 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.04; acc: 0.2
Batch: 20; loss: 1.88; acc: 0.27
Batch: 40; loss: 1.99; acc: 0.25
Batch: 60; loss: 2.15; acc: 0.17
Batch: 80; loss: 2.13; acc: 0.2
Batch: 100; loss: 1.99; acc: 0.25
Batch: 120; loss: 2.0; acc: 0.23
Batch: 140; loss: 1.92; acc: 0.31
Batch: 160; loss: 2.1; acc: 0.16
Batch: 180; loss: 1.9; acc: 0.28
Batch: 200; loss: 2.08; acc: 0.22
Batch: 220; loss: 1.93; acc: 0.33
Batch: 240; loss: 2.06; acc: 0.22
Batch: 260; loss: 2.03; acc: 0.28
Batch: 280; loss: 2.1; acc: 0.23
Batch: 300; loss: 1.87; acc: 0.22
Batch: 320; loss: 2.0; acc: 0.22
Batch: 340; loss: 1.93; acc: 0.23
Batch: 360; loss: 1.86; acc: 0.28
Batch: 380; loss: 1.87; acc: 0.33
Batch: 400; loss: 1.96; acc: 0.22
Batch: 420; loss: 1.98; acc: 0.25
Batch: 440; loss: 2.08; acc: 0.23
Batch: 460; loss: 2.0; acc: 0.23
Batch: 480; loss: 1.99; acc: 0.27
Batch: 500; loss: 2.22; acc: 0.2
Batch: 520; loss: 2.09; acc: 0.23
Batch: 540; loss: 1.97; acc: 0.23
Batch: 560; loss: 1.96; acc: 0.2
Batch: 580; loss: 2.17; acc: 0.22
Batch: 600; loss: 1.88; acc: 0.3
Batch: 620; loss: 2.15; acc: 0.16
Batch: 640; loss: 1.95; acc: 0.36
Batch: 660; loss: 2.05; acc: 0.17
Batch: 680; loss: 1.96; acc: 0.23
Batch: 700; loss: 2.0; acc: 0.2
Batch: 720; loss: 1.98; acc: 0.28
Batch: 740; loss: 1.91; acc: 0.23
Batch: 760; loss: 2.09; acc: 0.22
Batch: 780; loss: 2.05; acc: 0.25
Train Epoch over. train_loss: 2.0; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.2
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9852236691553882; val_accuracy: 0.2554737261146497 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.01; acc: 0.25
Batch: 20; loss: 1.96; acc: 0.27
Batch: 40; loss: 1.98; acc: 0.25
Batch: 60; loss: 1.9; acc: 0.31
Batch: 80; loss: 1.89; acc: 0.3
Batch: 100; loss: 1.97; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.23
Batch: 140; loss: 1.95; acc: 0.27
Batch: 160; loss: 1.9; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.17
Batch: 200; loss: 2.15; acc: 0.22
Batch: 220; loss: 2.01; acc: 0.23
Batch: 240; loss: 1.92; acc: 0.27
Batch: 260; loss: 1.86; acc: 0.31
Batch: 280; loss: 2.0; acc: 0.25
Batch: 300; loss: 1.92; acc: 0.3
Batch: 320; loss: 1.86; acc: 0.34
Batch: 340; loss: 1.99; acc: 0.27
Batch: 360; loss: 1.94; acc: 0.38
Batch: 380; loss: 2.0; acc: 0.2
Batch: 400; loss: 2.02; acc: 0.2
Batch: 420; loss: 1.96; acc: 0.34
Batch: 440; loss: 2.1; acc: 0.27
Batch: 460; loss: 2.02; acc: 0.27
Batch: 480; loss: 2.08; acc: 0.23
Batch: 500; loss: 2.1; acc: 0.17
Batch: 520; loss: 1.97; acc: 0.23
Batch: 540; loss: 2.0; acc: 0.22
Batch: 560; loss: 1.91; acc: 0.25
Batch: 580; loss: 2.02; acc: 0.19
Batch: 600; loss: 2.14; acc: 0.17
Batch: 620; loss: 1.99; acc: 0.25
Batch: 640; loss: 2.05; acc: 0.28
Batch: 660; loss: 2.05; acc: 0.25
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 2.15; acc: 0.19
Batch: 720; loss: 1.94; acc: 0.28
Batch: 740; loss: 1.98; acc: 0.22
Batch: 760; loss: 1.95; acc: 0.31
Batch: 780; loss: 2.07; acc: 0.2
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.985157716046473; val_accuracy: 0.25537420382165604 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.99; acc: 0.25
Batch: 20; loss: 1.98; acc: 0.31
Batch: 40; loss: 2.06; acc: 0.22
Batch: 60; loss: 1.87; acc: 0.28
Batch: 80; loss: 1.98; acc: 0.28
Batch: 100; loss: 1.95; acc: 0.25
Batch: 120; loss: 1.97; acc: 0.23
Batch: 140; loss: 2.04; acc: 0.22
Batch: 160; loss: 1.97; acc: 0.23
Batch: 180; loss: 1.94; acc: 0.3
Batch: 200; loss: 1.86; acc: 0.39
Batch: 220; loss: 2.07; acc: 0.16
Batch: 240; loss: 2.05; acc: 0.28
Batch: 260; loss: 2.04; acc: 0.25
Batch: 280; loss: 2.05; acc: 0.23
Batch: 300; loss: 1.96; acc: 0.2
Batch: 320; loss: 1.99; acc: 0.23
Batch: 340; loss: 2.01; acc: 0.23
Batch: 360; loss: 2.03; acc: 0.28
Batch: 380; loss: 2.05; acc: 0.23
Batch: 400; loss: 1.96; acc: 0.31
Batch: 420; loss: 1.97; acc: 0.25
Batch: 440; loss: 2.05; acc: 0.22
Batch: 460; loss: 2.03; acc: 0.23
Batch: 480; loss: 2.06; acc: 0.2
Batch: 500; loss: 2.11; acc: 0.2
Batch: 520; loss: 1.98; acc: 0.27
Batch: 540; loss: 1.97; acc: 0.28
Batch: 560; loss: 1.9; acc: 0.23
Batch: 580; loss: 1.92; acc: 0.23
Batch: 600; loss: 2.01; acc: 0.22
Batch: 620; loss: 2.0; acc: 0.23
Batch: 640; loss: 2.02; acc: 0.19
Batch: 660; loss: 2.03; acc: 0.23
Batch: 680; loss: 1.98; acc: 0.3
Batch: 700; loss: 2.01; acc: 0.27
Batch: 720; loss: 2.08; acc: 0.25
Batch: 740; loss: 1.94; acc: 0.23
Batch: 760; loss: 1.98; acc: 0.25
Batch: 780; loss: 2.1; acc: 0.28
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.98512386364542; val_accuracy: 0.2552746815286624 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 1.98; acc: 0.25
Batch: 40; loss: 1.97; acc: 0.3
Batch: 60; loss: 1.95; acc: 0.27
Batch: 80; loss: 2.07; acc: 0.2
Batch: 100; loss: 1.96; acc: 0.25
Batch: 120; loss: 2.01; acc: 0.28
Batch: 140; loss: 1.93; acc: 0.27
Batch: 160; loss: 2.15; acc: 0.17
Batch: 180; loss: 1.82; acc: 0.39
Batch: 200; loss: 1.96; acc: 0.27
Batch: 220; loss: 1.96; acc: 0.25
Batch: 240; loss: 1.96; acc: 0.25
Batch: 260; loss: 1.98; acc: 0.25
Batch: 280; loss: 2.12; acc: 0.22
Batch: 300; loss: 1.93; acc: 0.31
Batch: 320; loss: 2.02; acc: 0.22
Batch: 340; loss: 2.07; acc: 0.27
Batch: 360; loss: 2.17; acc: 0.19
Batch: 380; loss: 2.07; acc: 0.27
Batch: 400; loss: 2.01; acc: 0.22
Batch: 420; loss: 2.05; acc: 0.28
Batch: 440; loss: 1.94; acc: 0.36
Batch: 460; loss: 2.02; acc: 0.25
Batch: 480; loss: 2.08; acc: 0.14
Batch: 500; loss: 1.98; acc: 0.25
Batch: 520; loss: 2.07; acc: 0.23
Batch: 540; loss: 2.06; acc: 0.22
Batch: 560; loss: 2.02; acc: 0.22
Batch: 580; loss: 1.98; acc: 0.3
Batch: 600; loss: 2.09; acc: 0.19
Batch: 620; loss: 2.0; acc: 0.17
Batch: 640; loss: 1.91; acc: 0.27
Batch: 660; loss: 1.95; acc: 0.3
Batch: 680; loss: 2.07; acc: 0.2
Batch: 700; loss: 2.08; acc: 0.17
Batch: 720; loss: 2.02; acc: 0.2
Batch: 740; loss: 1.96; acc: 0.27
Batch: 760; loss: 1.98; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.27
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.23
Batch: 20; loss: 2.02; acc: 0.23
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.25
Batch: 100; loss: 1.89; acc: 0.22
Batch: 120; loss: 1.95; acc: 0.33
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9851726574502933; val_accuracy: 0.25308519108280253 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.96; acc: 0.3
Batch: 20; loss: 1.95; acc: 0.27
Batch: 40; loss: 2.02; acc: 0.33
Batch: 60; loss: 1.91; acc: 0.33
Batch: 80; loss: 1.99; acc: 0.2
Batch: 100; loss: 2.07; acc: 0.22
Batch: 120; loss: 2.12; acc: 0.28
Batch: 140; loss: 2.06; acc: 0.25
Batch: 160; loss: 1.98; acc: 0.28
Batch: 180; loss: 1.87; acc: 0.3
Batch: 200; loss: 2.03; acc: 0.27
Batch: 220; loss: 2.01; acc: 0.23
Batch: 240; loss: 2.07; acc: 0.23
Batch: 260; loss: 1.9; acc: 0.27
Batch: 280; loss: 1.92; acc: 0.27
Batch: 300; loss: 1.97; acc: 0.25
Batch: 320; loss: 2.09; acc: 0.2
Batch: 340; loss: 2.06; acc: 0.23
Batch: 360; loss: 1.98; acc: 0.23
Batch: 380; loss: 1.9; acc: 0.22
Batch: 400; loss: 1.91; acc: 0.34
Batch: 420; loss: 2.11; acc: 0.25
Batch: 440; loss: 2.21; acc: 0.16
Batch: 460; loss: 2.04; acc: 0.27
Batch: 480; loss: 2.03; acc: 0.19
Batch: 500; loss: 2.1; acc: 0.19
Batch: 520; loss: 2.08; acc: 0.2
Batch: 540; loss: 1.85; acc: 0.38
Batch: 560; loss: 2.05; acc: 0.2
Batch: 580; loss: 2.03; acc: 0.22
Batch: 600; loss: 2.04; acc: 0.3
Batch: 620; loss: 1.99; acc: 0.28
Batch: 640; loss: 1.81; acc: 0.36
Batch: 660; loss: 1.91; acc: 0.28
Batch: 680; loss: 1.96; acc: 0.3
Batch: 700; loss: 2.07; acc: 0.17
Batch: 720; loss: 1.99; acc: 0.25
Batch: 740; loss: 2.16; acc: 0.2
Batch: 760; loss: 2.12; acc: 0.16
Batch: 780; loss: 2.09; acc: 0.22
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.985049937940707; val_accuracy: 0.2548765923566879 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.81; acc: 0.3
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.15; acc: 0.14
Batch: 60; loss: 2.05; acc: 0.22
Batch: 80; loss: 2.06; acc: 0.19
Batch: 100; loss: 1.85; acc: 0.27
Batch: 120; loss: 1.98; acc: 0.22
Batch: 140; loss: 1.98; acc: 0.23
Batch: 160; loss: 1.87; acc: 0.28
Batch: 180; loss: 2.06; acc: 0.27
Batch: 200; loss: 2.02; acc: 0.23
Batch: 220; loss: 1.95; acc: 0.23
Batch: 240; loss: 2.05; acc: 0.23
Batch: 260; loss: 2.02; acc: 0.28
Batch: 280; loss: 1.99; acc: 0.19
Batch: 300; loss: 1.93; acc: 0.27
Batch: 320; loss: 1.92; acc: 0.22
Batch: 340; loss: 1.95; acc: 0.28
Batch: 360; loss: 2.01; acc: 0.25
Batch: 380; loss: 2.08; acc: 0.2
Batch: 400; loss: 1.97; acc: 0.3
Batch: 420; loss: 2.16; acc: 0.2
Batch: 440; loss: 2.04; acc: 0.2
Batch: 460; loss: 2.05; acc: 0.22
Batch: 480; loss: 1.83; acc: 0.34
Batch: 500; loss: 1.82; acc: 0.33
Batch: 520; loss: 1.97; acc: 0.25
Batch: 540; loss: 2.05; acc: 0.23
Batch: 560; loss: 2.07; acc: 0.19
Batch: 580; loss: 1.85; acc: 0.38
Batch: 600; loss: 2.04; acc: 0.22
Batch: 620; loss: 1.9; acc: 0.27
Batch: 640; loss: 1.93; acc: 0.22
Batch: 660; loss: 2.03; acc: 0.31
Batch: 680; loss: 1.95; acc: 0.27
Batch: 700; loss: 2.16; acc: 0.23
Batch: 720; loss: 1.77; acc: 0.44
Batch: 740; loss: 1.97; acc: 0.17
Batch: 760; loss: 1.92; acc: 0.28
Batch: 780; loss: 1.91; acc: 0.28
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.985105169047216; val_accuracy: 0.2552746815286624 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.09; acc: 0.25
Batch: 20; loss: 2.01; acc: 0.23
Batch: 40; loss: 2.04; acc: 0.19
Batch: 60; loss: 1.95; acc: 0.25
Batch: 80; loss: 2.02; acc: 0.28
Batch: 100; loss: 2.06; acc: 0.19
Batch: 120; loss: 1.91; acc: 0.41
Batch: 140; loss: 2.01; acc: 0.23
Batch: 160; loss: 2.03; acc: 0.28
Batch: 180; loss: 2.08; acc: 0.23
Batch: 200; loss: 2.02; acc: 0.3
Batch: 220; loss: 1.87; acc: 0.34
Batch: 240; loss: 2.0; acc: 0.19
Batch: 260; loss: 2.05; acc: 0.22
Batch: 280; loss: 2.02; acc: 0.27
Batch: 300; loss: 2.1; acc: 0.22
Batch: 320; loss: 1.93; acc: 0.3
Batch: 340; loss: 1.94; acc: 0.3
Batch: 360; loss: 1.85; acc: 0.31
Batch: 380; loss: 2.05; acc: 0.2
Batch: 400; loss: 2.01; acc: 0.22
Batch: 420; loss: 2.02; acc: 0.25
Batch: 440; loss: 2.14; acc: 0.19
Batch: 460; loss: 1.93; acc: 0.33
Batch: 480; loss: 2.02; acc: 0.27
Batch: 500; loss: 1.96; acc: 0.22
Batch: 520; loss: 1.88; acc: 0.28
Batch: 540; loss: 2.04; acc: 0.19
Batch: 560; loss: 2.05; acc: 0.2
Batch: 580; loss: 1.96; acc: 0.31
Batch: 600; loss: 1.9; acc: 0.34
Batch: 620; loss: 2.07; acc: 0.16
Batch: 640; loss: 1.9; acc: 0.25
Batch: 660; loss: 1.96; acc: 0.25
Batch: 680; loss: 2.08; acc: 0.16
Batch: 700; loss: 1.85; acc: 0.27
Batch: 720; loss: 1.86; acc: 0.31
Batch: 740; loss: 2.04; acc: 0.23
Batch: 760; loss: 2.01; acc: 0.27
Batch: 780; loss: 2.13; acc: 0.2
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.985116364090306; val_accuracy: 0.25567277070063693 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.99; acc: 0.27
Batch: 20; loss: 2.05; acc: 0.25
Batch: 40; loss: 2.14; acc: 0.22
Batch: 60; loss: 2.04; acc: 0.27
Batch: 80; loss: 1.87; acc: 0.28
Batch: 100; loss: 2.09; acc: 0.22
Batch: 120; loss: 1.91; acc: 0.28
Batch: 140; loss: 2.08; acc: 0.17
Batch: 160; loss: 2.0; acc: 0.27
Batch: 180; loss: 2.05; acc: 0.23
Batch: 200; loss: 1.9; acc: 0.34
Batch: 220; loss: 1.91; acc: 0.31
Batch: 240; loss: 2.0; acc: 0.25
Batch: 260; loss: 1.94; acc: 0.28
Batch: 280; loss: 1.91; acc: 0.19
Batch: 300; loss: 1.99; acc: 0.31
Batch: 320; loss: 1.92; acc: 0.34
Batch: 340; loss: 2.04; acc: 0.25
Batch: 360; loss: 1.98; acc: 0.33
Batch: 380; loss: 1.93; acc: 0.3
Batch: 400; loss: 1.9; acc: 0.3
Batch: 420; loss: 1.9; acc: 0.31
Batch: 440; loss: 2.0; acc: 0.19
Batch: 460; loss: 1.87; acc: 0.33
Batch: 480; loss: 1.92; acc: 0.28
Batch: 500; loss: 1.86; acc: 0.28
Batch: 520; loss: 1.85; acc: 0.31
Batch: 540; loss: 1.96; acc: 0.28
Batch: 560; loss: 1.9; acc: 0.27
Batch: 580; loss: 2.06; acc: 0.22
Batch: 600; loss: 2.07; acc: 0.23
Batch: 620; loss: 1.99; acc: 0.25
Batch: 640; loss: 1.98; acc: 0.28
Batch: 660; loss: 2.05; acc: 0.27
Batch: 680; loss: 1.99; acc: 0.25
Batch: 700; loss: 1.89; acc: 0.3
Batch: 720; loss: 2.0; acc: 0.31
Batch: 740; loss: 2.01; acc: 0.2
Batch: 760; loss: 2.25; acc: 0.17
Batch: 780; loss: 1.99; acc: 0.3
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9850208053163663; val_accuracy: 0.25507563694267515 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.23; acc: 0.14
Batch: 20; loss: 2.04; acc: 0.2
Batch: 40; loss: 2.06; acc: 0.2
Batch: 60; loss: 1.77; acc: 0.33
Batch: 80; loss: 2.18; acc: 0.16
Batch: 100; loss: 2.01; acc: 0.2
Batch: 120; loss: 1.95; acc: 0.25
Batch: 140; loss: 2.07; acc: 0.2
Batch: 160; loss: 1.96; acc: 0.27
Batch: 180; loss: 2.1; acc: 0.2
Batch: 200; loss: 1.99; acc: 0.28
Batch: 220; loss: 1.97; acc: 0.23
Batch: 240; loss: 2.02; acc: 0.27
Batch: 260; loss: 2.04; acc: 0.22
Batch: 280; loss: 1.88; acc: 0.3
Batch: 300; loss: 2.06; acc: 0.28
Batch: 320; loss: 1.94; acc: 0.34
Batch: 340; loss: 1.97; acc: 0.25
Batch: 360; loss: 1.92; acc: 0.31
Batch: 380; loss: 2.11; acc: 0.22
Batch: 400; loss: 2.05; acc: 0.23
Batch: 420; loss: 2.02; acc: 0.17
Batch: 440; loss: 2.06; acc: 0.2
Batch: 460; loss: 2.02; acc: 0.23
Batch: 480; loss: 2.03; acc: 0.2
Batch: 500; loss: 2.01; acc: 0.17
Batch: 520; loss: 1.88; acc: 0.31
Batch: 540; loss: 1.88; acc: 0.3
Batch: 560; loss: 2.17; acc: 0.19
Batch: 580; loss: 1.94; acc: 0.34
Batch: 600; loss: 1.95; acc: 0.23
Batch: 620; loss: 1.87; acc: 0.31
Batch: 640; loss: 1.99; acc: 0.3
Batch: 660; loss: 1.93; acc: 0.2
Batch: 680; loss: 2.06; acc: 0.27
Batch: 700; loss: 2.16; acc: 0.19
Batch: 720; loss: 2.02; acc: 0.17
Batch: 740; loss: 2.06; acc: 0.19
Batch: 760; loss: 2.14; acc: 0.25
Batch: 780; loss: 2.06; acc: 0.2
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9851561572141707; val_accuracy: 0.25557324840764334 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.99; acc: 0.25
Batch: 20; loss: 1.96; acc: 0.23
Batch: 40; loss: 2.01; acc: 0.28
Batch: 60; loss: 1.93; acc: 0.28
Batch: 80; loss: 1.86; acc: 0.36
Batch: 100; loss: 1.86; acc: 0.36
Batch: 120; loss: 1.96; acc: 0.3
Batch: 140; loss: 2.03; acc: 0.23
Batch: 160; loss: 1.93; acc: 0.3
Batch: 180; loss: 1.99; acc: 0.2
Batch: 200; loss: 1.96; acc: 0.25
Batch: 220; loss: 1.95; acc: 0.28
Batch: 240; loss: 1.89; acc: 0.31
Batch: 260; loss: 2.19; acc: 0.2
Batch: 280; loss: 1.89; acc: 0.25
Batch: 300; loss: 2.05; acc: 0.2
Batch: 320; loss: 1.96; acc: 0.22
Batch: 340; loss: 1.97; acc: 0.3
Batch: 360; loss: 1.9; acc: 0.27
Batch: 380; loss: 2.01; acc: 0.23
Batch: 400; loss: 2.01; acc: 0.2
Batch: 420; loss: 2.02; acc: 0.19
Batch: 440; loss: 1.94; acc: 0.25
Batch: 460; loss: 1.87; acc: 0.28
Batch: 480; loss: 1.98; acc: 0.28
Batch: 500; loss: 2.01; acc: 0.22
Batch: 520; loss: 2.01; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.23
Batch: 560; loss: 1.9; acc: 0.34
Batch: 580; loss: 2.0; acc: 0.25
Batch: 600; loss: 1.99; acc: 0.33
Batch: 620; loss: 2.05; acc: 0.23
Batch: 640; loss: 2.06; acc: 0.23
Batch: 660; loss: 2.21; acc: 0.17
Batch: 680; loss: 2.08; acc: 0.23
Batch: 700; loss: 1.95; acc: 0.23
Batch: 720; loss: 2.01; acc: 0.17
Batch: 740; loss: 2.0; acc: 0.28
Batch: 760; loss: 2.07; acc: 0.2
Batch: 780; loss: 1.96; acc: 0.23
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9850524823377087; val_accuracy: 0.2552746815286624 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.03; acc: 0.3
Batch: 20; loss: 2.06; acc: 0.19
Batch: 40; loss: 2.04; acc: 0.27
Batch: 60; loss: 2.04; acc: 0.23
Batch: 80; loss: 1.92; acc: 0.34
Batch: 100; loss: 1.78; acc: 0.36
Batch: 120; loss: 2.06; acc: 0.17
Batch: 140; loss: 2.0; acc: 0.23
Batch: 160; loss: 2.02; acc: 0.3
Batch: 180; loss: 1.82; acc: 0.36
Batch: 200; loss: 1.98; acc: 0.25
Batch: 220; loss: 1.87; acc: 0.39
Batch: 240; loss: 2.11; acc: 0.25
Batch: 260; loss: 2.05; acc: 0.22
Batch: 280; loss: 2.03; acc: 0.2
Batch: 300; loss: 1.95; acc: 0.38
Batch: 320; loss: 1.81; acc: 0.39
Batch: 340; loss: 1.99; acc: 0.3
Batch: 360; loss: 2.0; acc: 0.22
Batch: 380; loss: 1.93; acc: 0.3
Batch: 400; loss: 1.97; acc: 0.28
Batch: 420; loss: 1.93; acc: 0.27
Batch: 440; loss: 1.93; acc: 0.31
Batch: 460; loss: 1.98; acc: 0.23
Batch: 480; loss: 2.03; acc: 0.22
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.07; acc: 0.23
Batch: 540; loss: 1.95; acc: 0.23
Batch: 560; loss: 1.89; acc: 0.28
Batch: 580; loss: 1.95; acc: 0.25
Batch: 600; loss: 1.96; acc: 0.27
Batch: 620; loss: 2.07; acc: 0.17
Batch: 640; loss: 1.91; acc: 0.39
Batch: 660; loss: 2.03; acc: 0.28
Batch: 680; loss: 2.01; acc: 0.23
Batch: 700; loss: 2.01; acc: 0.23
Batch: 720; loss: 2.09; acc: 0.22
Batch: 740; loss: 2.03; acc: 0.23
Batch: 760; loss: 2.01; acc: 0.16
Batch: 780; loss: 2.03; acc: 0.2
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9851726020217701; val_accuracy: 0.2552746815286624 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.87; acc: 0.27
Batch: 20; loss: 1.96; acc: 0.3
Batch: 40; loss: 2.0; acc: 0.28
Batch: 60; loss: 2.02; acc: 0.22
Batch: 80; loss: 1.97; acc: 0.27
Batch: 100; loss: 2.02; acc: 0.25
Batch: 120; loss: 1.94; acc: 0.25
Batch: 140; loss: 1.88; acc: 0.3
Batch: 160; loss: 1.93; acc: 0.31
Batch: 180; loss: 2.04; acc: 0.22
Batch: 200; loss: 1.93; acc: 0.34
Batch: 220; loss: 1.88; acc: 0.33
Batch: 240; loss: 2.06; acc: 0.28
Batch: 260; loss: 1.99; acc: 0.25
Batch: 280; loss: 2.08; acc: 0.2
Batch: 300; loss: 2.04; acc: 0.22
Batch: 320; loss: 2.07; acc: 0.17
Batch: 340; loss: 2.08; acc: 0.2
Batch: 360; loss: 1.98; acc: 0.33
Batch: 380; loss: 1.77; acc: 0.39
Batch: 400; loss: 1.94; acc: 0.23
Batch: 420; loss: 1.95; acc: 0.23
Batch: 440; loss: 1.92; acc: 0.22
Batch: 460; loss: 2.01; acc: 0.27
Batch: 480; loss: 1.99; acc: 0.25
Batch: 500; loss: 2.09; acc: 0.27
Batch: 520; loss: 1.94; acc: 0.23
Batch: 540; loss: 2.06; acc: 0.22
Batch: 560; loss: 2.03; acc: 0.25
Batch: 580; loss: 1.94; acc: 0.28
Batch: 600; loss: 2.15; acc: 0.19
Batch: 620; loss: 1.97; acc: 0.28
Batch: 640; loss: 1.89; acc: 0.34
Batch: 660; loss: 1.87; acc: 0.33
Batch: 680; loss: 1.96; acc: 0.3
Batch: 700; loss: 2.07; acc: 0.17
Batch: 720; loss: 2.06; acc: 0.27
Batch: 740; loss: 2.06; acc: 0.19
Batch: 760; loss: 1.93; acc: 0.31
Batch: 780; loss: 2.03; acc: 0.16
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850570031791737; val_accuracy: 0.2546775477707006 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.06; acc: 0.17
Batch: 20; loss: 1.92; acc: 0.3
Batch: 40; loss: 2.01; acc: 0.31
Batch: 60; loss: 2.09; acc: 0.25
Batch: 80; loss: 2.04; acc: 0.19
Batch: 100; loss: 2.02; acc: 0.22
Batch: 120; loss: 2.02; acc: 0.31
Batch: 140; loss: 2.0; acc: 0.3
Batch: 160; loss: 1.91; acc: 0.31
Batch: 180; loss: 1.89; acc: 0.36
Batch: 200; loss: 1.8; acc: 0.3
Batch: 220; loss: 2.05; acc: 0.2
Batch: 240; loss: 2.02; acc: 0.27
Batch: 260; loss: 1.9; acc: 0.34
Batch: 280; loss: 2.06; acc: 0.23
Batch: 300; loss: 1.95; acc: 0.27
Batch: 320; loss: 2.05; acc: 0.23
Batch: 340; loss: 1.95; acc: 0.3
Batch: 360; loss: 1.97; acc: 0.27
Batch: 380; loss: 2.01; acc: 0.27
Batch: 400; loss: 2.07; acc: 0.14
Batch: 420; loss: 1.91; acc: 0.3
Batch: 440; loss: 2.01; acc: 0.22
Batch: 460; loss: 2.22; acc: 0.16
Batch: 480; loss: 2.01; acc: 0.22
Batch: 500; loss: 2.12; acc: 0.22
Batch: 520; loss: 1.99; acc: 0.23
Batch: 540; loss: 1.94; acc: 0.3
Batch: 560; loss: 2.01; acc: 0.19
Batch: 580; loss: 1.97; acc: 0.28
Batch: 600; loss: 2.02; acc: 0.25
Batch: 620; loss: 1.95; acc: 0.28
Batch: 640; loss: 1.99; acc: 0.28
Batch: 660; loss: 2.07; acc: 0.2
Batch: 680; loss: 1.86; acc: 0.34
Batch: 700; loss: 1.95; acc: 0.28
Batch: 720; loss: 2.01; acc: 0.28
Batch: 740; loss: 2.1; acc: 0.23
Batch: 760; loss: 2.07; acc: 0.17
Batch: 780; loss: 1.94; acc: 0.27
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.02; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.28
Batch: 80; loss: 2.0; acc: 0.25
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.984910802476725; val_accuracy: 0.25437898089171973 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.14
Batch: 20; loss: 2.02; acc: 0.25
Batch: 40; loss: 1.93; acc: 0.28
Batch: 60; loss: 2.12; acc: 0.19
Batch: 80; loss: 1.92; acc: 0.28
Batch: 100; loss: 1.97; acc: 0.31
Batch: 120; loss: 1.97; acc: 0.23
Batch: 140; loss: 1.98; acc: 0.33
Batch: 160; loss: 2.04; acc: 0.22
Batch: 180; loss: 1.97; acc: 0.2
Batch: 200; loss: 2.14; acc: 0.2
Batch: 220; loss: 2.0; acc: 0.23
Batch: 240; loss: 1.85; acc: 0.34
Batch: 260; loss: 2.11; acc: 0.19
Batch: 280; loss: 2.12; acc: 0.31
Batch: 300; loss: 1.92; acc: 0.31
Batch: 320; loss: 1.99; acc: 0.27
Batch: 340; loss: 2.16; acc: 0.2
Batch: 360; loss: 2.16; acc: 0.22
Batch: 380; loss: 2.07; acc: 0.22
Batch: 400; loss: 2.23; acc: 0.17
Batch: 420; loss: 2.01; acc: 0.22
Batch: 440; loss: 2.06; acc: 0.2
Batch: 460; loss: 1.94; acc: 0.25
Batch: 480; loss: 1.93; acc: 0.25
Batch: 500; loss: 1.73; acc: 0.39
Batch: 520; loss: 1.93; acc: 0.23
Batch: 540; loss: 2.18; acc: 0.19
Batch: 560; loss: 1.88; acc: 0.34
Batch: 580; loss: 1.98; acc: 0.25
Batch: 600; loss: 2.05; acc: 0.17
Batch: 620; loss: 2.03; acc: 0.19
Batch: 640; loss: 2.11; acc: 0.17
Batch: 660; loss: 1.99; acc: 0.27
Batch: 680; loss: 1.96; acc: 0.27
Batch: 700; loss: 1.98; acc: 0.22
Batch: 720; loss: 1.89; acc: 0.27
Batch: 740; loss: 1.84; acc: 0.34
Batch: 760; loss: 2.02; acc: 0.25
Batch: 780; loss: 1.98; acc: 0.19
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.25
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9849081153322936; val_accuracy: 0.2541799363057325 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.03; acc: 0.27
Batch: 20; loss: 1.98; acc: 0.23
Batch: 40; loss: 2.08; acc: 0.25
Batch: 60; loss: 2.08; acc: 0.25
Batch: 80; loss: 1.87; acc: 0.31
Batch: 100; loss: 1.91; acc: 0.3
Batch: 120; loss: 1.97; acc: 0.23
Batch: 140; loss: 1.98; acc: 0.22
Batch: 160; loss: 2.0; acc: 0.28
Batch: 180; loss: 2.04; acc: 0.22
Batch: 200; loss: 1.91; acc: 0.28
Batch: 220; loss: 2.07; acc: 0.3
Batch: 240; loss: 2.07; acc: 0.22
Batch: 260; loss: 1.95; acc: 0.25
Batch: 280; loss: 1.95; acc: 0.25
Batch: 300; loss: 2.09; acc: 0.2
Batch: 320; loss: 1.99; acc: 0.22
Batch: 340; loss: 1.92; acc: 0.23
Batch: 360; loss: 2.02; acc: 0.19
Batch: 380; loss: 2.0; acc: 0.16
Batch: 400; loss: 2.03; acc: 0.22
Batch: 420; loss: 1.91; acc: 0.31
Batch: 440; loss: 2.11; acc: 0.19
Batch: 460; loss: 1.86; acc: 0.3
Batch: 480; loss: 1.97; acc: 0.31
Batch: 500; loss: 2.07; acc: 0.2
Batch: 520; loss: 1.93; acc: 0.3
Batch: 540; loss: 2.0; acc: 0.23
Batch: 560; loss: 1.87; acc: 0.3
Batch: 580; loss: 2.12; acc: 0.23
Batch: 600; loss: 2.02; acc: 0.23
Batch: 620; loss: 2.12; acc: 0.22
Batch: 640; loss: 1.98; acc: 0.3
Batch: 660; loss: 2.24; acc: 0.11
Batch: 680; loss: 2.14; acc: 0.23
Batch: 700; loss: 1.97; acc: 0.27
Batch: 720; loss: 1.8; acc: 0.31
Batch: 740; loss: 2.05; acc: 0.17
Batch: 760; loss: 2.02; acc: 0.28
Batch: 780; loss: 2.02; acc: 0.31
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850019816380398; val_accuracy: 0.2549761146496815 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.06; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.28
Batch: 40; loss: 2.16; acc: 0.2
Batch: 60; loss: 2.06; acc: 0.25
Batch: 80; loss: 1.95; acc: 0.3
Batch: 100; loss: 2.0; acc: 0.31
Batch: 120; loss: 1.99; acc: 0.19
Batch: 140; loss: 1.94; acc: 0.23
Batch: 160; loss: 1.84; acc: 0.31
Batch: 180; loss: 2.11; acc: 0.16
Batch: 200; loss: 2.03; acc: 0.2
Batch: 220; loss: 2.06; acc: 0.25
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 1.98; acc: 0.23
Batch: 280; loss: 2.13; acc: 0.16
Batch: 300; loss: 2.0; acc: 0.14
Batch: 320; loss: 2.07; acc: 0.23
Batch: 340; loss: 2.05; acc: 0.2
Batch: 360; loss: 2.0; acc: 0.2
Batch: 380; loss: 1.94; acc: 0.28
Batch: 400; loss: 1.99; acc: 0.28
Batch: 420; loss: 2.06; acc: 0.22
Batch: 440; loss: 1.94; acc: 0.22
Batch: 460; loss: 2.03; acc: 0.23
Batch: 480; loss: 2.04; acc: 0.23
Batch: 500; loss: 1.9; acc: 0.28
Batch: 520; loss: 2.03; acc: 0.27
Batch: 540; loss: 2.0; acc: 0.22
Batch: 560; loss: 1.99; acc: 0.27
Batch: 580; loss: 1.94; acc: 0.31
Batch: 600; loss: 2.04; acc: 0.2
Batch: 620; loss: 1.92; acc: 0.33
Batch: 640; loss: 1.93; acc: 0.23
Batch: 660; loss: 1.96; acc: 0.2
Batch: 680; loss: 2.11; acc: 0.17
Batch: 700; loss: 2.0; acc: 0.23
Batch: 720; loss: 2.12; acc: 0.19
Batch: 740; loss: 2.06; acc: 0.22
Batch: 760; loss: 2.02; acc: 0.28
Batch: 780; loss: 1.92; acc: 0.23
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.9850552309850218; val_accuracy: 0.2551751592356688 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.96; acc: 0.22
Batch: 20; loss: 2.05; acc: 0.23
Batch: 40; loss: 1.95; acc: 0.23
Batch: 60; loss: 1.93; acc: 0.22
Batch: 80; loss: 1.99; acc: 0.23
Batch: 100; loss: 2.11; acc: 0.2
Batch: 120; loss: 1.86; acc: 0.34
Batch: 140; loss: 1.96; acc: 0.23
Batch: 160; loss: 2.0; acc: 0.28
Batch: 180; loss: 2.13; acc: 0.19
Batch: 200; loss: 2.08; acc: 0.2
Batch: 220; loss: 2.01; acc: 0.27
Batch: 240; loss: 2.09; acc: 0.25
Batch: 260; loss: 1.86; acc: 0.3
Batch: 280; loss: 1.76; acc: 0.34
Batch: 300; loss: 2.02; acc: 0.23
Batch: 320; loss: 1.99; acc: 0.19
Batch: 340; loss: 2.01; acc: 0.2
Batch: 360; loss: 1.97; acc: 0.33
Batch: 380; loss: 1.99; acc: 0.17
Batch: 400; loss: 2.09; acc: 0.22
Batch: 420; loss: 2.04; acc: 0.27
Batch: 440; loss: 2.16; acc: 0.19
Batch: 460; loss: 1.96; acc: 0.28
Batch: 480; loss: 2.09; acc: 0.17
Batch: 500; loss: 1.82; acc: 0.33
Batch: 520; loss: 2.01; acc: 0.23
Batch: 540; loss: 2.05; acc: 0.17
Batch: 560; loss: 1.93; acc: 0.27
Batch: 580; loss: 1.87; acc: 0.39
Batch: 600; loss: 2.0; acc: 0.27
Batch: 620; loss: 2.09; acc: 0.23
Batch: 640; loss: 1.94; acc: 0.33
Batch: 660; loss: 1.97; acc: 0.28
Batch: 680; loss: 2.0; acc: 0.25
Batch: 700; loss: 1.91; acc: 0.3
Batch: 720; loss: 2.14; acc: 0.19
Batch: 740; loss: 2.03; acc: 0.25
Batch: 760; loss: 2.03; acc: 0.19
Batch: 780; loss: 1.9; acc: 0.36
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.25
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.23
Val Epoch over. val_loss: 1.985086299811199; val_accuracy: 0.25537420382165604 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.11; acc: 0.22
Batch: 20; loss: 2.04; acc: 0.2
Batch: 40; loss: 2.09; acc: 0.23
Batch: 60; loss: 1.85; acc: 0.33
Batch: 80; loss: 2.09; acc: 0.27
Batch: 100; loss: 1.98; acc: 0.23
Batch: 120; loss: 2.0; acc: 0.27
Batch: 140; loss: 2.04; acc: 0.28
Batch: 160; loss: 2.07; acc: 0.23
Batch: 180; loss: 2.03; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.23
Batch: 220; loss: 2.08; acc: 0.25
Batch: 240; loss: 2.06; acc: 0.2
Batch: 260; loss: 1.93; acc: 0.36
Batch: 280; loss: 1.76; acc: 0.38
Batch: 300; loss: 2.02; acc: 0.22
Batch: 320; loss: 1.93; acc: 0.25
Batch: 340; loss: 1.91; acc: 0.31
Batch: 360; loss: 2.04; acc: 0.23
Batch: 380; loss: 2.07; acc: 0.17
Batch: 400; loss: 2.24; acc: 0.19
Batch: 420; loss: 1.87; acc: 0.33
Batch: 440; loss: 1.92; acc: 0.25
Batch: 460; loss: 2.01; acc: 0.22
Batch: 480; loss: 1.85; acc: 0.23
Batch: 500; loss: 1.97; acc: 0.23
Batch: 520; loss: 2.05; acc: 0.19
Batch: 540; loss: 1.94; acc: 0.31
Batch: 560; loss: 1.95; acc: 0.3
Batch: 580; loss: 2.01; acc: 0.27
Batch: 600; loss: 1.98; acc: 0.2
Batch: 620; loss: 2.0; acc: 0.22
Batch: 640; loss: 2.01; acc: 0.17
Batch: 660; loss: 1.93; acc: 0.23
Batch: 680; loss: 2.08; acc: 0.22
Batch: 700; loss: 1.95; acc: 0.25
Batch: 720; loss: 2.02; acc: 0.2
Batch: 740; loss: 1.95; acc: 0.3
Batch: 760; loss: 2.08; acc: 0.22
Batch: 780; loss: 2.13; acc: 0.22
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.985011648220621; val_accuracy: 0.254578025477707 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.05; acc: 0.2
Batch: 20; loss: 2.02; acc: 0.2
Batch: 40; loss: 1.81; acc: 0.36
Batch: 60; loss: 2.06; acc: 0.25
Batch: 80; loss: 2.01; acc: 0.2
Batch: 100; loss: 1.9; acc: 0.27
Batch: 120; loss: 2.06; acc: 0.27
Batch: 140; loss: 2.11; acc: 0.19
Batch: 160; loss: 1.96; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.27
Batch: 200; loss: 2.06; acc: 0.16
Batch: 220; loss: 2.06; acc: 0.23
Batch: 240; loss: 1.99; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.98; acc: 0.2
Batch: 300; loss: 1.98; acc: 0.25
Batch: 320; loss: 2.11; acc: 0.2
Batch: 340; loss: 1.9; acc: 0.22
Batch: 360; loss: 1.93; acc: 0.34
Batch: 380; loss: 1.98; acc: 0.2
Batch: 400; loss: 1.85; acc: 0.33
Batch: 420; loss: 2.05; acc: 0.19
Batch: 440; loss: 1.89; acc: 0.28
Batch: 460; loss: 1.94; acc: 0.25
Batch: 480; loss: 2.02; acc: 0.27
Batch: 500; loss: 2.08; acc: 0.23
Batch: 520; loss: 1.98; acc: 0.3
Batch: 540; loss: 1.98; acc: 0.23
Batch: 560; loss: 2.1; acc: 0.25
Batch: 580; loss: 1.94; acc: 0.23
Batch: 600; loss: 1.92; acc: 0.33
Batch: 620; loss: 2.1; acc: 0.2
Batch: 640; loss: 1.99; acc: 0.28
Batch: 660; loss: 1.97; acc: 0.38
Batch: 680; loss: 1.96; acc: 0.28
Batch: 700; loss: 1.95; acc: 0.3
Batch: 720; loss: 2.03; acc: 0.25
Batch: 740; loss: 1.89; acc: 0.3
Batch: 760; loss: 1.98; acc: 0.28
Batch: 780; loss: 1.96; acc: 0.27
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850200558923612; val_accuracy: 0.25477707006369427 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.96; acc: 0.22
Batch: 20; loss: 1.93; acc: 0.3
Batch: 40; loss: 2.06; acc: 0.16
Batch: 60; loss: 2.09; acc: 0.23
Batch: 80; loss: 2.05; acc: 0.16
Batch: 100; loss: 1.89; acc: 0.3
Batch: 120; loss: 1.99; acc: 0.28
Batch: 140; loss: 2.07; acc: 0.16
Batch: 160; loss: 1.92; acc: 0.28
Batch: 180; loss: 2.13; acc: 0.25
Batch: 200; loss: 2.09; acc: 0.22
Batch: 220; loss: 2.08; acc: 0.19
Batch: 240; loss: 2.04; acc: 0.23
Batch: 260; loss: 2.03; acc: 0.27
Batch: 280; loss: 2.09; acc: 0.16
Batch: 300; loss: 2.03; acc: 0.23
Batch: 320; loss: 2.03; acc: 0.19
Batch: 340; loss: 2.07; acc: 0.17
Batch: 360; loss: 1.95; acc: 0.3
Batch: 380; loss: 1.94; acc: 0.28
Batch: 400; loss: 2.06; acc: 0.27
Batch: 420; loss: 1.9; acc: 0.27
Batch: 440; loss: 1.93; acc: 0.33
Batch: 460; loss: 2.0; acc: 0.28
Batch: 480; loss: 1.99; acc: 0.23
Batch: 500; loss: 2.09; acc: 0.22
Batch: 520; loss: 1.89; acc: 0.27
Batch: 540; loss: 2.04; acc: 0.27
Batch: 560; loss: 2.11; acc: 0.2
Batch: 580; loss: 1.92; acc: 0.28
Batch: 600; loss: 2.03; acc: 0.19
Batch: 620; loss: 2.03; acc: 0.22
Batch: 640; loss: 1.9; acc: 0.25
Batch: 660; loss: 1.98; acc: 0.3
Batch: 680; loss: 2.08; acc: 0.17
Batch: 700; loss: 1.98; acc: 0.22
Batch: 720; loss: 1.84; acc: 0.3
Batch: 740; loss: 1.95; acc: 0.27
Batch: 760; loss: 1.88; acc: 0.3
Batch: 780; loss: 2.12; acc: 0.19
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850370770047425; val_accuracy: 0.2549761146496815 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.92; acc: 0.36
Batch: 20; loss: 2.05; acc: 0.33
Batch: 40; loss: 2.13; acc: 0.2
Batch: 60; loss: 2.04; acc: 0.19
Batch: 80; loss: 1.95; acc: 0.25
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 1.96; acc: 0.25
Batch: 140; loss: 2.08; acc: 0.22
Batch: 160; loss: 1.84; acc: 0.31
Batch: 180; loss: 1.97; acc: 0.27
Batch: 200; loss: 1.92; acc: 0.34
Batch: 220; loss: 1.99; acc: 0.3
Batch: 240; loss: 2.04; acc: 0.25
Batch: 260; loss: 1.92; acc: 0.33
Batch: 280; loss: 1.91; acc: 0.31
Batch: 300; loss: 2.1; acc: 0.25
Batch: 320; loss: 1.93; acc: 0.3
Batch: 340; loss: 1.91; acc: 0.22
Batch: 360; loss: 1.99; acc: 0.3
Batch: 380; loss: 2.08; acc: 0.23
Batch: 400; loss: 1.98; acc: 0.27
Batch: 420; loss: 1.99; acc: 0.22
Batch: 440; loss: 1.91; acc: 0.27
Batch: 460; loss: 1.95; acc: 0.25
Batch: 480; loss: 2.02; acc: 0.27
Batch: 500; loss: 2.05; acc: 0.22
Batch: 520; loss: 2.13; acc: 0.23
Batch: 540; loss: 1.92; acc: 0.33
Batch: 560; loss: 1.9; acc: 0.33
Batch: 580; loss: 1.99; acc: 0.19
Batch: 600; loss: 2.04; acc: 0.23
Batch: 620; loss: 2.05; acc: 0.23
Batch: 640; loss: 1.89; acc: 0.33
Batch: 660; loss: 1.96; acc: 0.25
Batch: 680; loss: 1.98; acc: 0.2
Batch: 700; loss: 1.99; acc: 0.23
Batch: 720; loss: 2.04; acc: 0.2
Batch: 740; loss: 1.92; acc: 0.28
Batch: 760; loss: 1.96; acc: 0.23
Batch: 780; loss: 2.01; acc: 0.25
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850118107097163; val_accuracy: 0.254578025477707 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.9; acc: 0.27
Batch: 20; loss: 2.11; acc: 0.23
Batch: 40; loss: 1.88; acc: 0.33
Batch: 60; loss: 2.18; acc: 0.17
Batch: 80; loss: 2.01; acc: 0.2
Batch: 100; loss: 2.06; acc: 0.23
Batch: 120; loss: 2.15; acc: 0.16
Batch: 140; loss: 2.12; acc: 0.17
Batch: 160; loss: 1.92; acc: 0.3
Batch: 180; loss: 1.91; acc: 0.28
Batch: 200; loss: 2.07; acc: 0.22
Batch: 220; loss: 2.07; acc: 0.23
Batch: 240; loss: 1.96; acc: 0.23
Batch: 260; loss: 1.92; acc: 0.3
Batch: 280; loss: 2.0; acc: 0.25
Batch: 300; loss: 2.09; acc: 0.16
Batch: 320; loss: 1.98; acc: 0.23
Batch: 340; loss: 1.96; acc: 0.33
Batch: 360; loss: 1.97; acc: 0.14
Batch: 380; loss: 2.0; acc: 0.22
Batch: 400; loss: 2.1; acc: 0.25
Batch: 420; loss: 2.02; acc: 0.25
Batch: 440; loss: 1.89; acc: 0.34
Batch: 460; loss: 1.99; acc: 0.28
Batch: 480; loss: 1.8; acc: 0.41
Batch: 500; loss: 2.14; acc: 0.2
Batch: 520; loss: 2.06; acc: 0.25
Batch: 540; loss: 2.08; acc: 0.23
Batch: 560; loss: 2.05; acc: 0.23
Batch: 580; loss: 2.01; acc: 0.3
Batch: 600; loss: 1.84; acc: 0.39
Batch: 620; loss: 1.88; acc: 0.25
Batch: 640; loss: 1.93; acc: 0.22
Batch: 660; loss: 2.09; acc: 0.14
Batch: 680; loss: 2.06; acc: 0.22
Batch: 700; loss: 1.97; acc: 0.23
Batch: 720; loss: 1.95; acc: 0.3
Batch: 740; loss: 2.09; acc: 0.14
Batch: 760; loss: 1.88; acc: 0.31
Batch: 780; loss: 2.05; acc: 0.23
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9849936779896924; val_accuracy: 0.2546775477707006 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.11; acc: 0.2
Batch: 20; loss: 1.97; acc: 0.25
Batch: 40; loss: 1.96; acc: 0.34
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 1.86; acc: 0.33
Batch: 100; loss: 1.95; acc: 0.27
Batch: 120; loss: 1.88; acc: 0.34
Batch: 140; loss: 1.95; acc: 0.39
Batch: 160; loss: 1.97; acc: 0.36
Batch: 180; loss: 2.0; acc: 0.23
Batch: 200; loss: 1.92; acc: 0.28
Batch: 220; loss: 1.95; acc: 0.27
Batch: 240; loss: 1.92; acc: 0.3
Batch: 260; loss: 2.05; acc: 0.31
Batch: 280; loss: 2.09; acc: 0.16
Batch: 300; loss: 2.09; acc: 0.28
Batch: 320; loss: 1.96; acc: 0.25
Batch: 340; loss: 1.95; acc: 0.2
Batch: 360; loss: 1.94; acc: 0.31
Batch: 380; loss: 1.96; acc: 0.3
Batch: 400; loss: 2.0; acc: 0.28
Batch: 420; loss: 1.92; acc: 0.28
Batch: 440; loss: 2.09; acc: 0.2
Batch: 460; loss: 2.1; acc: 0.16
Batch: 480; loss: 2.04; acc: 0.17
Batch: 500; loss: 2.11; acc: 0.2
Batch: 520; loss: 1.94; acc: 0.31
Batch: 540; loss: 1.98; acc: 0.27
Batch: 560; loss: 2.08; acc: 0.25
Batch: 580; loss: 2.18; acc: 0.27
Batch: 600; loss: 1.97; acc: 0.23
Batch: 620; loss: 1.94; acc: 0.34
Batch: 640; loss: 1.91; acc: 0.3
Batch: 660; loss: 1.97; acc: 0.28
Batch: 680; loss: 2.03; acc: 0.27
Batch: 700; loss: 2.09; acc: 0.22
Batch: 720; loss: 2.11; acc: 0.25
Batch: 740; loss: 2.12; acc: 0.16
Batch: 760; loss: 1.97; acc: 0.34
Batch: 780; loss: 2.05; acc: 0.17
Train Epoch over. train_loss: 1.99; train_accuracy: 0.25 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850173421726105; val_accuracy: 0.2549761146496815 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.07; acc: 0.2
Batch: 20; loss: 2.19; acc: 0.22
Batch: 40; loss: 1.97; acc: 0.33
Batch: 60; loss: 1.89; acc: 0.28
Batch: 80; loss: 1.99; acc: 0.28
Batch: 100; loss: 2.08; acc: 0.25
Batch: 120; loss: 2.06; acc: 0.2
Batch: 140; loss: 2.02; acc: 0.25
Batch: 160; loss: 2.09; acc: 0.2
Batch: 180; loss: 2.14; acc: 0.17
Batch: 200; loss: 2.14; acc: 0.19
Batch: 220; loss: 2.1; acc: 0.19
Batch: 240; loss: 1.97; acc: 0.3
Batch: 260; loss: 2.07; acc: 0.22
Batch: 280; loss: 2.03; acc: 0.22
Batch: 300; loss: 2.03; acc: 0.17
Batch: 320; loss: 2.1; acc: 0.22
Batch: 340; loss: 1.93; acc: 0.3
Batch: 360; loss: 2.04; acc: 0.27
Batch: 380; loss: 1.94; acc: 0.34
Batch: 400; loss: 2.15; acc: 0.19
Batch: 420; loss: 2.08; acc: 0.19
Batch: 440; loss: 2.07; acc: 0.22
Batch: 460; loss: 1.96; acc: 0.28
Batch: 480; loss: 1.99; acc: 0.23
Batch: 500; loss: 2.0; acc: 0.34
Batch: 520; loss: 2.02; acc: 0.2
Batch: 540; loss: 2.14; acc: 0.12
Batch: 560; loss: 2.04; acc: 0.2
Batch: 580; loss: 1.99; acc: 0.23
Batch: 600; loss: 1.98; acc: 0.3
Batch: 620; loss: 1.97; acc: 0.28
Batch: 640; loss: 2.09; acc: 0.2
Batch: 660; loss: 1.92; acc: 0.3
Batch: 680; loss: 1.97; acc: 0.33
Batch: 700; loss: 2.03; acc: 0.22
Batch: 720; loss: 1.87; acc: 0.28
Batch: 740; loss: 1.97; acc: 0.27
Batch: 760; loss: 1.93; acc: 0.27
Batch: 780; loss: 1.96; acc: 0.25
Train Epoch over. train_loss: 1.99; train_accuracy: 0.26 

Batch: 0; loss: 2.03; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.25
Batch: 60; loss: 1.91; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 1.89; acc: 0.23
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 2.01; acc: 0.22
Val Epoch over. val_loss: 1.9850290846672787; val_accuracy: 0.25507563694267515 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 10677
elements in E: 2249500
fraction nonzero: 0.004746388086241387
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.14
Batch: 280; loss: 2.29; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.29; acc: 0.19
Batch: 340; loss: 2.31; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.29; acc: 0.14
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.3; acc: 0.16
Batch: 500; loss: 2.3; acc: 0.16
Batch: 520; loss: 2.3; acc: 0.05
Batch: 540; loss: 2.29; acc: 0.14
Batch: 560; loss: 2.3; acc: 0.09
Batch: 580; loss: 2.29; acc: 0.19
Batch: 600; loss: 2.29; acc: 0.12
Batch: 620; loss: 2.29; acc: 0.19
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.3; acc: 0.05
Batch: 760; loss: 2.29; acc: 0.06
Batch: 780; loss: 2.28; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.12 

Batch: 0; loss: 2.28; acc: 0.11
Batch: 20; loss: 2.28; acc: 0.23
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.17
Val Epoch over. val_loss: 2.2889101657138506; val_accuracy: 0.12430334394904459 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.28; acc: 0.12
Batch: 160; loss: 2.28; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.28; acc: 0.17
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.11
Batch: 260; loss: 2.27; acc: 0.16
Batch: 280; loss: 2.27; acc: 0.17
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.27; acc: 0.19
Batch: 340; loss: 2.28; acc: 0.22
Batch: 360; loss: 2.27; acc: 0.09
Batch: 380; loss: 2.26; acc: 0.2
Batch: 400; loss: 2.27; acc: 0.23
Batch: 420; loss: 2.25; acc: 0.25
Batch: 440; loss: 2.26; acc: 0.19
Batch: 460; loss: 2.25; acc: 0.34
Batch: 480; loss: 2.24; acc: 0.25
Batch: 500; loss: 2.24; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.25
Batch: 540; loss: 2.25; acc: 0.36
Batch: 560; loss: 2.25; acc: 0.22
Batch: 580; loss: 2.23; acc: 0.33
Batch: 600; loss: 2.24; acc: 0.27
Batch: 620; loss: 2.23; acc: 0.28
Batch: 640; loss: 2.21; acc: 0.39
Batch: 660; loss: 2.17; acc: 0.44
Batch: 680; loss: 2.21; acc: 0.31
Batch: 700; loss: 2.2; acc: 0.39
Batch: 720; loss: 2.14; acc: 0.45
Batch: 740; loss: 2.15; acc: 0.36
Batch: 760; loss: 2.11; acc: 0.42
Batch: 780; loss: 2.14; acc: 0.31
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.08; acc: 0.42
Batch: 20; loss: 2.12; acc: 0.33
Batch: 40; loss: 2.06; acc: 0.56
Batch: 60; loss: 2.05; acc: 0.48
Batch: 80; loss: 2.06; acc: 0.33
Batch: 100; loss: 2.16; acc: 0.34
Batch: 120; loss: 2.04; acc: 0.56
Batch: 140; loss: 2.13; acc: 0.36
Val Epoch over. val_loss: 2.100902903611493; val_accuracy: 0.39828821656050956 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.12; acc: 0.44
Batch: 20; loss: 2.15; acc: 0.38
Batch: 40; loss: 2.09; acc: 0.44
Batch: 60; loss: 1.95; acc: 0.47
Batch: 80; loss: 1.98; acc: 0.45
Batch: 100; loss: 1.86; acc: 0.5
Batch: 120; loss: 1.81; acc: 0.47
Batch: 140; loss: 1.81; acc: 0.41
Batch: 160; loss: 1.85; acc: 0.39
Batch: 180; loss: 1.68; acc: 0.53
Batch: 200; loss: 1.59; acc: 0.45
Batch: 220; loss: 1.66; acc: 0.45
Batch: 240; loss: 1.56; acc: 0.48
Batch: 260; loss: 1.52; acc: 0.55
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.54; acc: 0.47
Batch: 340; loss: 1.63; acc: 0.44
Batch: 360; loss: 1.42; acc: 0.5
Batch: 380; loss: 1.44; acc: 0.52
Batch: 400; loss: 1.41; acc: 0.56
Batch: 420; loss: 1.42; acc: 0.52
Batch: 440; loss: 1.38; acc: 0.55
Batch: 460; loss: 1.41; acc: 0.53
Batch: 480; loss: 1.45; acc: 0.52
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.45; acc: 0.52
Batch: 540; loss: 1.07; acc: 0.66
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 1.73; acc: 0.5
Batch: 600; loss: 1.46; acc: 0.47
Batch: 620; loss: 1.07; acc: 0.62
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.24; acc: 0.58
Batch: 680; loss: 1.47; acc: 0.5
Batch: 700; loss: 1.46; acc: 0.53
Batch: 720; loss: 1.31; acc: 0.47
Batch: 740; loss: 1.08; acc: 0.59
Batch: 760; loss: 1.02; acc: 0.73
Batch: 780; loss: 1.42; acc: 0.55
Train Epoch over. train_loss: 1.52; train_accuracy: 0.52 

Batch: 0; loss: 1.38; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.08; acc: 0.62
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.3621759486805862; val_accuracy: 0.5625995222929936 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.55
Batch: 20; loss: 1.21; acc: 0.56
Batch: 40; loss: 1.25; acc: 0.58
Batch: 60; loss: 1.26; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.56
Batch: 100; loss: 1.32; acc: 0.53
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.12; acc: 0.64
Batch: 160; loss: 1.31; acc: 0.55
Batch: 180; loss: 1.18; acc: 0.61
Batch: 200; loss: 1.2; acc: 0.56
Batch: 220; loss: 1.48; acc: 0.58
Batch: 240; loss: 1.65; acc: 0.44
Batch: 260; loss: 1.31; acc: 0.61
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 1.17; acc: 0.64
Batch: 320; loss: 1.46; acc: 0.61
Batch: 340; loss: 1.11; acc: 0.66
Batch: 360; loss: 1.03; acc: 0.72
Batch: 380; loss: 1.17; acc: 0.58
Batch: 400; loss: 0.93; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.59
Batch: 440; loss: 0.94; acc: 0.69
Batch: 460; loss: 1.0; acc: 0.69
Batch: 480; loss: 1.11; acc: 0.58
Batch: 500; loss: 1.11; acc: 0.66
Batch: 520; loss: 1.37; acc: 0.58
Batch: 540; loss: 1.45; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.61
Batch: 580; loss: 0.9; acc: 0.67
Batch: 600; loss: 0.99; acc: 0.67
Batch: 620; loss: 1.55; acc: 0.48
Batch: 640; loss: 1.35; acc: 0.61
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 1.15; acc: 0.64
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.49; acc: 0.56
Batch: 740; loss: 0.83; acc: 0.8
Batch: 760; loss: 1.08; acc: 0.73
Batch: 780; loss: 1.08; acc: 0.69
Train Epoch over. train_loss: 1.19; train_accuracy: 0.62 

Batch: 0; loss: 1.16; acc: 0.58
Batch: 20; loss: 1.43; acc: 0.48
Batch: 40; loss: 0.91; acc: 0.64
Batch: 60; loss: 1.01; acc: 0.59
Batch: 80; loss: 0.83; acc: 0.69
Batch: 100; loss: 1.26; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 0.82; acc: 0.75
Val Epoch over. val_loss: 1.0912839743741758; val_accuracy: 0.6533638535031847 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.59
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 0.84; acc: 0.7
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 1.33; acc: 0.58
Batch: 100; loss: 1.32; acc: 0.56
Batch: 120; loss: 1.1; acc: 0.61
Batch: 140; loss: 1.33; acc: 0.64
Batch: 160; loss: 1.08; acc: 0.7
Batch: 180; loss: 1.17; acc: 0.62
Batch: 200; loss: 1.11; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.59
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 1.26; acc: 0.53
Batch: 280; loss: 0.87; acc: 0.62
Batch: 300; loss: 1.26; acc: 0.64
Batch: 320; loss: 1.06; acc: 0.67
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.07; acc: 0.62
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 0.97; acc: 0.73
Batch: 420; loss: 1.41; acc: 0.56
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 1.17; acc: 0.5
Batch: 480; loss: 1.11; acc: 0.61
Batch: 500; loss: 1.35; acc: 0.47
Batch: 520; loss: 1.04; acc: 0.66
Batch: 540; loss: 1.44; acc: 0.52
Batch: 560; loss: 0.91; acc: 0.7
Batch: 580; loss: 0.87; acc: 0.75
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 1.56; acc: 0.5
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.64
Batch: 680; loss: 1.02; acc: 0.66
Batch: 700; loss: 1.26; acc: 0.66
Batch: 720; loss: 1.26; acc: 0.59
Batch: 740; loss: 1.23; acc: 0.56
Batch: 760; loss: 1.03; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.67
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.18; acc: 0.52
Batch: 20; loss: 1.46; acc: 0.45
Batch: 40; loss: 0.9; acc: 0.69
Batch: 60; loss: 0.99; acc: 0.64
Batch: 80; loss: 0.82; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.64
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.79; acc: 0.75
Val Epoch over. val_loss: 1.0915346502498458; val_accuracy: 0.6476910828025477 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.48
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 1.14; acc: 0.61
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.12; acc: 0.58
Batch: 100; loss: 0.98; acc: 0.69
Batch: 120; loss: 0.89; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 1.6; acc: 0.48
Batch: 200; loss: 0.97; acc: 0.66
Batch: 220; loss: 0.92; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.59
Batch: 260; loss: 1.01; acc: 0.72
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.87; acc: 0.7
Batch: 320; loss: 1.26; acc: 0.64
Batch: 340; loss: 1.01; acc: 0.66
Batch: 360; loss: 1.32; acc: 0.55
Batch: 380; loss: 1.31; acc: 0.58
Batch: 400; loss: 1.22; acc: 0.62
Batch: 420; loss: 1.08; acc: 0.59
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.62
Batch: 500; loss: 1.27; acc: 0.69
Batch: 520; loss: 1.16; acc: 0.64
Batch: 540; loss: 1.04; acc: 0.64
Batch: 560; loss: 1.26; acc: 0.69
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 1.15; acc: 0.64
Batch: 620; loss: 1.38; acc: 0.55
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 1.03; acc: 0.69
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 0.89; acc: 0.72
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 1.02; acc: 0.62
Batch: 780; loss: 0.81; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.07; acc: 0.59
Batch: 20; loss: 1.51; acc: 0.45
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 0.74; acc: 0.8
Val Epoch over. val_loss: 1.0615868017931653; val_accuracy: 0.6571457006369427 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.61
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.58
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 1.51; acc: 0.48
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 1.28; acc: 0.64
Batch: 240; loss: 0.85; acc: 0.8
Batch: 260; loss: 1.02; acc: 0.62
Batch: 280; loss: 0.88; acc: 0.75
Batch: 300; loss: 0.98; acc: 0.64
Batch: 320; loss: 0.98; acc: 0.7
Batch: 340; loss: 1.14; acc: 0.58
Batch: 360; loss: 1.34; acc: 0.59
Batch: 380; loss: 1.15; acc: 0.66
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 1.04; acc: 0.66
Batch: 440; loss: 1.14; acc: 0.61
Batch: 460; loss: 1.13; acc: 0.55
Batch: 480; loss: 1.32; acc: 0.58
Batch: 500; loss: 1.24; acc: 0.59
Batch: 520; loss: 1.07; acc: 0.64
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.03; acc: 0.67
Batch: 600; loss: 1.3; acc: 0.58
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.02; acc: 0.66
Batch: 660; loss: 0.94; acc: 0.64
Batch: 680; loss: 1.14; acc: 0.66
Batch: 700; loss: 1.28; acc: 0.56
Batch: 720; loss: 1.11; acc: 0.61
Batch: 740; loss: 0.96; acc: 0.69
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 1.36; acc: 0.55
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.2; acc: 0.55
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.61
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.86; acc: 0.73
Val Epoch over. val_loss: 1.1203261274061385; val_accuracy: 0.6400278662420382 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 1.26; acc: 0.59
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.08; acc: 0.64
Batch: 100; loss: 0.94; acc: 0.62
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 1.17; acc: 0.62
Batch: 160; loss: 0.98; acc: 0.66
Batch: 180; loss: 0.98; acc: 0.72
Batch: 200; loss: 0.86; acc: 0.73
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.38; acc: 0.58
Batch: 260; loss: 1.05; acc: 0.64
Batch: 280; loss: 1.07; acc: 0.59
Batch: 300; loss: 1.06; acc: 0.59
Batch: 320; loss: 1.0; acc: 0.7
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 1.34; acc: 0.5
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 1.28; acc: 0.53
Batch: 420; loss: 1.0; acc: 0.64
Batch: 440; loss: 1.27; acc: 0.56
Batch: 460; loss: 1.37; acc: 0.55
Batch: 480; loss: 0.95; acc: 0.64
Batch: 500; loss: 1.15; acc: 0.66
Batch: 520; loss: 1.3; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.64
Batch: 560; loss: 1.24; acc: 0.55
Batch: 580; loss: 1.04; acc: 0.66
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.07; acc: 0.67
Batch: 640; loss: 1.33; acc: 0.56
Batch: 660; loss: 1.09; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.64
Batch: 700; loss: 1.06; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.61
Batch: 740; loss: 1.24; acc: 0.58
Batch: 760; loss: 1.2; acc: 0.62
Batch: 780; loss: 0.82; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.65 

Batch: 0; loss: 1.06; acc: 0.58
Batch: 20; loss: 1.51; acc: 0.44
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.93; acc: 0.67
Batch: 80; loss: 0.88; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 0.77; acc: 0.78
Val Epoch over. val_loss: 1.0799561066991965; val_accuracy: 0.640625 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.61
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 0.89; acc: 0.72
Batch: 160; loss: 1.1; acc: 0.62
Batch: 180; loss: 1.34; acc: 0.58
Batch: 200; loss: 0.92; acc: 0.72
Batch: 220; loss: 1.03; acc: 0.67
Batch: 240; loss: 1.13; acc: 0.59
Batch: 260; loss: 1.1; acc: 0.64
Batch: 280; loss: 1.24; acc: 0.58
Batch: 300; loss: 1.24; acc: 0.61
Batch: 320; loss: 1.01; acc: 0.7
Batch: 340; loss: 1.31; acc: 0.55
Batch: 360; loss: 1.33; acc: 0.52
Batch: 380; loss: 1.1; acc: 0.64
Batch: 400; loss: 1.26; acc: 0.58
Batch: 420; loss: 1.7; acc: 0.48
Batch: 440; loss: 0.96; acc: 0.61
Batch: 460; loss: 1.41; acc: 0.61
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 1.26; acc: 0.61
Batch: 520; loss: 0.96; acc: 0.7
Batch: 540; loss: 1.23; acc: 0.64
Batch: 560; loss: 1.14; acc: 0.62
Batch: 580; loss: 1.36; acc: 0.53
Batch: 600; loss: 1.12; acc: 0.59
Batch: 620; loss: 0.88; acc: 0.72
Batch: 640; loss: 1.29; acc: 0.64
Batch: 660; loss: 1.12; acc: 0.66
Batch: 680; loss: 1.33; acc: 0.55
Batch: 700; loss: 1.06; acc: 0.58
Batch: 720; loss: 1.29; acc: 0.59
Batch: 740; loss: 1.32; acc: 0.5
Batch: 760; loss: 1.03; acc: 0.64
Batch: 780; loss: 0.89; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.64 

Batch: 0; loss: 1.07; acc: 0.62
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.79; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 0.72; acc: 0.8
Val Epoch over. val_loss: 1.0277191804852455; val_accuracy: 0.681031050955414 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.77
Batch: 60; loss: 1.26; acc: 0.59
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.12; acc: 0.58
Batch: 120; loss: 1.38; acc: 0.56
Batch: 140; loss: 1.11; acc: 0.61
Batch: 160; loss: 0.99; acc: 0.7
Batch: 180; loss: 1.06; acc: 0.69
Batch: 200; loss: 1.07; acc: 0.66
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.19; acc: 0.64
Batch: 260; loss: 0.89; acc: 0.72
Batch: 280; loss: 1.22; acc: 0.59
Batch: 300; loss: 1.11; acc: 0.56
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.2; acc: 0.61
Batch: 380; loss: 0.97; acc: 0.69
Batch: 400; loss: 1.03; acc: 0.64
Batch: 420; loss: 1.08; acc: 0.7
Batch: 440; loss: 1.21; acc: 0.56
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 1.36; acc: 0.56
Batch: 500; loss: 1.03; acc: 0.67
Batch: 520; loss: 0.95; acc: 0.7
Batch: 540; loss: 0.96; acc: 0.7
Batch: 560; loss: 1.28; acc: 0.62
Batch: 580; loss: 1.03; acc: 0.58
Batch: 600; loss: 1.03; acc: 0.62
Batch: 620; loss: 0.95; acc: 0.69
Batch: 640; loss: 1.12; acc: 0.62
Batch: 660; loss: 0.97; acc: 0.73
Batch: 680; loss: 1.28; acc: 0.59
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 1.05; acc: 0.7
Batch: 760; loss: 1.07; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.56
Train Epoch over. train_loss: 1.11; train_accuracy: 0.65 

Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 1.4; acc: 0.55
Batch: 40; loss: 0.86; acc: 0.67
Batch: 60; loss: 0.94; acc: 0.66
Batch: 80; loss: 0.79; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.59
Batch: 140; loss: 0.79; acc: 0.73
Val Epoch over. val_loss: 1.0661793294226287; val_accuracy: 0.6620222929936306 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.37; acc: 0.64
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.61
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 1.13; acc: 0.64
Batch: 100; loss: 1.1; acc: 0.61
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 0.99; acc: 0.73
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.87; acc: 0.7
Batch: 220; loss: 1.06; acc: 0.69
Batch: 240; loss: 1.06; acc: 0.66
Batch: 260; loss: 1.08; acc: 0.59
Batch: 280; loss: 0.99; acc: 0.72
Batch: 300; loss: 0.96; acc: 0.7
Batch: 320; loss: 1.01; acc: 0.67
Batch: 340; loss: 1.31; acc: 0.59
Batch: 360; loss: 1.07; acc: 0.66
Batch: 380; loss: 1.16; acc: 0.61
Batch: 400; loss: 0.96; acc: 0.62
Batch: 420; loss: 1.03; acc: 0.62
Batch: 440; loss: 0.98; acc: 0.66
Batch: 460; loss: 0.92; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.72
Batch: 500; loss: 1.36; acc: 0.66
Batch: 520; loss: 0.95; acc: 0.72
Batch: 540; loss: 1.26; acc: 0.56
Batch: 560; loss: 1.18; acc: 0.7
Batch: 580; loss: 1.44; acc: 0.55
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 1.12; acc: 0.66
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.01; acc: 0.69
Batch: 680; loss: 1.08; acc: 0.67
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 0.82; acc: 0.73
Batch: 760; loss: 1.0; acc: 0.64
Batch: 780; loss: 1.29; acc: 0.55
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.04; acc: 0.64
Batch: 20; loss: 1.37; acc: 0.53
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.66
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.007566746822588; val_accuracy: 0.6857085987261147 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.04; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.67
Batch: 40; loss: 1.01; acc: 0.72
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.21; acc: 0.58
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 1.02; acc: 0.66
Batch: 180; loss: 1.03; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.58
Batch: 220; loss: 1.0; acc: 0.66
Batch: 240; loss: 1.03; acc: 0.67
Batch: 260; loss: 0.96; acc: 0.77
Batch: 280; loss: 1.1; acc: 0.67
Batch: 300; loss: 0.97; acc: 0.66
Batch: 320; loss: 0.9; acc: 0.77
Batch: 340; loss: 0.92; acc: 0.7
Batch: 360; loss: 0.81; acc: 0.7
Batch: 380; loss: 1.14; acc: 0.58
Batch: 400; loss: 1.17; acc: 0.59
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 0.99; acc: 0.7
Batch: 460; loss: 1.06; acc: 0.56
Batch: 480; loss: 0.98; acc: 0.64
Batch: 500; loss: 1.36; acc: 0.56
Batch: 520; loss: 0.91; acc: 0.73
Batch: 540; loss: 0.93; acc: 0.77
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 1.29; acc: 0.59
Batch: 600; loss: 1.05; acc: 0.59
Batch: 620; loss: 1.03; acc: 0.7
Batch: 640; loss: 1.01; acc: 0.7
Batch: 660; loss: 1.32; acc: 0.56
Batch: 680; loss: 1.01; acc: 0.62
Batch: 700; loss: 1.19; acc: 0.59
Batch: 720; loss: 1.25; acc: 0.58
Batch: 740; loss: 1.24; acc: 0.59
Batch: 760; loss: 1.17; acc: 0.67
Batch: 780; loss: 1.27; acc: 0.58
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.02; acc: 0.59
Batch: 20; loss: 1.47; acc: 0.47
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.72
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 0.72; acc: 0.77
Val Epoch over. val_loss: 1.0384749174118042; val_accuracy: 0.6659036624203821 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.69
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 1.22; acc: 0.56
Batch: 80; loss: 1.2; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.7
Batch: 160; loss: 0.94; acc: 0.69
Batch: 180; loss: 1.32; acc: 0.64
Batch: 200; loss: 1.08; acc: 0.64
Batch: 220; loss: 1.17; acc: 0.66
Batch: 240; loss: 0.92; acc: 0.66
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 0.98; acc: 0.69
Batch: 320; loss: 1.14; acc: 0.61
Batch: 340; loss: 0.91; acc: 0.73
Batch: 360; loss: 1.17; acc: 0.62
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 1.24; acc: 0.61
Batch: 420; loss: 1.3; acc: 0.52
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 1.09; acc: 0.62
Batch: 480; loss: 1.0; acc: 0.66
Batch: 500; loss: 1.05; acc: 0.64
Batch: 520; loss: 1.16; acc: 0.64
Batch: 540; loss: 1.1; acc: 0.62
Batch: 560; loss: 1.11; acc: 0.61
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 0.86; acc: 0.69
Batch: 620; loss: 1.06; acc: 0.7
Batch: 640; loss: 1.28; acc: 0.59
Batch: 660; loss: 1.33; acc: 0.55
Batch: 680; loss: 0.97; acc: 0.72
Batch: 700; loss: 1.28; acc: 0.59
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.19; acc: 0.62
Batch: 760; loss: 1.02; acc: 0.7
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.05; acc: 0.64
Batch: 20; loss: 1.44; acc: 0.45
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.89; acc: 0.69
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 0.72; acc: 0.81
Val Epoch over. val_loss: 1.0206956827336815; val_accuracy: 0.6769506369426752 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.62
Batch: 20; loss: 1.09; acc: 0.55
Batch: 40; loss: 1.12; acc: 0.64
Batch: 60; loss: 0.89; acc: 0.69
Batch: 80; loss: 1.39; acc: 0.55
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.67
Batch: 160; loss: 0.92; acc: 0.67
Batch: 180; loss: 1.18; acc: 0.56
Batch: 200; loss: 1.23; acc: 0.55
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 1.05; acc: 0.62
Batch: 260; loss: 0.94; acc: 0.73
Batch: 280; loss: 1.01; acc: 0.69
Batch: 300; loss: 0.94; acc: 0.7
Batch: 320; loss: 1.16; acc: 0.59
Batch: 340; loss: 1.25; acc: 0.58
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.08; acc: 0.64
Batch: 400; loss: 0.87; acc: 0.73
Batch: 420; loss: 0.94; acc: 0.69
Batch: 440; loss: 1.3; acc: 0.62
Batch: 460; loss: 1.39; acc: 0.56
Batch: 480; loss: 1.09; acc: 0.64
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 0.88; acc: 0.72
Batch: 540; loss: 0.96; acc: 0.77
Batch: 560; loss: 1.26; acc: 0.59
Batch: 580; loss: 1.15; acc: 0.64
Batch: 600; loss: 1.24; acc: 0.58
Batch: 620; loss: 1.2; acc: 0.61
Batch: 640; loss: 1.27; acc: 0.52
Batch: 660; loss: 1.05; acc: 0.62
Batch: 680; loss: 1.0; acc: 0.69
Batch: 700; loss: 1.27; acc: 0.61
Batch: 720; loss: 1.21; acc: 0.66
Batch: 740; loss: 1.3; acc: 0.59
Batch: 760; loss: 1.48; acc: 0.62
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.07; acc: 0.59
Batch: 20; loss: 1.41; acc: 0.52
Batch: 40; loss: 0.79; acc: 0.75
Batch: 60; loss: 0.89; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.59
Batch: 140; loss: 0.75; acc: 0.77
Val Epoch over. val_loss: 1.028506681607787; val_accuracy: 0.680234872611465 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.21; acc: 0.55
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.2; acc: 0.56
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 1.31; acc: 0.59
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.12; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 1.14; acc: 0.66
Batch: 260; loss: 0.97; acc: 0.61
Batch: 280; loss: 1.03; acc: 0.75
Batch: 300; loss: 1.21; acc: 0.58
Batch: 320; loss: 1.06; acc: 0.69
Batch: 340; loss: 1.3; acc: 0.58
Batch: 360; loss: 1.12; acc: 0.66
Batch: 380; loss: 1.21; acc: 0.59
Batch: 400; loss: 1.26; acc: 0.67
Batch: 420; loss: 1.18; acc: 0.61
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.59
Batch: 480; loss: 1.14; acc: 0.64
Batch: 500; loss: 1.22; acc: 0.62
Batch: 520; loss: 1.03; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.59
Batch: 560; loss: 1.41; acc: 0.58
Batch: 580; loss: 1.3; acc: 0.61
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.02; acc: 0.72
Batch: 640; loss: 1.19; acc: 0.61
Batch: 660; loss: 0.9; acc: 0.73
Batch: 680; loss: 0.99; acc: 0.66
Batch: 700; loss: 1.36; acc: 0.58
Batch: 720; loss: 0.89; acc: 0.64
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.07; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.59
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.11; acc: 0.58
Batch: 20; loss: 1.38; acc: 0.48
Batch: 40; loss: 0.83; acc: 0.7
Batch: 60; loss: 0.93; acc: 0.66
Batch: 80; loss: 0.8; acc: 0.67
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 0.76; acc: 0.75
Val Epoch over. val_loss: 1.0389416908762257; val_accuracy: 0.6690883757961783 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.61
Batch: 80; loss: 1.08; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.69
Batch: 160; loss: 0.88; acc: 0.69
Batch: 180; loss: 1.04; acc: 0.66
Batch: 200; loss: 0.95; acc: 0.66
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 0.84; acc: 0.7
Batch: 260; loss: 1.07; acc: 0.62
Batch: 280; loss: 1.29; acc: 0.62
Batch: 300; loss: 1.23; acc: 0.58
Batch: 320; loss: 1.52; acc: 0.47
Batch: 340; loss: 1.02; acc: 0.72
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 0.91; acc: 0.69
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 0.87; acc: 0.67
Batch: 440; loss: 1.12; acc: 0.55
Batch: 460; loss: 1.4; acc: 0.55
Batch: 480; loss: 1.08; acc: 0.61
Batch: 500; loss: 1.07; acc: 0.75
Batch: 520; loss: 1.08; acc: 0.61
Batch: 540; loss: 1.2; acc: 0.61
Batch: 560; loss: 0.97; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.12; acc: 0.66
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 1.25; acc: 0.67
Batch: 660; loss: 1.24; acc: 0.53
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 0.87; acc: 0.69
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 0.87; acc: 0.72
Batch: 760; loss: 1.01; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.61
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.04; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.48
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 0.72; acc: 0.8
Val Epoch over. val_loss: 1.0175308831937753; val_accuracy: 0.6788415605095541 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 0.98; acc: 0.67
Batch: 40; loss: 1.01; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 1.16; acc: 0.64
Batch: 100; loss: 1.12; acc: 0.64
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.59
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.62
Batch: 200; loss: 0.96; acc: 0.69
Batch: 220; loss: 0.87; acc: 0.7
Batch: 240; loss: 0.99; acc: 0.77
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 1.4; acc: 0.55
Batch: 300; loss: 1.06; acc: 0.64
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 1.02; acc: 0.66
Batch: 360; loss: 1.03; acc: 0.69
Batch: 380; loss: 0.97; acc: 0.69
Batch: 400; loss: 1.11; acc: 0.66
Batch: 420; loss: 1.15; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.62
Batch: 460; loss: 1.17; acc: 0.61
Batch: 480; loss: 0.86; acc: 0.72
Batch: 500; loss: 0.85; acc: 0.72
Batch: 520; loss: 1.15; acc: 0.61
Batch: 540; loss: 1.33; acc: 0.59
Batch: 560; loss: 1.0; acc: 0.64
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 1.44; acc: 0.62
Batch: 620; loss: 0.95; acc: 0.69
Batch: 640; loss: 0.99; acc: 0.62
Batch: 660; loss: 0.9; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.29; acc: 0.61
Batch: 720; loss: 0.96; acc: 0.69
Batch: 740; loss: 0.99; acc: 0.69
Batch: 760; loss: 1.04; acc: 0.7
Batch: 780; loss: 1.27; acc: 0.61
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.07; acc: 0.61
Batch: 20; loss: 1.42; acc: 0.47
Batch: 40; loss: 0.78; acc: 0.7
Batch: 60; loss: 0.91; acc: 0.64
Batch: 80; loss: 0.78; acc: 0.72
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 0.75; acc: 0.78
Val Epoch over. val_loss: 1.0231244824114878; val_accuracy: 0.6703821656050956 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.55
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 1.04; acc: 0.75
Batch: 60; loss: 1.19; acc: 0.56
Batch: 80; loss: 0.99; acc: 0.62
Batch: 100; loss: 1.11; acc: 0.61
Batch: 120; loss: 1.04; acc: 0.59
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 1.21; acc: 0.59
Batch: 180; loss: 1.05; acc: 0.64
Batch: 200; loss: 0.81; acc: 0.72
Batch: 220; loss: 1.0; acc: 0.64
Batch: 240; loss: 1.21; acc: 0.72
Batch: 260; loss: 0.87; acc: 0.69
Batch: 280; loss: 0.91; acc: 0.66
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 1.08; acc: 0.72
Batch: 340; loss: 1.21; acc: 0.64
Batch: 360; loss: 1.16; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.59
Batch: 400; loss: 1.0; acc: 0.72
Batch: 420; loss: 1.07; acc: 0.59
Batch: 440; loss: 0.99; acc: 0.67
Batch: 460; loss: 1.3; acc: 0.5
Batch: 480; loss: 1.27; acc: 0.61
Batch: 500; loss: 1.53; acc: 0.56
Batch: 520; loss: 1.17; acc: 0.64
Batch: 540; loss: 1.45; acc: 0.55
Batch: 560; loss: 1.17; acc: 0.56
Batch: 580; loss: 1.07; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.64
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 1.1; acc: 0.66
Batch: 680; loss: 1.08; acc: 0.66
Batch: 700; loss: 0.88; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.67
Batch: 740; loss: 1.4; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.56
Batch: 780; loss: 0.98; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.06; acc: 0.59
Batch: 20; loss: 1.49; acc: 0.48
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.62
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0403926205483212; val_accuracy: 0.6652070063694268 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 0.99; acc: 0.66
Batch: 80; loss: 0.82; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.62
Batch: 160; loss: 1.24; acc: 0.62
Batch: 180; loss: 1.45; acc: 0.53
Batch: 200; loss: 1.06; acc: 0.64
Batch: 220; loss: 0.99; acc: 0.62
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 1.06; acc: 0.64
Batch: 280; loss: 1.06; acc: 0.61
Batch: 300; loss: 1.14; acc: 0.66
Batch: 320; loss: 0.76; acc: 0.73
Batch: 340; loss: 1.0; acc: 0.66
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 1.13; acc: 0.58
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.09; acc: 0.62
Batch: 480; loss: 0.95; acc: 0.67
Batch: 500; loss: 1.08; acc: 0.66
Batch: 520; loss: 1.15; acc: 0.56
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.98; acc: 0.73
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 1.13; acc: 0.59
Batch: 640; loss: 1.05; acc: 0.72
Batch: 660; loss: 1.17; acc: 0.61
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.02; acc: 0.66
Batch: 720; loss: 1.17; acc: 0.58
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 1.1; acc: 0.62
Batch: 780; loss: 1.16; acc: 0.61
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.09; acc: 0.58
Batch: 20; loss: 1.42; acc: 0.5
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 0.92; acc: 0.66
Batch: 80; loss: 0.77; acc: 0.73
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.75; acc: 0.78
Val Epoch over. val_loss: 1.0389693324353284; val_accuracy: 0.6739649681528662 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 1.51; acc: 0.53
Batch: 80; loss: 0.9; acc: 0.72
Batch: 100; loss: 0.97; acc: 0.72
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 1.05; acc: 0.62
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 0.93; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.22; acc: 0.62
Batch: 240; loss: 0.91; acc: 0.64
Batch: 260; loss: 1.1; acc: 0.61
Batch: 280; loss: 1.47; acc: 0.55
Batch: 300; loss: 1.18; acc: 0.59
Batch: 320; loss: 1.0; acc: 0.7
Batch: 340; loss: 0.88; acc: 0.73
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.04; acc: 0.66
Batch: 400; loss: 1.24; acc: 0.7
Batch: 420; loss: 1.04; acc: 0.7
Batch: 440; loss: 0.94; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.59
Batch: 480; loss: 0.97; acc: 0.67
Batch: 500; loss: 1.04; acc: 0.72
Batch: 520; loss: 1.15; acc: 0.69
Batch: 540; loss: 1.17; acc: 0.59
Batch: 560; loss: 1.34; acc: 0.58
Batch: 580; loss: 1.13; acc: 0.61
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.56
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.07; acc: 0.58
Batch: 700; loss: 1.07; acc: 0.69
Batch: 720; loss: 1.14; acc: 0.62
Batch: 740; loss: 1.12; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.64
Batch: 780; loss: 1.04; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.04; acc: 0.59
Batch: 20; loss: 1.45; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 0.74; acc: 0.75
Val Epoch over. val_loss: 1.0257537129578318; val_accuracy: 0.6792396496815286 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.09; acc: 0.62
Batch: 40; loss: 1.33; acc: 0.58
Batch: 60; loss: 0.81; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.58
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.69
Batch: 160; loss: 0.91; acc: 0.75
Batch: 180; loss: 1.16; acc: 0.59
Batch: 200; loss: 1.11; acc: 0.59
Batch: 220; loss: 1.22; acc: 0.61
Batch: 240; loss: 1.38; acc: 0.64
Batch: 260; loss: 1.17; acc: 0.64
Batch: 280; loss: 1.02; acc: 0.66
Batch: 300; loss: 1.21; acc: 0.59
Batch: 320; loss: 1.28; acc: 0.61
Batch: 340; loss: 0.87; acc: 0.81
Batch: 360; loss: 0.96; acc: 0.73
Batch: 380; loss: 1.17; acc: 0.55
Batch: 400; loss: 1.1; acc: 0.69
Batch: 420; loss: 0.98; acc: 0.75
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 0.95; acc: 0.73
Batch: 480; loss: 1.15; acc: 0.62
Batch: 500; loss: 1.13; acc: 0.67
Batch: 520; loss: 1.21; acc: 0.56
Batch: 540; loss: 1.09; acc: 0.7
Batch: 560; loss: 1.03; acc: 0.62
Batch: 580; loss: 1.06; acc: 0.69
Batch: 600; loss: 1.11; acc: 0.66
Batch: 620; loss: 1.02; acc: 0.62
Batch: 640; loss: 1.11; acc: 0.61
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.61
Batch: 720; loss: 0.75; acc: 0.77
Batch: 740; loss: 1.16; acc: 0.56
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.29; acc: 0.66
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.05; acc: 0.59
Batch: 20; loss: 1.39; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.86; acc: 0.67
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0086187266620101; val_accuracy: 0.6865047770700637 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.34; acc: 0.59
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.99; acc: 0.67
Batch: 60; loss: 0.99; acc: 0.62
Batch: 80; loss: 1.22; acc: 0.59
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 1.11; acc: 0.59
Batch: 160; loss: 0.99; acc: 0.66
Batch: 180; loss: 1.18; acc: 0.56
Batch: 200; loss: 1.15; acc: 0.59
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 1.05; acc: 0.66
Batch: 260; loss: 1.08; acc: 0.59
Batch: 280; loss: 1.43; acc: 0.55
Batch: 300; loss: 1.02; acc: 0.73
Batch: 320; loss: 1.19; acc: 0.61
Batch: 340; loss: 1.04; acc: 0.77
Batch: 360; loss: 1.45; acc: 0.62
Batch: 380; loss: 1.14; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 1.24; acc: 0.61
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.42; acc: 0.53
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 0.95; acc: 0.75
Batch: 520; loss: 1.02; acc: 0.64
Batch: 540; loss: 1.02; acc: 0.81
Batch: 560; loss: 1.01; acc: 0.64
Batch: 580; loss: 0.87; acc: 0.69
Batch: 600; loss: 1.06; acc: 0.62
Batch: 620; loss: 0.86; acc: 0.72
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.62
Batch: 680; loss: 1.19; acc: 0.69
Batch: 700; loss: 1.16; acc: 0.55
Batch: 720; loss: 1.01; acc: 0.7
Batch: 740; loss: 1.43; acc: 0.52
Batch: 760; loss: 0.96; acc: 0.73
Batch: 780; loss: 1.02; acc: 0.64
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.04; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.59
Batch: 140; loss: 0.7; acc: 0.8
Val Epoch over. val_loss: 1.0043029375137038; val_accuracy: 0.6876990445859873 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.64
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.3; acc: 0.58
Batch: 160; loss: 0.96; acc: 0.75
Batch: 180; loss: 1.13; acc: 0.66
Batch: 200; loss: 1.09; acc: 0.66
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 0.94; acc: 0.64
Batch: 260; loss: 0.9; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.62
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.97; acc: 0.67
Batch: 340; loss: 0.97; acc: 0.66
Batch: 360; loss: 1.05; acc: 0.62
Batch: 380; loss: 1.29; acc: 0.62
Batch: 400; loss: 1.01; acc: 0.66
Batch: 420; loss: 1.15; acc: 0.62
Batch: 440; loss: 1.11; acc: 0.64
Batch: 460; loss: 1.2; acc: 0.59
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.06; acc: 0.69
Batch: 520; loss: 1.17; acc: 0.58
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 0.84; acc: 0.81
Batch: 600; loss: 1.34; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.61
Batch: 640; loss: 1.06; acc: 0.62
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 0.94; acc: 0.7
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 1.12; acc: 0.56
Batch: 740; loss: 0.93; acc: 0.8
Batch: 760; loss: 1.07; acc: 0.62
Batch: 780; loss: 0.96; acc: 0.64
Train Epoch over. train_loss: 1.09; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.8
Val Epoch over. val_loss: 1.008436313290505; val_accuracy: 0.6843152866242038 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.22; acc: 0.56
Batch: 40; loss: 1.3; acc: 0.62
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.56
Batch: 100; loss: 1.32; acc: 0.59
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 1.08; acc: 0.53
Batch: 160; loss: 1.29; acc: 0.55
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 0.99; acc: 0.66
Batch: 220; loss: 0.99; acc: 0.69
Batch: 240; loss: 1.11; acc: 0.7
Batch: 260; loss: 1.09; acc: 0.64
Batch: 280; loss: 1.04; acc: 0.66
Batch: 300; loss: 1.08; acc: 0.7
Batch: 320; loss: 1.07; acc: 0.64
Batch: 340; loss: 0.96; acc: 0.64
Batch: 360; loss: 1.45; acc: 0.53
Batch: 380; loss: 1.09; acc: 0.59
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 1.29; acc: 0.55
Batch: 440; loss: 1.0; acc: 0.72
Batch: 460; loss: 0.9; acc: 0.7
Batch: 480; loss: 1.07; acc: 0.69
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.01; acc: 0.61
Batch: 560; loss: 1.32; acc: 0.64
Batch: 580; loss: 1.16; acc: 0.61
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.64; acc: 0.48
Batch: 640; loss: 1.06; acc: 0.62
Batch: 660; loss: 1.24; acc: 0.62
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 1.09; acc: 0.67
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.18; acc: 0.58
Batch: 780; loss: 0.94; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.05; acc: 0.62
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.006147823515971; val_accuracy: 0.6867038216560509 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 1.58; acc: 0.53
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 1.08; acc: 0.69
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 1.02; acc: 0.7
Batch: 240; loss: 1.04; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.62
Batch: 280; loss: 0.92; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.67
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.07; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.61
Batch: 400; loss: 0.91; acc: 0.75
Batch: 420; loss: 0.94; acc: 0.73
Batch: 440; loss: 1.13; acc: 0.69
Batch: 460; loss: 1.05; acc: 0.66
Batch: 480; loss: 1.09; acc: 0.64
Batch: 500; loss: 0.94; acc: 0.69
Batch: 520; loss: 1.05; acc: 0.75
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.1; acc: 0.69
Batch: 600; loss: 1.02; acc: 0.62
Batch: 620; loss: 0.94; acc: 0.75
Batch: 640; loss: 1.43; acc: 0.58
Batch: 660; loss: 1.03; acc: 0.72
Batch: 680; loss: 1.05; acc: 0.66
Batch: 700; loss: 1.02; acc: 0.62
Batch: 720; loss: 1.11; acc: 0.61
Batch: 740; loss: 1.08; acc: 0.64
Batch: 760; loss: 1.23; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.62
Train Epoch over. train_loss: 1.09; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.61
Batch: 20; loss: 1.39; acc: 0.55
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0050562403763934; val_accuracy: 0.6884952229299363 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.08; acc: 0.62
Batch: 20; loss: 0.89; acc: 0.7
Batch: 40; loss: 1.5; acc: 0.59
Batch: 60; loss: 1.29; acc: 0.53
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 1.11; acc: 0.66
Batch: 160; loss: 1.1; acc: 0.64
Batch: 180; loss: 0.96; acc: 0.69
Batch: 200; loss: 1.07; acc: 0.67
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 1.18; acc: 0.58
Batch: 260; loss: 0.96; acc: 0.67
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.2; acc: 0.61
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 1.03; acc: 0.64
Batch: 360; loss: 1.32; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.61
Batch: 400; loss: 1.19; acc: 0.55
Batch: 420; loss: 0.91; acc: 0.67
Batch: 440; loss: 1.04; acc: 0.64
Batch: 460; loss: 1.18; acc: 0.58
Batch: 480; loss: 0.96; acc: 0.73
Batch: 500; loss: 1.44; acc: 0.58
Batch: 520; loss: 1.14; acc: 0.61
Batch: 540; loss: 0.9; acc: 0.69
Batch: 560; loss: 1.2; acc: 0.55
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 1.29; acc: 0.53
Batch: 620; loss: 0.93; acc: 0.72
Batch: 640; loss: 1.18; acc: 0.61
Batch: 660; loss: 1.13; acc: 0.64
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 1.16; acc: 0.62
Batch: 740; loss: 1.27; acc: 0.53
Batch: 760; loss: 1.0; acc: 0.67
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 1.09; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.8
Val Epoch over. val_loss: 1.0044419805335392; val_accuracy: 0.6862062101910829 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.03; acc: 0.64
Batch: 40; loss: 1.01; acc: 0.69
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.34; acc: 0.58
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 1.05; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.55
Batch: 200; loss: 1.03; acc: 0.67
Batch: 220; loss: 0.93; acc: 0.67
Batch: 240; loss: 1.18; acc: 0.66
Batch: 260; loss: 0.93; acc: 0.77
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 0.91; acc: 0.72
Batch: 320; loss: 0.97; acc: 0.69
Batch: 340; loss: 0.94; acc: 0.7
Batch: 360; loss: 1.26; acc: 0.58
Batch: 380; loss: 1.27; acc: 0.67
Batch: 400; loss: 0.94; acc: 0.73
Batch: 420; loss: 1.31; acc: 0.58
Batch: 440; loss: 1.38; acc: 0.56
Batch: 460; loss: 1.14; acc: 0.64
Batch: 480; loss: 0.91; acc: 0.7
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 0.98; acc: 0.7
Batch: 560; loss: 1.16; acc: 0.59
Batch: 580; loss: 0.98; acc: 0.66
Batch: 600; loss: 0.94; acc: 0.72
Batch: 620; loss: 1.29; acc: 0.58
Batch: 640; loss: 1.18; acc: 0.59
Batch: 660; loss: 1.03; acc: 0.67
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 0.98; acc: 0.62
Batch: 720; loss: 1.08; acc: 0.61
Batch: 740; loss: 1.22; acc: 0.58
Batch: 760; loss: 1.23; acc: 0.58
Batch: 780; loss: 0.93; acc: 0.62
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.04; acc: 0.61
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.69
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0066465623439498; val_accuracy: 0.6861066878980892 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.06; acc: 0.59
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.59
Batch: 180; loss: 1.39; acc: 0.64
Batch: 200; loss: 1.19; acc: 0.64
Batch: 220; loss: 1.12; acc: 0.66
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.07; acc: 0.69
Batch: 280; loss: 1.14; acc: 0.62
Batch: 300; loss: 1.05; acc: 0.66
Batch: 320; loss: 1.08; acc: 0.64
Batch: 340; loss: 1.18; acc: 0.62
Batch: 360; loss: 1.09; acc: 0.69
Batch: 380; loss: 0.95; acc: 0.66
Batch: 400; loss: 1.11; acc: 0.61
Batch: 420; loss: 1.21; acc: 0.58
Batch: 440; loss: 1.35; acc: 0.53
Batch: 460; loss: 1.13; acc: 0.62
Batch: 480; loss: 1.49; acc: 0.52
Batch: 500; loss: 1.03; acc: 0.64
Batch: 520; loss: 0.96; acc: 0.69
Batch: 540; loss: 1.29; acc: 0.61
Batch: 560; loss: 1.08; acc: 0.61
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 1.05; acc: 0.64
Batch: 620; loss: 1.27; acc: 0.62
Batch: 640; loss: 1.14; acc: 0.64
Batch: 660; loss: 1.01; acc: 0.66
Batch: 680; loss: 1.14; acc: 0.62
Batch: 700; loss: 1.32; acc: 0.61
Batch: 720; loss: 1.08; acc: 0.62
Batch: 740; loss: 1.02; acc: 0.64
Batch: 760; loss: 1.29; acc: 0.53
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.09; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.61
Batch: 20; loss: 1.38; acc: 0.48
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.8
Val Epoch over. val_loss: 1.0097912925823478; val_accuracy: 0.6822253184713376 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.18; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 1.15; acc: 0.66
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 1.12; acc: 0.64
Batch: 100; loss: 1.31; acc: 0.59
Batch: 120; loss: 1.09; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.58
Batch: 160; loss: 1.02; acc: 0.59
Batch: 180; loss: 0.96; acc: 0.72
Batch: 200; loss: 1.05; acc: 0.7
Batch: 220; loss: 1.14; acc: 0.56
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.03; acc: 0.73
Batch: 300; loss: 1.26; acc: 0.59
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 1.05; acc: 0.64
Batch: 360; loss: 1.37; acc: 0.61
Batch: 380; loss: 1.01; acc: 0.77
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 1.02; acc: 0.66
Batch: 440; loss: 1.33; acc: 0.58
Batch: 460; loss: 1.01; acc: 0.7
Batch: 480; loss: 1.29; acc: 0.58
Batch: 500; loss: 1.4; acc: 0.64
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.07; acc: 0.7
Batch: 560; loss: 0.93; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.69
Batch: 600; loss: 0.97; acc: 0.67
Batch: 620; loss: 0.94; acc: 0.77
Batch: 640; loss: 1.0; acc: 0.7
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.26; acc: 0.67
Batch: 700; loss: 1.05; acc: 0.72
Batch: 720; loss: 1.3; acc: 0.53
Batch: 740; loss: 1.18; acc: 0.62
Batch: 760; loss: 1.1; acc: 0.64
Batch: 780; loss: 0.92; acc: 0.64
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.02; acc: 0.62
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 0.76; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0049165445528212; val_accuracy: 0.6870023885350318 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.55
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.61
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.62
Batch: 180; loss: 1.02; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.64
Batch: 220; loss: 1.06; acc: 0.62
Batch: 240; loss: 1.04; acc: 0.62
Batch: 260; loss: 1.21; acc: 0.64
Batch: 280; loss: 1.17; acc: 0.58
Batch: 300; loss: 1.01; acc: 0.58
Batch: 320; loss: 1.23; acc: 0.67
Batch: 340; loss: 1.25; acc: 0.67
Batch: 360; loss: 1.18; acc: 0.64
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 0.87; acc: 0.69
Batch: 420; loss: 0.91; acc: 0.73
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.21; acc: 0.61
Batch: 480; loss: 1.1; acc: 0.61
Batch: 500; loss: 0.99; acc: 0.7
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.11; acc: 0.69
Batch: 560; loss: 1.1; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.62
Batch: 600; loss: 0.96; acc: 0.62
Batch: 620; loss: 1.07; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.64
Batch: 660; loss: 1.07; acc: 0.59
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.44; acc: 0.52
Batch: 720; loss: 1.43; acc: 0.53
Batch: 740; loss: 1.06; acc: 0.67
Batch: 760; loss: 0.87; acc: 0.7
Batch: 780; loss: 1.01; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.65 

Batch: 0; loss: 1.02; acc: 0.62
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 0.76; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0045247286747976; val_accuracy: 0.6874004777070064 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.56
Batch: 40; loss: 1.09; acc: 0.62
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.2; acc: 0.55
Batch: 140; loss: 1.18; acc: 0.56
Batch: 160; loss: 1.13; acc: 0.66
Batch: 180; loss: 0.88; acc: 0.73
Batch: 200; loss: 0.96; acc: 0.69
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 1.01; acc: 0.66
Batch: 260; loss: 1.09; acc: 0.67
Batch: 280; loss: 0.98; acc: 0.66
Batch: 300; loss: 1.13; acc: 0.62
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 0.9; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.59
Batch: 420; loss: 1.36; acc: 0.55
Batch: 440; loss: 1.1; acc: 0.7
Batch: 460; loss: 1.41; acc: 0.53
Batch: 480; loss: 1.08; acc: 0.62
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.0; acc: 0.62
Batch: 540; loss: 1.2; acc: 0.58
Batch: 560; loss: 1.07; acc: 0.66
Batch: 580; loss: 1.13; acc: 0.64
Batch: 600; loss: 1.3; acc: 0.52
Batch: 620; loss: 1.07; acc: 0.56
Batch: 640; loss: 0.99; acc: 0.69
Batch: 660; loss: 1.02; acc: 0.67
Batch: 680; loss: 1.02; acc: 0.66
Batch: 700; loss: 1.15; acc: 0.72
Batch: 720; loss: 0.93; acc: 0.67
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.04; acc: 0.64
Batch: 780; loss: 1.18; acc: 0.58
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0027705288616715; val_accuracy: 0.6879976114649682 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.7
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 1.14; acc: 0.69
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 1.24; acc: 0.58
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.16; acc: 0.56
Batch: 140; loss: 0.98; acc: 0.64
Batch: 160; loss: 1.09; acc: 0.61
Batch: 180; loss: 0.96; acc: 0.67
Batch: 200; loss: 0.98; acc: 0.7
Batch: 220; loss: 1.14; acc: 0.64
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 1.36; acc: 0.55
Batch: 280; loss: 1.26; acc: 0.59
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 0.79; acc: 0.81
Batch: 340; loss: 1.08; acc: 0.55
Batch: 360; loss: 0.93; acc: 0.67
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.05; acc: 0.66
Batch: 420; loss: 1.11; acc: 0.61
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 0.91; acc: 0.72
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.88; acc: 0.66
Batch: 520; loss: 0.89; acc: 0.77
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.31; acc: 0.59
Batch: 580; loss: 0.96; acc: 0.72
Batch: 600; loss: 1.21; acc: 0.59
Batch: 620; loss: 1.09; acc: 0.62
Batch: 640; loss: 0.96; acc: 0.66
Batch: 660; loss: 0.82; acc: 0.77
Batch: 680; loss: 0.94; acc: 0.62
Batch: 700; loss: 1.14; acc: 0.56
Batch: 720; loss: 1.02; acc: 0.69
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 0.98; acc: 0.64
Batch: 780; loss: 1.24; acc: 0.55
Train Epoch over. train_loss: 1.09; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.002397493002521; val_accuracy: 0.6884952229299363 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.36; acc: 0.53
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 1.11; acc: 0.58
Batch: 60; loss: 1.23; acc: 0.61
Batch: 80; loss: 1.16; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.58
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 1.16; acc: 0.59
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.55
Batch: 200; loss: 1.08; acc: 0.59
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 1.15; acc: 0.58
Batch: 260; loss: 1.14; acc: 0.61
Batch: 280; loss: 0.96; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.59
Batch: 320; loss: 1.01; acc: 0.64
Batch: 340; loss: 1.16; acc: 0.64
Batch: 360; loss: 0.9; acc: 0.64
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 0.98; acc: 0.69
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 1.29; acc: 0.62
Batch: 460; loss: 0.87; acc: 0.72
Batch: 480; loss: 0.88; acc: 0.73
Batch: 500; loss: 1.05; acc: 0.59
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 1.03; acc: 0.69
Batch: 600; loss: 1.15; acc: 0.59
Batch: 620; loss: 1.38; acc: 0.59
Batch: 640; loss: 1.15; acc: 0.59
Batch: 660; loss: 1.0; acc: 0.72
Batch: 680; loss: 0.99; acc: 0.66
Batch: 700; loss: 0.88; acc: 0.7
Batch: 720; loss: 0.94; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.62
Batch: 760; loss: 1.06; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.53
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.8
Val Epoch over. val_loss: 1.0052887058941422; val_accuracy: 0.6873009554140127 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.1; acc: 0.7
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 1.21; acc: 0.56
Batch: 60; loss: 1.22; acc: 0.61
Batch: 80; loss: 0.82; acc: 0.73
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 1.06; acc: 0.61
Batch: 140; loss: 1.09; acc: 0.66
Batch: 160; loss: 1.18; acc: 0.66
Batch: 180; loss: 0.93; acc: 0.72
Batch: 200; loss: 0.81; acc: 0.69
Batch: 220; loss: 1.03; acc: 0.67
Batch: 240; loss: 1.03; acc: 0.67
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 1.03; acc: 0.67
Batch: 300; loss: 1.25; acc: 0.53
Batch: 320; loss: 1.05; acc: 0.66
Batch: 340; loss: 1.05; acc: 0.67
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 0.94; acc: 0.72
Batch: 400; loss: 1.15; acc: 0.66
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.0; acc: 0.66
Batch: 480; loss: 1.21; acc: 0.61
Batch: 500; loss: 1.17; acc: 0.62
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.69
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 1.26; acc: 0.61
Batch: 600; loss: 1.19; acc: 0.61
Batch: 620; loss: 1.33; acc: 0.58
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 1.01; acc: 0.64
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.62
Batch: 720; loss: 0.96; acc: 0.69
Batch: 740; loss: 1.04; acc: 0.64
Batch: 760; loss: 1.53; acc: 0.5
Batch: 780; loss: 0.94; acc: 0.69
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.05; acc: 0.59
Batch: 20; loss: 1.39; acc: 0.53
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.87; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 1.11; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 0.73; acc: 0.77
Val Epoch over. val_loss: 1.0088140691161915; val_accuracy: 0.6845143312101911 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.28; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.0; acc: 0.73
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.64
Batch: 160; loss: 1.2; acc: 0.59
Batch: 180; loss: 1.03; acc: 0.7
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.12; acc: 0.69
Batch: 240; loss: 1.04; acc: 0.66
Batch: 260; loss: 1.13; acc: 0.7
Batch: 280; loss: 1.18; acc: 0.64
Batch: 300; loss: 1.35; acc: 0.66
Batch: 320; loss: 1.31; acc: 0.58
Batch: 340; loss: 1.07; acc: 0.72
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 1.21; acc: 0.62
Batch: 400; loss: 0.98; acc: 0.72
Batch: 420; loss: 1.16; acc: 0.58
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 1.28; acc: 0.66
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 1.07; acc: 0.73
Batch: 520; loss: 1.27; acc: 0.55
Batch: 540; loss: 1.02; acc: 0.69
Batch: 560; loss: 1.08; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.59
Batch: 600; loss: 0.89; acc: 0.69
Batch: 620; loss: 1.06; acc: 0.66
Batch: 640; loss: 1.31; acc: 0.55
Batch: 660; loss: 1.11; acc: 0.64
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.03; acc: 0.64
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 1.08; acc: 0.61
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0036280024203525; val_accuracy: 0.6882961783439491 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.84; acc: 0.69
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.67
Batch: 60; loss: 0.91; acc: 0.59
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.9; acc: 0.7
Batch: 160; loss: 1.14; acc: 0.59
Batch: 180; loss: 1.06; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.72
Batch: 220; loss: 1.03; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.64
Batch: 260; loss: 1.3; acc: 0.55
Batch: 280; loss: 1.05; acc: 0.64
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.15; acc: 0.64
Batch: 340; loss: 1.21; acc: 0.59
Batch: 360; loss: 1.31; acc: 0.59
Batch: 380; loss: 1.19; acc: 0.56
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.28; acc: 0.58
Batch: 440; loss: 0.91; acc: 0.67
Batch: 460; loss: 1.01; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.61
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 1.22; acc: 0.61
Batch: 540; loss: 0.96; acc: 0.7
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.27; acc: 0.56
Batch: 620; loss: 1.16; acc: 0.58
Batch: 640; loss: 1.24; acc: 0.64
Batch: 660; loss: 1.07; acc: 0.64
Batch: 680; loss: 1.3; acc: 0.58
Batch: 700; loss: 0.94; acc: 0.69
Batch: 720; loss: 1.15; acc: 0.55
Batch: 740; loss: 1.37; acc: 0.56
Batch: 760; loss: 1.1; acc: 0.64
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.61
Batch: 20; loss: 1.37; acc: 0.53
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.66
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0044449347599296; val_accuracy: 0.6865047770700637 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.37; acc: 0.56
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.83; acc: 0.69
Batch: 100; loss: 1.0; acc: 0.67
Batch: 120; loss: 1.23; acc: 0.56
Batch: 140; loss: 1.22; acc: 0.58
Batch: 160; loss: 1.03; acc: 0.62
Batch: 180; loss: 1.19; acc: 0.64
Batch: 200; loss: 1.32; acc: 0.55
Batch: 220; loss: 1.08; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.67
Batch: 260; loss: 0.97; acc: 0.72
Batch: 280; loss: 1.05; acc: 0.59
Batch: 300; loss: 0.87; acc: 0.77
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 1.08; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.69
Batch: 380; loss: 1.42; acc: 0.59
Batch: 400; loss: 1.16; acc: 0.59
Batch: 420; loss: 1.31; acc: 0.56
Batch: 440; loss: 0.78; acc: 0.77
Batch: 460; loss: 0.96; acc: 0.66
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 1.31; acc: 0.58
Batch: 520; loss: 1.11; acc: 0.69
Batch: 540; loss: 1.25; acc: 0.62
Batch: 560; loss: 0.98; acc: 0.66
Batch: 580; loss: 1.1; acc: 0.62
Batch: 600; loss: 1.06; acc: 0.66
Batch: 620; loss: 0.97; acc: 0.64
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 1.28; acc: 0.67
Batch: 680; loss: 1.01; acc: 0.69
Batch: 700; loss: 1.22; acc: 0.64
Batch: 720; loss: 1.14; acc: 0.66
Batch: 740; loss: 0.96; acc: 0.7
Batch: 760; loss: 1.42; acc: 0.61
Batch: 780; loss: 1.0; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.61
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.77
Val Epoch over. val_loss: 1.0043750657777117; val_accuracy: 0.6873009554140127 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 1.24; acc: 0.52
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 1.37; acc: 0.61
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 1.06; acc: 0.7
Batch: 160; loss: 0.91; acc: 0.75
Batch: 180; loss: 1.18; acc: 0.64
Batch: 200; loss: 0.85; acc: 0.77
Batch: 220; loss: 0.99; acc: 0.7
Batch: 240; loss: 1.04; acc: 0.66
Batch: 260; loss: 0.92; acc: 0.78
Batch: 280; loss: 1.06; acc: 0.7
Batch: 300; loss: 0.9; acc: 0.67
Batch: 320; loss: 1.19; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 0.98; acc: 0.7
Batch: 380; loss: 0.87; acc: 0.7
Batch: 400; loss: 1.03; acc: 0.62
Batch: 420; loss: 1.04; acc: 0.7
Batch: 440; loss: 1.12; acc: 0.67
Batch: 460; loss: 0.9; acc: 0.73
Batch: 480; loss: 0.98; acc: 0.64
Batch: 500; loss: 1.31; acc: 0.61
Batch: 520; loss: 1.1; acc: 0.59
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 1.07; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.66
Batch: 600; loss: 1.25; acc: 0.61
Batch: 620; loss: 1.18; acc: 0.61
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 1.02; acc: 0.69
Batch: 680; loss: 0.97; acc: 0.62
Batch: 700; loss: 1.08; acc: 0.62
Batch: 720; loss: 1.15; acc: 0.62
Batch: 740; loss: 1.05; acc: 0.64
Batch: 760; loss: 1.0; acc: 0.67
Batch: 780; loss: 0.89; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.37; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0021672457646413; val_accuracy: 0.6870023885350318 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.66
Batch: 40; loss: 1.13; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.09; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.04; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.61
Batch: 160; loss: 0.95; acc: 0.73
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.82; acc: 0.72
Batch: 220; loss: 1.31; acc: 0.61
Batch: 240; loss: 1.15; acc: 0.66
Batch: 260; loss: 0.94; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.61
Batch: 320; loss: 0.98; acc: 0.64
Batch: 340; loss: 1.02; acc: 0.62
Batch: 360; loss: 1.12; acc: 0.62
Batch: 380; loss: 1.03; acc: 0.72
Batch: 400; loss: 1.21; acc: 0.61
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 1.23; acc: 0.61
Batch: 460; loss: 1.12; acc: 0.69
Batch: 480; loss: 1.14; acc: 0.64
Batch: 500; loss: 1.3; acc: 0.53
Batch: 520; loss: 1.24; acc: 0.62
Batch: 540; loss: 0.94; acc: 0.69
Batch: 560; loss: 0.87; acc: 0.73
Batch: 580; loss: 1.39; acc: 0.62
Batch: 600; loss: 1.44; acc: 0.58
Batch: 620; loss: 1.05; acc: 0.64
Batch: 640; loss: 1.0; acc: 0.73
Batch: 660; loss: 1.16; acc: 0.62
Batch: 680; loss: 1.18; acc: 0.59
Batch: 700; loss: 1.07; acc: 0.61
Batch: 720; loss: 0.94; acc: 0.73
Batch: 740; loss: 1.38; acc: 0.62
Batch: 760; loss: 1.28; acc: 0.66
Batch: 780; loss: 1.01; acc: 0.64
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0023643607926216; val_accuracy: 0.6878980891719745 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.07; acc: 0.62
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.48; acc: 0.55
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.99; acc: 0.64
Batch: 160; loss: 1.41; acc: 0.53
Batch: 180; loss: 1.2; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 0.86; acc: 0.78
Batch: 240; loss: 0.89; acc: 0.72
Batch: 260; loss: 1.24; acc: 0.7
Batch: 280; loss: 1.29; acc: 0.55
Batch: 300; loss: 0.98; acc: 0.7
Batch: 320; loss: 0.97; acc: 0.7
Batch: 340; loss: 1.14; acc: 0.62
Batch: 360; loss: 0.96; acc: 0.69
Batch: 380; loss: 1.37; acc: 0.58
Batch: 400; loss: 1.5; acc: 0.55
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.09; acc: 0.59
Batch: 460; loss: 0.98; acc: 0.73
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.18; acc: 0.62
Batch: 520; loss: 1.3; acc: 0.59
Batch: 540; loss: 0.91; acc: 0.77
Batch: 560; loss: 1.31; acc: 0.64
Batch: 580; loss: 0.96; acc: 0.66
Batch: 600; loss: 1.09; acc: 0.62
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.07; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.64
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 1.47; acc: 0.55
Batch: 720; loss: 0.82; acc: 0.72
Batch: 740; loss: 1.07; acc: 0.64
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.19; acc: 0.59
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.61
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.77
Val Epoch over. val_loss: 1.002865840864789; val_accuracy: 0.6878980891719745 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 0.93; acc: 0.67
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.24; acc: 0.53
Batch: 80; loss: 1.1; acc: 0.56
Batch: 100; loss: 1.28; acc: 0.62
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.66
Batch: 160; loss: 0.93; acc: 0.67
Batch: 180; loss: 1.11; acc: 0.56
Batch: 200; loss: 1.25; acc: 0.61
Batch: 220; loss: 0.77; acc: 0.72
Batch: 240; loss: 1.27; acc: 0.55
Batch: 260; loss: 1.12; acc: 0.62
Batch: 280; loss: 0.8; acc: 0.7
Batch: 300; loss: 1.15; acc: 0.66
Batch: 320; loss: 0.8; acc: 0.72
Batch: 340; loss: 1.16; acc: 0.59
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.32; acc: 0.56
Batch: 400; loss: 1.18; acc: 0.69
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.1; acc: 0.64
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 0.9; acc: 0.7
Batch: 540; loss: 1.09; acc: 0.66
Batch: 560; loss: 1.26; acc: 0.48
Batch: 580; loss: 1.42; acc: 0.55
Batch: 600; loss: 1.14; acc: 0.61
Batch: 620; loss: 1.24; acc: 0.62
Batch: 640; loss: 1.3; acc: 0.58
Batch: 660; loss: 1.38; acc: 0.58
Batch: 680; loss: 1.12; acc: 0.64
Batch: 700; loss: 1.19; acc: 0.59
Batch: 720; loss: 0.97; acc: 0.75
Batch: 740; loss: 1.06; acc: 0.67
Batch: 760; loss: 0.87; acc: 0.64
Batch: 780; loss: 1.14; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0022741977576237; val_accuracy: 0.6871019108280255 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.03; acc: 0.66
Batch: 40; loss: 1.4; acc: 0.61
Batch: 60; loss: 1.04; acc: 0.62
Batch: 80; loss: 0.98; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.15; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.61
Batch: 180; loss: 1.19; acc: 0.59
Batch: 200; loss: 1.36; acc: 0.52
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 1.4; acc: 0.59
Batch: 260; loss: 0.99; acc: 0.78
Batch: 280; loss: 1.18; acc: 0.64
Batch: 300; loss: 1.0; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.59
Batch: 340; loss: 1.39; acc: 0.55
Batch: 360; loss: 1.19; acc: 0.69
Batch: 380; loss: 1.12; acc: 0.62
Batch: 400; loss: 1.17; acc: 0.72
Batch: 420; loss: 1.2; acc: 0.62
Batch: 440; loss: 0.9; acc: 0.7
Batch: 460; loss: 1.03; acc: 0.66
Batch: 480; loss: 1.03; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.67
Batch: 520; loss: 1.03; acc: 0.67
Batch: 540; loss: 0.87; acc: 0.67
Batch: 560; loss: 0.92; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.64
Batch: 600; loss: 1.09; acc: 0.64
Batch: 620; loss: 0.97; acc: 0.64
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.99; acc: 0.67
Batch: 680; loss: 0.92; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.64
Batch: 720; loss: 1.07; acc: 0.62
Batch: 740; loss: 0.93; acc: 0.7
Batch: 760; loss: 1.13; acc: 0.58
Batch: 780; loss: 1.08; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.002601698895169; val_accuracy: 0.6872014331210191 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.59
Batch: 40; loss: 0.92; acc: 0.69
Batch: 60; loss: 1.08; acc: 0.58
Batch: 80; loss: 1.07; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 0.9; acc: 0.72
Batch: 160; loss: 1.12; acc: 0.59
Batch: 180; loss: 1.03; acc: 0.59
Batch: 200; loss: 1.12; acc: 0.64
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.2; acc: 0.62
Batch: 260; loss: 0.91; acc: 0.7
Batch: 280; loss: 0.89; acc: 0.69
Batch: 300; loss: 1.09; acc: 0.59
Batch: 320; loss: 0.93; acc: 0.66
Batch: 340; loss: 1.12; acc: 0.67
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 1.19; acc: 0.64
Batch: 400; loss: 1.2; acc: 0.58
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 1.11; acc: 0.64
Batch: 540; loss: 1.29; acc: 0.59
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.95; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.59
Batch: 620; loss: 1.02; acc: 0.66
Batch: 640; loss: 0.98; acc: 0.69
Batch: 660; loss: 1.1; acc: 0.59
Batch: 680; loss: 1.51; acc: 0.47
Batch: 700; loss: 1.0; acc: 0.69
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 0.92; acc: 0.77
Batch: 760; loss: 1.24; acc: 0.69
Batch: 780; loss: 1.01; acc: 0.67
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0033188741298238; val_accuracy: 0.6870023885350318 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.25; acc: 0.7
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 1.08; acc: 0.58
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.58
Batch: 100; loss: 1.12; acc: 0.61
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.28; acc: 0.62
Batch: 180; loss: 1.19; acc: 0.66
Batch: 200; loss: 0.95; acc: 0.64
Batch: 220; loss: 1.16; acc: 0.64
Batch: 240; loss: 1.15; acc: 0.64
Batch: 260; loss: 1.01; acc: 0.64
Batch: 280; loss: 0.88; acc: 0.67
Batch: 300; loss: 1.06; acc: 0.62
Batch: 320; loss: 0.93; acc: 0.66
Batch: 340; loss: 1.15; acc: 0.69
Batch: 360; loss: 1.14; acc: 0.58
Batch: 380; loss: 1.05; acc: 0.62
Batch: 400; loss: 1.14; acc: 0.62
Batch: 420; loss: 0.92; acc: 0.69
Batch: 440; loss: 0.98; acc: 0.64
Batch: 460; loss: 0.83; acc: 0.7
Batch: 480; loss: 1.18; acc: 0.66
Batch: 500; loss: 1.06; acc: 0.64
Batch: 520; loss: 1.12; acc: 0.64
Batch: 540; loss: 0.95; acc: 0.69
Batch: 560; loss: 1.08; acc: 0.66
Batch: 580; loss: 1.37; acc: 0.52
Batch: 600; loss: 0.96; acc: 0.66
Batch: 620; loss: 1.05; acc: 0.69
Batch: 640; loss: 1.23; acc: 0.59
Batch: 660; loss: 1.01; acc: 0.69
Batch: 680; loss: 0.94; acc: 0.7
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 1.15; acc: 0.7
Batch: 740; loss: 0.9; acc: 0.61
Batch: 760; loss: 1.11; acc: 0.69
Batch: 780; loss: 1.17; acc: 0.56
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0020686917623896; val_accuracy: 0.6875995222929936 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.06; acc: 0.66
Batch: 20; loss: 0.94; acc: 0.77
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.02; acc: 0.66
Batch: 80; loss: 1.12; acc: 0.59
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 1.2; acc: 0.58
Batch: 140; loss: 1.21; acc: 0.66
Batch: 160; loss: 1.14; acc: 0.56
Batch: 180; loss: 1.22; acc: 0.61
Batch: 200; loss: 1.05; acc: 0.7
Batch: 220; loss: 1.48; acc: 0.55
Batch: 240; loss: 1.16; acc: 0.66
Batch: 260; loss: 1.05; acc: 0.73
Batch: 280; loss: 1.01; acc: 0.67
Batch: 300; loss: 1.05; acc: 0.66
Batch: 320; loss: 0.98; acc: 0.7
Batch: 340; loss: 1.03; acc: 0.66
Batch: 360; loss: 1.33; acc: 0.67
Batch: 380; loss: 1.25; acc: 0.52
Batch: 400; loss: 0.92; acc: 0.77
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 0.81; acc: 0.7
Batch: 460; loss: 0.88; acc: 0.73
Batch: 480; loss: 1.14; acc: 0.58
Batch: 500; loss: 1.02; acc: 0.64
Batch: 520; loss: 1.21; acc: 0.58
Batch: 540; loss: 1.12; acc: 0.72
Batch: 560; loss: 1.1; acc: 0.69
Batch: 580; loss: 0.92; acc: 0.7
Batch: 600; loss: 0.97; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 0.9; acc: 0.69
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 1.0; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.66
Batch: 720; loss: 1.64; acc: 0.56
Batch: 740; loss: 1.26; acc: 0.58
Batch: 760; loss: 0.95; acc: 0.69
Batch: 780; loss: 1.08; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0025509817964713; val_accuracy: 0.6863057324840764 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.61
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.21; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.93; acc: 0.73
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 1.12; acc: 0.59
Batch: 160; loss: 0.94; acc: 0.72
Batch: 180; loss: 1.28; acc: 0.62
Batch: 200; loss: 1.01; acc: 0.64
Batch: 220; loss: 1.09; acc: 0.73
Batch: 240; loss: 0.92; acc: 0.7
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 1.25; acc: 0.62
Batch: 300; loss: 1.35; acc: 0.59
Batch: 320; loss: 1.07; acc: 0.64
Batch: 340; loss: 1.09; acc: 0.64
Batch: 360; loss: 0.95; acc: 0.73
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 1.37; acc: 0.52
Batch: 420; loss: 0.87; acc: 0.72
Batch: 440; loss: 1.03; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 0.89; acc: 0.7
Batch: 500; loss: 1.43; acc: 0.58
Batch: 520; loss: 1.06; acc: 0.7
Batch: 540; loss: 1.21; acc: 0.66
Batch: 560; loss: 1.04; acc: 0.67
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 1.14; acc: 0.56
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.56
Batch: 660; loss: 1.13; acc: 0.62
Batch: 680; loss: 1.2; acc: 0.59
Batch: 700; loss: 0.99; acc: 0.77
Batch: 720; loss: 0.96; acc: 0.72
Batch: 740; loss: 1.22; acc: 0.55
Batch: 760; loss: 1.17; acc: 0.62
Batch: 780; loss: 1.22; acc: 0.5
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.0025107063305605; val_accuracy: 0.6875995222929936 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.93; acc: 0.7
Batch: 40; loss: 1.14; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.66
Batch: 80; loss: 1.06; acc: 0.62
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 1.0; acc: 0.73
Batch: 160; loss: 0.92; acc: 0.75
Batch: 180; loss: 0.94; acc: 0.66
Batch: 200; loss: 0.96; acc: 0.7
Batch: 220; loss: 1.07; acc: 0.69
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 0.92; acc: 0.69
Batch: 280; loss: 1.28; acc: 0.58
Batch: 300; loss: 1.16; acc: 0.62
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 1.13; acc: 0.72
Batch: 380; loss: 1.24; acc: 0.64
Batch: 400; loss: 1.05; acc: 0.66
Batch: 420; loss: 1.11; acc: 0.69
Batch: 440; loss: 0.94; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.55
Batch: 480; loss: 1.32; acc: 0.53
Batch: 500; loss: 1.16; acc: 0.64
Batch: 520; loss: 1.23; acc: 0.55
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.06; acc: 0.64
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.12; acc: 0.62
Batch: 620; loss: 1.03; acc: 0.69
Batch: 640; loss: 0.98; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.7
Batch: 720; loss: 1.19; acc: 0.62
Batch: 740; loss: 1.14; acc: 0.64
Batch: 760; loss: 0.96; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.69
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0034162273072893; val_accuracy: 0.6871019108280255 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 1.38; acc: 0.56
Batch: 40; loss: 1.06; acc: 0.75
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.14; acc: 0.59
Batch: 140; loss: 1.05; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.64
Batch: 180; loss: 1.13; acc: 0.69
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.11; acc: 0.58
Batch: 240; loss: 0.88; acc: 0.67
Batch: 260; loss: 0.82; acc: 0.73
Batch: 280; loss: 1.11; acc: 0.61
Batch: 300; loss: 1.03; acc: 0.67
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 1.01; acc: 0.7
Batch: 360; loss: 0.99; acc: 0.72
Batch: 380; loss: 1.05; acc: 0.73
Batch: 400; loss: 1.32; acc: 0.58
Batch: 420; loss: 1.05; acc: 0.69
Batch: 440; loss: 0.98; acc: 0.72
Batch: 460; loss: 1.14; acc: 0.59
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 1.18; acc: 0.61
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 1.1; acc: 0.62
Batch: 600; loss: 1.3; acc: 0.67
Batch: 620; loss: 0.79; acc: 0.72
Batch: 640; loss: 0.9; acc: 0.72
Batch: 660; loss: 1.15; acc: 0.61
Batch: 680; loss: 1.07; acc: 0.66
Batch: 700; loss: 0.96; acc: 0.72
Batch: 720; loss: 1.01; acc: 0.66
Batch: 740; loss: 1.11; acc: 0.67
Batch: 760; loss: 1.01; acc: 0.64
Batch: 780; loss: 1.08; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.64
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 1.0018766110490083; val_accuracy: 0.6872014331210191 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 0.91; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.64
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 1.2; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.1; acc: 0.61
Batch: 200; loss: 1.08; acc: 0.66
Batch: 220; loss: 0.96; acc: 0.7
Batch: 240; loss: 0.89; acc: 0.69
Batch: 260; loss: 1.5; acc: 0.5
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.23; acc: 0.56
Batch: 320; loss: 1.05; acc: 0.62
Batch: 340; loss: 0.91; acc: 0.73
Batch: 360; loss: 1.28; acc: 0.64
Batch: 380; loss: 0.93; acc: 0.73
Batch: 400; loss: 1.06; acc: 0.67
Batch: 420; loss: 0.99; acc: 0.72
Batch: 440; loss: 1.02; acc: 0.67
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 1.22; acc: 0.58
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 1.07; acc: 0.62
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 1.11; acc: 0.61
Batch: 580; loss: 1.16; acc: 0.62
Batch: 600; loss: 1.29; acc: 0.61
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 1.08; acc: 0.7
Batch: 660; loss: 0.94; acc: 0.72
Batch: 680; loss: 1.21; acc: 0.66
Batch: 700; loss: 1.44; acc: 0.55
Batch: 720; loss: 0.99; acc: 0.7
Batch: 740; loss: 1.25; acc: 0.56
Batch: 760; loss: 1.32; acc: 0.53
Batch: 780; loss: 1.2; acc: 0.58
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.03; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.52
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.78
Val Epoch over. val_loss: 1.0022839664653609; val_accuracy: 0.6869028662420382 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.59
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 1.43; acc: 0.56
Batch: 60; loss: 1.03; acc: 0.58
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.19; acc: 0.59
Batch: 120; loss: 1.1; acc: 0.61
Batch: 140; loss: 1.32; acc: 0.62
Batch: 160; loss: 0.89; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.5
Batch: 200; loss: 0.94; acc: 0.69
Batch: 220; loss: 1.05; acc: 0.66
Batch: 240; loss: 1.19; acc: 0.61
Batch: 260; loss: 1.23; acc: 0.7
Batch: 280; loss: 1.12; acc: 0.59
Batch: 300; loss: 0.86; acc: 0.69
Batch: 320; loss: 1.07; acc: 0.69
Batch: 340; loss: 1.09; acc: 0.66
Batch: 360; loss: 1.08; acc: 0.59
Batch: 380; loss: 1.13; acc: 0.67
Batch: 400; loss: 1.33; acc: 0.5
Batch: 420; loss: 1.1; acc: 0.7
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 0.97; acc: 0.67
Batch: 480; loss: 1.05; acc: 0.73
Batch: 500; loss: 1.07; acc: 0.62
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.08; acc: 0.62
Batch: 560; loss: 1.33; acc: 0.56
Batch: 580; loss: 0.84; acc: 0.73
Batch: 600; loss: 1.05; acc: 0.67
Batch: 620; loss: 1.28; acc: 0.61
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.69
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 1.15; acc: 0.58
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.07; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.67
Batch: 780; loss: 1.06; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.67
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.71; acc: 0.78
Val Epoch over. val_loss: 1.002656989416499; val_accuracy: 0.6874004777070064 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 16127
elements in E: 3374250
fraction nonzero: 0.004779432466474031
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.31; acc: 0.17
Batch: 160; loss: 2.31; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.16
Batch: 240; loss: 2.3; acc: 0.14
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.14
Batch: 300; loss: 2.29; acc: 0.12
Batch: 320; loss: 2.29; acc: 0.19
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.16
Batch: 380; loss: 2.29; acc: 0.16
Batch: 400; loss: 2.29; acc: 0.27
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.29; acc: 0.08
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.14
Batch: 500; loss: 2.29; acc: 0.12
Batch: 520; loss: 2.28; acc: 0.11
Batch: 540; loss: 2.29; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.06
Batch: 580; loss: 2.28; acc: 0.12
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.27; acc: 0.16
Batch: 640; loss: 2.29; acc: 0.08
Batch: 660; loss: 2.27; acc: 0.14
Batch: 680; loss: 2.27; acc: 0.12
Batch: 700; loss: 2.27; acc: 0.19
Batch: 720; loss: 2.26; acc: 0.12
Batch: 740; loss: 2.28; acc: 0.12
Batch: 760; loss: 2.27; acc: 0.09
Batch: 780; loss: 2.25; acc: 0.17
Train Epoch over. train_loss: 2.29; train_accuracy: 0.13 

Batch: 0; loss: 2.26; acc: 0.22
Batch: 20; loss: 2.27; acc: 0.16
Batch: 40; loss: 2.26; acc: 0.19
Batch: 60; loss: 2.26; acc: 0.16
Batch: 80; loss: 2.25; acc: 0.2
Batch: 100; loss: 2.26; acc: 0.12
Batch: 120; loss: 2.25; acc: 0.22
Batch: 140; loss: 2.27; acc: 0.16
Val Epoch over. val_loss: 2.2658991449198145; val_accuracy: 0.15346337579617833 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.19
Batch: 20; loss: 2.26; acc: 0.14
Batch: 40; loss: 2.26; acc: 0.14
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.22
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.23; acc: 0.28
Batch: 160; loss: 2.24; acc: 0.27
Batch: 180; loss: 2.26; acc: 0.28
Batch: 200; loss: 2.23; acc: 0.39
Batch: 220; loss: 2.25; acc: 0.33
Batch: 240; loss: 2.23; acc: 0.31
Batch: 260; loss: 2.22; acc: 0.36
Batch: 280; loss: 2.21; acc: 0.36
Batch: 300; loss: 2.2; acc: 0.36
Batch: 320; loss: 2.21; acc: 0.33
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.2; acc: 0.33
Batch: 380; loss: 2.17; acc: 0.47
Batch: 400; loss: 2.21; acc: 0.34
Batch: 420; loss: 2.12; acc: 0.55
Batch: 440; loss: 2.13; acc: 0.45
Batch: 460; loss: 2.09; acc: 0.47
Batch: 480; loss: 2.09; acc: 0.39
Batch: 500; loss: 2.03; acc: 0.48
Batch: 520; loss: 1.99; acc: 0.52
Batch: 540; loss: 1.99; acc: 0.47
Batch: 560; loss: 2.02; acc: 0.38
Batch: 580; loss: 1.98; acc: 0.47
Batch: 600; loss: 1.93; acc: 0.42
Batch: 620; loss: 1.88; acc: 0.41
Batch: 640; loss: 1.79; acc: 0.47
Batch: 660; loss: 1.56; acc: 0.56
Batch: 680; loss: 1.66; acc: 0.5
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.6; acc: 0.55
Batch: 740; loss: 1.41; acc: 0.61
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.5; acc: 0.53
Train Epoch over. train_loss: 2.05; train_accuracy: 0.38 

Batch: 0; loss: 1.46; acc: 0.53
Batch: 20; loss: 1.48; acc: 0.61
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 1.39; acc: 0.5
Val Epoch over. val_loss: 1.3908481248624764; val_accuracy: 0.5691679936305732 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.47
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 1.46; acc: 0.52
Batch: 60; loss: 1.27; acc: 0.53
Batch: 80; loss: 1.24; acc: 0.56
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.72
Batch: 160; loss: 1.22; acc: 0.59
Batch: 180; loss: 1.07; acc: 0.62
Batch: 200; loss: 1.04; acc: 0.66
Batch: 220; loss: 1.09; acc: 0.69
Batch: 240; loss: 1.01; acc: 0.73
Batch: 260; loss: 0.9; acc: 0.7
Batch: 280; loss: 1.12; acc: 0.66
Batch: 300; loss: 1.02; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.59
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 0.91; acc: 0.77
Batch: 400; loss: 0.88; acc: 0.69
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 0.99; acc: 0.73
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 1.04; acc: 0.7
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 1.03; acc: 0.62
Batch: 540; loss: 0.85; acc: 0.67
Batch: 560; loss: 0.74; acc: 0.75
Batch: 580; loss: 1.02; acc: 0.66
Batch: 600; loss: 0.9; acc: 0.75
Batch: 620; loss: 0.88; acc: 0.73
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 1.1; acc: 0.7
Batch: 700; loss: 1.05; acc: 0.72
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 1.01; train_accuracy: 0.69 

Batch: 0; loss: 1.0; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 0.6; acc: 0.77
Batch: 60; loss: 0.88; acc: 0.69
Batch: 80; loss: 0.72; acc: 0.72
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 0.5; acc: 0.81
Val Epoch over. val_loss: 0.8296504935641198; val_accuracy: 0.7333797770700637 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.84; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 1.17; acc: 0.58
Batch: 100; loss: 1.02; acc: 0.67
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.84; acc: 0.73
Batch: 180; loss: 1.0; acc: 0.64
Batch: 200; loss: 1.11; acc: 0.64
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 0.94; acc: 0.66
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.76; acc: 0.75
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 0.88; acc: 0.7
Batch: 360; loss: 0.88; acc: 0.69
Batch: 380; loss: 0.64; acc: 0.77
Batch: 400; loss: 0.7; acc: 0.75
Batch: 420; loss: 0.91; acc: 0.72
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.73
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 0.92; acc: 0.75
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 1.04; acc: 0.7
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 1.06; acc: 0.73
Batch: 660; loss: 0.88; acc: 0.69
Batch: 680; loss: 0.9; acc: 0.72
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 0.74; acc: 0.77
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 0.83; train_accuracy: 0.75 

Batch: 0; loss: 0.82; acc: 0.69
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.87; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.7689580192231829; val_accuracy: 0.7585589171974523 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.7
Batch: 20; loss: 0.76; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 0.96; acc: 0.72
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.72; acc: 0.75
Batch: 200; loss: 0.88; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 0.57; acc: 0.75
Batch: 260; loss: 0.68; acc: 0.8
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.82; acc: 0.75
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.92; acc: 0.75
Batch: 480; loss: 0.73; acc: 0.75
Batch: 500; loss: 0.68; acc: 0.73
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.73
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.92; acc: 0.77
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 0.82; acc: 0.72
Batch: 680; loss: 0.87; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.96; acc: 0.64
Batch: 740; loss: 0.81; acc: 0.77
Batch: 760; loss: 0.82; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.75 

Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.7
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.7150838457664866; val_accuracy: 0.7788614649681529 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.77
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.92; acc: 0.73
Batch: 200; loss: 0.72; acc: 0.81
Batch: 220; loss: 0.7; acc: 0.75
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 1.13; acc: 0.61
Batch: 340; loss: 0.67; acc: 0.75
Batch: 360; loss: 0.93; acc: 0.66
Batch: 380; loss: 1.09; acc: 0.58
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.83; acc: 0.7
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.78; acc: 0.73
Batch: 540; loss: 0.76; acc: 0.73
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.73
Batch: 620; loss: 0.96; acc: 0.72
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.84; acc: 0.73
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.77; acc: 0.73
Batch: 760; loss: 0.79; acc: 0.75
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.77; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.75
Batch: 120; loss: 0.82; acc: 0.7
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.6862027253125124; val_accuracy: 0.7897093949044586 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 0.82; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.73
Batch: 160; loss: 0.64; acc: 0.77
Batch: 180; loss: 1.18; acc: 0.62
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.78
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.73
Batch: 280; loss: 0.63; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.73; acc: 0.78
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.73
Batch: 380; loss: 1.02; acc: 0.7
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.75
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.96; acc: 0.72
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.95; acc: 0.69
Batch: 620; loss: 0.78; acc: 0.7
Batch: 640; loss: 0.72; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.72
Batch: 720; loss: 0.86; acc: 0.73
Batch: 740; loss: 0.6; acc: 0.77
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.94; acc: 0.67
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 0.77; acc: 0.72
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.6992660736202434; val_accuracy: 0.7819466560509554 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.7
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.67
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.87; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.64; acc: 0.73
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 1.11; acc: 0.72
Batch: 300; loss: 0.71; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 1.0; acc: 0.69
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 0.72; acc: 0.75
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.81; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.92; acc: 0.69
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.72; acc: 0.77
Batch: 600; loss: 0.7; acc: 0.73
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.96; acc: 0.67
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.85; acc: 0.73
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.8; acc: 0.67
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.89; acc: 0.67
Batch: 20; loss: 0.84; acc: 0.69
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.61; acc: 0.75
Batch: 100; loss: 0.96; acc: 0.67
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.7588005508207212; val_accuracy: 0.7532842356687898 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.88; acc: 0.67
Batch: 100; loss: 0.81; acc: 0.72
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 0.67; acc: 0.77
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 1.01; acc: 0.72
Batch: 260; loss: 0.87; acc: 0.7
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.86; acc: 0.73
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.88; acc: 0.67
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.98; acc: 0.73
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.93; acc: 0.69
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.77; acc: 0.75
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.81; acc: 0.73
Batch: 600; loss: 0.82; acc: 0.77
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.94; acc: 0.7
Batch: 660; loss: 0.94; acc: 0.69
Batch: 680; loss: 0.73; acc: 0.77
Batch: 700; loss: 0.85; acc: 0.7
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.76; train_accuracy: 0.77 

Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.6800928436646796; val_accuracy: 0.7891122611464968 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.7
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.77
Batch: 160; loss: 0.9; acc: 0.72
Batch: 180; loss: 0.84; acc: 0.73
Batch: 200; loss: 0.95; acc: 0.73
Batch: 220; loss: 0.9; acc: 0.75
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.87; acc: 0.7
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.81
Batch: 360; loss: 0.72; acc: 0.72
Batch: 380; loss: 0.72; acc: 0.75
Batch: 400; loss: 0.83; acc: 0.75
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 0.9; acc: 0.69
Batch: 460; loss: 0.69; acc: 0.75
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.73; acc: 0.73
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.77
Batch: 600; loss: 0.72; acc: 0.69
Batch: 620; loss: 1.06; acc: 0.72
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 0.85; acc: 0.77
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.72
Batch: 780; loss: 0.61; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.6883935072239796; val_accuracy: 0.789609872611465 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.69; acc: 0.7
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 1.15; acc: 0.67
Batch: 160; loss: 0.95; acc: 0.72
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.73
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.73
Batch: 320; loss: 0.78; acc: 0.7
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 0.58; acc: 0.77
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.89; acc: 0.69
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.88; acc: 0.66
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.88; acc: 0.77
Batch: 580; loss: 0.91; acc: 0.77
Batch: 600; loss: 0.84; acc: 0.83
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.84; acc: 0.78
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 1.05; acc: 0.69
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.79; acc: 0.7
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.72; acc: 0.73
Batch: 780; loss: 0.88; acc: 0.67
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6651196790159128; val_accuracy: 0.7970740445859873 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.65; acc: 0.75
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.98; acc: 0.69
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.78
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.8; acc: 0.83
Batch: 260; loss: 0.73; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.69; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.8
Batch: 460; loss: 0.89; acc: 0.73
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.73
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.86; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.59
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.96; acc: 0.72
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.97; acc: 0.67
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6602236338102134; val_accuracy: 0.79796974522293 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.75
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 1.18; acc: 0.7
Batch: 280; loss: 0.95; acc: 0.67
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 1.09; acc: 0.66
Batch: 380; loss: 0.74; acc: 0.7
Batch: 400; loss: 0.7; acc: 0.73
Batch: 420; loss: 0.84; acc: 0.72
Batch: 440; loss: 0.78; acc: 0.8
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.86; acc: 0.73
Batch: 520; loss: 0.84; acc: 0.73
Batch: 540; loss: 0.84; acc: 0.7
Batch: 560; loss: 0.83; acc: 0.73
Batch: 580; loss: 0.97; acc: 0.72
Batch: 600; loss: 0.82; acc: 0.72
Batch: 620; loss: 0.86; acc: 0.73
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.53; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.85; acc: 0.75
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.78
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.6806339891093551; val_accuracy: 0.7905055732484076 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.72
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.85; acc: 0.72
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.57; acc: 0.78
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 0.89; acc: 0.73
Batch: 380; loss: 0.73; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 0.93; acc: 0.69
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.77
Batch: 520; loss: 0.75; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.87; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.8
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.92; acc: 0.66
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.87; acc: 0.64
Batch: 740; loss: 0.81; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.77
Batch: 780; loss: 0.94; acc: 0.66
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.6809976452095493; val_accuracy: 0.7911027070063694 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 1.13; acc: 0.64
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.99; acc: 0.69
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.87; acc: 0.72
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.65; acc: 0.8
Batch: 300; loss: 0.78; acc: 0.75
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.87; acc: 0.77
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.89; acc: 0.72
Batch: 420; loss: 0.81; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.89; acc: 0.69
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.75; acc: 0.78
Batch: 520; loss: 0.93; acc: 0.7
Batch: 540; loss: 0.78; acc: 0.72
Batch: 560; loss: 0.96; acc: 0.69
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.75; acc: 0.8
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.72
Batch: 660; loss: 0.65; acc: 0.77
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.81; acc: 0.77
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6655041514688237; val_accuracy: 0.7970740445859873 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.94; acc: 0.7
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.82; acc: 0.73
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.78
Batch: 200; loss: 0.84; acc: 0.75
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.72
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.9; acc: 0.75
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.92; acc: 0.7
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.57; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.75
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.8; acc: 0.69
Batch: 500; loss: 0.64; acc: 0.75
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.72; acc: 0.75
Batch: 660; loss: 1.02; acc: 0.73
Batch: 680; loss: 0.65; acc: 0.75
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.76; acc: 0.67
Batch: 760; loss: 0.68; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.6609541867758818; val_accuracy: 0.7981687898089171 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 0.81; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.75
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 0.9; acc: 0.78
Batch: 320; loss: 0.8; acc: 0.78
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.78
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.87; acc: 0.72
Batch: 560; loss: 1.09; acc: 0.62
Batch: 580; loss: 1.01; acc: 0.67
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 0.8; acc: 0.72
Batch: 640; loss: 0.73; acc: 0.73
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.72
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.75
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.6782982582878915; val_accuracy: 0.790406050955414 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.75
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.93; acc: 0.7
Batch: 80; loss: 0.67; acc: 0.75
Batch: 100; loss: 0.8; acc: 0.73
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.89; acc: 0.67
Batch: 200; loss: 0.77; acc: 0.73
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.89; acc: 0.72
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.63; acc: 0.75
Batch: 300; loss: 0.89; acc: 0.69
Batch: 320; loss: 0.91; acc: 0.69
Batch: 340; loss: 0.9; acc: 0.75
Batch: 360; loss: 0.84; acc: 0.73
Batch: 380; loss: 0.96; acc: 0.73
Batch: 400; loss: 0.59; acc: 0.78
Batch: 420; loss: 0.83; acc: 0.75
Batch: 440; loss: 0.63; acc: 0.75
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.95; acc: 0.67
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.87; acc: 0.69
Batch: 580; loss: 0.74; acc: 0.72
Batch: 600; loss: 0.75; acc: 0.83
Batch: 620; loss: 0.75; acc: 0.73
Batch: 640; loss: 0.86; acc: 0.77
Batch: 660; loss: 0.94; acc: 0.72
Batch: 680; loss: 0.54; acc: 0.78
Batch: 700; loss: 0.78; acc: 0.75
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.92; acc: 0.77
Batch: 760; loss: 0.84; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6647510166950287; val_accuracy: 0.7971735668789809 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.91; acc: 0.77
Batch: 160; loss: 0.7; acc: 0.8
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.78
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.9; acc: 0.72
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.72
Batch: 400; loss: 0.66; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.78
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.73
Batch: 540; loss: 0.91; acc: 0.69
Batch: 560; loss: 0.91; acc: 0.69
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.8; acc: 0.8
Batch: 660; loss: 0.82; acc: 0.69
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.73
Batch: 720; loss: 0.98; acc: 0.64
Batch: 740; loss: 0.73; acc: 0.75
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.6698040060556618; val_accuracy: 0.7947850318471338 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.69
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 0.94; acc: 0.64
Batch: 80; loss: 0.76; acc: 0.72
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.83; acc: 0.72
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.75
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.83; acc: 0.72
Batch: 320; loss: 0.81; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.85; acc: 0.7
Batch: 380; loss: 0.64; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.87; acc: 0.72
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.81
Batch: 540; loss: 0.93; acc: 0.7
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.77; acc: 0.78
Batch: 600; loss: 0.74; acc: 0.7
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.77
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.85; acc: 0.7
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.6698755475745839; val_accuracy: 0.7929936305732485 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.85; acc: 0.72
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.73
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.86; acc: 0.75
Batch: 200; loss: 0.78; acc: 0.73
Batch: 220; loss: 0.75; acc: 0.77
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 1.01; acc: 0.67
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 1.04; acc: 0.67
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.75; acc: 0.72
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.75
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.8
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.5; acc: 0.8
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.93; acc: 0.75
Batch: 780; loss: 0.81; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6594268246820778; val_accuracy: 0.7984673566878981 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.67; acc: 0.75
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.9; acc: 0.77
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.82; acc: 0.7
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 1.05; acc: 0.66
Batch: 260; loss: 1.01; acc: 0.73
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.66
Batch: 380; loss: 1.01; acc: 0.66
Batch: 400; loss: 0.94; acc: 0.66
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.96; acc: 0.64
Batch: 460; loss: 0.83; acc: 0.73
Batch: 480; loss: 0.59; acc: 0.78
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.62; acc: 0.77
Batch: 600; loss: 0.76; acc: 0.75
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.99; acc: 0.73
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6586016505766826; val_accuracy: 0.7991640127388535 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.84; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.72
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.69
Batch: 220; loss: 0.76; acc: 0.72
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 0.73; acc: 0.75
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.71; acc: 0.75
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.84; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.87; acc: 0.64
Batch: 540; loss: 0.78; acc: 0.75
Batch: 560; loss: 0.61; acc: 0.77
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.74; acc: 0.72
Batch: 620; loss: 0.77; acc: 0.72
Batch: 640; loss: 0.87; acc: 0.67
Batch: 660; loss: 0.73; acc: 0.77
Batch: 680; loss: 0.79; acc: 0.66
Batch: 700; loss: 0.65; acc: 0.81
Batch: 720; loss: 0.8; acc: 0.69
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6616867877495517; val_accuracy: 0.7953821656050956 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.7
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.8; acc: 0.78
Batch: 220; loss: 0.86; acc: 0.77
Batch: 240; loss: 0.93; acc: 0.72
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.82; acc: 0.75
Batch: 300; loss: 0.84; acc: 0.73
Batch: 320; loss: 0.81; acc: 0.72
Batch: 340; loss: 0.66; acc: 0.75
Batch: 360; loss: 0.95; acc: 0.64
Batch: 380; loss: 0.7; acc: 0.78
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.77
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.78
Batch: 520; loss: 0.78; acc: 0.69
Batch: 540; loss: 0.76; acc: 0.75
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.94; acc: 0.73
Batch: 600; loss: 0.83; acc: 0.72
Batch: 620; loss: 0.97; acc: 0.69
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.83; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.81
Batch: 700; loss: 0.81; acc: 0.75
Batch: 720; loss: 1.09; acc: 0.72
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6602470538798412; val_accuracy: 0.79796974522293 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.75
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.73
Batch: 160; loss: 0.71; acc: 0.77
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.73
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.73
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.78
Batch: 400; loss: 0.71; acc: 0.75
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.77; acc: 0.73
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.75
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.74; acc: 0.77
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 1.01; acc: 0.7
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.85; acc: 0.77
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6606907881559081; val_accuracy: 0.7969745222929936 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.86; acc: 0.72
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.8; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.83; acc: 0.72
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.73
Batch: 360; loss: 1.0; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.77
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.74; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.7; acc: 0.73
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.98; acc: 0.69
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.71; acc: 0.77
Batch: 640; loss: 0.94; acc: 0.69
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.77; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.6621915420908837; val_accuracy: 0.7983678343949044 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 1.19; acc: 0.66
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.85; acc: 0.73
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.89; acc: 0.73
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.67; acc: 0.78
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.92; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.77
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.85; acc: 0.72
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.81; acc: 0.73
Batch: 680; loss: 0.68; acc: 0.75
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.85; acc: 0.69
Batch: 760; loss: 0.93; acc: 0.78
Batch: 780; loss: 0.78; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.6598965859716865; val_accuracy: 0.7972730891719745 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.78
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.72; acc: 0.72
Batch: 180; loss: 0.98; acc: 0.7
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.8
Batch: 400; loss: 0.8; acc: 0.73
Batch: 420; loss: 0.75; acc: 0.73
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.79; acc: 0.77
Batch: 500; loss: 0.82; acc: 0.72
Batch: 520; loss: 0.73; acc: 0.78
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.97; acc: 0.75
Batch: 600; loss: 0.76; acc: 0.69
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.87; acc: 0.64
Batch: 720; loss: 0.89; acc: 0.72
Batch: 740; loss: 0.82; acc: 0.72
Batch: 760; loss: 0.82; acc: 0.73
Batch: 780; loss: 0.78; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6595872951920625; val_accuracy: 0.7983678343949044 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.7
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.98; acc: 0.73
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 0.57; acc: 0.77
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.83; acc: 0.75
Batch: 260; loss: 0.72; acc: 0.73
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.77
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.83; acc: 0.69
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.77; acc: 0.69
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.79; acc: 0.78
Batch: 500; loss: 1.02; acc: 0.67
Batch: 520; loss: 0.74; acc: 0.77
Batch: 540; loss: 0.77; acc: 0.72
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.62; acc: 0.78
Batch: 620; loss: 0.8; acc: 0.81
Batch: 640; loss: 0.64; acc: 0.77
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 0.78; acc: 0.73
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.8
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6581469096575573; val_accuracy: 0.7990644904458599 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.12; acc: 0.69
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.76; acc: 0.75
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.89; acc: 0.67
Batch: 160; loss: 0.77; acc: 0.75
Batch: 180; loss: 0.7; acc: 0.77
Batch: 200; loss: 0.59; acc: 0.78
Batch: 220; loss: 0.89; acc: 0.7
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.83; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.73
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.81; acc: 0.72
Batch: 360; loss: 0.84; acc: 0.73
Batch: 380; loss: 0.93; acc: 0.75
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.85; acc: 0.78
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.97; acc: 0.75
Batch: 480; loss: 0.76; acc: 0.7
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.86; acc: 0.72
Batch: 560; loss: 1.01; acc: 0.66
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.72
Batch: 660; loss: 1.02; acc: 0.67
Batch: 680; loss: 0.97; acc: 0.73
Batch: 700; loss: 0.79; acc: 0.73
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.6621101409386677; val_accuracy: 0.7969745222929936 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.77
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.69; acc: 0.73
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.69; acc: 0.75
Batch: 300; loss: 0.6; acc: 0.78
Batch: 320; loss: 0.89; acc: 0.72
Batch: 340; loss: 0.91; acc: 0.69
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.87; acc: 0.7
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 0.72; acc: 0.78
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 1.0; acc: 0.66
Batch: 520; loss: 0.81; acc: 0.72
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 0.99; acc: 0.73
Batch: 640; loss: 0.9; acc: 0.69
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.75; acc: 0.72
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.75; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 0.82; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6591001729107206; val_accuracy: 0.7997611464968153 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 1.1; acc: 0.7
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.75
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.78
Batch: 220; loss: 0.72; acc: 0.73
Batch: 240; loss: 0.8; acc: 0.78
Batch: 260; loss: 0.9; acc: 0.72
Batch: 280; loss: 0.81; acc: 0.7
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.77
Batch: 360; loss: 0.56; acc: 0.77
Batch: 380; loss: 0.81; acc: 0.72
Batch: 400; loss: 0.99; acc: 0.77
Batch: 420; loss: 0.79; acc: 0.72
Batch: 440; loss: 0.82; acc: 0.72
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.77
Batch: 560; loss: 1.09; acc: 0.62
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.75
Batch: 620; loss: 0.84; acc: 0.7
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.71; acc: 0.75
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.91; acc: 0.72
Batch: 760; loss: 0.61; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.6575760243424944; val_accuracy: 0.7997611464968153 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.9; acc: 0.7
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.71; acc: 0.84
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.71; acc: 0.73
Batch: 300; loss: 0.79; acc: 0.72
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.7; acc: 0.69
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.77; acc: 0.73
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 0.67; acc: 0.75
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.8
Batch: 580; loss: 0.67; acc: 0.77
Batch: 600; loss: 0.68; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 1.11; acc: 0.64
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.77
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.72
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6600928018047552; val_accuracy: 0.7994625796178344 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.67
Batch: 40; loss: 0.97; acc: 0.7
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.75
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.85; acc: 0.69
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.72
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 0.95; acc: 0.69
Batch: 420; loss: 0.84; acc: 0.8
Batch: 440; loss: 0.83; acc: 0.72
Batch: 460; loss: 0.78; acc: 0.72
Batch: 480; loss: 1.0; acc: 0.66
Batch: 500; loss: 0.64; acc: 0.75
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 1.08; acc: 0.69
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.89; acc: 0.69
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.64; acc: 0.75
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.81; acc: 0.77
Batch: 700; loss: 0.81; acc: 0.7
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.77; acc: 0.78
Batch: 760; loss: 0.95; acc: 0.73
Batch: 780; loss: 0.92; acc: 0.7
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6581889199223488; val_accuracy: 0.7990644904458599 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.73
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.94; acc: 0.72
Batch: 100; loss: 0.78; acc: 0.75
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 0.87; acc: 0.67
Batch: 200; loss: 1.17; acc: 0.69
Batch: 220; loss: 0.79; acc: 0.7
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.87; acc: 0.73
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.83; acc: 0.81
Batch: 320; loss: 0.85; acc: 0.73
Batch: 340; loss: 1.01; acc: 0.62
Batch: 360; loss: 0.99; acc: 0.73
Batch: 380; loss: 0.7; acc: 0.77
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.86; acc: 0.69
Batch: 460; loss: 0.76; acc: 0.72
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 0.81; acc: 0.72
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.75; acc: 0.72
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.79; acc: 0.72
Batch: 780; loss: 0.84; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6591292830409518; val_accuracy: 0.7989649681528662 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 1.07; acc: 0.61
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.89; acc: 0.69
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.87; acc: 0.69
Batch: 260; loss: 0.88; acc: 0.72
Batch: 280; loss: 0.86; acc: 0.75
Batch: 300; loss: 0.71; acc: 0.69
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 0.94; acc: 0.69
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.9; acc: 0.75
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.84; acc: 0.69
Batch: 520; loss: 0.75; acc: 0.75
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.65; acc: 0.72
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.65; acc: 0.75
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.82; acc: 0.72
Batch: 760; loss: 0.84; acc: 0.73
Batch: 780; loss: 0.69; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6591013973685587; val_accuracy: 0.798765923566879 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.67
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.82; acc: 0.73
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.73
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.97; acc: 0.7
Batch: 500; loss: 0.94; acc: 0.69
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.67; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.75
Batch: 580; loss: 0.95; acc: 0.73
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.76; acc: 0.7
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.84; acc: 0.77
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.86; acc: 0.67
Batch: 780; loss: 0.75; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6589310891499185; val_accuracy: 0.7983678343949044 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 1.04; acc: 0.62
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.66
Batch: 240; loss: 0.92; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.72
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.99; acc: 0.73
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 0.93; acc: 0.7
Batch: 520; loss: 0.69; acc: 0.73
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.89; acc: 0.72
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6573971705451892; val_accuracy: 0.7990644904458599 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.69
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 1.14; acc: 0.69
Batch: 160; loss: 0.78; acc: 0.75
Batch: 180; loss: 0.66; acc: 0.73
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.92; acc: 0.75
Batch: 240; loss: 0.95; acc: 0.72
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.91; acc: 0.75
Batch: 300; loss: 0.63; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.75
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.83; acc: 0.73
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.86; acc: 0.77
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.81; acc: 0.72
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.75
Batch: 600; loss: 0.89; acc: 0.66
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 0.78; acc: 0.7
Batch: 680; loss: 0.71; acc: 0.75
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.6581299812740581; val_accuracy: 0.7992635350318471 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.72
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.73
Batch: 280; loss: 0.95; acc: 0.7
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.77
Batch: 360; loss: 0.83; acc: 0.8
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.84; acc: 0.77
Batch: 420; loss: 0.57; acc: 0.8
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.85; acc: 0.75
Batch: 500; loss: 0.71; acc: 0.73
Batch: 520; loss: 0.63; acc: 0.75
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.74; acc: 0.75
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.96; acc: 0.72
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.86; acc: 0.75
Batch: 660; loss: 1.04; acc: 0.73
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.79; acc: 0.81
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6588713373917683; val_accuracy: 0.7996616242038217 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 1.0; acc: 0.66
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.81
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.74; acc: 0.75
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 1.01; acc: 0.73
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.75
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.81
Batch: 400; loss: 0.92; acc: 0.72
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.83; acc: 0.75
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.86; acc: 0.72
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.78
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.77
Batch: 600; loss: 0.73; acc: 0.77
Batch: 620; loss: 0.89; acc: 0.72
Batch: 640; loss: 0.58; acc: 0.8
Batch: 660; loss: 0.89; acc: 0.72
Batch: 680; loss: 0.63; acc: 0.78
Batch: 700; loss: 0.97; acc: 0.66
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.69; acc: 0.77
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.83; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6589151117832038; val_accuracy: 0.798765923566879 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.64
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.84; acc: 0.73
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.83; acc: 0.81
Batch: 160; loss: 0.9; acc: 0.69
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 1.1; acc: 0.64
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.77
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.86; acc: 0.7
Batch: 300; loss: 0.87; acc: 0.75
Batch: 320; loss: 0.73; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.69; acc: 0.78
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.73; acc: 0.77
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.78
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.82; acc: 0.72
Batch: 720; loss: 0.85; acc: 0.78
Batch: 740; loss: 0.75; acc: 0.77
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.657553607016612; val_accuracy: 0.799562101910828 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.78
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.8
Batch: 160; loss: 0.66; acc: 0.77
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.75
Batch: 240; loss: 0.96; acc: 0.67
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 1.0; acc: 0.67
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 0.9; acc: 0.75
Batch: 460; loss: 0.6; acc: 0.77
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.78
Batch: 580; loss: 0.67; acc: 0.7
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.8; acc: 0.78
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.78
Batch: 760; loss: 0.87; acc: 0.77
Batch: 780; loss: 0.9; acc: 0.72
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.6581569163093142; val_accuracy: 0.7985668789808917 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.8; acc: 0.69
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.73
Batch: 160; loss: 0.98; acc: 0.69
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.75
Batch: 220; loss: 0.91; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.72
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.7
Batch: 420; loss: 0.69; acc: 0.77
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.67; acc: 0.73
Batch: 480; loss: 1.0; acc: 0.78
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 0.65; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.73
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.77; acc: 0.73
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.79; acc: 0.78
Batch: 780; loss: 0.91; acc: 0.7
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6582968788351983; val_accuracy: 0.798765923566879 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.77
Batch: 160; loss: 0.95; acc: 0.69
Batch: 180; loss: 0.87; acc: 0.72
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.77
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.73; acc: 0.77
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.72
Batch: 360; loss: 1.06; acc: 0.69
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.75; acc: 0.72
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.86; acc: 0.61
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.82; acc: 0.77
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.83; acc: 0.73
Batch: 680; loss: 0.69; acc: 0.72
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.96; acc: 0.69
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.57; acc: 0.8
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6593963071988647; val_accuracy: 0.7983678343949044 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.78
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.76; acc: 0.75
Batch: 280; loss: 0.83; acc: 0.78
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.8; acc: 0.75
Batch: 420; loss: 0.76; acc: 0.73
Batch: 440; loss: 0.73; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.98; acc: 0.67
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 0.9; acc: 0.78
Batch: 680; loss: 0.79; acc: 0.73
Batch: 700; loss: 0.78; acc: 0.81
Batch: 720; loss: 0.85; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.75
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6582404196642007; val_accuracy: 0.7993630573248408 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 0.68; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.7
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.78; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.79; acc: 0.73
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.88; acc: 0.69
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.94; acc: 0.72
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.83; acc: 0.78
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.96; acc: 0.67
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.67
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.95; acc: 0.72
Batch: 500; loss: 0.7; acc: 0.75
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 0.85; acc: 0.78
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 0.55; acc: 0.78
Batch: 620; loss: 0.71; acc: 0.75
Batch: 640; loss: 0.85; acc: 0.64
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.73
Batch: 740; loss: 0.58; acc: 0.77
Batch: 760; loss: 0.76; acc: 0.73
Batch: 780; loss: 0.89; acc: 0.72
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.6593815902615808; val_accuracy: 0.8001592356687898 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.92; acc: 0.69
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.73
Batch: 180; loss: 0.78; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.69
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.9; acc: 0.72
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 0.6; acc: 0.77
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.75
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.7
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.94; acc: 0.72
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.88; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.61; acc: 0.73
Batch: 740; loss: 0.76; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6586730113834333; val_accuracy: 0.7983678343949044 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.79; acc: 0.73
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.75
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 0.99; acc: 0.73
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.91; acc: 0.67
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.7; acc: 0.75
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.59; acc: 0.78
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.68; acc: 0.8
Batch: 400; loss: 0.98; acc: 0.69
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.8; acc: 0.75
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.92; acc: 0.77
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.74; acc: 0.77
Batch: 620; loss: 0.88; acc: 0.7
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.85; acc: 0.73
Batch: 760; loss: 0.86; acc: 0.66
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6588079608549737; val_accuracy: 0.7994625796178344 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 1.31; acc: 0.64
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 0.68; acc: 0.73
Batch: 280; loss: 0.71; acc: 0.75
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.86; acc: 0.72
Batch: 400; loss: 0.78; acc: 0.7
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 1.05; acc: 0.72
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 1.02; acc: 0.69
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.81; acc: 0.72
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.74; acc: 0.78
Batch: 660; loss: 0.87; acc: 0.75
Batch: 680; loss: 0.55; acc: 0.81
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6582863184676808; val_accuracy: 0.7994625796178344 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 21258
elements in E: 4499000
fraction nonzero: 0.004725050011113581
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.11
Batch: 420; loss: 2.29; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.28; acc: 0.06
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.16
Batch: 580; loss: 2.27; acc: 0.23
Batch: 600; loss: 2.28; acc: 0.2
Batch: 620; loss: 2.28; acc: 0.19
Batch: 640; loss: 2.26; acc: 0.22
Batch: 660; loss: 2.26; acc: 0.27
Batch: 680; loss: 2.27; acc: 0.19
Batch: 700; loss: 2.26; acc: 0.23
Batch: 720; loss: 2.26; acc: 0.27
Batch: 740; loss: 2.28; acc: 0.22
Batch: 760; loss: 2.26; acc: 0.27
Batch: 780; loss: 2.23; acc: 0.31
Train Epoch over. train_loss: 2.29; train_accuracy: 0.13 

Batch: 0; loss: 2.25; acc: 0.3
Batch: 20; loss: 2.28; acc: 0.17
Batch: 40; loss: 2.25; acc: 0.25
Batch: 60; loss: 2.25; acc: 0.23
Batch: 80; loss: 2.26; acc: 0.27
Batch: 100; loss: 2.26; acc: 0.19
Batch: 120; loss: 2.25; acc: 0.25
Batch: 140; loss: 2.25; acc: 0.17
Val Epoch over. val_loss: 2.256153162877271; val_accuracy: 0.2307921974522293 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.22
Batch: 20; loss: 2.23; acc: 0.3
Batch: 40; loss: 2.24; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.33
Batch: 80; loss: 2.22; acc: 0.33
Batch: 100; loss: 2.24; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.38
Batch: 140; loss: 2.2; acc: 0.36
Batch: 160; loss: 2.16; acc: 0.41
Batch: 180; loss: 2.19; acc: 0.44
Batch: 200; loss: 2.16; acc: 0.31
Batch: 220; loss: 2.14; acc: 0.47
Batch: 240; loss: 2.05; acc: 0.48
Batch: 260; loss: 1.95; acc: 0.47
Batch: 280; loss: 1.85; acc: 0.47
Batch: 300; loss: 1.76; acc: 0.41
Batch: 320; loss: 1.83; acc: 0.31
Batch: 340; loss: 1.7; acc: 0.38
Batch: 360; loss: 1.37; acc: 0.56
Batch: 380; loss: 1.19; acc: 0.67
Batch: 400; loss: 1.34; acc: 0.59
Batch: 420; loss: 1.01; acc: 0.69
Batch: 440; loss: 0.9; acc: 0.78
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 1.03; acc: 0.67
Batch: 500; loss: 0.84; acc: 0.75
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.96; acc: 0.77
Batch: 560; loss: 0.86; acc: 0.72
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 1.14; acc: 0.72
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.92; acc: 0.72
Batch: 720; loss: 0.98; acc: 0.69
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 1.11; acc: 0.67
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 1.46; train_accuracy: 0.55 

Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 1.25; acc: 0.59
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 1.02; acc: 0.7
Batch: 140; loss: 0.64; acc: 0.77
Val Epoch over. val_loss: 0.7742161057936917; val_accuracy: 0.772093949044586 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.98; acc: 0.67
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.72
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.84; acc: 0.73
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.75
Batch: 300; loss: 0.63; acc: 0.75
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.67; acc: 0.77
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.78
Batch: 420; loss: 0.69; acc: 0.73
Batch: 440; loss: 0.63; acc: 0.72
Batch: 460; loss: 0.74; acc: 0.72
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.86; acc: 0.72
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.72
Batch: 600; loss: 0.79; acc: 0.73
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.81; acc: 0.69
Batch: 660; loss: 0.86; acc: 0.73
Batch: 680; loss: 0.93; acc: 0.75
Batch: 700; loss: 0.98; acc: 0.7
Batch: 720; loss: 1.11; acc: 0.62
Batch: 740; loss: 0.61; acc: 0.78
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.77 

Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.55
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.78; acc: 0.78
Val Epoch over. val_loss: 0.9302131188143591; val_accuracy: 0.7089968152866242 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.67
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.86; acc: 0.67
Batch: 240; loss: 1.01; acc: 0.69
Batch: 260; loss: 0.73; acc: 0.72
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.95; acc: 0.72
Batch: 340; loss: 0.93; acc: 0.69
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.79; acc: 0.77
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.65; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 0.92; acc: 0.7
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 1.03; acc: 0.66
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.79; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.53; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.5947948104826508; val_accuracy: 0.816281847133758 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.67; acc: 0.67
Batch: 100; loss: 0.86; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.77
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.59; acc: 0.8
Batch: 300; loss: 0.57; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.7
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.85; acc: 0.69
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.77; acc: 0.72
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.8; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.79; acc: 0.73
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.5723809763124794; val_accuracy: 0.8275278662420382 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.42; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.77
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.79; acc: 0.72
Batch: 400; loss: 0.51; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.77
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.5322286927016677; val_accuracy: 0.8375796178343949 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.72
Batch: 180; loss: 0.91; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.81; acc: 0.77
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.64; acc: 0.78
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.52; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5220781523900427; val_accuracy: 0.8452428343949044 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.75; acc: 0.72
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.6; acc: 0.78
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.5534713386919847; val_accuracy: 0.8307125796178344 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.77; acc: 0.75
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.83
Batch: 540; loss: 0.65; acc: 0.77
Batch: 560; loss: 0.91; acc: 0.77
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.75
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.8
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6482266200955507; val_accuracy: 0.7982683121019108 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.57; acc: 0.77
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.81
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.77
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.8
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.8
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.49; acc: 0.78
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.83; acc: 0.77
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.84 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.88
Val Epoch over. val_loss: 0.5715780510644245; val_accuracy: 0.8284235668789809 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.77
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.77
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.4951964171638914; val_accuracy: 0.8527070063694268 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.81
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.76; acc: 0.75
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.91; acc: 0.78
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.62; acc: 0.77
Batch: 760; loss: 0.65; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.521108059461709; val_accuracy: 0.8457404458598726 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.75
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.72; acc: 0.78
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.8
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.81
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.48646171495413326; val_accuracy: 0.8577826433121019 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.8
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.8
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.68; acc: 0.72
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.49472235048272806; val_accuracy: 0.8542993630573248 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.77
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.77
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.72
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.48879566399534796; val_accuracy: 0.8554936305732485 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.81; acc: 0.73
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.8
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.8
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.8
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.72
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.8
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.45; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.4884140686054898; val_accuracy: 0.8522093949044586 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.53; acc: 0.78
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.75
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.5175029509196616; val_accuracy: 0.8450437898089171 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.73
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.69; acc: 0.75
Batch: 180; loss: 0.53; acc: 0.77
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.82; acc: 0.75
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.8; acc: 0.77
Batch: 360; loss: 0.69; acc: 0.77
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.78
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.74; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.48621648625963054; val_accuracy: 0.8530055732484076 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.68; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.91; acc: 0.78
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.48619973232412034; val_accuracy: 0.8527070063694268 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.78
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.75
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4913465316128579; val_accuracy: 0.851015127388535 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.52; acc: 0.8
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.47711415693258785; val_accuracy: 0.8561902866242038 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.77
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.85; acc: 0.7
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.4757636488433097; val_accuracy: 0.8548964968152867 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.76; acc: 0.81
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.8
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.81
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.8
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.73
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.78
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.8
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.78
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.59; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.48128436837986016; val_accuracy: 0.8532046178343949 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.78
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.77
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4761058783075612; val_accuracy: 0.8558917197452229 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.75
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.8
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.36; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.47783518150733534; val_accuracy: 0.8549960191082803 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.76; acc: 0.77
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.77; acc: 0.77
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.47555595911612175; val_accuracy: 0.8554936305732485 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.78
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.4750755975011048; val_accuracy: 0.8556926751592356 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.91; acc: 0.72
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.78
Batch: 520; loss: 0.52; acc: 0.78
Batch: 540; loss: 0.72; acc: 0.8
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.75
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.75; acc: 0.72
Batch: 720; loss: 0.79; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.8
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4883040510545111; val_accuracy: 0.850218949044586 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.8
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.8; acc: 0.7
Batch: 700; loss: 0.9; acc: 0.77
Batch: 720; loss: 0.38; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.47232446520571497; val_accuracy: 0.8558917197452229 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.71; acc: 0.75
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.8
Batch: 280; loss: 0.59; acc: 0.89
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.92; acc: 0.77
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.47463519197360726; val_accuracy: 0.8550955414012739 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.56; acc: 0.78
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.71; acc: 0.78
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.4728863161460609; val_accuracy: 0.8558917197452229 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.89; acc: 0.75
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.78; acc: 0.75
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.78
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.4716485626758284; val_accuracy: 0.8554936305732485 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.78
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.78
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.4707080859477353; val_accuracy: 0.8548964968152867 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.8
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.8
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.4703384101580662; val_accuracy: 0.8576831210191083 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.55; acc: 0.78
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.32; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.47139531611257296; val_accuracy: 0.857484076433121 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.71; acc: 0.78
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.57; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.75
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.78
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.57; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.81
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.48; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.4705857642137321; val_accuracy: 0.8554936305732485 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.72; acc: 0.75
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.78
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4713414915048393; val_accuracy: 0.8543988853503185 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.78
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.91
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.72
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.81
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.55; acc: 0.77
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.47106367871639837; val_accuracy: 0.8552945859872612 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.47120192295806423; val_accuracy: 0.8539012738853503 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.73
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.92; acc: 0.77
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.86; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.77
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.69
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.4703611913759997; val_accuracy: 0.8554936305732485 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.8
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.74; acc: 0.73
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.44; acc: 0.81
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.85; acc: 0.72
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.47096151588069407; val_accuracy: 0.8553941082802548 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.75
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.47061513316859105; val_accuracy: 0.854796974522293 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.55; acc: 0.77
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.8
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.85; acc: 0.73
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.4698626653403993; val_accuracy: 0.8557921974522293 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.9; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.47015825777676457; val_accuracy: 0.854796974522293 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.64; acc: 0.77
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4702297476636376; val_accuracy: 0.8556926751592356 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.56; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.77
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.47006443532029535; val_accuracy: 0.8546974522292994 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.75
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.73
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.75
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.8
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.4696927361047951; val_accuracy: 0.8562898089171974 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.97; acc: 0.72
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.4696728717179815; val_accuracy: 0.855593152866242 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.42; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.77
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.8
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.77
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.78
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4694088780955904; val_accuracy: 0.8565883757961783 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.65; acc: 0.78
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.71; acc: 0.77
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.8
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.469727384436662; val_accuracy: 0.8548964968152867 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 23286
elements in E: 4948900
fraction nonzero: 0.004705288043807715
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.28; acc: 0.14
Batch: 320; loss: 2.27; acc: 0.22
Batch: 340; loss: 2.28; acc: 0.2
Batch: 360; loss: 2.28; acc: 0.2
Batch: 380; loss: 2.27; acc: 0.27
Batch: 400; loss: 2.26; acc: 0.31
Batch: 420; loss: 2.26; acc: 0.39
Batch: 440; loss: 2.26; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.39
Batch: 480; loss: 2.25; acc: 0.31
Batch: 500; loss: 2.26; acc: 0.33
Batch: 520; loss: 2.21; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.42
Batch: 560; loss: 2.17; acc: 0.45
Batch: 580; loss: 2.16; acc: 0.39
Batch: 600; loss: 2.12; acc: 0.52
Batch: 620; loss: 2.09; acc: 0.45
Batch: 640; loss: 2.04; acc: 0.34
Batch: 660; loss: 1.95; acc: 0.41
Batch: 680; loss: 1.81; acc: 0.45
Batch: 700; loss: 1.75; acc: 0.47
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.36; acc: 0.66
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.32; acc: 0.55
Train Epoch over. train_loss: 2.15; train_accuracy: 0.27 

Batch: 0; loss: 1.2; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.5
Batch: 40; loss: 1.0; acc: 0.78
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.28; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.64
Val Epoch over. val_loss: 1.2595115402701553; val_accuracy: 0.6048964968152867 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.52
Batch: 20; loss: 1.4; acc: 0.61
Batch: 40; loss: 1.04; acc: 0.62
Batch: 60; loss: 0.91; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 0.89; acc: 0.64
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.9; acc: 0.73
Batch: 160; loss: 0.92; acc: 0.7
Batch: 180; loss: 1.11; acc: 0.69
Batch: 200; loss: 0.76; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.78
Batch: 260; loss: 0.78; acc: 0.72
Batch: 280; loss: 0.93; acc: 0.69
Batch: 300; loss: 0.78; acc: 0.75
Batch: 320; loss: 0.91; acc: 0.69
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.76; acc: 0.67
Batch: 380; loss: 0.87; acc: 0.77
Batch: 400; loss: 0.91; acc: 0.67
Batch: 420; loss: 0.85; acc: 0.75
Batch: 440; loss: 0.86; acc: 0.7
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.81; acc: 0.7
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.7; acc: 0.75
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.87; acc: 0.73
Batch: 600; loss: 0.84; acc: 0.7
Batch: 620; loss: 1.03; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.78
Batch: 700; loss: 0.88; acc: 0.69
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 1.15; acc: 0.55
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.78
Val Epoch over. val_loss: 0.9006805173151052; val_accuracy: 0.7242237261146497 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.72
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.8
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.77; acc: 0.7
Batch: 480; loss: 0.7; acc: 0.77
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.88; acc: 0.69
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.75; acc: 0.77
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.8
Batch: 700; loss: 0.88; acc: 0.73
Batch: 720; loss: 0.95; acc: 0.73
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.8
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.4; acc: 0.83
Val Epoch over. val_loss: 0.5913698431223061; val_accuracy: 0.8225517515923567 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.7
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.61; acc: 0.78
Batch: 220; loss: 0.72; acc: 0.77
Batch: 240; loss: 0.9; acc: 0.75
Batch: 260; loss: 0.68; acc: 0.8
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.75
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.82 

Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.6111084977342824; val_accuracy: 0.8129976114649682 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.63; acc: 0.8
Batch: 280; loss: 0.61; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.73
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.91; acc: 0.78
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.62; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.73
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.85; acc: 0.78
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.69
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.69; acc: 0.78
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.5438078111334211; val_accuracy: 0.8282245222929936 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.78
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.75
Batch: 340; loss: 0.41; acc: 0.83
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.88; acc: 0.73
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.41; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.6163024189555721; val_accuracy: 0.8027468152866242 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.7
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.71; acc: 0.73
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.8
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.6; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.5112440803438235; val_accuracy: 0.8376791401273885 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.78
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.48; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.78
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.69
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.5619119888847801; val_accuracy: 0.8180732484076433 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.6; acc: 0.77
Batch: 380; loss: 0.4; acc: 0.83
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.81
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.75
Batch: 740; loss: 0.47; acc: 0.8
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.5424996511476814; val_accuracy: 0.8491242038216561 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.77
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.77
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.83
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.85; acc: 0.75
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.4544825605621004; val_accuracy: 0.8627587579617835 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.54; acc: 0.8
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.78
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4239032090089883; val_accuracy: 0.8694267515923567 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.78; acc: 0.75
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.81
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.46096673384783377; val_accuracy: 0.8599721337579618 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4261718972758123; val_accuracy: 0.872312898089172 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.78
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.43411211337253547; val_accuracy: 0.8670382165605095 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.8
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.73
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.78
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.64; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.45; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4328820603858134; val_accuracy: 0.866640127388535 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.84
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.81
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.4157937629397508; val_accuracy: 0.8745023885350318 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.73
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.8
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.4198952547399102; val_accuracy: 0.8717157643312102 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.58; acc: 0.8
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.92; acc: 0.78
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.78
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.4269984875135361; val_accuracy: 0.8691281847133758 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.83
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.8
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.78
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.4307342749207642; val_accuracy: 0.8674363057324841 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.75
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.67; acc: 0.73
Batch: 420; loss: 0.53; acc: 0.8
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.78
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.8
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.410539997089061; val_accuracy: 0.8793789808917197 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3961112494491468; val_accuracy: 0.8820660828025477 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.89; acc: 0.77
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.39393428860196644; val_accuracy: 0.8845541401273885 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.75
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.45; acc: 0.8
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.81
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3910723245067961; val_accuracy: 0.8810708598726115 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.77
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.92
Val Epoch over. val_loss: 0.3896907680448453; val_accuracy: 0.8848527070063694 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.83
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.91
Val Epoch over. val_loss: 0.38681855721838154; val_accuracy: 0.8863455414012739 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.8
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.77
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.38986252238796015; val_accuracy: 0.8835589171974523 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.39761865841355293; val_accuracy: 0.8812699044585988 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.38301715629685457; val_accuracy: 0.8850517515923567 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.81
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.38414570722420505; val_accuracy: 0.886843152866242 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.38173513328004033; val_accuracy: 0.8864450636942676 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.75
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.8
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.85; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.75
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.37619991163918926; val_accuracy: 0.8896297770700637 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.77
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37669957590520764; val_accuracy: 0.8901273885350318 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.73; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37886267928940476; val_accuracy: 0.8905254777070064 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.77; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.66; acc: 0.78
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.69; acc: 0.7
Batch: 780; loss: 0.39; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37526929335799186; val_accuracy: 0.8905254777070064 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.81
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.27; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3780228444819997; val_accuracy: 0.8893312101910829 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3772019446845267; val_accuracy: 0.88953025477707 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3746310563603784; val_accuracy: 0.8912221337579618 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.8
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.37543901222147; val_accuracy: 0.8916202229299363 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.81; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.57; acc: 0.78
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3734191250839051; val_accuracy: 0.8927149681528662 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3739724439230694; val_accuracy: 0.8928144904458599 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.78
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3734775656347821; val_accuracy: 0.8931130573248408 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.37371189949239136; val_accuracy: 0.8927149681528662 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.28; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.81
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.37481536872827326; val_accuracy: 0.8915207006369427 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.8; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.8
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.37389198410662877; val_accuracy: 0.8922173566878981 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.81
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.78
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.373004289427001; val_accuracy: 0.8932125796178344 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.69; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.78
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.374728870714546; val_accuracy: 0.8919187898089171 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.66; acc: 0.75
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.78
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3733324516256144; val_accuracy: 0.8919187898089171 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.5; acc: 0.8
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.73; acc: 0.78
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3725334120214365; val_accuracy: 0.8922173566878981 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.3; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.61; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.8
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3725144597850028; val_accuracy: 0.8923168789808917 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.78
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.72; acc: 0.75
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.97; acc: 0.77
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3725810438204723; val_accuracy: 0.8932125796178344 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_110_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 25348
elements in E: 5398800
fraction nonzero: 0.004695117433503742
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.17
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.11
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.19
Batch: 340; loss: 2.29; acc: 0.2
Batch: 360; loss: 2.29; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.19
Batch: 400; loss: 2.28; acc: 0.2
Batch: 420; loss: 2.27; acc: 0.36
Batch: 440; loss: 2.28; acc: 0.19
Batch: 460; loss: 2.27; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.2
Batch: 500; loss: 2.28; acc: 0.2
Batch: 520; loss: 2.26; acc: 0.3
Batch: 540; loss: 2.26; acc: 0.34
Batch: 560; loss: 2.25; acc: 0.42
Batch: 580; loss: 2.25; acc: 0.39
Batch: 600; loss: 2.24; acc: 0.42
Batch: 620; loss: 2.25; acc: 0.27
Batch: 640; loss: 2.24; acc: 0.3
Batch: 660; loss: 2.23; acc: 0.38
Batch: 680; loss: 2.23; acc: 0.36
Batch: 700; loss: 2.22; acc: 0.28
Batch: 720; loss: 2.19; acc: 0.44
Batch: 740; loss: 2.2; acc: 0.41
Batch: 760; loss: 2.16; acc: 0.47
Batch: 780; loss: 2.17; acc: 0.31
Train Epoch over. train_loss: 2.27; train_accuracy: 0.22 

Batch: 0; loss: 2.18; acc: 0.34
Batch: 20; loss: 2.2; acc: 0.36
Batch: 40; loss: 2.14; acc: 0.45
Batch: 60; loss: 2.16; acc: 0.44
Batch: 80; loss: 2.17; acc: 0.41
Batch: 100; loss: 2.17; acc: 0.39
Batch: 120; loss: 2.17; acc: 0.34
Batch: 140; loss: 2.17; acc: 0.33
Val Epoch over. val_loss: 2.173284671868488; val_accuracy: 0.3499203821656051 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.18; acc: 0.39
Batch: 20; loss: 2.18; acc: 0.33
Batch: 40; loss: 2.13; acc: 0.41
Batch: 60; loss: 2.1; acc: 0.34
Batch: 80; loss: 2.09; acc: 0.38
Batch: 100; loss: 2.09; acc: 0.31
Batch: 120; loss: 2.0; acc: 0.45
Batch: 140; loss: 1.97; acc: 0.45
Batch: 160; loss: 1.89; acc: 0.44
Batch: 180; loss: 1.79; acc: 0.55
Batch: 200; loss: 1.74; acc: 0.5
Batch: 220; loss: 1.71; acc: 0.45
Batch: 240; loss: 1.47; acc: 0.59
Batch: 260; loss: 1.44; acc: 0.58
Batch: 280; loss: 1.37; acc: 0.48
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 1.4; acc: 0.53
Batch: 340; loss: 1.2; acc: 0.61
Batch: 360; loss: 1.02; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.2; acc: 0.59
Batch: 420; loss: 0.89; acc: 0.62
Batch: 440; loss: 0.96; acc: 0.69
Batch: 460; loss: 0.88; acc: 0.8
Batch: 480; loss: 1.16; acc: 0.56
Batch: 500; loss: 1.18; acc: 0.62
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.82; acc: 0.67
Batch: 560; loss: 0.74; acc: 0.75
Batch: 580; loss: 0.82; acc: 0.75
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 1.17; acc: 0.72
Batch: 640; loss: 0.79; acc: 0.7
Batch: 660; loss: 0.86; acc: 0.72
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.81; acc: 0.67
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 1.24; train_accuracy: 0.61 

Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.97; acc: 0.67
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.73
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.41; acc: 0.92
Val Epoch over. val_loss: 0.6829049928932432; val_accuracy: 0.7816480891719745 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.78; acc: 0.72
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.73
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.74; acc: 0.77
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 0.61; acc: 0.75
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.85; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.52; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.7
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.81; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.65; acc: 0.75
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.84; acc: 0.66
Batch: 700; loss: 0.47; acc: 0.78
Batch: 720; loss: 0.9; acc: 0.75
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.75
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 0.55; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 1.25; acc: 0.58
Batch: 140; loss: 0.68; acc: 0.73
Val Epoch over. val_loss: 0.7336205593339956; val_accuracy: 0.7519904458598726 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.73; acc: 0.78
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.75
Batch: 440; loss: 0.57; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.77
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.81
Batch: 720; loss: 0.96; acc: 0.73
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.53; acc: 0.77
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.6360706475320136; val_accuracy: 0.7931926751592356 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.77
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.78
Batch: 220; loss: 0.95; acc: 0.77
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.8
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.73
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.75
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 1.06; acc: 0.67
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.5347065489003613; val_accuracy: 0.8268312101910829 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.8; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.46; acc: 0.78
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.75; acc: 0.69
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.48441471795367586; val_accuracy: 0.8489251592356688 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.92; acc: 0.73
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.75
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.78
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.52; acc: 0.77
Batch: 20; loss: 0.96; acc: 0.66
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.75
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 1.02; acc: 0.67
Batch: 140; loss: 0.61; acc: 0.77
Val Epoch over. val_loss: 0.6158642520190803; val_accuracy: 0.7990644904458599 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.72
Batch: 140; loss: 0.57; acc: 0.78
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.78; acc: 0.73
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.9; acc: 0.81
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.81
Batch: 680; loss: 0.63; acc: 0.77
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.78
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.43; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 0.44; acc: 0.83
Val Epoch over. val_loss: 0.5453121065144326; val_accuracy: 0.8222531847133758 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.78
Batch: 240; loss: 0.76; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.77
Batch: 380; loss: 0.63; acc: 0.78
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.81
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.52; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5148292380818136; val_accuracy: 0.8407643312101911 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.63; acc: 0.77
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.73
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.44097105114702967; val_accuracy: 0.8660429936305732 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.77
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.8
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.77
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.77
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.41519444663623334; val_accuracy: 0.8746019108280255 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.8
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.75
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.77
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.42511366312481036; val_accuracy: 0.8686305732484076 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.77
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.41372733088625463; val_accuracy: 0.8728105095541401 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.79; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.8
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.78
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.76; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.42843935923401716; val_accuracy: 0.869327229299363 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.75
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.31; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.75
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.4221446573924107; val_accuracy: 0.8738057324840764 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.78; acc: 0.75
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.8
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.71; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4010675176503552; val_accuracy: 0.8782842356687898 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.75
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.7
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.75
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.4124696243815361; val_accuracy: 0.869327229299363 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.8
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.8
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.83
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.97; acc: 0.77
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.44058240499276263; val_accuracy: 0.8614649681528662 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.92; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.75
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.78
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.77
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.75
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.54; acc: 0.8
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.43043741764156684; val_accuracy: 0.8675358280254777 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.81
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.67; acc: 0.81
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.40698526856625916; val_accuracy: 0.8751990445859873 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.8
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.78
Batch: 240; loss: 0.47; acc: 0.78
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.83
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3946471542689451; val_accuracy: 0.8780851910828026 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.78
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.55; acc: 0.77
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.78
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.39148152434522177; val_accuracy: 0.8784832802547771 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.8
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.73
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.398014440135971; val_accuracy: 0.8758957006369427 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 1.04; acc: 0.77
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.393419569749741; val_accuracy: 0.8798765923566879 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.81
Batch: 160; loss: 0.36; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.77
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3910595076099323; val_accuracy: 0.8779856687898089 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.78
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.3923318397941863; val_accuracy: 0.8762937898089171 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.77
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.81
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.38811643910445986; val_accuracy: 0.8802746815286624 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.77
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3924717358343161; val_accuracy: 0.8783837579617835 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3870983872633831; val_accuracy: 0.8778861464968153 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.81
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.72; acc: 0.75
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.53; acc: 0.77
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3880137627481655; val_accuracy: 0.8803742038216561 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.78
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.38363522974548825; val_accuracy: 0.880672770700637 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.73
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.38208919578486944; val_accuracy: 0.8813694267515924 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.77
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.78
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3836605030639916; val_accuracy: 0.8790804140127388 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.83
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.77; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.8
Batch: 500; loss: 0.39; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.4; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.8
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.38471889405683346; val_accuracy: 0.8798765923566879 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.78
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.38006010907850446; val_accuracy: 0.8816679936305732 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.75
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.81
Batch: 580; loss: 0.38; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.38069313204592203; val_accuracy: 0.8813694267515924 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.379915226179703; val_accuracy: 0.8819665605095541 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.37791365869106003; val_accuracy: 0.8845541401273885 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.37668119532287503; val_accuracy: 0.8846536624203821 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.87; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.375561530755204; val_accuracy: 0.8853503184713376 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.78
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.37664937617103006; val_accuracy: 0.8831608280254777 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.8
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.95; acc: 0.77
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.78
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.81
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3751938607377611; val_accuracy: 0.8852507961783439 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.75
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.27; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.78
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.37457128500293013; val_accuracy: 0.8851512738853503 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.83
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.37475956013058404; val_accuracy: 0.8838574840764332 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.77
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.73
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3741985568005568; val_accuracy: 0.8850517515923567 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.92; acc: 0.73
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.37410546165363046; val_accuracy: 0.8851512738853503 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3752953334693696; val_accuracy: 0.8843550955414012 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.75
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.38; acc: 0.81
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.81
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.78
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.75
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.37359613759122834; val_accuracy: 0.8847531847133758 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.8
Batch: 320; loss: 0.4; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.78
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.71; acc: 0.75
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.37323274776624266; val_accuracy: 0.8862460191082803 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.8
Batch: 280; loss: 0.46; acc: 0.78
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.37271445012016663; val_accuracy: 0.8855493630573248 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_120_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 27846
elements in E: 5848700
fraction nonzero: 0.004761058012891754
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.31; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.14
Batch: 200; loss: 2.29; acc: 0.12
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.29; acc: 0.14
Batch: 280; loss: 2.28; acc: 0.17
Batch: 300; loss: 2.28; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.17
Batch: 340; loss: 2.29; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.19
Batch: 380; loss: 2.28; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.27
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.26; acc: 0.12
Batch: 460; loss: 2.27; acc: 0.14
Batch: 480; loss: 2.26; acc: 0.17
Batch: 500; loss: 2.26; acc: 0.27
Batch: 520; loss: 2.23; acc: 0.34
Batch: 540; loss: 2.24; acc: 0.25
Batch: 560; loss: 2.21; acc: 0.27
Batch: 580; loss: 2.21; acc: 0.33
Batch: 600; loss: 2.19; acc: 0.36
Batch: 620; loss: 2.18; acc: 0.47
Batch: 640; loss: 2.15; acc: 0.27
Batch: 660; loss: 2.09; acc: 0.38
Batch: 680; loss: 2.01; acc: 0.42
Batch: 700; loss: 1.94; acc: 0.53
Batch: 720; loss: 1.83; acc: 0.44
Batch: 740; loss: 1.62; acc: 0.56
Batch: 760; loss: 1.43; acc: 0.61
Batch: 780; loss: 1.3; acc: 0.56
Train Epoch over. train_loss: 2.19; train_accuracy: 0.24 

Batch: 0; loss: 1.43; acc: 0.55
Batch: 20; loss: 1.54; acc: 0.55
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 1.26; acc: 0.55
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.56
Batch: 120; loss: 1.25; acc: 0.62
Batch: 140; loss: 1.26; acc: 0.53
Val Epoch over. val_loss: 1.3084238217135145; val_accuracy: 0.5710589171974523 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.55
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.94; acc: 0.69
Batch: 60; loss: 1.1; acc: 0.59
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 0.96; acc: 0.69
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.87; acc: 0.73
Batch: 160; loss: 0.77; acc: 0.77
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.65; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.73
Batch: 240; loss: 0.83; acc: 0.73
Batch: 260; loss: 0.92; acc: 0.67
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.64; acc: 0.77
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.72
Batch: 420; loss: 1.35; acc: 0.58
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.82; acc: 0.7
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.63; acc: 0.78
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.75
Batch: 680; loss: 0.57; acc: 0.77
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.77
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.69; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.64
Batch: 140; loss: 0.37; acc: 0.88
Val Epoch over. val_loss: 0.5932585294269452; val_accuracy: 0.8059315286624203 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.71; acc: 0.75
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.77
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.77
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.81; acc: 0.72
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.78
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.8
Batch: 540; loss: 0.54; acc: 0.78
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.75
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.72
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.4882362549472007; val_accuracy: 0.8356886942675159 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.8
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.83
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4012899352771461; val_accuracy: 0.87609474522293 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.77; acc: 0.78
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.8
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.77
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.9; acc: 0.78
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.4467867678802484; val_accuracy: 0.8597730891719745 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.69; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.9; acc: 0.7
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.78
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.45706807352175377; val_accuracy: 0.8532046178343949 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.78
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.78
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.84
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3900314000002138; val_accuracy: 0.8780851910828026 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.83
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.78
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.91
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.81
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.77
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.87; acc: 0.8
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.8
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.72
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 0.39; acc: 0.86
Val Epoch over. val_loss: 0.6483342125537289; val_accuracy: 0.7910031847133758 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.81
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.8
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.47647327535850986; val_accuracy: 0.861265923566879 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.77
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.61; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4470641089567713; val_accuracy: 0.8581807324840764 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.8
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.3552177571187353; val_accuracy: 0.8964968152866242 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.65; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.78
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.81
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.81; acc: 0.81
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3500520754961451; val_accuracy: 0.8959992038216561 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3710725301295329; val_accuracy: 0.8928144904458599 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3864855824667177; val_accuracy: 0.8856488853503185 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3688012021742049; val_accuracy: 0.8894307324840764 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.34757140872015313; val_accuracy: 0.8985867834394905 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.85; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.54; acc: 0.78
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.36519826331715677; val_accuracy: 0.8922173566878981 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.9; acc: 0.78
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3617068927758818; val_accuracy: 0.8898288216560509 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.363804146172894; val_accuracy: 0.890625 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.85; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.36465153031668085; val_accuracy: 0.892515923566879 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3550329866350456; val_accuracy: 0.8973925159235668 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3399715129357235; val_accuracy: 0.900577229299363 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.77
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3457485353396197; val_accuracy: 0.898984872611465 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.98
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.86
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.63; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34610521489647544; val_accuracy: 0.8967953821656051 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.78
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.34772636636996723; val_accuracy: 0.8950039808917197 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3391180544332334; val_accuracy: 0.8992834394904459 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.84
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.34693964115183823; val_accuracy: 0.8958001592356688 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.34087823587618055; val_accuracy: 0.8986863057324841 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.84
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.34216091523219827; val_accuracy: 0.8968949044585988 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.83
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.72; acc: 0.78
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.8
Batch: 600; loss: 0.37; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3378298387026331; val_accuracy: 0.8994824840764332 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.81
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.65; acc: 0.77
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.81
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3395199132667985; val_accuracy: 0.8995820063694268 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 1.0
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.93; acc: 0.83
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.77
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3371528260836935; val_accuracy: 0.9008757961783439 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.74; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.33546185149413765; val_accuracy: 0.9014729299363057 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3372525492576277; val_accuracy: 0.9006767515923567 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3368517704260577; val_accuracy: 0.902468152866242 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3356137902114042; val_accuracy: 0.9000796178343949 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3359211228170972; val_accuracy: 0.9000796178343949 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3368051035009372; val_accuracy: 0.8998805732484076 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.65; acc: 0.91
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.91
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.77
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3362774498238685; val_accuracy: 0.9009753184713376 

Epoch 40 start
The current lr is: 0.06400000000000002
slurmstepd: error: _is_a_lwp: 1 read() attempts on /proc/235898/status failed: No such process
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 1.06; acc: 0.77
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.89; acc: 0.75
Batch: 720; loss: 0.27; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3413566566861359; val_accuracy: 0.8993829617834395 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.33345252769008565; val_accuracy: 0.9019705414012739 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.8
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.76; acc: 0.75
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.33323574991552696; val_accuracy: 0.9032643312101911 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.33402864502114094; val_accuracy: 0.9007762738853503 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3334689824634297; val_accuracy: 0.9021695859872612 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.81
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.33299941915994996; val_accuracy: 0.9021695859872612 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.82; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3340391312624998; val_accuracy: 0.9026671974522293 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3342441296216789; val_accuracy: 0.9018710191082803 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.63; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.33482522384566105; val_accuracy: 0.9011743630573248 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3335812924678918; val_accuracy: 0.9009753184713376 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.33459214647864083; val_accuracy: 0.9012738853503185 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_130_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 29673
elements in E: 6298600
fraction nonzero: 0.004711046899310958
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.28; acc: 0.14
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.17
Batch: 260; loss: 2.28; acc: 0.19
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.27; acc: 0.2
Batch: 320; loss: 2.27; acc: 0.27
Batch: 340; loss: 2.27; acc: 0.31
Batch: 360; loss: 2.28; acc: 0.22
Batch: 380; loss: 2.26; acc: 0.25
Batch: 400; loss: 2.25; acc: 0.27
Batch: 420; loss: 2.24; acc: 0.34
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.24; acc: 0.36
Batch: 480; loss: 2.24; acc: 0.23
Batch: 500; loss: 2.25; acc: 0.23
Batch: 520; loss: 2.2; acc: 0.31
Batch: 540; loss: 2.2; acc: 0.34
Batch: 560; loss: 2.17; acc: 0.33
Batch: 580; loss: 2.17; acc: 0.28
Batch: 600; loss: 2.15; acc: 0.31
Batch: 620; loss: 2.11; acc: 0.31
Batch: 640; loss: 2.04; acc: 0.39
Batch: 660; loss: 2.01; acc: 0.34
Batch: 680; loss: 1.96; acc: 0.38
Batch: 700; loss: 1.91; acc: 0.41
Batch: 720; loss: 1.71; acc: 0.53
Batch: 740; loss: 1.68; acc: 0.48
Batch: 760; loss: 1.53; acc: 0.47
Batch: 780; loss: 1.32; acc: 0.58
Train Epoch over. train_loss: 2.17; train_accuracy: 0.25 

Batch: 0; loss: 1.43; acc: 0.44
Batch: 20; loss: 1.61; acc: 0.42
Batch: 40; loss: 1.22; acc: 0.58
Batch: 60; loss: 1.4; acc: 0.58
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.39; acc: 0.61
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 1.4; acc: 0.44
Val Epoch over. val_loss: 1.4002066653245573; val_accuracy: 0.5068670382165605 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.5
Batch: 20; loss: 1.27; acc: 0.52
Batch: 40; loss: 1.03; acc: 0.69
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.84; acc: 0.67
Batch: 100; loss: 0.94; acc: 0.64
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.67
Batch: 160; loss: 0.73; acc: 0.75
Batch: 180; loss: 0.99; acc: 0.59
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.67; acc: 0.72
Batch: 240; loss: 0.77; acc: 0.77
Batch: 260; loss: 0.86; acc: 0.72
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.89; acc: 0.69
Batch: 340; loss: 0.7; acc: 0.75
Batch: 360; loss: 0.62; acc: 0.7
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.7
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.73
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 1.09; acc: 0.78
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.8
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 1.27; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.7578325566782314; val_accuracy: 0.76671974522293 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.77
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.8
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.4848914275978022; val_accuracy: 0.8511146496815286 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.75
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.77
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.78
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.73
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.41658755391836166; val_accuracy: 0.8682324840764332 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.81
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.39569648419311093; val_accuracy: 0.8770899681528662 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.8
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.73
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3592689666351315; val_accuracy: 0.8877388535031847 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.57; acc: 0.78
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.31; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.86
Val Epoch over. val_loss: 0.581777396190698; val_accuracy: 0.8103105095541401 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.75
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.8
Batch: 360; loss: 0.5; acc: 0.81
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.45732428831089833; val_accuracy: 0.8500199044585988 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.26; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.75
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.81; acc: 0.72
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.72
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.39490852550051775; val_accuracy: 0.8756966560509554 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.87 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.33516729561386593; val_accuracy: 0.8916202229299363 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.30085386601602954; val_accuracy: 0.9036624203821656 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.83
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.71; acc: 0.81
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3243978206234373; val_accuracy: 0.8996815286624203 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.75
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.35661160860471663; val_accuracy: 0.8877388535031847 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.83
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.32708437486913555; val_accuracy: 0.9004777070063694 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.72
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3217866519691458; val_accuracy: 0.8957006369426752 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.13; acc: 1.0
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2912772401550393; val_accuracy: 0.9090366242038217 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.81
Batch: 420; loss: 0.3; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.78
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.83
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.31621585954811166; val_accuracy: 0.899781050955414 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.8
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.83
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.82; acc: 0.7
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4269255330892885; val_accuracy: 0.8670382165605095 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.62; acc: 0.75
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3514766051511096; val_accuracy: 0.8907245222929936 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.89
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.75
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.334890606629241; val_accuracy: 0.8959992038216561 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.2923852286663405; val_accuracy: 0.9088375796178344 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2941817130157902; val_accuracy: 0.9091361464968153 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.2; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.28688584742652384; val_accuracy: 0.9112261146496815 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.78
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.34345654948691656; val_accuracy: 0.894406847133758 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.81
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.86
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.2931971233219478; val_accuracy: 0.9086385350318471 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.88
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2835033946688388; val_accuracy: 0.910828025477707 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.29322577407880196; val_accuracy: 0.9104299363057324 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.8
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.28460840100220813; val_accuracy: 0.9125199044585988 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27938648503107627; val_accuracy: 0.9131170382165605 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.28710746513620305; val_accuracy: 0.9109275477707006 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.83
Batch: 440; loss: 0.52; acc: 0.8
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.17; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.8
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.28365606712592634; val_accuracy: 0.9130175159235668 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27967189280850113; val_accuracy: 0.9120222929936306 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2788129070666945; val_accuracy: 0.9131170382165605 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.81
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.28333794440670756; val_accuracy: 0.913515127388535 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.28449116468097374; val_accuracy: 0.9109275477707006 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.81
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2804963757776341; val_accuracy: 0.9126194267515924 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.38; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.83
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27837158829733066; val_accuracy: 0.9144108280254777 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.81
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.84
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2758765875059328; val_accuracy: 0.9153065286624203 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.81
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2773347694403047; val_accuracy: 0.9141122611464968 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.277388885071513; val_accuracy: 0.9130175159235668 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.81
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.35; acc: 0.83
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2759738778042945; val_accuracy: 0.9148089171974523 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2753112934007766; val_accuracy: 0.9145103503184714 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.86
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.49; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2761235367625382; val_accuracy: 0.9141122611464968 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27576791056117433; val_accuracy: 0.9143113057324841 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27395274359614225; val_accuracy: 0.9155055732484076 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27653602997113946; val_accuracy: 0.9143113057324841 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.83
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.27718872700337394; val_accuracy: 0.913515127388535 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.27473138695120053; val_accuracy: 0.9143113057324841 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.27579298938155933; val_accuracy: 0.9145103503184714 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.98
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.27637321732131537; val_accuracy: 0.9142117834394905 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_140_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 31793
elements in E: 6748500
fraction nonzero: 0.0047111209898495965
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.05
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.12
Batch: 260; loss: 2.27; acc: 0.16
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.25; acc: 0.17
Batch: 320; loss: 2.24; acc: 0.25
Batch: 340; loss: 2.24; acc: 0.22
Batch: 360; loss: 2.23; acc: 0.27
Batch: 380; loss: 2.18; acc: 0.28
Batch: 400; loss: 2.16; acc: 0.31
Batch: 420; loss: 2.11; acc: 0.33
Batch: 440; loss: 1.98; acc: 0.42
Batch: 460; loss: 1.93; acc: 0.28
Batch: 480; loss: 1.85; acc: 0.41
Batch: 500; loss: 1.51; acc: 0.62
Batch: 520; loss: 1.55; acc: 0.45
Batch: 540; loss: 1.39; acc: 0.48
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 1.27; acc: 0.58
Batch: 600; loss: 1.06; acc: 0.62
Batch: 620; loss: 1.0; acc: 0.7
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.8
Batch: 760; loss: 1.04; acc: 0.7
Batch: 780; loss: 0.64; acc: 0.72
Train Epoch over. train_loss: 1.78; train_accuracy: 0.36 

Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.69
Batch: 80; loss: 0.55; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.63; acc: 0.75
Val Epoch over. val_loss: 0.7094372746291434; val_accuracy: 0.7567675159235668 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 1.02; acc: 0.64
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.57; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.84; acc: 0.73
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.73
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.81; acc: 0.73
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.89; acc: 0.84
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.62
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 0.4; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.64
Batch: 140; loss: 0.43; acc: 0.83
Val Epoch over. val_loss: 0.6622476789412225; val_accuracy: 0.787718949044586 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.81
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.72
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.29; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.38283684745336033; val_accuracy: 0.8767914012738853 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.8
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.41642761553169055; val_accuracy: 0.8675358280254777 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.8
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.8
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.39251773790189415; val_accuracy: 0.8751990445859873 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.7; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.3540545896075334; val_accuracy: 0.8865445859872612 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.98
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.8
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.4116584521949671; val_accuracy: 0.8659434713375797 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.37393164397424955; val_accuracy: 0.8803742038216561 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.8
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.84
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4176123507178513; val_accuracy: 0.8641520700636943 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.81
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.3352132652690456; val_accuracy: 0.8963972929936306 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.83
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.83
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.81
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.29086361865803695; val_accuracy: 0.9098328025477707 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.34060194158250356; val_accuracy: 0.8939092356687898 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.32358364440548193; val_accuracy: 0.8974920382165605 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3171432245831201; val_accuracy: 0.9017714968152867 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.31135255418670404; val_accuracy: 0.9037619426751592 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.28249342386035403; val_accuracy: 0.9125199044585988 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.94; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.31968608856865555; val_accuracy: 0.9002786624203821 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.78
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.311782575242079; val_accuracy: 0.9022691082802548 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.30256442941582884; val_accuracy: 0.90625 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3022534632046891; val_accuracy: 0.9053542993630573 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.2739355237146092; val_accuracy: 0.9155055732484076 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.83
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.83
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.2837879882686457; val_accuracy: 0.9124203821656051 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.8
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.2788896209020523; val_accuracy: 0.912718949044586 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.81
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.281115232474485; val_accuracy: 0.9130175159235668 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.81
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.2786462847499331; val_accuracy: 0.9138136942675159 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2712611355077309; val_accuracy: 0.9157046178343949 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.2725315472929721; val_accuracy: 0.9165007961783439 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.88
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.27382984147614736; val_accuracy: 0.9151074840764332 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2723402371214833; val_accuracy: 0.9160031847133758 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.83
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.84
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2688392721543646; val_accuracy: 0.9171974522292994 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.83
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.83
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.92
Val Epoch over. val_loss: 0.27005178953526887; val_accuracy: 0.9164012738853503 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.26969118162420147; val_accuracy: 0.9178941082802548 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2686471384564403; val_accuracy: 0.917296974522293 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.83
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.2682621244364863; val_accuracy: 0.9179936305732485 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.84
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.18; acc: 0.98
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2677209646838486; val_accuracy: 0.9166003184713376 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.2698849957365139; val_accuracy: 0.9173964968152867 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2673378405724741; val_accuracy: 0.9170979299363057 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.86
Batch: 360; loss: 0.2; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.81
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.26526438233673955; val_accuracy: 0.918093152866242 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.11; acc: 1.0
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.26710822596956213; val_accuracy: 0.9174960191082803 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2670287457383742; val_accuracy: 0.918093152866242 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.19; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.26516551692869256; val_accuracy: 0.9183917197452229 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2663549979685978; val_accuracy: 0.9182921974522293 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.83
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.26596363807085216; val_accuracy: 0.9175955414012739 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.265348835735564; val_accuracy: 0.9182921974522293 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2645002810201447; val_accuracy: 0.9175955414012739 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2663183063743221; val_accuracy: 0.9184912420382165 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.61; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.26655027723521185; val_accuracy: 0.918093152866242 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2649758842530524; val_accuracy: 0.9185907643312102 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.84
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.26516784941125066; val_accuracy: 0.9181926751592356 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.08; acc: 1.0
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2656634784760369; val_accuracy: 0.918093152866242 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_150_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 33815
elements in E: 7198400
fraction nonzero: 0.004697571682596133
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.19
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.16
Batch: 240; loss: 2.29; acc: 0.16
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.28; acc: 0.14
Batch: 320; loss: 2.27; acc: 0.23
Batch: 340; loss: 2.29; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.19
Batch: 380; loss: 2.28; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.19
Batch: 420; loss: 2.26; acc: 0.34
Batch: 440; loss: 2.27; acc: 0.2
Batch: 460; loss: 2.28; acc: 0.17
Batch: 480; loss: 2.26; acc: 0.2
Batch: 500; loss: 2.27; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.2
Batch: 540; loss: 2.24; acc: 0.3
Batch: 560; loss: 2.23; acc: 0.27
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.22; acc: 0.33
Batch: 620; loss: 2.22; acc: 0.33
Batch: 640; loss: 2.21; acc: 0.36
Batch: 660; loss: 2.18; acc: 0.5
Batch: 680; loss: 2.16; acc: 0.42
Batch: 700; loss: 2.15; acc: 0.47
Batch: 720; loss: 2.11; acc: 0.5
Batch: 740; loss: 2.1; acc: 0.48
Batch: 760; loss: 2.01; acc: 0.48
Batch: 780; loss: 1.94; acc: 0.53
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.0; acc: 0.52
Batch: 20; loss: 2.03; acc: 0.39
Batch: 40; loss: 1.88; acc: 0.56
Batch: 60; loss: 1.9; acc: 0.53
Batch: 80; loss: 1.93; acc: 0.5
Batch: 100; loss: 1.99; acc: 0.52
Batch: 120; loss: 1.94; acc: 0.53
Batch: 140; loss: 1.94; acc: 0.52
Val Epoch over. val_loss: 1.9765357318197845; val_accuracy: 0.46238057324840764 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.98; acc: 0.41
Batch: 20; loss: 1.9; acc: 0.45
Batch: 40; loss: 1.77; acc: 0.53
Batch: 60; loss: 1.63; acc: 0.59
Batch: 80; loss: 1.52; acc: 0.45
Batch: 100; loss: 1.4; acc: 0.52
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 1.11; acc: 0.61
Batch: 160; loss: 1.05; acc: 0.75
Batch: 180; loss: 1.27; acc: 0.59
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 1.0; acc: 0.67
Batch: 240; loss: 1.09; acc: 0.67
Batch: 260; loss: 1.52; acc: 0.59
Batch: 280; loss: 1.31; acc: 0.55
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 1.56; acc: 0.48
Batch: 340; loss: 0.88; acc: 0.72
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.93; acc: 0.64
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.82; acc: 0.73
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.88; acc: 0.72
Batch: 520; loss: 0.64; acc: 0.75
Batch: 540; loss: 0.99; acc: 0.7
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.74; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.75; acc: 0.69
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.75
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 0.97; train_accuracy: 0.7 

Batch: 0; loss: 1.66; acc: 0.55
Batch: 20; loss: 2.1; acc: 0.44
Batch: 40; loss: 0.76; acc: 0.73
Batch: 60; loss: 1.71; acc: 0.59
Batch: 80; loss: 1.24; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.58
Batch: 120; loss: 1.9; acc: 0.53
Batch: 140; loss: 1.18; acc: 0.66
Val Epoch over. val_loss: 1.3825963298985913; val_accuracy: 0.59265525477707 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.49; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.52; acc: 0.8
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.45; acc: 0.81
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.81; acc: 0.8
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.5654621620656578; val_accuracy: 0.8129976114649682 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.79; acc: 0.69
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.81
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.83
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.78
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.92
Val Epoch over. val_loss: 0.4361534302781342; val_accuracy: 0.8672372611464968 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.81
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.72; acc: 0.73
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.95; acc: 0.73
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.40811016026196206; val_accuracy: 0.8690286624203821 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.8
Batch: 380; loss: 0.46; acc: 0.78
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.77
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.81
Batch: 680; loss: 0.71; acc: 0.75
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.36706398306473803; val_accuracy: 0.8896297770700637 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.72; acc: 0.75
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.8
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.81
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.44253106320359903; val_accuracy: 0.8670382165605095 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.78
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.73; acc: 0.78
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.8
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.4479678374282114; val_accuracy: 0.8546974522292994 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.44; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.88
Val Epoch over. val_loss: 0.632360310975913; val_accuracy: 0.8058320063694268 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.83
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.66; acc: 0.77
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.8
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.23; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.29; acc: 0.88
Val Epoch over. val_loss: 0.622732685819553; val_accuracy: 0.8036425159235668 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.78
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.30710228648819743; val_accuracy: 0.9054538216560509 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3607171450261098; val_accuracy: 0.8882364649681529 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.33317694129647724; val_accuracy: 0.8966958598726115 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.37390876395307526; val_accuracy: 0.8840565286624203 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.8
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.8
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.54; acc: 0.81
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.3198725267485448; val_accuracy: 0.896297770700637 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.83
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31144122079394426; val_accuracy: 0.9033638535031847 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.84
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.83
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.33937147164800363; val_accuracy: 0.894406847133758 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3482967088841329; val_accuracy: 0.8922173566878981 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.84
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3228752808586048; val_accuracy: 0.8996815286624203 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.32120919517081253; val_accuracy: 0.9008757961783439 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2806082292560749; val_accuracy: 0.9144108280254777 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2622986673881674; val_accuracy: 0.9201831210191083 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2720135702591413; val_accuracy: 0.9157046178343949 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.84
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2978007565638062; val_accuracy: 0.908140923566879 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.83
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26935420508027835; val_accuracy: 0.9181926751592356 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.26691701132685514; val_accuracy: 0.9167993630573248 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.27968905436669944; val_accuracy: 0.9121218152866242 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27579792482173365; val_accuracy: 0.9168988853503185 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.18; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2579084324893678; val_accuracy: 0.9198845541401274 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.83
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.81
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.84
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26288007038414096; val_accuracy: 0.9196855095541401 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.26105040416216396; val_accuracy: 0.9186902866242038 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25446131490882795; val_accuracy: 0.9223726114649682 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26190142746374107; val_accuracy: 0.918093152866242 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.51; acc: 0.78
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25950037047361874; val_accuracy: 0.9190883757961783 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.19; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2601598365006933; val_accuracy: 0.9198845541401274 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26090840924127845; val_accuracy: 0.9190883757961783 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25627662784829264; val_accuracy: 0.9200835987261147 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25562274804825236; val_accuracy: 0.9215764331210191 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25719285481105186; val_accuracy: 0.9185907643312102 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.83
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25566784800237913; val_accuracy: 0.9209792993630573 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25263108028348086; val_accuracy: 0.9220740445859873 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.17; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.81
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.88
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2508186084354759; val_accuracy: 0.9208797770700637 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.61; acc: 0.77
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.252738504249389; val_accuracy: 0.9208797770700637 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.25452071385588615; val_accuracy: 0.9192874203821656 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25175512707821884; val_accuracy: 0.9220740445859873 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2519527524946981; val_accuracy: 0.9205812101910829 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.83
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25565552241673134; val_accuracy: 0.9200835987261147 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25073512494089495; val_accuracy: 0.9227707006369427 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2524431865116593; val_accuracy: 0.9206807324840764 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2501657563884547; val_accuracy: 0.9225716560509554 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_160_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 35718
elements in E: 7648300
fraction nonzero: 0.00467005739837611
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.19
Batch: 160; loss: 2.29; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.19
Batch: 200; loss: 2.27; acc: 0.31
Batch: 220; loss: 2.26; acc: 0.45
Batch: 240; loss: 2.26; acc: 0.38
Batch: 260; loss: 2.25; acc: 0.3
Batch: 280; loss: 2.24; acc: 0.31
Batch: 300; loss: 2.21; acc: 0.39
Batch: 320; loss: 2.2; acc: 0.34
Batch: 340; loss: 2.19; acc: 0.45
Batch: 360; loss: 2.14; acc: 0.27
Batch: 380; loss: 2.04; acc: 0.44
Batch: 400; loss: 2.03; acc: 0.31
Batch: 420; loss: 1.79; acc: 0.5
Batch: 440; loss: 1.63; acc: 0.56
Batch: 460; loss: 1.65; acc: 0.41
Batch: 480; loss: 1.39; acc: 0.62
Batch: 500; loss: 1.2; acc: 0.61
Batch: 520; loss: 1.2; acc: 0.7
Batch: 540; loss: 1.1; acc: 0.61
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 1.34; acc: 0.53
Batch: 600; loss: 0.93; acc: 0.66
Batch: 620; loss: 1.0; acc: 0.7
Batch: 640; loss: 0.71; acc: 0.73
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.69
Batch: 700; loss: 0.82; acc: 0.75
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 1.66; train_accuracy: 0.45 

Batch: 0; loss: 0.96; acc: 0.58
Batch: 20; loss: 1.26; acc: 0.55
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.78
Val Epoch over. val_loss: 0.6957931012674502; val_accuracy: 0.7784633757961783 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.75
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.75; acc: 0.75
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.75
Batch: 280; loss: 0.84; acc: 0.72
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.81; acc: 0.77
Batch: 340; loss: 0.67; acc: 0.73
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 0.99; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 1.08; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 1.03; acc: 0.7
Batch: 140; loss: 0.39; acc: 0.86
Val Epoch over. val_loss: 0.6898855884933168; val_accuracy: 0.7783638535031847 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.75
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.77
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.78
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.49024662911702116; val_accuracy: 0.8443471337579618 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.81
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.75; acc: 0.81
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.77
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.84
Val Epoch over. val_loss: 0.46687141307599983; val_accuracy: 0.849422770700637 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.78
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.94
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3640359987024289; val_accuracy: 0.8867436305732485 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.71; acc: 0.75
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.88; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.81
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3594476731529661; val_accuracy: 0.8927149681528662 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.3920270198849356; val_accuracy: 0.8797770700636943 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.42738917702512375; val_accuracy: 0.8625597133757962 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.88
Val Epoch over. val_loss: 0.4280971698700243; val_accuracy: 0.8659434713375797 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.78
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.81
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.32620376713906124; val_accuracy: 0.8981886942675159 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.2830693870308293; val_accuracy: 0.9140127388535032 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.28229107821633104; val_accuracy: 0.9159036624203821 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.83
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.283836855914942; val_accuracy: 0.9142117834394905 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3350199704906743; val_accuracy: 0.8992834394904459 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.84
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2762815557942269; val_accuracy: 0.9185907643312102 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2729383483529091; val_accuracy: 0.9221735668789809 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2878483036995693; val_accuracy: 0.9151074840764332 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.83
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.27794568401992703; val_accuracy: 0.9184912420382165 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.83
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.29776123688099493; val_accuracy: 0.9088375796178344 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.277573282266878; val_accuracy: 0.9171974522292994 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.19; acc: 0.98
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.27052337526800524; val_accuracy: 0.9204816878980892 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.81
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.84
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2662948827692278; val_accuracy: 0.9223726114649682 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.83
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2691054620133464; val_accuracy: 0.9217754777070064 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.2739080200624314; val_accuracy: 0.9182921974522293 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.8
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.27290353665401224; val_accuracy: 0.9195859872611465 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.13; acc: 1.0
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.13; acc: 1.0
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.26906300480863093; val_accuracy: 0.9210788216560509 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.26834747261682135; val_accuracy: 0.921875 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.27707391801723247; val_accuracy: 0.9184912420382165 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.26374263579773294; val_accuracy: 0.9211783439490446 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.81
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.26342738199101134; val_accuracy: 0.9219745222929936 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.8
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.26245077960430435; val_accuracy: 0.9219745222929936 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2621023719477805; val_accuracy: 0.9227707006369427 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2628793091436101; val_accuracy: 0.92296974522293 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2611617556043491; val_accuracy: 0.9236664012738853 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2647201323015675; val_accuracy: 0.9235668789808917 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2645133974825501; val_accuracy: 0.9222730891719745 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2612480676857529; val_accuracy: 0.9225716560509554 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.26118589334996645; val_accuracy: 0.92296974522293 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.84
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2611090311673796; val_accuracy: 0.9233678343949044 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.85; acc: 0.8
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2617893791787184; val_accuracy: 0.9227707006369427 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26160332029032857; val_accuracy: 0.9225716560509554 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.84
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2608999215113889; val_accuracy: 0.9238654458598726 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.26149263346840623; val_accuracy: 0.9233678343949044 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2602689569922769; val_accuracy: 0.9235668789808917 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.2596518401127712; val_accuracy: 0.9230692675159236 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.1; acc: 1.0
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2604975140398475; val_accuracy: 0.9233678343949044 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.81
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2612866771638773; val_accuracy: 0.9231687898089171 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.86
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2603328358500626; val_accuracy: 0.9228702229299363 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.8
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26037047988480067; val_accuracy: 0.9228702229299363 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.84
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26056053432499526; val_accuracy: 0.9224721337579618 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_170_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 38249
elements in E: 8098200
fraction nonzero: 0.0047231483539552
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.27; acc: 0.2
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.27; acc: 0.09
Batch: 400; loss: 2.27; acc: 0.17
Batch: 420; loss: 2.25; acc: 0.34
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.26; acc: 0.3
Batch: 480; loss: 2.26; acc: 0.17
Batch: 500; loss: 2.26; acc: 0.28
Batch: 520; loss: 2.22; acc: 0.31
Batch: 540; loss: 2.22; acc: 0.27
Batch: 560; loss: 2.19; acc: 0.38
Batch: 580; loss: 2.2; acc: 0.36
Batch: 600; loss: 2.17; acc: 0.36
Batch: 620; loss: 2.15; acc: 0.34
Batch: 640; loss: 2.11; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.38
Batch: 680; loss: 1.99; acc: 0.39
Batch: 700; loss: 1.98; acc: 0.42
Batch: 720; loss: 1.74; acc: 0.62
Batch: 740; loss: 1.7; acc: 0.47
Batch: 760; loss: 1.35; acc: 0.62
Batch: 780; loss: 1.27; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.23 

Batch: 0; loss: 1.4; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.59
Batch: 40; loss: 0.98; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.24; acc: 0.75
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.11; acc: 0.61
Val Epoch over. val_loss: 1.2542599208036047; val_accuracy: 0.6075835987261147 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.56
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 0.97; acc: 0.66
Batch: 60; loss: 1.12; acc: 0.66
Batch: 80; loss: 0.96; acc: 0.66
Batch: 100; loss: 0.95; acc: 0.69
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 0.72; acc: 0.75
Batch: 180; loss: 0.83; acc: 0.72
Batch: 200; loss: 0.67; acc: 0.8
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.73; acc: 0.73
Batch: 260; loss: 1.12; acc: 0.69
Batch: 280; loss: 0.78; acc: 0.73
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.92; acc: 0.7
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.72; acc: 0.75
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.75
Batch: 620; loss: 1.05; acc: 0.75
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.77
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.77
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6734582542613813; val_accuracy: 0.7832404458598726 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.8
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.8
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.8
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.73
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.7
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.48244159132432024; val_accuracy: 0.8429538216560509 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.62; acc: 0.75
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.78
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.66; acc: 0.78
Batch: 540; loss: 0.57; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.73; acc: 0.75
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.38666435032133845; val_accuracy: 0.883359872611465 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3759356712934318; val_accuracy: 0.8835589171974523 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.9; acc: 0.77
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.95; acc: 0.73
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3331283464268514; val_accuracy: 0.8968949044585988 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.73
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3791385266431578; val_accuracy: 0.8792794585987261 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.78
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.18; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.81
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3382253715187121; val_accuracy: 0.895203025477707 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.8
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3898217793862531; val_accuracy: 0.8765923566878981 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3246384348933864; val_accuracy: 0.8991839171974523 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.43; acc: 0.8
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2765082493661695; val_accuracy: 0.9159036624203821 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2786681432348148; val_accuracy: 0.9152070063694268 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.51; acc: 0.8
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27231649316515133; val_accuracy: 0.9164012738853503 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.78
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.3131869489410121; val_accuracy: 0.9042595541401274 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.81
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.2; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.3089514484593443; val_accuracy: 0.9033638535031847 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.24427121403111016; val_accuracy: 0.9247611464968153 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.25800532126312803; val_accuracy: 0.9250597133757962 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2906516798220243; val_accuracy: 0.9105294585987261 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2650938650984673; val_accuracy: 0.9225716560509554 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.86
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.26432374205179276; val_accuracy: 0.9232683121019108 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.86
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2382541175480861; val_accuracy: 0.9307324840764332 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23123603250095798; val_accuracy: 0.931827229299363 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.23228546737390718; val_accuracy: 0.9311305732484076 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.23803526922396034; val_accuracy: 0.931031050955414 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.88
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2304007592522035; val_accuracy: 0.9321257961783439 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22954417069911198; val_accuracy: 0.9322253184713376 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.84
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22752712759527433; val_accuracy: 0.9326234076433121 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.36; acc: 0.83
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2319552196296537; val_accuracy: 0.9330214968152867 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22398528743796287; val_accuracy: 0.9353105095541401 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2295117278340136; val_accuracy: 0.9341162420382165 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2275229662680512; val_accuracy: 0.9349124203821656 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22090388628043187; val_accuracy: 0.9345143312101911 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22041591277025688; val_accuracy: 0.9355095541401274 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.86
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22112482295009742; val_accuracy: 0.93640525477707 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.84
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22215617647406402; val_accuracy: 0.9355095541401274 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.83
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21903620414720598; val_accuracy: 0.9370023885350318 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.84
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2177795507488357; val_accuracy: 0.9355095541401274 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.91
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2209784672328621; val_accuracy: 0.9351114649681529 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21950312585208068; val_accuracy: 0.9361066878980892 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.84
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2166783170074604; val_accuracy: 0.935609076433121 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2210847035904599; val_accuracy: 0.9351114649681529 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21638118781766313; val_accuracy: 0.9375995222929936 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21627548532738428; val_accuracy: 0.9371019108280255 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2165554985166735; val_accuracy: 0.9369028662420382 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21613637023385923; val_accuracy: 0.9369028662420382 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21621652390024845; val_accuracy: 0.9380971337579618 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.98
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2165971027722784; val_accuracy: 0.9378980891719745 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21650699900973375; val_accuracy: 0.9372014331210191 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.81
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21698585926157654; val_accuracy: 0.9376990445859873 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21648710631545942; val_accuracy: 0.9371019108280255 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_180_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 40116
elements in E: 8548100
fraction nonzero: 0.004692972707385267
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.12
Batch: 300; loss: 2.27; acc: 0.12
Batch: 320; loss: 2.27; acc: 0.27
Batch: 340; loss: 2.29; acc: 0.25
Batch: 360; loss: 2.28; acc: 0.23
Batch: 380; loss: 2.26; acc: 0.23
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.24; acc: 0.36
Batch: 440; loss: 2.23; acc: 0.3
Batch: 460; loss: 2.23; acc: 0.31
Batch: 480; loss: 2.22; acc: 0.39
Batch: 500; loss: 2.19; acc: 0.34
Batch: 520; loss: 2.07; acc: 0.5
Batch: 540; loss: 2.06; acc: 0.52
Batch: 560; loss: 1.82; acc: 0.56
Batch: 580; loss: 1.71; acc: 0.47
Batch: 600; loss: 1.33; acc: 0.61
Batch: 620; loss: 1.07; acc: 0.72
Batch: 640; loss: 0.91; acc: 0.78
Batch: 660; loss: 0.92; acc: 0.72
Batch: 680; loss: 0.89; acc: 0.73
Batch: 700; loss: 1.03; acc: 0.66
Batch: 720; loss: 0.56; acc: 0.8
Batch: 740; loss: 0.7; acc: 0.77
Batch: 760; loss: 0.87; acc: 0.75
Batch: 780; loss: 0.73; acc: 0.77
Train Epoch over. train_loss: 1.91; train_accuracy: 0.35 

Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.62
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.43; acc: 0.88
Val Epoch over. val_loss: 0.6773761302042919; val_accuracy: 0.7884156050955414 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.72
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.81
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.76; acc: 0.75
Batch: 280; loss: 0.63; acc: 0.73
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.62; acc: 0.77
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.78
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.77
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.84
Batch: 620; loss: 0.9; acc: 0.77
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.97
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.40534298482594217; val_accuracy: 0.8762937898089171 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.83
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 0.35; acc: 0.83
Val Epoch over. val_loss: 0.42762770574943276; val_accuracy: 0.8611664012738853 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.88
Val Epoch over. val_loss: 0.3809197543627897; val_accuracy: 0.8800756369426752 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3104611610531048; val_accuracy: 0.9048566878980892 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.69; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2786058784717587; val_accuracy: 0.9174960191082803 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.4097319481194399; val_accuracy: 0.8644506369426752 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.75
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3175605089896044; val_accuracy: 0.8993829617834395 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.27; acc: 0.84
Val Epoch over. val_loss: 0.5254200884870662; val_accuracy: 0.8394705414012739 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.25993221836864566; val_accuracy: 0.9212778662420382 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.23268966853713532; val_accuracy: 0.9317277070063694 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.256008835782291; val_accuracy: 0.9202826433121019 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2544685712760421; val_accuracy: 0.9196855095541401 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.268859924072293; val_accuracy: 0.9184912420382165 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.27; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.24212465297644306; val_accuracy: 0.9257563694267515 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.24050912666757396; val_accuracy: 0.925656847133758 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.25183067629766315; val_accuracy: 0.9241640127388535 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23634075717466652; val_accuracy: 0.9281449044585988 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.27832531682245293; val_accuracy: 0.9130175159235668 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.24588896692463547; val_accuracy: 0.9276472929936306 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2113911256099203; val_accuracy: 0.9382961783439491 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2087466611888758; val_accuracy: 0.9363057324840764 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.21290241924535697; val_accuracy: 0.935609076433121 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.86
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2090700397113706; val_accuracy: 0.9370023885350318 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.89
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.20817102470500454; val_accuracy: 0.9378980891719745 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.86
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.20519891495157958; val_accuracy: 0.9383957006369427 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.20746551242888353; val_accuracy: 0.9370023885350318 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.22003010400588724; val_accuracy: 0.9341162420382165 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.20261322082892344; val_accuracy: 0.9399880573248408 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.20696886155493321; val_accuracy: 0.939390923566879 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.20227332560309938; val_accuracy: 0.9406847133757962 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.08; acc: 1.0
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.19960121505531916; val_accuracy: 0.9414808917197452 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.20100334467022282; val_accuracy: 0.9403861464968153 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.20069467136339778; val_accuracy: 0.941281847133758 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.94
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.08; acc: 1.0
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2027622657786509; val_accuracy: 0.939390923566879 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2005969470805803; val_accuracy: 0.941281847133758 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2000984408341016; val_accuracy: 0.9405851910828026 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.38; acc: 0.81
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19927983315792053; val_accuracy: 0.9405851910828026 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 1.0
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19872186430225705; val_accuracy: 0.9417794585987261 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.88
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.19915531435684794; val_accuracy: 0.9409832802547771 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.15; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.1978251273227725; val_accuracy: 0.941281847133758 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.15; acc: 0.92
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.1977565675428149; val_accuracy: 0.9423765923566879 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.19790117230242604; val_accuracy: 0.9413813694267515 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.19835935525928333; val_accuracy: 0.9424761146496815 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.88
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.07; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19842726098978594; val_accuracy: 0.942078025477707 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.88
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.1977169316760294; val_accuracy: 0.9421775477707006 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.91
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.91
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.19828004344918165; val_accuracy: 0.941281847133758 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19817956570227435; val_accuracy: 0.9406847133757962 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19825891115862854; val_accuracy: 0.9416799363057324 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.19762420126349683; val_accuracy: 0.9419785031847133 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_190_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 42070
elements in E: 8998000
fraction nonzero: 0.004675483440764614
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.17
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.06
Batch: 200; loss: 2.28; acc: 0.12
Batch: 220; loss: 2.27; acc: 0.16
Batch: 240; loss: 2.27; acc: 0.22
Batch: 260; loss: 2.26; acc: 0.31
Batch: 280; loss: 2.26; acc: 0.27
Batch: 300; loss: 2.24; acc: 0.41
Batch: 320; loss: 2.25; acc: 0.25
Batch: 340; loss: 2.23; acc: 0.31
Batch: 360; loss: 2.2; acc: 0.3
Batch: 380; loss: 2.18; acc: 0.31
Batch: 400; loss: 2.13; acc: 0.28
Batch: 420; loss: 2.05; acc: 0.36
Batch: 440; loss: 1.91; acc: 0.47
Batch: 460; loss: 1.85; acc: 0.47
Batch: 480; loss: 1.67; acc: 0.47
Batch: 500; loss: 1.29; acc: 0.66
Batch: 520; loss: 1.3; acc: 0.56
Batch: 540; loss: 1.93; acc: 0.44
Batch: 560; loss: 0.94; acc: 0.73
Batch: 580; loss: 1.3; acc: 0.55
Batch: 600; loss: 0.94; acc: 0.64
Batch: 620; loss: 1.09; acc: 0.62
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 0.86; acc: 0.67
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.9; acc: 0.77
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 1.74; train_accuracy: 0.41 

Batch: 0; loss: 0.64; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.62
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.5; acc: 0.8
Val Epoch over. val_loss: 0.6120580062744724; val_accuracy: 0.8058320063694268 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.75
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.77
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.78
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.95; acc: 0.8
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.46761392664377854; val_accuracy: 0.8487261146496815 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.69; acc: 0.77
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.8
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.75
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.73
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.83
Val Epoch over. val_loss: 0.4623298245915182; val_accuracy: 0.8390724522292994 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.5492880314493634; val_accuracy: 0.8247412420382165 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.62; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.34040754378601246; val_accuracy: 0.8942078025477707 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.2940883253030716; val_accuracy: 0.9090366242038217 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.2908946304659175; val_accuracy: 0.9129179936305732 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.86
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.81
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.32203476017068144; val_accuracy: 0.8992834394904459 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.40400876119068474; val_accuracy: 0.8839570063694268 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.86
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.26906838552777174; val_accuracy: 0.9204816878980892 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21118741088612064; val_accuracy: 0.9354100318471338 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21427269531473234; val_accuracy: 0.9354100318471338 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22931415791724138; val_accuracy: 0.9300358280254777 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.88
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2396707668141195; val_accuracy: 0.9260549363057324 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.20927370040659693; val_accuracy: 0.934812898089172 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20660364243445123; val_accuracy: 0.9375 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.88
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.89
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2153420256581276; val_accuracy: 0.9347133757961783 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21906035355511744; val_accuracy: 0.933718152866242 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21039430721170582; val_accuracy: 0.93640525477707 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23022777271593453; val_accuracy: 0.9353105095541401 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.83
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.20478312910836974; val_accuracy: 0.9394904458598726 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.20258608496018277; val_accuracy: 0.9390923566878981 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20320398977417856; val_accuracy: 0.9392914012738853 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2402929859651122; val_accuracy: 0.9254578025477707 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1974309204253042; val_accuracy: 0.9388933121019108 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19634311696051793; val_accuracy: 0.9399880573248408 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20143775321590673; val_accuracy: 0.9394904458598726 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.88
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19842186489492464; val_accuracy: 0.9403861464968153 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19688565032497332; val_accuracy: 0.9406847133757962 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.19985695230733058; val_accuracy: 0.9399880573248408 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19375588970294425; val_accuracy: 0.9411823248407644 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19300990043931707; val_accuracy: 0.9415804140127388 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.86
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19585624712098176; val_accuracy: 0.9398885350318471 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.14; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1936145196342544; val_accuracy: 0.9418789808917197 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19407018848285554; val_accuracy: 0.9409832802547771 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.84
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.194003869773476; val_accuracy: 0.9410828025477707 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1945827922243981; val_accuracy: 0.941281847133758 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.07; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19370674052435882; val_accuracy: 0.9419785031847133 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1940847038036319; val_accuracy: 0.9402866242038217 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.86
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19400418775191733; val_accuracy: 0.9410828025477707 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.88
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.43; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.192487706661604; val_accuracy: 0.9413813694267515 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.09; acc: 1.0
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19228852604320096; val_accuracy: 0.9413813694267515 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1916427407532361; val_accuracy: 0.9422770700636943 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19158537386898783; val_accuracy: 0.942078025477707 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.84
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1916612651507566; val_accuracy: 0.9408837579617835 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19207841836532968; val_accuracy: 0.9418789808917197 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1925964853993267; val_accuracy: 0.942078025477707 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19139116138789306; val_accuracy: 0.9415804140127388 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19189104798493112; val_accuracy: 0.9423765923566879 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19141787285827527; val_accuracy: 0.9424761146496815 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 53096
elements in E: 11247500
fraction nonzero: 0.004720693487441654
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.2
Batch: 160; loss: 2.28; acc: 0.19
Batch: 180; loss: 2.28; acc: 0.17
Batch: 200; loss: 2.25; acc: 0.3
Batch: 220; loss: 2.24; acc: 0.41
Batch: 240; loss: 2.24; acc: 0.28
Batch: 260; loss: 2.21; acc: 0.31
Batch: 280; loss: 2.2; acc: 0.3
Batch: 300; loss: 2.11; acc: 0.34
Batch: 320; loss: 2.07; acc: 0.3
Batch: 340; loss: 1.9; acc: 0.52
Batch: 360; loss: 1.63; acc: 0.56
Batch: 380; loss: 1.25; acc: 0.66
Batch: 400; loss: 1.25; acc: 0.53
Batch: 420; loss: 0.75; acc: 0.75
Batch: 440; loss: 1.14; acc: 0.62
Batch: 460; loss: 0.97; acc: 0.72
Batch: 480; loss: 0.91; acc: 0.72
Batch: 500; loss: 0.69; acc: 0.7
Batch: 520; loss: 0.83; acc: 0.8
Batch: 540; loss: 0.81; acc: 0.75
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 0.74; acc: 0.73
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 1.42; train_accuracy: 0.53 

Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.7
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.45095347855121465; val_accuracy: 0.854796974522293 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.78
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.57; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.77
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.88; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.7
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4405342664118785; val_accuracy: 0.865843949044586 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.37609083623073664; val_accuracy: 0.8847531847133758 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3787492142076705; val_accuracy: 0.8790804140127388 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3024007207980961; val_accuracy: 0.9057523885350318 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.67; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.15; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.7; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.24909703730113186; val_accuracy: 0.9267515923566879 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.86
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4345922979769433; val_accuracy: 0.867734872611465 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.33; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.31830233786326306; val_accuracy: 0.8991839171974523 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.81
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.84
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.3074920499210904; val_accuracy: 0.9054538216560509 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.86
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.23448421549835022; val_accuracy: 0.9319267515923567 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.91
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.19374345746009972; val_accuracy: 0.9461584394904459 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2633233503646152; val_accuracy: 0.9204816878980892 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2085456187083463; val_accuracy: 0.9413813694267515 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.205855842015356; val_accuracy: 0.9397890127388535 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1988165894416487; val_accuracy: 0.9405851910828026 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.18328435185134032; val_accuracy: 0.9468550955414012 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20031857303088638; val_accuracy: 0.9419785031847133 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.7; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.21562013174792763; val_accuracy: 0.9371019108280255 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22131233282719448; val_accuracy: 0.9322253184713376 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20060622108400247; val_accuracy: 0.9436703821656051 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18692743400954137; val_accuracy: 0.9456608280254777 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.18629692198269687; val_accuracy: 0.9477507961783439 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1827461366915399; val_accuracy: 0.9467555732484076 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18838419535072745; val_accuracy: 0.9460589171974523 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17753213843342605; val_accuracy: 0.9483479299363057 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17640338112024745; val_accuracy: 0.9499402866242038 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17634929624048007; val_accuracy: 0.9499402866242038 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.4; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.185812176104374; val_accuracy: 0.9468550955414012 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.86
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17748691722940488; val_accuracy: 0.948546974522293 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.97
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18259944116613666; val_accuracy: 0.946656050955414 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17358489920663986; val_accuracy: 0.9511345541401274 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17600694640426878; val_accuracy: 0.9502388535031847 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17553901643889724; val_accuracy: 0.9498407643312102 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17550072731201055; val_accuracy: 0.9505374203821656 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17341483548093753; val_accuracy: 0.9506369426751592 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17736533360116802; val_accuracy: 0.9500398089171974 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17610911937181356; val_accuracy: 0.9510350318471338 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.173005110851139; val_accuracy: 0.9513335987261147 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17299968201168783; val_accuracy: 0.950437898089172 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1729694319426254; val_accuracy: 0.9517316878980892 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17228154178447785; val_accuracy: 0.9500398089171974 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.88
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.07; acc: 1.0
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.91
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17158633377996219; val_accuracy: 0.9510350318471338 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.83
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17082917080468432; val_accuracy: 0.9518312101910829 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17221239109517664; val_accuracy: 0.9501393312101911 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1716704284119758; val_accuracy: 0.950437898089172 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17317565139027158; val_accuracy: 0.9503383757961783 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17233895723986778; val_accuracy: 0.9507364649681529 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17231284269386796; val_accuracy: 0.950437898089172 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17363727937458426; val_accuracy: 0.9495421974522293 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.86
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17185900984391286; val_accuracy: 0.9510350318471338 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_250_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 63629
elements in E: 13497000
fraction nonzero: 0.00471430688301104
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.06
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.28; acc: 0.11
Batch: 180; loss: 2.28; acc: 0.2
Batch: 200; loss: 2.22; acc: 0.36
Batch: 220; loss: 2.2; acc: 0.42
Batch: 240; loss: 2.11; acc: 0.34
Batch: 260; loss: 1.94; acc: 0.44
Batch: 280; loss: 1.79; acc: 0.34
Batch: 300; loss: 1.58; acc: 0.47
Batch: 320; loss: 1.28; acc: 0.56
Batch: 340; loss: 1.13; acc: 0.61
Batch: 360; loss: 1.27; acc: 0.55
Batch: 380; loss: 1.89; acc: 0.45
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.77
Batch: 480; loss: 0.8; acc: 0.72
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.72
Batch: 600; loss: 0.52; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.78
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 1.25; train_accuracy: 0.57 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3867466328724934; val_accuracy: 0.8818670382165605 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.77
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.72; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.32137156716862303; val_accuracy: 0.900577229299363 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.86
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.2749708083926872; val_accuracy: 0.9128184713375797 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.19605144011271988; val_accuracy: 0.9400875796178344 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17374393626288243; val_accuracy: 0.9490445859872612 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18781161823185386; val_accuracy: 0.9422770700636943 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.98
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.1; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2533100749941389; val_accuracy: 0.9194864649681529 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.15802395562077784; val_accuracy: 0.9536226114649682 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.16139189664060902; val_accuracy: 0.9515326433121019 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.15095033323404136; val_accuracy: 0.9575039808917197 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.13071064641521235; val_accuracy: 0.9625796178343949 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.1356356127579121; val_accuracy: 0.959593949044586 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.13379303726610864; val_accuracy: 0.961484872611465 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.14096180055361646; val_accuracy: 0.9590963375796179 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12692482892874699; val_accuracy: 0.9632762738853503 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.12675059404058062; val_accuracy: 0.9632762738853503 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.13157807155304654; val_accuracy: 0.9624800955414012 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.13979981308150444; val_accuracy: 0.9589968152866242 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1673083250310011; val_accuracy: 0.9479498407643312 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.47; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.13309901715463893; val_accuracy: 0.9604896496815286 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.12650885319064378; val_accuracy: 0.964171974522293 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.89
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12405865086586612; val_accuracy: 0.9638734076433121 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.12666725208330307; val_accuracy: 0.9633757961783439 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.125829987702476; val_accuracy: 0.9638734076433121 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.08; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.1249844263408594; val_accuracy: 0.9635748407643312 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12329986952482515; val_accuracy: 0.9652667197452229 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.12464347310886262; val_accuracy: 0.9629777070063694 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.89
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12340111181045034; val_accuracy: 0.9640724522292994 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.12214097769776727; val_accuracy: 0.9646695859872612 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.88
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12419379203562524; val_accuracy: 0.9632762738853503 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12280777832315226; val_accuracy: 0.964968152866242 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12362458722986233; val_accuracy: 0.9647691082802548 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12420011954797301; val_accuracy: 0.9643710191082803 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12149175626646941; val_accuracy: 0.9655652866242038 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12248809365140405; val_accuracy: 0.9647691082802548 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.18; acc: 0.89
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.1240885124607071; val_accuracy: 0.9644705414012739 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12146816898588163; val_accuracy: 0.964968152866242 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12191687002303495; val_accuracy: 0.9651671974522293 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12211627330464922; val_accuracy: 0.9646695859872612 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12162634455094672; val_accuracy: 0.9645700636942676 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12179298781950003; val_accuracy: 0.9643710191082803 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.88
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12126194880266858; val_accuracy: 0.9650676751592356 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.1210885707549988; val_accuracy: 0.9643710191082803 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12136965605673516; val_accuracy: 0.964968152866242 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12137283612588409; val_accuracy: 0.9653662420382165 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12157258967969828; val_accuracy: 0.9652667197452229 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.92
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.12155735699139583; val_accuracy: 0.9655652866242038 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12141833461488888; val_accuracy: 0.964968152866242 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.1221145396921665; val_accuracy: 0.9655652866242038 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.12139279006203268; val_accuracy: 0.9651671974522293 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 74672
elements in E: 15746500
fraction nonzero: 0.004742133172451021
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.29; acc: 0.06
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.28; acc: 0.28
Batch: 180; loss: 2.28; acc: 0.28
Batch: 200; loss: 2.24; acc: 0.56
Batch: 220; loss: 2.22; acc: 0.53
Batch: 240; loss: 2.2; acc: 0.31
Batch: 260; loss: 2.16; acc: 0.45
Batch: 280; loss: 2.1; acc: 0.45
Batch: 300; loss: 1.83; acc: 0.66
Batch: 320; loss: 1.66; acc: 0.48
Batch: 340; loss: 1.28; acc: 0.56
Batch: 360; loss: 1.15; acc: 0.59
Batch: 380; loss: 0.95; acc: 0.67
Batch: 400; loss: 0.79; acc: 0.72
Batch: 420; loss: 0.58; acc: 0.77
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.65; acc: 0.75
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.94; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 1.28; train_accuracy: 0.59 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3605782883182453; val_accuracy: 0.8871417197452229 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.24016775404381904; val_accuracy: 0.9253582802547771 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22220560754085802; val_accuracy: 0.929140127388535 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17666309063506733; val_accuracy: 0.9460589171974523 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.84
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15802537488519766; val_accuracy: 0.9536226114649682 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.41; acc: 0.83
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.09; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1415697851330991; val_accuracy: 0.9549164012738853 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16497924266632197; val_accuracy: 0.9460589171974523 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1228333146186771; val_accuracy: 0.9625796178343949 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13401383216119117; val_accuracy: 0.9604896496815286 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11098255755700123; val_accuracy: 0.9689490445859873 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10032718438821234; val_accuracy: 0.96984474522293 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11374818118419616; val_accuracy: 0.9658638535031847 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.91
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10583937417264956; val_accuracy: 0.9678542993630573 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11210407062795512; val_accuracy: 0.9664609872611465 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09795336508352286; val_accuracy: 0.9703423566878981 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.94
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09453005732814218; val_accuracy: 0.9710390127388535 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09904485060625774; val_accuracy: 0.9707404458598726 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.45; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0978648495759554; val_accuracy: 0.9694466560509554 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09838597023278285; val_accuracy: 0.9713375796178344 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0995654949479422; val_accuracy: 0.9688495222929936 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0917438517567838; val_accuracy: 0.9715366242038217 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09418106202486974; val_accuracy: 0.9713375796178344 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.92
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09349519529255332; val_accuracy: 0.9704418789808917 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09566757925281859; val_accuracy: 0.9716361464968153 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09358383435162769; val_accuracy: 0.9724323248407644 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09164965976100818; val_accuracy: 0.9736265923566879 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.91
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09217535849114891; val_accuracy: 0.9732285031847133 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09167495284490525; val_accuracy: 0.9719347133757962 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09034354901712412; val_accuracy: 0.9728304140127388 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09080645778946056; val_accuracy: 0.9727308917197452 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0911431534180216; val_accuracy: 0.9730294585987261 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08985834084688478; val_accuracy: 0.9730294585987261 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0900177401816769; val_accuracy: 0.9738256369426752 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09093494203155207; val_accuracy: 0.973328025477707 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08973536853957328; val_accuracy: 0.973328025477707 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.03; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09022062013198616; val_accuracy: 0.9732285031847133 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09036133348182508; val_accuracy: 0.9741242038216561 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08979106046686507; val_accuracy: 0.9732285031847133 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09029391294072388; val_accuracy: 0.9729299363057324 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0899051594174212; val_accuracy: 0.9737261146496815 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0892166077236461; val_accuracy: 0.9734275477707006 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08905583342454236; val_accuracy: 0.9730294585987261 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08877546897834274; val_accuracy: 0.9741242038216561 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0889022376649319; val_accuracy: 0.973328025477707 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08896214595645856; val_accuracy: 0.9736265923566879 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08896589872373897; val_accuracy: 0.9742237261146497 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.07; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08896285397516694; val_accuracy: 0.9741242038216561 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08917175200144956; val_accuracy: 0.9736265923566879 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08912103594678222; val_accuracy: 0.9736265923566879 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08891728558358114; val_accuracy: 0.9741242038216561 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_350_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 84601
elements in E: 17996000
fraction nonzero: 0.004701100244498778
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.06
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.27; acc: 0.23
Batch: 160; loss: 2.25; acc: 0.31
Batch: 180; loss: 2.24; acc: 0.27
Batch: 200; loss: 2.18; acc: 0.45
Batch: 220; loss: 2.07; acc: 0.52
Batch: 240; loss: 1.99; acc: 0.36
Batch: 260; loss: 1.66; acc: 0.53
Batch: 280; loss: 1.74; acc: 0.41
Batch: 300; loss: 0.83; acc: 0.73
Batch: 320; loss: 1.5; acc: 0.52
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.78
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.09; acc: 1.0
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 1.07; train_accuracy: 0.64 

Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.33764286960955636; val_accuracy: 0.88953025477707 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.95
Val Epoch over. val_loss: 0.2453407783796833; val_accuracy: 0.9228702229299363 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.25465480864617474; val_accuracy: 0.9174960191082803 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.92
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1604761012183253; val_accuracy: 0.9491441082802548 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15182331258988685; val_accuracy: 0.9552149681528662 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1324954472102557; val_accuracy: 0.960390127388535 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.15554809254730584; val_accuracy: 0.9530254777070064 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13126033245568064; val_accuracy: 0.9612858280254777 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.30513276719743276; val_accuracy: 0.9138136942675159 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11799193805997159; val_accuracy: 0.9663614649681529 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11408295082580891; val_accuracy: 0.9655652866242038 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10789837487706333; val_accuracy: 0.9679538216560509 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11316383381486889; val_accuracy: 0.9655652866242038 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.15254635751769421; val_accuracy: 0.9562101910828026 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.89
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10173299791777779; val_accuracy: 0.9690485668789809 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10828713250530374; val_accuracy: 0.9672571656050956 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10694878315849668; val_accuracy: 0.9666600318471338 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11627679071418799; val_accuracy: 0.964968152866242 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10211936242664886; val_accuracy: 0.9692476114649682 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.109724540512558; val_accuracy: 0.9672571656050956 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09954501321883338; val_accuracy: 0.9700437898089171 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1013951219238673; val_accuracy: 0.9694466560509554 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10014316612842736; val_accuracy: 0.96984474522293 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10288655348596679; val_accuracy: 0.9690485668789809 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10590303285866026; val_accuracy: 0.9679538216560509 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09647945914963249; val_accuracy: 0.9716361464968153 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.06; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09917975158377249; val_accuracy: 0.9699442675159236 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.92
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09634203781154316; val_accuracy: 0.9705414012738853 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09686147560407023; val_accuracy: 0.9697452229299363 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09699092505226849; val_accuracy: 0.9699442675159236 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09590222474163884; val_accuracy: 0.9704418789808917 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09525119044645956; val_accuracy: 0.9711385350318471 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09478296688906136; val_accuracy: 0.9718351910828026 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09560509905171623; val_accuracy: 0.9716361464968153 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.89
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.92
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09604090337706789; val_accuracy: 0.9704418789808917 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09559078338692427; val_accuracy: 0.9705414012738853 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0941365110409108; val_accuracy: 0.9718351910828026 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09326494738435859; val_accuracy: 0.9720342356687898 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.31; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0946644155462836; val_accuracy: 0.9713375796178344 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09513466163971432; val_accuracy: 0.9713375796178344 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09351760860247786; val_accuracy: 0.9717356687898089 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09407905369380097; val_accuracy: 0.9720342356687898 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09365604610840796; val_accuracy: 0.9715366242038217 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09380496441606123; val_accuracy: 0.971437101910828 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09406568404809115; val_accuracy: 0.971437101910828 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09495976585657544; val_accuracy: 0.9723328025477707 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0941344421641652; val_accuracy: 0.9720342356687898 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09408338095067413; val_accuracy: 0.9708399681528662 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09410970894157127; val_accuracy: 0.9722332802547771 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09379925403838894; val_accuracy: 0.9719347133757962 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 95052
elements in E: 20245500
fraction nonzero: 0.004694969252426465
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.22
Batch: 140; loss: 2.24; acc: 0.28
Batch: 160; loss: 2.21; acc: 0.38
Batch: 180; loss: 2.11; acc: 0.36
Batch: 200; loss: 1.75; acc: 0.5
Batch: 220; loss: 1.29; acc: 0.56
Batch: 240; loss: 1.27; acc: 0.59
Batch: 260; loss: 0.94; acc: 0.69
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.84; acc: 0.67
Batch: 340; loss: 0.77; acc: 0.75
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.73; acc: 0.78
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.99; train_accuracy: 0.67 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.16; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.25843062731111127; val_accuracy: 0.921875 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.81
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2360414827040806; val_accuracy: 0.9258558917197452 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.08; acc: 1.0
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.17880764124309942; val_accuracy: 0.9454617834394905 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.88
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.88
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14710661806878012; val_accuracy: 0.955015923566879 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.14312509110399113; val_accuracy: 0.9580015923566879 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14210690610158216; val_accuracy: 0.9566082802547771 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.18682276789739632; val_accuracy: 0.9419785031847133 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13362123219260744; val_accuracy: 0.959593949044586 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.42602286900684333; val_accuracy: 0.8652468152866242 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.43; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11731724673585528; val_accuracy: 0.9669585987261147 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10440084812747445; val_accuracy: 0.9708399681528662 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.91
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10319016892818889; val_accuracy: 0.9707404458598726 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10050030214012048; val_accuracy: 0.9692476114649682 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12347433198789123; val_accuracy: 0.9642714968152867 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09684153185908202; val_accuracy: 0.9721337579617835 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09670167890893426; val_accuracy: 0.9715366242038217 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.92
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09876232665435523; val_accuracy: 0.9710390127388535 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09952247546167131; val_accuracy: 0.9696457006369427 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09605274914176601; val_accuracy: 0.972531847133758 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10248541756040731; val_accuracy: 0.9707404458598726 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.06; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09277661296592396; val_accuracy: 0.9730294585987261 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09227480136664809; val_accuracy: 0.9731289808917197 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.17; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.95
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09186368748830383; val_accuracy: 0.9740246815286624 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09195327934375994; val_accuracy: 0.9747213375796179 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09058524146201505; val_accuracy: 0.9741242038216561 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08953940498221452; val_accuracy: 0.9748208598726115 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08881606659881629; val_accuracy: 0.9753184713375797 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.92
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08837885347900877; val_accuracy: 0.9768113057324841 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09189727351923657; val_accuracy: 0.9739251592356688 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.18; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08936142114696989; val_accuracy: 0.976015127388535 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08683938210367396; val_accuracy: 0.9763136942675159 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08908541249051975; val_accuracy: 0.974422770700637 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08781020125006414; val_accuracy: 0.9758160828025477 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08826522866062297; val_accuracy: 0.976015127388535 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08761566663813439; val_accuracy: 0.9758160828025477 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08892524380023313; val_accuracy: 0.9763136942675159 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08707416176226489; val_accuracy: 0.9761146496815286 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08545879170203664; val_accuracy: 0.9770103503184714 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0871515217575298; val_accuracy: 0.9762141719745223 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.92
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08536859108194424; val_accuracy: 0.9766122611464968 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08572281365561638; val_accuracy: 0.9765127388535032 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0861518038021531; val_accuracy: 0.977109872611465 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08570914952808124; val_accuracy: 0.977109872611465 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08577657282162623; val_accuracy: 0.977109872611465 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08622919820296536; val_accuracy: 0.9764132165605095 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08621611759351318; val_accuracy: 0.9770103503184714 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08641512250634516; val_accuracy: 0.9770103503184714 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.89
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08618685807202273; val_accuracy: 0.9766122611464968 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08680682804933779; val_accuracy: 0.9766122611464968 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08548967217563823; val_accuracy: 0.977109872611465 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_450_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 105616
elements in E: 22495000
fraction nonzero: 0.004695087797288286
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.19
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.23
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.13; acc: 0.39
Batch: 180; loss: 1.95; acc: 0.36
Batch: 200; loss: 1.44; acc: 0.53
Batch: 220; loss: 0.95; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.62
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.75
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.92; train_accuracy: 0.69 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.22981294773661406; val_accuracy: 0.9286425159235668 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.97
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.27239889110539367; val_accuracy: 0.9107285031847133 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.92
Val Epoch over. val_loss: 0.16499421299452993; val_accuracy: 0.9482484076433121 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1220233996345359; val_accuracy: 0.9621815286624203 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.13084251585469883; val_accuracy: 0.9605891719745223 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11007649408783882; val_accuracy: 0.9674562101910829 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.23639695849388268; val_accuracy: 0.9336186305732485 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09829933246134952; val_accuracy: 0.9716361464968153 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.96 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3645193902360406; val_accuracy: 0.88953025477707 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.17394960728610398; val_accuracy: 0.9500398089171974 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.09117412752216789; val_accuracy: 0.9731289808917197 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09149081627749334; val_accuracy: 0.9722332802547771 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09171003364263826; val_accuracy: 0.9737261146496815 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.10827444866299629; val_accuracy: 0.9684514331210191 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0886824396290597; val_accuracy: 0.9743232484076433 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08793266136555156; val_accuracy: 0.9726313694267515 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10443056526647251; val_accuracy: 0.9717356687898089 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12393846334355652; val_accuracy: 0.9619824840764332 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08945621429070545; val_accuracy: 0.9736265923566879 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.23; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09375359625763195; val_accuracy: 0.972531847133758 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08652880270579819; val_accuracy: 0.9759156050955414 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.94
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08844521081751319; val_accuracy: 0.9740246815286624 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08757079278777359; val_accuracy: 0.9750199044585988 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.09234497734126013; val_accuracy: 0.9745222929936306 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08922758503894138; val_accuracy: 0.9730294585987261 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08487306350165871; val_accuracy: 0.9765127388535032 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0853706540385629; val_accuracy: 0.9750199044585988 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08685040872567779; val_accuracy: 0.9753184713375797 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08646226365854785; val_accuracy: 0.9758160828025477 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08667261706321103; val_accuracy: 0.975218949044586 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08546480227997348; val_accuracy: 0.975218949044586 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08578357762496942; val_accuracy: 0.975218949044586 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08545492698622358; val_accuracy: 0.9755175159235668 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08662485729926711; val_accuracy: 0.9750199044585988 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08670529547580488; val_accuracy: 0.9758160828025477 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08651008551857274; val_accuracy: 0.975218949044586 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08517936163931895; val_accuracy: 0.976015127388535 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461287413622923; val_accuracy: 0.9759156050955414 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.89
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08578486318231389; val_accuracy: 0.9758160828025477 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08431222088112952; val_accuracy: 0.9770103503184714 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08435773666781984; val_accuracy: 0.976015127388535 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08477749457215047; val_accuracy: 0.9757165605095541 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0839406148453427; val_accuracy: 0.9764132165605095 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08399880368998096; val_accuracy: 0.9764132165605095 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0840864830715641; val_accuracy: 0.9762141719745223 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08472923478882784; val_accuracy: 0.9762141719745223 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08423852346315505; val_accuracy: 0.9763136942675159 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08456380926309877; val_accuracy: 0.9763136942675159 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08479795894425386; val_accuracy: 0.9759156050955414 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

/home/llang/thesis-intrinsic-dimension/logging_helper.py:44: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax1 = plt.subplots()
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08400044277025635; val_accuracy: 0.9765127388535032 

plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/reg_lenet_3/2020-01-19 22:01:20/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4386313/slurm_script: line 25: --print_freq=20: command not found
